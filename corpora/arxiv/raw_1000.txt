cs.AI:Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.
cs.AI:Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.
cs.AI:We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.
cs.AI:As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.
cs.AI:To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime.
cs.AI:Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.
cs.AI:A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.
cs.AI:Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.
cs.AI:The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.
cs.AI:The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.
cs.AI:We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.
cs.AI:This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.
cs.AI:In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.
cs.AI:Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,...,N} that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.
cs.AI:Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.
cs.AI:This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.
cs.AI:This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.
cs.AI:The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.
cs.AI:This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.
cs.AI:For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.
cs.AI:Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k ``classes''). The definition is acquired by studying collections of training examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.
cs.AI:The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.
cs.AI:Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda &gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.
cs.AI:This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search.
cs.AI:Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.
cs.AI:Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n^2d^2) complexity (instead of O(n^3d^3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.
cs.AI:We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.
cs.AI:Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.
cs.AI:We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional ``basecase'' oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.
cs.AI:In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.
cs.AI:There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the "best" possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.
cs.AI:This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).
cs.AI:ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.
cs.AI:Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.
cs.AI:Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.
cs.AI:This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.
cs.AI:Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset.
cs.AI:Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a ``measure of goodness'' or ``membership value'' with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.
cs.AI:This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.
cs.AI:OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.
cs.AI:The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.
cs.AI:In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.
cs.AI:We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.
cs.AI:Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.
cs.AI:This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.
cs.AI:We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.
cs.AI:Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.
cs.AI:The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.
cs.AI:Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.
cs.AI:We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.
cs.AI:We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.
cs.AI:A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.
cs.AI:For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.
cs.AI:Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a ``difference matching'' procedure. The critic then proposes lemmas and generalizations which ``ripple'' these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.
cs.AI:Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.
cs.AI:Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.
cs.AI:This paper presents new experimental evidence against the utility of Occam's razor. A~systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.
cs.AI:The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.
cs.AI:This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
cs.AI:Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.
cs.AI:Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.
cs.AI:A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.
cs.AI:An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.
cs.AI:Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.
cs.AI:Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods. We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines.
cs.AI:Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Omega_1,Omega_2,..., with the following properties: first, Omega_1 is the class of all stratified knowledge bases; second, if a knowledge base Pi is in Omega_k, then Pi has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find the minimum k such that Pi belongs to Omega_k in time polynomial in the size of Pi; and, last, where K is the class of all knowledge bases, it is the case that union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some class in the hierarchy.
cs.AI:We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by UCPOP to select plans for refinement. The other is based on preferring ``zero commitment'' (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of UCPOP, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default UCPOP search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices.
cs.AI:Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.
cs.AI:This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol (UNP) is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals. Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one 'strict', the other 'tolerant', and analyze their affects on the stability and efficiency of negotiation outcomes.
cs.AI:First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization.
cs.AI:This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP.
cs.AI:A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as ``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.
cs.AI:Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a personUs name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains.
cs.AI:Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data.
cs.AI:Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.
cs.AI:Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken- language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.
cs.AI:Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic muALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualified number restrictions, muALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of muALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus.
cs.AI:We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the Toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment.
cs.AI:We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks (we show how they can be generated using clustering and conditioning algorithms). The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching.
cs.AI:An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the REGENT algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.
cs.AI:Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.
cs.AI:We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total.
cs.AI:The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions.
cs.AI:This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time.
cs.AI:Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.
cs.AI:Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.
cs.AI:SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.
cs.AI:Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.
cs.AI:Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.
cs.AI:The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback, while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.
cs.AI:Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms.
cs.AI:The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered.
cs.AI:Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.
cs.AI:An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.
cs.AI:Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.
cs.AI:This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.
cs.AI:In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.
cs.AI:In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.
cs.AI:This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.
cs.AI:One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.
cs.AI:We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the process of proving that certain planning problems are complete for NP^PP, we introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.
cs.AI:In this paper we describe SYNERGY, which is a highly parallelizable, linear planning system that is based on the genetic programming paradigm. Rather than reasoning about the world it is planning for, SYNERGY uses artificial selection, recombination and fitness measure to generate linear plans that solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase problem and a few variants of the robot navigation problem), and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by UCPOP. In order to facilitate the search reduction and to enhance the expressive power of SYNERGY, we also propose two major extensions to our planning system: a formalism for using hierarchical planning operators, and a framework for planning in dynamic environments.
cs.AI:We show that several constraint propagation algorithms (also called (local) consistency, consistency enforcing, Waltz, filtering or narrowing algorithms) are instances of algorithms that deal with chaotic iteration. To this end we propose a simple abstract framework that allows us to classify and compare these algorithms and to establish in a uniform way their basic properties.
cs.AI:This paper examines the phenomenon of daydreaming: spontaneously recalling or imagining personal or vicarious experiences in the past or future. The following important roles of daydreaming in human cognition are postulated: plan preparation and rehearsal, learning from failures and successes, support for processes of creativity, emotion regulation, and motivation.   A computational theory of daydreaming and its implementation as the program DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based on relaxed planning, 2) a dynamic episodic memory of experiences used by the scenario generator, 3) a collection of personal goals and control goals which guide the scenario generator, 4) an emotion component in which daydreams initiate, and are initiated by, emotional states arising from goal outcomes, and 5) domain knowledge of interpersonal relations and common everyday occurrences.   The role of emotions and control goals in daydreaming is discussed. Four control goals commonly used in guiding daydreaming are presented: rationalization, failure/success reversal, revenge, and preparation. The role of episodic memory in daydreaming is considered, including how daydreamed information is incorporated into memory and later used. An initial version of DAYDREAMER which produces several daydreams (in English) is currently running.
cs.AI:Real world combinatorial optimization problems such as scheduling are typically too complex to solve with exact methods. Additionally, the problems often have to observe vaguely specified constraints of different importance, the available data may be uncertain, and compromises between antagonistic criteria may be necessary. We present a combination of approximate reasoning based constraints and iterative optimization based heuristics that help to model and solve such problems in a framework of C++ software libraries called StarFLIP++. While initially developed to schedule continuous caster units in steel plants, we present in this paper results from reusing the library components in a shift scheduling system for the workforce of an industrial production plant.
cs.AI:The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman & Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.
cs.AI:How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) "iconic representations," which are analogs of the proximal sensory projections of distal objects and events, and (2) "categorical representations," which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) "symbolic representations," grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., "An X is a Y that is Z").
cs.AI:In tree search problem the best-first search algorithm needs too much of space . To remove such drawbacks of these algorithms the IDA* was developed which is both space and time cost efficient. But again IDA* can give an optimal solution for real valued problems like Flow shop scheduling, Travelling Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further modifications are done on it and the Iterative Deepening Branch and Bound Search Algorithms is developed which meets the requirements. We have tried using this algorithm for the Flow Shop Scheduling Problem and have found that it is quite effective.
cs.AI:Agents are small programs that autonomously take actions based on changes in their environment or ``state.'' Over the last few years, there have been an increasing number of efforts to build agents that can interact and/or collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top of legacy code. However, their framework assumes that agent states are completely determined, and there is no uncertainty in an agent's state. Thus, their framework allows an agent developer to specify how his agents will react when the agent is 100% sure about what is true/false in the world state. In this paper, we propose the concept of a \emph{probabilistic agent program} and show how, given an arbitrary program written in any imperative language, we may build a declarative ``probabilistic'' agent program on top of it which supports decision making in the presence of uncertainty. We provide two alternative semantics for probabilistic agent programs. We show that the second semantics, though more epistemically appealing, is more complex to compute. We provide sound and complete algorithms to compute the semantics of \emph{positive} agent programs.
cs.AI:The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.
cs.AI:We revisit the issue of connections between two leading formalisms in nonmonotonic reasoning: autoepistemic logic and default logic. For each logic we develop a comprehensive semantic framework based on the notion of a belief pair. The set of all belief pairs together with the so called knowledge ordering forms a complete lattice. For each logic, we introduce several semantics by means of fixpoints of operators on the lattice of belief pairs. Our results elucidate an underlying isomorphism of the respective semantic constructions. In particular, we show that the interpretation of defaults as modal formulas proposed by Konolige allows us to represent all semantics for default logic in terms of the corresponding semantics for autoepistemic logic. Thus, our results conclusively establish that default logic can indeed be viewed as a fragment of autoepistemic logic. However, as we also demonstrate, the semantics of Moore and Reiter are given by different operators and occupy different locations in their corresponding families of semantics. This result explains the source of the longstanding difficulty to formally relate these two semantics. In the paper, we also discuss approximating skeptical reasoning with autoepistemic and default logics and establish constructive principles behind such approximations.
cs.AI:Randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables. However, these algorithms are not complete. That is, they provide no guarantee that a satisfying assignment, if one exists, will be found. Thus, when studying randomized algorithms, there are two important characteristics that need to be considered: the running time and, even more importantly, the accuracy --- a measure of likelihood that a satisfying assignment will be found, provided one exists. In fact, we argue that without a reference to the accuracy, the notion of the running time for randomized algorithms is not well-defined. In this paper, we introduce a formal notion of accuracy. We use it to define a concept of the running time. We use both notions to study the random walk strategy GSAT algorithm. We investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments. We demonstrate that the running time of GSAT grows exponentially in the number of variables of the input formula for randomly generated 3-CNF formulas and for the formulas encoding 3- and 4-colorability of graphs.
cs.AI:Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentioned as a process that may have been used to obtain a primordial biological code necessary open-ended evolution.
cs.AI:This paper presents a method of computing a revision of a function-free normal logic program. If an added rule is inconsistent with a program, that is, if it leads to a situation such that no stable model exists for a new program, then deletion and addition of rules are performed to avoid inconsistency. We specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules. To compute such deletion and addition, we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule. We compute a minimally revised program, by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure.
cs.AI:This is a system description for the OSCAR defeasible reasoner.
cs.AI:Diagnostic reasoning has been characterized logically as consistency-based reasoning or abductive reasoning. Previous analyses in the literature have shown, on the one hand, that choosing the (in general more restrictive) abductive definition may be appropriate or not, depending on the content of the knowledge base [Console&Torasso91], and, on the other hand, that, depending on the choice of the definition the same knowledge should be expressed in different form [Poole94].   Since in Model-Based Diagnosis a major problem is finding the right way of abstracting the behavior of the system to be modeled, this paper discusses the relation between modeling, and in particular abstraction in the model, and the notion of diagnosis.
cs.AI:ACLP is a system which combines abductive reasoning and constraint solving by integrating the frameworks of Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). It forms a general high-level knowledge representation environment for abductive problems in Artificial Intelligence and other areas. In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems. The ACLP system is currently implemented on top of the CLP language of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver for finite domains. It has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the (lower level) constraint solving framework of CLP on which it is built. These experiments provide evidence that the abductive framework of ACLP does not compromise significantly the computational efficiency of the solutions. Other experiments show the natural ability of ACLP to accommodate easily and in a robust way new or changing requirements of the original problem.
cs.AI:We present a method for relevance sensitive non-monotonic inference from belief sequences which incorporates insights pertaining to prioritized inference and relevance sensitive, inconsistency tolerant belief revision.   Our model uses a finite, logically open sequence of propositional formulas as a representation for beliefs and defines a notion of inference from maxiconsistent subsets of formulas guided by two orderings: a temporal sequencing and an ordering based on relevance relations between the conclusion and formulas in the sequence. The relevance relations are ternary (using context as a parameter) as opposed to standard binary axiomatizations. The inference operation thus defined easily handles iterated revision by maintaining a revision history, blocks the derivation of inconsistent answers from a possibly inconsistent sequence and maintains the distinction between explicit and implicit beliefs. In doing so, it provides a finitely presented formalism and a plausible model of reasoning for automated agents.
cs.AI:We propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. In detail, we generalize the notions of Pearl's entailment in system Z, Lehmann's lexicographic entailment, and Geffner's conditional entailment to conditional constraints. We give some examples that show that the new notions of z-, lexicographic, and conditional entailment have similar properties like their classical counterparts. Moreover, we show that the new notions of z-, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints.
cs.AI:This paper describes a system, called PLP, for compiling ordered logic programs into standard logic programs under the answer set semantics. In an ordered logic program, rules are named by unique terms, and preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Since the result of the translation is an extended logic program, existing logic programming systems can be used as underlying reasoning engine. In particular, PLP is conceived as a front-end to the logic programming systems dlv and smodels.
cs.AI:The SLDNFA-system results from the LP+ project at the K.U.Leuven, which investigates logics and proof procedures for these logics for declarative knowledge representation. Within this project inductive definition logic (ID-logic) is used as representation logic. Different solvers are being developed for this logic and one of these is SLDNFA. A prototype of the system is available and used for investigating how to solve efficiently problems represented in ID-logic.
cs.AI:We describe an approach for compiling preferences into logic programs under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows both the specification of static orderings (as found in most previous work), in which preferences are external to a logic program, as well as orderings on sets of rules. In large part then, we are interested in describing a general methodology for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.
cs.AI:This paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base. The abductive inference processes described here depend on the semantic of the rule. We distinguish two classes of interpretation of a fuzzy rule, certainty generation rules and possible generation rules. In this paper we present the architecture of abductive inference in the first class of interpretation. We give two kinds of problem that we can resolve by using the proposed models of inference.
cs.AI:The goal of the LP+ project at the K.U.Leuven is to design an expressive logic, suitable for declarative knowledge representation, and to develop intelligent systems based on Logic Programming technology for solving computational problems using the declarative specifications. The ID-logic is an integration of typed classical logic and a definition logic. Different abductive solvers for this language are being developed. This paper is a report of the integration of high order aggregates into ID-logic and the consequences on the solver SLDNFA.
cs.AI:We propose a new approach to belief revision that provides a way to change knowledge bases with a minimum of effort. We call this way of revising belief states optimal belief revision. Our revision method gives special attention to the fact that most belief revision processes are directed to a specific informational objective. This approach to belief change is founded on notions such as optimal context and accessibility. For the sentential model of belief states we provide both a formal description of contexts as sub-theories determined by three parameters and a method to construct contexts. Next, we introduce an accessibility ordering for belief sets, which we then use for selecting the best (optimal) contexts with respect to the processing effort involved in the revision. Then, for finitely axiomatizable knowledge bases, we characterize a finite accessibility ranking from which the accessibility ordering for the entire base is generated and show how to determine the ranking of an arbitrary sentence in the language. Finally, we define the adjustment of the accessibility ranking of a revised base of a belief set.
cs.AI:High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like \congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting \ccgolog, a variant of \congolog which is based on the extended situation calculus. One benefit of \ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism.
cs.AI:The Smodels system implements the stable model semantics for normal logic programs. It handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints. On top of this core engine more involved systems can be built. As an example, we have implemented total and partial stable model computation for disjunctive logic programs. An interesting application method is based on answer set programming, i.e., encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules. Smodels has been applied to a number of areas including planning, model checking, reachability analysis, product configuration, dynamic constraint satisfaction, and feature interaction.
cs.AI:E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.
cs.AI:In this paper, we outline the prototype of an automated inference tool, called QUIP, which provides a uniform implementation for several nonmonotonic reasoning formalisms. The theoretical basis of QUIP is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae.
cs.AI:Over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information. One such language is the language of epistemic specifications. As is usual with logic programming languages, the problem of answering queries is intractable in the general case. For extended disjunctive logic programs, an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets. In this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications. Furthermore, an extension of the splitting set theorem will be presented. Also, a characterization of stratified epistemic specifications will be given in terms of splitting sets. This characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs.
cs.AI:The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.
cs.AI:We generalize a theorem by Francois Fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure. The study of this relationship is important in connection with the emergence of answer set programming. Whenever the two semantics are equivalent, answer sets can be computed by a satisfiability solver, and the use of answer set solvers such as smodels and dlv is unnecessary. A logic programming representation of the blocks world due to Ilkka Niemelae is discussed as an example.
cs.AI:We introduced decomposable negation normal form (DNNF) recently as a tractable form of propositional theories, and provided a number of powerful logical operations that can be performed on it in polynomial time. We also presented an algorithm for compiling any conjunctive normal form (CNF) into DNNF and provided a structure-based guarantee on its space and time complexity. We present in this paper a linear-time algorithm for converting an ordered binary decision diagram (OBDD) representation of a propositional theory into an equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the previous complexity guarantees on compiling DNNF continue to hold for this stricter subclass, which has stronger properties. In particular, we present a new operation on d-DNNF which allows us to count its models under the assertion, retraction and flipping of every literal by traversing the d-DNNF twice. That is, after such traversal, we can test in constant-time: the entailment of any literal by the d-DNNF, and the consistency of the d-DNNF under the retraction or flipping of any literal. We demonstrate the significance of these new operations by showing how they allow us to implement linear-time, complete truth maintenance systems and linear-time, complete belief revision systems for two important classes of propositional theories.
cs.AI:The paper reports on first preliminary results and insights gained in a project aiming at implementing the fluent calculus using methods and techniques based on binary decision diagrams. After reporting on an initial experiment showing promising results we discuss our findings concerning various techniques and heuristics used to speed up the reasoning process.
cs.AI:Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.
cs.AI:In an earlier work, we have presented operations of belief change which only affect the relevant part of a belief base. In this paper, we propose the application of the same strategy to the problem of model-based diangosis. We first isolate the subset of the system description which is relevant for a given observation and then solve the diagnosis problem for this subset.
cs.AI:We present a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such ``extension'' represents the revised state; alternately revision consists of the intersection of all such extensions.   The most general formulation of our approach is flexible enough to express other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: the choice revision operator gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A.
cs.AI:SATEN is an object-oriented web-based extraction and belief revision engine. It runs on any computer via a Java 1.1 enabled browser such as Netscape 4. SATEN performs belief revision based on the AGM approach. The extraction and belief revision reasoning engines operate on a user specified ranking of information. One of the features of SATEN is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm. We describe here an ASP system, DATALOG with constraints or DC, based on non-monotonic logic. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax, use and implementation of DC and provide experimental results.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm well attuned to search problems in AI, constraint satisfaction and combinatorics. Propositional logic is, arguably, the simplest ASP system with an intuitive semantics supporting direct modeling of problem constraints. However, for some applications, especially those requiring that transitive closure be computed, it requires additional variables and results in large theories. Consequently, it may not be a practical computational tool for such problems. On the other hand, ASP systems based on nonmonotonic logics, such as stable logic programming, can handle transitive closure computation efficiently and, in general, yield very concise theories as problem representations. Their semantics is, however, more complex. Searching for the middle ground, in this paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax and semantics of DC, and study its properties. We discuss an implementation of DC and present results of experimental study of the effectiveness of DC, comparing it with CSAT, a satisfiability checker and SMODELS implementation of stable logic programming. Our results show that DC is competitive with the other two approaches, in case of many search problems, often yielding much more efficient solutions.
cs.AI:We study here the well-known propagation rules for Boolean constraints. First we propose a simple notion of completeness for sets of such rules and establish a completeness result. Then we show an equivalence in an appropriate sense between Boolean constraint propagation and unit propagation, a form of resolution for propositional logic.   Subsequently we characterize one set of such rules by means of the notion of hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify the status of a similar, though different, set of rules introduced in (Simonis 1989a) and more fully in (Codognet and Diaz 1996).
cs.AI:A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.
cs.AI:In this paper we present a rule based formalism for filtering variables domains of constraints. This formalism is well adapted for solving dynamic CSP. We take diagnosis as an instance problem to illustrate the use of these rules. A diagnosis problem is seen like finding all the minimal sets of constraints to be relaxed in the constraint network that models the device to be diagnosed
cs.AI:Despite the effort of many researchers in the area of multi-agent systems (MAS) for designing and programming agents, a few years ago the research community began to take into account that common features among different MAS exists. Based on these common features, several tools have tackled the problem of agent development on specific application domains or specific types of agents. As a consequence, their scope is restricted to a subset of the huge application domain of MAS. In this paper we propose a generic infrastructure for programming agents whose name is Brainstorm/J. The infrastructure has been implemented as an object oriented framework. As a consequence, our approach supports a broader scope of MAS applications than previous efforts, being flexible and reusable.
cs.AI:In fuzzy propositional logic, to a proposition a partial truth in [0,1] is assigned. It is well known that under certain circumstances, fuzzy logic collapses to classical logic. In this paper, we will show that under dual conditions, fuzzy logic collapses to four-valued (relevance) logic, where propositions have truth-value true, false, unknown, or contradiction. As a consequence, fuzzy entailment may be considered as ``in between'' four-valued (relevance) entailment and classical entailment.
cs.AI:We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.
cs.AI:Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are systems based on stable model semantics, abductive systems, and first order logic model generators which compute solutions as models of some theory. This paper compares these different approaches from the point of view of knowledge representation (how declarative are the programs) and from the point of view of performance (how good are they at solving typical problems).
cs.AI:In this paper, we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery. This theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules. We show how to realize this theory in a learning system named CFRule. CFRule is a parallel weight-based model, but it departs from traditional neural computing in that its internal knowledge is comprehensible. Furthermore, when the model converges upon training, each channel converges to a target rule. The model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent. Since, however, gradient descent only guarantees local optimization, a multi-channel regression-based optimization strategy is developed to effectively deal with this problem. Formally, we prove that the CFRule model can explicitly and precisely encode any given rule set. Also, we prove a property related to asynchronous parallel convergence, which is a critical element of the multi-channel parallel adaptation theory for rule learning. Thanks to the quantizability nature of the CFRule model, rules can be extracted completely and soundly via a threshold-based mechanism. Finally, the practical application of the theory is demonstrated in DNA promoter recognition and hepatitis prognosis prediction.
cs.AI:We present an approach for modelling the structure and coarse content of legal documents with a view to providing automated support for the drafting of contracts and contract database retrieval. The approach is designed to be applicable where contract drafting is based on model-form contracts or on existing examples of a similar type. The main features of the approach are: (1) the representation addresses the structure and the interrelationships between the constituent parts of contracts, but not the text of the document itself; (2) the representation of documents is separated from the mechanisms that manipulate it; and (3) the drafting process is subject to a collection of explicitly stated constraints that govern the structure of the documents. We describe the representation of document instances and of 'generic documents', which are data structures used to drive the creation of new document instances, and we show extracts from a sample session to illustrate the features of a prototype system implemented in MacProlog.
cs.AI:One influential approach to assessing the "goodness" of arguments is offered by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an approach that originates in discourse analysis. In p-d terms an argument is good if it avoids committing a fallacy, whereas in RST terms an argument is good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for providing only a partially functional account of argument, and similar criticisms have been raised in the Natural Language Generation (NLG) community-particularly by Moore & Pollack (1992)- with regards to its account of intentionality in text in general. Mann and Thompson themselves note that although RST can be successfully applied to a wide range of texts from diverse domains, it fails to characterise some types of text, most notably legal contracts. There is ongoing research in the Artificial Intelligence and Law community exploring the potential for providing electronic support to contract negotiators, focusing on long-term, complex engineering agreements (see for example Daskalopulu & Sergot 1997). This paper provides a brief introduction to RST and illustrates its shortcomings with respect to contractual text. An alternative approach for modelling argument structure is presented which not only caters for contractual text, but also overcomes the aforementioned limitations of RST.
cs.AI:Information Integration is a young and exciting field with enormous research and commercial significance in the new world of the Information Society. It stands at the crossroad of Databases and Artificial Intelligence requiring novel techniques that bring together different methods from these fields. Information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible, transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program. The field although relatively young has already found many practical applications particularly for integrating information over the World Wide Web. This paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas. It attempts to evaluate the current and potential role of Computational Logic in this and suggests some of the problems where logic-based techniques could be used.
cs.AI:Constraint propagation is a general algorithmic approach for pruning the search space of a CSP. In a uniform way, K. R. Apt has defined a computation as an iteration of reduction functions over a domain. He has also demonstrated the need for integrating static properties of reduction functions (commutativity and semi-commutativity) to design specialized algorithms such as AC3 and DAC. We introduce here a set of operators for modeling compositions of reduction functions. Two of the major goals are to tackle parallel computations, and dynamic behaviours (such as slow convergence).
cs.AI:We consider an approach to update nonmonotonic knowledge bases represented as extended logic programs under answer set semantics. New information is incorporated into the current knowledge base subject to a causal rejection principle enforcing that, in case of conflicts, more recent rules are preferred and older rules are overridden. Such a rejection principle is also exploited in other approaches to update logic programs, e.g., in dynamic logic programming by Alferes et al. We give a thorough analysis of properties of our approach, to get a better understanding of the causal rejection principle. We review postulates for update and revision operators from the area of theory change and nonmonotonic reasoning, and some new properties are considered as well. We then consider refinements of our semantics which incorporate a notion of minimality of change. As well, we investigate the relationship to other approaches, showing that our approach is semantically equivalent to inheritance programs by Buccafurri et al. and that it coincides with certain classes of dynamic logic programs, for which we provide characterizations in terms of graph conditions. Therefore, most of our results about properties of causal rejection principle apply to these approaches as well. Finally, we deal with computational complexity of our approach, and outline how the update semantics and its refinements can be implemented on top of existing logic programming engines.
cs.AI:We introduce a learning method called ``gradient-based reinforcement planning'' (GREP). Unlike traditional DP methods that improve their policy backwards in time, GREP is a gradient-based method that plans ahead and improves its policy before it actually acts in the environment. We derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments.
cs.AI:Much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty. In this work we extend competitive analysis to the context of multi-agent systems. Unlike classical competitive analysis where the behavior of an agent's environment is taken to be arbitrary, we consider the case where an agent's environment consists of other agents. These agents will usually obey some (minimal) rationality constraints. This leads to the definition of rational competitive analysis. We introduce the concept of rational competitive analysis, and initiate the study of competitive analysis for multi-agent systems. We also discuss the application of rational competitive analysis to the context of bidding games, as well as to the classical one-way trading problem.
cs.AI:This article aims at clarifying the language and practice of scientific experiment, mainly by hooking observability on calculability.
cs.AI:Many systems that exhibit nonmonotonic behavior have been described and studied already in the literature. The general notion of nonmonotonic reasoning, though, has almost always been described only negatively, by the property it does not enjoy, i.e. monotonicity. We study here general patterns of nonmonotonic reasoning and try to isolate properties that could help us map the field of nonmonotonic reasoning by reference to positive properties. We concentrate on a number of families of nonmonotonic consequence relations, defined in the style of Gentzen. Both proof-theoretic and semantic points of view are developed in parallel. The former point of view was pioneered by D. Gabbay, while the latter has been advocated by Y. Shoham in. Five such families are defined and characterized by representation theorems, relating the two points of view. One of the families of interest, that of preferential relations, turns out to have been studied by E. Adams. The "preferential" models proposed here are a much stronger tool than Adams' probabilistic semantics. The basic language used in this paper is that of propositional logic. The extension of our results to first order predicate calculi and the study of the computational complexity of the decision problems described in this paper will be treated in another paper.
cs.AI:This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type "if ... then ...", represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, S. Kraus and the authors defined and studied "preferential" consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, "rational" relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a "ranked" preferential model, or by a (non-standard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.
cs.AI:It is shown that Darwiche and Pearl's postulates imply an interesting property, not noticed by the authors.
cs.AI:A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions.
cs.AI:We give a semantics to iterated update by a preference relation on possible developments. An iterated update is a sequence of formulas, giving (incomplete) information about successive states of the world. A development is a sequence of models, describing a possible trajectory through time. We assume a principle of inertia and prefer those developments, which are compatible with the information, and avoid unnecessary changes. The logical properties of the updates defined in this way are considered, and a representation result is proved.
cs.AI:A. Tarski proposed the study of infinitary consequence operations as the central topic of mathematical logic. He considered monotonicity to be a property of all such operations. In this paper, we weaken the monotonicity requirement and consider more general operations, inference operations. These operations describe the nonmonotonic logics both humans and machines seem to be using when infering defeasible information from incomplete knowledge. We single out a number of interesting families of inference operations. This study of infinitary inference operations is inspired by the results of Kraus, Lehmann and Magidor on finitary nonmonotonic operations, but this paper is self-contained.
cs.AI:The Expansion property considered by researchers in Social Choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the {\em pure}, i.e., not involving connectives, version of a previously known weak rationality condition. The assumption that the union of two definable sets of models is definable is needed for the soundness part of the result.
cs.AI:The lexicographic closure of any given finite set D of normal defaults is defined. A conditional assertion "if a then b" is in this lexicographic closure if, given the defaults D and the fact a, one would conclude b. The lexicographic closure is essentially a rational extension of D, and of its rational closure, defined in a previous paper. It provides a logic of normal defaults that is different from the one proposed by R. Reiter and that is rich enough not to require the consideration of non-normal defaults. A large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind Reiter's logic of defaults.
cs.AI:We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).
cs.AI:Stereotypical reasoning assumes that the situation at hand is one of a kind and that it enjoys the properties generally associated with that kind of situation. It is one of the most basic forms of nonmonotonic reasoning. A formal model for stereotypical reasoning is proposed and the logical properties of this form of reasoning are studied. Stereotypical reasoning is shown to be cumulative under weak assumptions.
cs.AI:We introduce a methodology and framework for expressing general preference information in logic programming under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of form s < t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed program correspond with the preferred answer sets of the original program. Our approach allows the specification of dynamic orderings, in which preferences can appear arbitrarily within a program. Static orderings (in which preferences are external to a logic program) are a trivial restriction of the general dynamic case. First, we develop a specific approach to reasoning with preferences, wherein the preference ordering specifies the order in which rules are to be applied. We then demonstrate the wide range of applicability of our framework by showing how other approaches, among them that of Brewka and Eiter, can be captured within our framework. Since the result of each of these transformations is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a publicly available compiler as a front-end for these programming systems.
cs.AI:Prioritized default reasoning has illustrated its rich expressiveness and flexibility in knowledge representation and reasoning. However, many important aspects of prioritized default reasoning have yet to be thoroughly explored. In this paper, we investigate two properties of prioritized logic programs in the context of answer set semantics. Specifically, we reveal a close relationship between mutual defeasibility and uniqueness of the answer set for a prioritized logic program. We then explore how the splitting technique for extended logic programs can be extended to prioritized logic programs. We prove splitting theorems that can be used to simplify the evaluation of a prioritized logic program under certain conditions.
cs.AI:The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula, but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed.
cs.AI:We study fixpoints of operators on lattices. To this end we introduce the notion of an approximation of an operator. We order approximations by means of a precision ordering. We show that each lattice operator O has a unique most precise or ultimate approximation. We demonstrate that fixpoints of this ultimate approximation provide useful insights into fixpoints of the operator O.   We apply our theory to logic programming and introduce the ultimate Kripke-Kleene, well-founded and stable semantics. We show that the ultimate Kripke-Kleene and well-founded semantics are more precise then their standard counterparts We argue that ultimate semantics for logic programming have attractive epistemological properties and that, while in general they are computationally more complex than the standard semantics, for many classes of theories, their complexity is no worse.
cs.AI:Representing defeasibility is an important issue in common sense reasoning. In reasoning about action and change, this issue becomes more difficult because domain and action related defeasible information may conflict with general inertia rules. Furthermore, different types of defeasible information may also interfere with each other during the reasoning. In this paper, we develop a prioritized logic programming approach to handle defeasibilities in reasoning about action. In particular, we propose three action languages {\cal AT}^{0}, {\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in action domains named defeasible constraints, defeasible observations and actions with defeasible and abnormal effects respectively. Each language with a higher superscript can be viewed as an extension of the language with a lower superscript. These action languages inherit the simple syntax of {\cal A} language but their semantics is developed in terms of transition systems where transition functions are defined based on prioritized logic programs. By illustrating various examples, we show that our approach eventually provides a powerful mechanism to handle various defeasibilities in temporal prediction and postdiction. We also investigate semantic properties of these three action languages and characterize classes of action domains that present more desirable solutions in reasoning about action within the underlying action languages.
cs.AI:An anticipatory system for guiding plot development in interactive narratives is described. The executable model is a finite automaton that provides the implemented system with a look-ahead. The identification of undesirable future states in the model is used to guide the player, in a transparent manner. In this way, too radical twists of the plot can be avoided. Since the player participates in the development of the plot, such guidance can have many forms, depending on the environment of the player, on the behavior of the other players, and on the means of player interaction. We present a design method for interactive narratives which produces designs suitable for the implementation of anticipatory mechanisms. Use of the method is illustrated by application to our interactive computer game Kaktus.
cs.AI:Open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics. There are obvious analogies between open predicates and abducible predicates. However, despite superficial similarities, there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa. Similarly, open programs cannot be immediately simulated with answer set programming (ASP). In this paper we start a thorough investigation of the relationships between open inference, abduction and ASP. We shall prove that open programs generalize the other two frameworks. The generalized framework suggests interesting extensions of abduction under the generalized stable model semantics. In some cases, we will be able to reduce open inference to abduction and ASP, thereby estimating its computational complexity. At the same time, the aforementioned reduction opens the way to new applications of abduction and ASP.
cs.AI:In this paper we consider three different kinds of domain-dependent control knowledge (temporal, procedural and HTN-based) that are useful in planning. Our approach is declarative and relies on the language of logic programming with answer set semantics (AnsProlog*). AnsProlog* is designed to plan without control knowledge. We show how temporal, procedural and HTN-based control knowledge can be incorporated into AnsProlog* by the modular addition of a small number of domain-dependent rules, without the need to modify the planner. We formally prove the correctness of our planner, both in the absence and presence of the control knowledge. Finally, we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length.
cs.AI:Dung's abstract framework for argumentation enables a study of the interactions between arguments based solely on an ``attack'' binary relation on the set of arguments. Various ways to solve conflicts between contradictory pieces of information have been proposed in the context of argumentation, nonmonotonic reasoning or logic programming, and can be captured by appropriate semantics within Dung's framework. A common feature of these semantics is that one can always maximize in some sense the set of acceptable arguments. We propose in this paper to extend Dung's framework in order to allow for the representation of what we call ``restricted'' arguments: these arguments should only be used if absolutely necessary, that is, in order to support other arguments that would otherwise be defeated. We modify Dung's preferred semantics accordingly: a set of arguments becomes acceptable only if it contains a minimum of restricted arguments, for a maximum of unrestricted arguments.
cs.AI:We address a general representation problem for belief change, and describe two interrelated representations for iterative non-prioritized change: a logical representation in terms of persistent epistemic states, and a constructive representation in terms of flocks of bases.
cs.AI:An extension of an abstract argumentation framework, called collective argumentation, is introduced in which the attack relation is defined directly among sets of arguments. The extension turns out to be suitable, in particular, for representing semantics of disjunctive logic programs. Two special kinds of collective argumentation are considered in which the opponents can share their arguments.
cs.AI:Logic programs with ordered disjunction (LPODs) combine ideas underlying Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming. Logic programming under answer set semantics is extended with a new connective called ordered disjunction. The new connective allows us to represent alternative, ranked options for problem solutions in the heads of rules: A \times B intuitively means: if possible A, but if A is not possible then at least B. The semantics of logic programs with ordered disjunction is based on a preference relation on answer sets. LPODs are useful for applications in design and configuration and can serve as a basis for qualitative decision making.
cs.AI:In this paper, we investigate the extent to which knowledge compilation can be used to improve inference from propositional weighted bases. We present a general notion of compilation of a weighted base that is parametrized by any equivalence--preserving compilation function. Both negative and positive results are presented. On the one hand, complexity results are identified, showing that the inference problem from a compiled weighted base is as difficult as in the general case, when the prime implicates, Horn cover or renamable Horn cover classes are targeted. On the other hand, we show that the inference problem becomes tractable whenever DNNF-compilations are used and clausal queries are considered. Moreover, we show that the set of all preferred models of a DNNF-compilation of a weighted base can be computed in time polynomial in the output size. Finally, we sketch how our results can be used in model-based diagnosis in order to compute the most probable diagnoses of a system.
cs.AI:This paper studies the problem of modeling complex domains of actions and change within high-level action description languages. We investigate two main issues of concern: (a) can we represent complex domains that capture together different problems such as ramifications, non-determinism and concurrency of actions, at a high-level, close to the given natural ontology of the problem domain and (b) what features of such a representation can affect, and how, its computational behaviour. The paper describes the main problems faced in this representation task and presents the results of an empirical study, carried out through a series of controlled experiments, to analyze the computational performance of reasoning in these representations. The experiments compare different representations obtained, for example, by changing the basic ontology of the domain or by varying the degree of use of indirect effect laws through domain constraints. This study has helped to expose the main sources of computational difficulty in the reasoning and suggest some methodological guidelines for representing complex domains. Although our work has been carried out within one particular high-level description language, we believe that the results, especially those that relate to the problems of representation, are independent of the specific modeling language.
cs.AI:This paper introduces the notion of value-based argumentation frameworks, an extension of the standard argumentation frameworks proposed by Dung, which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote.
cs.AI:We analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory. We start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics. We then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs. The suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics. In particular, our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics. Finally, we indicate how an existing preferred well-founded semantics can be captured within our semantical framework.
cs.AI:In this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems. This transformation allows to characterize all notions of Reiter's default logic in the framework of argumentation systems. As a consequence, computing extensions, or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic. The transformation proposed is linear in the number of defaults.
cs.AI:In the present paper, the existence and multiplicity problems of extensions are addressed. The focus is on extension of the stable type. The main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification, a close cousin of the notion of admissibility. The characterization is given in the context of the particular logic for dialectical argumentation DEFLOG. The results are of direct relevance for several well-established models of defeasible reasoning (like default logic, logic programming and argumentation frameworks), since elsewhere dialectical argumentation has been shown to have close formal connections with these models.
cs.AI:Recently, it has been shown that probabilistic entailment under coherence is weaker than model-theoretic probabilistic entailment. Moreover, probabilistic entailment under coherence is a generalization of default entailment in System P. In this paper, we continue this line of research by presenting probabilistic generalizations of more sophisticated notions of classical default entailment that lie between model-theoretic probabilistic entailment and probabilistic entailment under coherence. That is, the new formalisms properly generalize their counterparts in classical default reasoning, they are weaker than model-theoretic probabilistic entailment, and they are stronger than probabilistic entailment under coherence. The new formalisms are useful especially for handling probabilistic inconsistencies related to conditioning on zero events. They can also be applied for probabilistic belief revision. More generally, in the same spirit as a similar previous paper, this paper sheds light on exciting new formalisms for probabilistic reasoning beyond the well-known standard ones.
cs.AI:We seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic. Rather than stipulating that the conclusion of an inference be true in all models in which the premises are true, we require that the conclusion of a nonmonotonic inference be true in ``almost all'' models of a certain sort in which the premises are true. This ``certain sort'' specification picks out the models that are relevant to the inference, taking into account factors such as specificity and vagueness, and previous inferences. The frequencies characterizing the relevant models reflect known frequencies in our actual world. The criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension. We show that this avoids the implausibilities that might otherwise result from the chaining of default inferences. The model proportions, when construed in terms of frequencies, provide a verifiable grounding of default rules, and can become the basis for generating default rules from statistics.
cs.AI:About ten years ago, various notions of preferential entailment have been introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM), one of the main competitor being a more general version defined by Makinson (MAK). These two versions have already been compared, but it is time to revisit these comparisons. Here are our three main results: (1) These two notions are equivalent, provided that we restrict our attention, as done in KLM, to the cases where the entailment respects logical equivalence (on the left and on the right). (2) A serious simplification of the description of the fundamental cases in which MAK is equivalent to KLM, including a natural passage in both ways. (3) The two previous results are given for preferential entailments more general than considered in some of the original texts, but they apply also to the original definitions and, for this particular case also, the models can be simplified.
cs.AI:This work analyses main features that should be present in knowledge representation. It suggests a model for representation and a way to implement this model in software. Representation takes care of both low-level sensor information and high-level concepts.
cs.AI:We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.
cs.AI:Recently, several approaches to updating knowledge bases modeled as extended logic programs have been introduced, ranging from basic methods to incorporate (sequences of) sets of rules into a logic program, to more elaborate methods which use an update policy for specifying how updates must be incorporated. In this paper, we introduce a framework for reasoning about evolving knowledge bases, which are represented as extended logic programs and maintained by an update policy. We first describe a formal model which captures various update approaches, and we define a logical language for expressing properties of evolving knowledge bases. We then investigate semantical and computational properties of our framework, where we focus on properties of knowledge states with respect to the canonical reasoning task of whether a given formula holds on a given evolving knowledge base. In particular, we present finitary characterizations of the evolution for certain classes of framework instances, which can be exploited for obtaining decidability results. In more detail, we characterize the complexity of reasoning for some meaningful classes of evolving knowledge bases, ranging from polynomial to double exponential space complexity.
cs.AI:In this thesis I present a virtual laboratory which implements five different models for controlling animats: a rule-based system, a behaviour-based system, a concept-based system, a neural network, and a Braitenberg architecture. Through different experiments, I compare the performance of the models and conclude that there is no "best" model, since different models are better for different things in different contexts.   The models I chose, although quite simple, represent different approaches for studying cognition. Using the results as an empirical philosophical aid,   I note that there is no "best" approach for studying cognition, since different approaches have all advantages and disadvantages, because they study different aspects of cognition from different contexts. This has implications for current debates on "proper" approaches for cognition: all approaches are a bit proper, but none will be "proper enough". I draw remarks on the notion of cognition abstracting from all the approaches used to study it, and propose a simple classification for different types of cognition.
cs.AI:This paper deals with the revision of partially ordered beliefs. It proposes a semantic representation of epistemic states by partial pre-orders on interpretations and a syntactic representation by partially ordered belief bases. Two revision operations, the revision stemming from the history of observations and the possibilistic revision, defined when the epistemic state is represented by a total pre-order, are generalized, at a semantic level, to the case of a partial pre-order on interpretations, and at a syntactic level, to the case of a partially ordered belief base. The equivalence between the two representations is shown for the two revision operations.
cs.AI:This is the first in a series of connected papers discussing the problem of a dynamically reconfigurable universal learning neurocomputer that could serve as a computational model for the whole human brain. The whole series is entitled "The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal Learning Neurocomputer." (For more information visit the website www.brain0.com.) This introductory paper is concerned with general methodology. Its main goal is to explain why it is critically important for both neural modeling and cognitive modeling to pay much attention to the basic requirements of the whole brain as a complex computing system. The author argues that it can be easier to develop an adequate computational model for the whole "unprogrammed" (untrained) human brain than to find adequate formal representations of some nontrivial parts of brain's performance. (In the same way as, for example, it is easier to describe the behavior of a complex analytical function than the behavior of its real and/or imaginary part.) The "curse of dimensionality" that plagues purely phenomenological ("brainless") cognitive theories is a natural penalty for an attempt to represent insufficiently large parts of brain's performance in a state space of insufficiently high dimensionality. A "partial" modeler encounters "Catch 22." An attempt to simplify a cognitive problem by artificially reducing its dimensionality makes the problem more difficult.
cs.AI:As a part of our effort for studying the evolution and development of cognition, we present results derived from synthetic experimentations in a virtual laboratory where animats develop koncepts adaptively and ground their meaning through action. We introduce the term "koncept" to avoid confusions and ambiguity derived from the wide use of the word "concept". We present the models which our animats use for abstracting koncepts from perceptions, plastically adapt koncepts, and associate koncepts with actions. On a more philosophical vein, we suggest that knowledge is a property of a cognitive system, not an element, and therefore observer-dependent.
cs.AI:This paper presents a model for dynamic adjustment of the motivation degree, using a reinforcement learning approach, in an action selection mechanism previously developed by the authors. The learning takes place in the modification of a parameter of the model of combination of internal and external stimuli. Experiments that show the claimed properties are presented, using a VR simulation developed for such purposes. The importance of adaptation by learning in action selection is also discussed.
cs.AI:This article analyses the properties of the Internal Behaviour network, an action selection mechanism previously proposed by the authors, with the aid of a simulation developed for such ends. A brief review of the Internal Behaviour network is followed by the explanation of the implementation of the simulation. Then, experiments are presented and discussed analysing the properties of the action selection in the proposed model.
cs.AI:This paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent, based in an action selection mechanism previously proposed by the authors. This combination model includes additive and multiplicative elements, which allows to incorporate new properties, which enhance the action selection. A given parameter a, which is part of the proposed model, allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity.
cs.AI:Reinforcement learning (RL) involves sequential decision making in uncertain environments. The aim of the decision-making agent is to maximize the benefit of acting in its environment over an extended period of time. Finding an optimal policy in RL may be very slow. To speed up learning, one often used solution is the integration of planning, for example, Sutton's Dyna algorithm, or various other methods using macro-actions.   Here we suggest to separate plannable, i.e., close to deterministic parts of the world, and focus planning efforts in this domain. A novel reinforcement learning method called plannable RL (pRL) is proposed here. pRL builds a simple model, which is used to search for macro actions. The simplicity of the model makes planning computationally inexpensive. It is shown that pRL finds an optimal policy, and that plannable macro actions found by pRL are near-optimal. In turn, it is unnecessary to try large numbers of macro actions, which enables fast learning. The utility of pRL is demonstrated by computer simulations.
cs.AI:Optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length. For interacting agents in time-critical applications, learning of the possibility of scheduling of subtasks (events) or the full task is an additional relevant issue. Besides, there exist highly stochastic problems where the actual trajectories show great variety from episode to episode, but completing the task takes almost the same amount of time. The identification of sub-problems of this nature may promote e.g., planning, scheduling and segmenting Markov decision processes. In this work, formulae for the average duration as well as the standard deviation of the duration of events are derived. The emerging Bellman-type equation is a simple extension of Sobel's work (1982). Methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension. Computer demonstration on a toy problem serve to highlight the principle.
cs.AI:Much work has been done on extending the well-founded semantics to general disjunctive logic programs and various approaches have been proposed. However, these semantics are different from each other and no consensus is reached about which semantics is the most intended. In this paper we look at disjunctive well-founded reasoning from different angles. We show that there is an intuitive form of the well-founded reasoning in disjunctive logic programming which can be characterized by slightly modifying some exisitng approaches to defining disjunctive well-founded semantics, including program transformations, argumentation, unfounded sets (and resolution-like procedure). We also provide a bottom-up procedure for this semantics. The significance of our work is not only in clarifying the relationship among different approaches, but also shed some light on what is an intended well-founded semantics for disjunctive logic programs.
q-bio.BM:We consider the regime in which the bands of the torsional acoustic (TA) and the hydrogen-bond-stretch (HBS) modes of the DNA interpenetrate each other. Within the framework of a model that accommodates the structure of the double helix, we find the three-wave interaction between the TA- and the HBS-modes, and show that microwave radiation could bring about torsional vibrations that could serve as a pump mode for maintaining the HBS-one. Rayleigh's threshold condition for the parametric resonance provides an estimate for the power density of the mw-field necessary for generating the HBS-mode.
q-bio.BM:Identifying the driving forces and the mechanism of association of huntingtin-exon1, a close marker for the progress of Huntington's disease, is an important prerequisite towards finding potential drug targets, and ultimately a cure. We introduce here a modelling framework based on a key analogy of the physico-chemical properties of the exon1 fragment to block copolymers. We use a systematic mesoscale methodology, based on Dissipative Particle Dynamics, which is capable of overcoming kinetic barriers, thus capturing the dynamics of significantly larger systems over longer times than considered before. Our results reveal that the relative hydrophobicity of the poly-glutamine block as compared to the rest of the (proline-based) exon1 fragment, ignored to date, constitutes a major factor in the initiation of the self-assembly process. We find that the assembly is governed by both the concentration of exon1 and the length of the poly-glutamine stretch, with a low length threshold for association even at the lowest volume fractions we considered. Moreover, this self-association occurs irrespective of whether the glutamine stretch is in random coil or hairpin configuration, leading to spherical or cylindrical assemblies, respectively. We discuss the implications of these results for reinterpretation of existing research within this context, including that the routes towards aggregation of exon1 may be distinct to those of the widely studied homopolymeric poly-glutamine peptides.
q-bio.BM:The molecular mechanism of the solvent motion that is required to instigate the protein structural relaxation above a critical hydration level or transition temperature has yet to be determined. In this work we use quasi-elastic neutron scattering (QENS) and molecular dynamics simulation to investigate hydration water dynamics near a greatly simplified protein surface. We consider the hydration water dynamics near the completely deuterated N-acetyl-leucine-methylamide (NALMA) solute, a hydrophobic amino acid side chain attached to a polar blocked polypeptide backbone, as a function of concentration between 0.5M-2.0M, under ambient conditions. In this Communication, we focus our results of hydration dynamics near a model protein surface on the issue of how enzymatic activity is restored once a critical hydration level is reached, and provide a hypothesis for the molecular mechanism of the solvent motion that is required to trigger protein structural relaxation when above the hydration transition.
q-bio.BM:We analyze the dependence of thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, N. Using lattice Go models we show that DeltaT/T_F ~ N^-1, where T_F is the folding transition temperature and DeltaT is the folding transition width. This finding is consistent with finite size effects expected for the systems undergoing a phase transition from a disordered to an ordered phase. The dependence of the folding rates k_F on N for lattice models and the dataset of 57 proteins and peptides shows that k_F = k_F^0 exp(-CN^beta) provides a good fit, if 0 < beta <= 2/3 and C is a constant. We find that k_F = k_F^0 exp(-1.1N^0.5) with k_F^0 =(0.4x10^-6 s)^-1 can estimate optimal protein folding rates to within an order of magnitude in most cases. By using this fit for a set of proteins with beta-sheet topology we find that k_F^0 is approximately equal to k_U^0, the prefactor for unfolding rates. The maximum ratio of k_U^0/k_F^0 is 10 for this class of proteins.
q-bio.BM:The asymmetry in the shapes of folded and unfolded states are probed using two parameters, one being a measure of the sphericity and the other that describes the shape. For the folded states, whose interiors are densely packed, the radii of gyration (Rg) and these two parameters are calculated using the coordinates of the experimentally determined structures. Although Rg scales as expected for maximally compact structures, the distributions of the shape parameters show that there is considerable asymmetry in the shapes of folded structures. The degree of asymmetry is greater for proteins that form oligomers. Analysis of the two- and three-body contacts in the native structures shows that the presence of near equal number of contacts between backbone and side-chains and between side-chains gives rise to dense packing. We suggest that proteins with relatively large values of shape parameters can tolerate volume mutations without greatly affecting the network of contacts or their stability. To probe shape characteristics of denatured states we have developed a model of a WW-like domain. The shape parameters, which are calculated using Langevin simulations, change dramatically in the course of coil to globule transition. Comparison of the values of shape parameters between the globular state and the folded state of WW domain shows that both energetic (especially dispersion in the hydrophobic interactions) and steric effects are important in determining packing in proteins.
q-bio.BM:Instead of conformation states of single residues, refined conformation states of quintuplets are proposed to reflect conformation correlation. Simple hidden Markov models combining with sliding window scores are used for predicting secondary structure of a protein from its amino acid sequence. Since the length of protein conformation segments varies in a narrow range, we ignore the duration effect of the length distribution. The window scores for residues are a window version of the Chou-Fasman propensities estimated under an approximation of conditional independency. Different window widths are examined, and the optimal width is found to be 17. A high accuracy about 70% is achieved.
q-bio.BM:Is protein secondary structure primarily determined by local interactions between residues closely spaced along the amino acid backbone, or by non-local tertiary interactions? To answer this question we have measured the entropy densities of primary structure and secondary structure sequences, and the local inter-sequence mutual information density. We find that the important inter-sequence interactions are short ranged, that correlations between neighboring amino acids are essentially uninformative, and that only 1/4 of the total information needed to determine the secondary structure is available from local inter-sequence correlations. Since the remaining information must come from non-local interactions, this observation supports the view that the majority of most proteins fold via a cooperative process where secondary and tertiary structure form concurrently. To provide a more direct comparison to existing secondary structure prediction methods, we construct a simple hidden Markov model (HMM) of the sequences. This HMM achieves a prediction accuracy comparable to other single sequence secondary structure prediction algorithms, and can extract almost all of the inter-sequence mutual information. This suggests that these algorithms are almost optimal, and that we should not expect a dramatic improvement in prediction accuracy. However, local correlations between secondary and primary structure are probably of under-appreciated importance in many tertiary structure prediction methods, such as threading.
q-bio.BM:The determination of the folding mechanisms of proteins is critical to understand the topological change that can propagate Alzheimer and Creutzfeld-Jakobs diseases, among others. The computational community has paid considerable attention to this problem; however, the associated time scale, typically on the order of milliseconds or more, represents a formidable challenge. Ab initio protein folding from long molecular dynamics (MD) simulations or ensemble dynamics is not feasible with ordinary computing facilities and new techniques must be introduced. Here we present a detailed study of the folding of a 16-residue beta-hairpin, described by a generic energy model and using the activation-relaxation technique. From a total of 90 trajectories at 300 K, three folding pathways emerge. All involve a simultaneous optimization of the complete hydrophobic and hydrogen bonding interactions. The first two follow closely those observed by previous theoretical studies. The third pathway, never observed by previous all-atom folding, unfolding and equilibrium simulations, can be described as a reptation move of one strand of the beta-sheet with respect to the other. This reptation move indicates that non-native interactions can play a dominant role in the folding of secondary structures. These results point to a more complex folding picture than expected for a simple beta-hairpin.
q-bio.BM:Analytic estimates for the forces and free energy generated by bilayer deformation reveal a compelling and intuitive model for MscL channel gating analogous to the nucleation of a second phase. We argue that the competition between hydrophobic mismatch and tension results in a surprisingly rich story which can provide both a quantitative comparison to measurements of opening tension for MscL when reconstituted in bilayers of different thickness and qualitative insights into the function of the MscL channel and other transmembrane proteins.
q-bio.BM:It is important to understand how protein folding and evolution influences each other. Several studies based on entropy calculation correlating experimental measurement of residue participation in folding nucleus and sequence conservation have reached different conclusions. Here we report analysis of conservation of folding nucleus using an evolutionary model alternative to entropy based approaches. We employ a continuous time Markov model of codon substitution to distinguish mutation fixed by evolution and mutation fixed by chance. This model takes into account bias in codon frequency, bias favoring transition over transversion, as well as explicit phylogenetic information. We measure selection pressure using the ratio $\omega$ of synonymous vs. non-synonymous substitution at individual residue site. The $\omega$-values are estimated using the {\sc Paml} method, a maximum-likelihood estimator. Our results show that there is little correlation between the extent of kinetic participation in protein folding nucleus as measured by experimental $\phi$-value and selection pressure as measured by $\omega$-value. In addition, two randomization tests failed to show that folding nucleus residues are significantly more conserved than the whole protein. These results suggest that at the level of codon substitution, there is no indication that folding nucleus residues are significantly more conserved than other residues. We further reconstruct candidate ancestral residues of the folding nucleus and suggest possible test tube mutation studies of ancient folding nucleus.
q-bio.BM:Many signalling functions in molecular biology require proteins bind to substrates such as DNA in response to environmental signals such as the simultaneous binding to a small molecule. Examples are repressor proteins which may transmit information via a conformational change in response to the ligand binding. An alternative entropic mechanism of ``allostery'' suggests that the inducer ligand changes the intramolecular vibrational entropy not just the static structure. We present a quantitative, coarse-grained model of entropic allostery that suggests design rules for internal cohesive potentials in proteins employing this effect. It also addresses the issue of how the signal information to bind or unbind is transmitted through the protein. The model may be applicable to a wide range of repressors and also to signalling in transmembrane proteins.
q-bio.BM:How DNA repair enzymes find the relatively rare sites of damage is not known in great detail. Recent experiments and molecular data suggest that the individual repair enzymes do not work independently of each other, but rather interact with each other through currents exchanged along DNA. A damaged site in DNA hinders this exchange and this makes it possible to quickly free up resources from error free stretches of DNA. Here the size of the speedup gained from this current exchange mechanism is calculated and the characteristic length and time scales are identified. In particular for Escherichia coli we estimate the speedup to be 50000/N, where N is the number of repair enzymes participating in the current exchange mechanism. Even though N is not exactly known a speedup of order 10 is not entirely unreasonable. Furthermore upon over expression of repair enzymes the detection time only varies as one over the squareroot of N and not as 1/N. This behavior is of interest in assessing the impact of stress full and radioactive environments on individual cell mutation rates.
q-bio.BM:We study DNA adsorption and renaturation in a water-phenol two-phase system, with or without shaking. In very dilute solutions, single-stranded DNA is adsorbed at the interface in a salt-dependent manner. At high salt concentrations the adsorption is irreversible. The adsorption of the single-stranded DNA is specific to phenol and relies on stacking and hydrogen bonding. We establish the interfacial nature of a DNA renaturation at a high salt concentration. In the absence of shaking, this reaction involves an efficient surface diffusion of the single-stranded DNA chains. In the presence of a vigorous shaking, the bimolecular rate of the reaction exceeds the Smoluchowski limit for a three-dimensional diffusion-controlled reaction. DNA renaturation in these conditions is known as the Phenol Emulsion Reassociation Technique or PERT. Our results establish the interfacial nature of PERT. A comparison of this interfacial reaction with other approaches shows that PERT is the most efficient technique and reveals similarities between PERT and the renaturation performed by single-stranded nucleic acid binding proteins. Our results lead to a better understanding of the partitioning of nucleic acids in two-phase systems, and should help design improved extraction procedures for damaged nucleic acids. We present arguments in favor of a role of phenol and water-phenol interface in prebiotic chemistry. The most efficient renaturation reactions (in the presence of condensing agents or with PERT) occur in heterogeneous systems. This reveals the limitations of homogeneous approaches to the biochemistry of nucleic acids. We propose a heterogeneous approach to overcome the limitations of the homogeneous viewpoint.
q-bio.BM:Hydrophobicity is thought to be one of the primary forces driving the folding of proteins. On average, hydrophobic residues occur preferentially in the core, whereas polar residues tends to occur at the surface of a folded protein. By analyzing the known protein structures, we quantify the degree to which the hydrophobicity sequence of a protein correlates with its pattern of surface exposure. We have assessed the statistical significance of this correlation for several hydrophobicity scales in the literature, and find that the computed correlations are significant but far from optimal. We show that this less than optimal correlation arises primarily from the large degree of mutations that naturally occurring proteins can tolerate. Lesser effects are due in part to forces other than hydrophobicity and we quantify this by analyzing the surface exposure distributions of all amino acids. Lastly we show that our database findings are consistent with those found from an off-lattice hydrophobic-polar model of protein folding.
q-bio.BM:The approach for the description of the DNA conformational transformations on the mesoscopic scales in the frame of the double helix is presented. Due to consideration of the joint motions of DNA structural elements along the conformational pathways the models for different transformations may be constructed in the unifying two-component form. One component of the model is the degree of freedom of the elastic rod and another component -- the effective coordinate of the conformational transformation. The internal and external model components are interrelated, as it is characteristic for the DNA structure organization. It is shown that the kinetic energy of the conformational transformation of heterogeneous DNA may be put in homogeneous form. In the frame of the developed approach the static excitations of the DNA structure under the transitions between the stable states are found for internal and external components. The comparison of the data obtained with the experiment on intrinsic DNA deformability shows good qualitative agreement. The conclusion is made that the found excitations in the DNA structure may be classificated as the static conformational solitons.
q-bio.BM:Molecular combing is a powerful and simple method for aligning DNA molecules onto a surface. Using this technique combined with fluorescence microscopy, we observed that the length of lambda-DNA molecules was extended to about 1.6 times their contour length (unextended length, 16.2 micrometers) by the combing method on hydrophobic polymethylmetacrylate (PMMA) coated surfaces. The effects of sodium and magnesium ions and pH of the DNA solution were investigated. Interestingly, we observed force-induced melting of single DNA molecules.
q-bio.BM:Using a Brownian dynamics simulation, we numerically studied the interaction of DNA with histone and proposed an octamer-rotation model to describe the process of nucleosome formation. Nucleosome disruption under stretching was also simulated. The theoretical curves of extension versus time as well as of force versus extension are consistent with previous experimental results.
q-bio.BM:We propose a two-dimensional model for a complete description of the dynamics of molecular motors, including both the processive movement along track filaments and the dissociation from the filaments. The theoretical results on the distributions of the run length and dwell time at a given ATP concentration, the dependences of mean run length, mean dwell time and mean velocity on ATP concentration and load are in good agreement with the previous experimental results.
q-bio.BM:Kinesin motors have been studied extensively both experimentally and theoretically. However, the microscopic mechanism of the processive movement of kinesin is still an open question. In this paper, we propose a hand-over-hand model for the processivity of kinesin, which is based on chemical, mechanical, and electrical couplings. In the model the processive movement does not need to rely on the two heads' coordination in their ATP hydrolysis and mechanical cycles. Rather, the ATP hydrolyses at the two heads are independent. The much higher ATPase rate at the trailing head than the leading head makes the motor walk processively in a natural way, with one ATP being hydrolyzed per step. The model is consistent with the structural study of kinesin and the measured pathway of the kinesin ATPase. Using the model the estimated driving force of ~ 5.8 pN is in agreements with the experimental results (5~7.5 pN). The prediction of the moving time in one step (~10 microseconds) is also consistent with the measured values of 0~50 microseconds. The previous observation of substeps within the 8-nm step is explained. The shapes of velocity-load (both positive and negative) curves show resemblance to previous experimental results.
q-bio.BM:Myosin V and myosin VI are two classes of two-headed molecular motors of the myosin superfamily that move processively along helical actin filaments in opposite directions. Here we present a hand-over-hand model for their processive movements. In the model, the moving direction of a dimeric molecular motor is automatically determined by the relative orientation between its two heads at free state and its head's binding orientation on track filament. This determines that myosin V moves toward the barbed end and myosin VI moves toward the pointed end of actin. During the moving period in one step, one head remains bound to actin for myosin V whereas two heads are detached for myosin VI: The moving manner is determined by the length of neck domain. This naturally explains the similar dynamic behaviors but opposite moving directions of myosin VI and mutant myosin V (the neck of which is truncated to only one-sixth of the native length). Because of different moving manners, myosin VI and mutant myosin V exhibit significantly broader step-size distribution than native myosin V. However, all three motors give the same mean step size of 36 nm (the pseudo-repeat of actin helix). Using the model we study the dynamics of myosin V quantitatively, with theoretical results in agreement with previous experimental ones.
q-bio.BM:We describe a faster and more accurate algorithm for computing the statistical mechanics of DNA denaturation according to the Poland-Scheraga type. Nearest neighbor thermodynamics is included in a complete and general way. The algorithm represents an optimization with respect to algorithmic complexity of the partition function algorithm of Yeramian et al.: We reduce the computation time for a base-pairing probability profile from O(N2) to O(N). This speed-up comes in addition to the speed-up due to a multiexponential approximation of the loop entropy factor as introduced by Fixman and Freire. The speed-up, however, is independent of the multiexponential approximation and reduces time from O(N3) to O(N2) in the exact case. In addition to calculating the standard base-pairing probability profiles, we propose to use the algorithm to calculate various other probabilities (loops, helices, tails) for a more direct view of the melting regions and their positions and sizes.
q-bio.BM:A joint experimental / theoretical investigation of the elastin-like octapeptide GVG(VPGVG) was carried out. In this paper a comprehensive molecular dynamics study of the temperature dependent folding and unfolding of the octapeptide is presented. The current study, as well as its experimental counterpart find that this peptide undergoes an "inverse temperature transition", ITT, leading to a folding at about 310-330 K. In addition, an unfolding transition is identified at unusually high temperatures approaching the boiling point of water. Due to the small size of the system two broad temperature regimes are found: the "ITT regime" (at about 280-320 K) and the "unfolding regime" at about T > 330 K, where the peptide has a maximum probability of being folded at approximately 330 K. A detailed molecular picture involving a thermodynamic order parameter, or reaction coordinate, for this process is presented along with a time-correlation function analysis of the hydrogen bond dynamics within the peptide as well as between the peptide and solvating water molecules. Correlation with experimental evidence and ramifications on the properties of elastin are discussed.
q-bio.BM:A simplified model for the closed circular DNA (ccDNA) is proposed to describe some specific features of the helix-coil transition in such molecule. The Hamiltonian of ccDNA is related to the one introduced earlier for the linear DNA. The basic assumption is that the reduced energy of the hydrogen bond is not constant through the transition process but depends effectively on the fraction of already broken bonds. A transformation formula is obtained which relates the temperature of ccDNA at a given degree of helicity during the transition to the temperature of the corresponding linear chain at the same degree of helicity. The formula provides a simple method to calculate the melting curve for the ccDNA from the experimental melting curve of the linear DNA with the same nucleotide sequence.
q-bio.BM:We develop a simple but rigorous model of protein-protein association kinetics based on diffusional association on free energy landscapes obtained by sampling configurations within and surrounding the native complex binding funnels. Guided by results obtained on exactly solvable model problems, we transform the problem of diffusion in a potential into free diffusion in the presence of an absorbing zone spanning the entrance to the binding funnel. The free diffusion problem is solved using a recently derived analytic expression for the rate of association of asymmetrically oriented molecules. Despite the required high steric specificity and the absence of long-range attractive interactions, the computed rates are typically on the order of 10^4-10^6 M-1 s-1, several orders of magnitude higher than rates obtained using a purely probabilistic model in which the association rate for free diffusion of uniformly reactive molecules is multiplied by the probability of a correct alignment of the two partners in a random collision. As the association rates of many protein-protein complexes are also in the 10^5-10^6 M-1 s-1, our results suggest that free energy barriers arising from desolvation and/or side-chain freezing during complex formation or increased ruggedness within the binding funnel, which are completely neglected in our simple diffusional model, do not contribute significantly to the dynamics of protein-protein association. The transparent physical interpretation of our approach that computes association rates directly from the size and geometry of protein-protein binding funnels makes it a useful complement to Brownian dynamics simulations.
q-bio.BM:Functional proteins must fold with some minimal stability to a structure that can perform a biochemical task. Here we use a simple model to investigate the relationship between the stability requirement and the capacity of a protein to evolve the function of binding to a ligand. Although our model contains no built-in tradeoff between stability and function, proteins evolved function more efficiently when the stability requirement was relaxed. Proteins with both high stability and high function evolved more efficiently when the stability requirement was gradually increased than when there was constant selection for high stability. These results show that in our model, the evolution of function is enhanced by allowing proteins to explore sequences corresponding to marginally stable structures, and that it is easier to improve stability while maintaining high function than to improve function while maintaining high stability. Our model also demonstrates that even in the absence of a fundamental biophysical tradeoff between stability and function, the speed with which function can evolve is limited by the stability requirement imposed on the protein.
q-bio.BM:Using the model for the processive movement of a dimeric kinesin we proposed before, we study the dynamics of a number of mutant homodimeric and heterodimeric kinesins that were constructed by Kaseda et al. (Kaseda, K., Higuchi, H. and Hirose, K. PNAS 99, 16058 (2002)). The theoretical results of ATPase rate per head, moving velocity, and stall force of the motors show good agreement with the experimental results by Kaseda et al.: The puzzling dynamic behaviors of heterodimeric kinesin that consists of two distinct heads compared with its parent homodimers can be easily explained by using independent ATPase rates of the two heads in our model. We also study the collective kinetic behaviors of kinesins in MT-gliding motility. The results explains well that the average MT-gliding velocity is independent of the number of bound motors and is equal to the moving velocity of a single kinesin relative to MT.
q-bio.BM:The simplest approximation of interaction potential between amino-acids in proteins is the contact potential, which defines the effective free energy of a protein conformation by a set of amino acid contacts formed in this conformation. Finding a contact potential capable of predicting free energies of protein states across a variety of protein families will aid protein folding and engineering in silico on a computationally tractable time-scale. We test the ability of contact potentials to accurately and transferably (across various protein families) predict stability changes of proteins upon mutations. We develop a new methodology to determine the contact potentials in proteins from experimental measurements of changes in protein thermodynamic stabilities (ddG) upon mutations. We apply our methodology to derive sets of contact interaction parameters for a hierarchy of interaction models including solvation and multi-body contact parameters. We test how well our models reproduce experimental measurements by statistical tests. We evaluate the maximum accuracy of predictions obtained by using contact potentials and the correlation between parameters derived from different data-sets of experimental ddG values. We argue that it is impossible to reach experimental accuracy and derive fully transferable contact parameters using the contact models of potentials. However, contact parameters can yield reliable predictions of ddG for datasets of mutations confined to specific amino-acid positions in the sequence of a single protein.
q-bio.BM:We first review how to determine the rate of vibrational energy relaxation (VER) using perturbation theory. We then apply those theoretical results to the problem of VER of a CD stretching mode in the protein cytochrome c. We model cytochrome c in vacuum as a normal mode system with the lowest-order anharmonic coupling elements. We find that, for the ``lifetime'' width parameter $\gamma=3 \sim 30$ cm$^{-1}$, the VER time is $0.2 \sim 0.3$ ps, which agrees rather well with the previous classical calculation using the quantum correction factor method, and is consistent with spectroscopic experiments by Romesberg's group. We decompose the VER rate into separate contributions from two modes, and find that the most significant contribution, which depends on the ``lifetime'' width parameter, comes from those modes most resonant with the CD vibrational mode.
q-bio.BM:The three-dimensional structures of two common repeat motifs Val$^1$-Pro$^2$-Gly$^3$-Val$^4$-Gly$^5$ and Val$^1$-Gly$^2$-Val$^3$-Pro$^4$-Gly$^5$-Val$^6$-Gly$^7$-Val$^8$-Pro$^9$ of tropoelastin are investigated by using the multicanonical simulation procedure. By minimizing the energy structures along the trajectory the thermodynamically most stable low-energy microstates of the molecule are determined. The structural predictions are in good agreement with X-ray diffraction experiments.
q-bio.BM:We address the controversial hot question concerning the validity of the loose-coupling versus the lever-arm models in the actomyosin dynamics by re-interpreting and extending the washboard potential model proposed by some of us in a previous paper. In the new theory, a loose-coupling mechanism co-exists with the deterministic lever-arm model. The synergetic action of a random component, originating from the harnessed thermal energy, and of the power-stroke generated by the lever-arm classical mechanism is seen to yield an excellent fit of the set of data obtained in T. Yanagida's laboratory on the sliding of Myosin II heads on actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of theory is tested via different combination of parameters and potential profiles.
q-bio.BM:In simple models side chains are often represented implicitly (e.g., by spin-states) or simplified as one atom. We study side chain effects using square lattice and tetrahedral lattice models, with explicitly side chains of two atoms. We distinguish effects due to chirality and effects due to side chain flexibilities, since residues in proteins are L-residues, and their side chains adopt different rotameric states. Short chains are enumerated exhaustively. For long chains, we sample effectively rare events (eg, compact conformations) and obtain complete pictures of ensemble properties of these models at all compactness region. We find that both chirality and reduced side chain flexibility lower the folding entropy significantly for globally compact conformations, suggesting that they are important properties of residues to ensure fast folding and stable native structure. This corresponds well with our finding that natural amino acid residues have reduced effective flexibility, as evidenced by analysis of rotamer libraries and side chain rotatable bonds. We further develop a method calculating the exact side-chain entropy for a given back bone structure. We show that simple rotamer counting often underestimates side chain entropy significantly, and side chain entropy does not always correlate well with main chain packing. Among compact backbones with maximum side chain entropy, helical structures emerges as the dominating configurations. Our results suggest that side chain entropy may be an important factor contributing to the formation of alpha helices for compact conformations.
q-bio.BM:We show that the contact map of the native structure of globular proteins can be reconstructed starting from the sole knowledge of the contact map's principal eigenvector, and present an exact algorithm for this purpose. Our algorithm yields a unique contact map for all 221 globular structures of PDBselect25 of length $N \le 120$. We also show that the reconstructed contact maps allow in turn for the accurate reconstruction of the three-dimensional structure. These results indicate that the reduced vectorial representation provided by the principal eigenvector of the contact map is equivalent to the protein structure itself. This representation is expected to provide a useful tool in bioinformatics algorithms for protein structure comparison and alignment, as well as a promising intermediate step towards protein structure prediction.
q-bio.BM:Function of proteins or a network of interacting proteins often involves communication between residues that are well separated in sequence. The classic example is the participation of distant residues in allosteric regulation. Bioinformatic and structural analysis methods have been introduced to infer residues that are correlated. Recently, increasing attention has been paid to obtain the sequence properties that determine the tendency of disease related proteins (Abeta peptides, prion proteins, transthyretin etc.) to aggregate and form fibrils. Motivated in part by the need to identify sequence characteristics that indicate a tendency to aggregate, we introduce a general method that probes covariations in charged residues along the sequence in a given protein family. The method, which involves computing the Sequence Correlation Entropy (SCE) using the quenched probability Psk(i,j) of finding a residue pair at a given sequence separation sk, allows us to classify protein families in terms of their SCE. Our general approach may be a useful way in obtaining evolutionary covariations of amino acid residues on a genome wide level.
q-bio.BM:We present an analysis of the effects of global topology on the structural stability of folded proteins in thermal equilibrium with a heat bath. For a large class of single domain proteins, we computed the harmonic spectrum within the Gaussian Network Model (GNM) and determined the spectral dimension, a parameter describing the low frequency behaviour of the density of modes. We find a surprisingly strong correlation between the spectral dimension and the number of amino acids of the protein. Considering that larger spectral dimension value relate to more topologically compact folded state, our results indicate that for a given temperature and length of the protein, the folded structure corresponds to the less compact folding compatible with thermodynamic stability.
q-bio.BM:We present a simple physical model which demonstrates that the native state folds of proteins can emerge on the basis of considerations of geometry and symmetry. We show that the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and hydrophobicity are sufficient to yield a free energy landscape with broad minima even for a homopolymer. These minima correspond to marginally compact structures comprising the menu of folds that proteins choose from to house their native-states in. Our results provide a general framework for understanding the common characteristics of globular proteins.
q-bio.BM:With the aim to study the relationship between protein sequences and their native structures, we adopt vectorial representations for both sequence and structure. The structural representation is based on the Principal Eigenvector of the fold's contact matrix (PE). As recently shown, the latter encodes sufficient information for reconstructing the whole contact matrix. The sequence is represented through a Hydrophobicity Profile (HP), using a generalized hydrophobicity scale that we obtain from the principal eigenvector of a residue-residue interaction matrix and denote it as interactivity scale. Using this novel scale, we define the optimal HP of a protein fold, and predict, by means of stability arguments, that it is strongly correlated with the PE of the fold's contact matrix. This prediction is confirmed through an evolutionary analysis, which shows that the PE correlates with the HP of each individual sequence adopting the same fold and, even more strongly, with the average HP of this set of sequences. Thus, protein sequences evolve in such a way that their average HP is close to the optimal one, implying that neutral evolution can be viewed as a kind of motion in sequence space around the optimal HP. Our results indicate that the correlation coefficient between N-dimensional vectors constitutes a natural metric in the vectorial space in which we represent both protein sequences and protein structures, which we call Vectorial Protein Space. In this way, we define a unified framework for sequence to sequence, sequence to structure, and structure to structure alignments. We show that the interactivity scale is nearly optimal both for the comparison of sequences with sequences and sequences with structures.
q-bio.BM:We fit the Fourier transforms of solvent accessibility and hydrophobicity profiles of a representative set of proteins to a joint multi-variable Gaussian. This allows us to separate the intrinsic tendencies of sequence and structure profiles from the interactions that correlate them; for example, the $\alpha$-helix periodicity in sequence hydrophobicity is dictated by the solvent accessibility of structures. The distinct intrinsic tendencies of sequence and structure profiles are most pronounced at long periods, where sequence hydrophobicity fluctuates more, while solvent accessibility fluctuations are less than average. Interestingly, correlations between the two profiles can be interpreted as the Boltzmann weight of the solvation energy at room temperature.
q-bio.BM:In this paper, we examine the mechanical role of the lipid bilayer in ion channel conformation and function with specific reference to the case of the mechanosensitive channel of large conductance (MscL). In a recent paper (Wiggins and Phillips, 2004), we argued that mechanotransduction very naturally arises from lipid-protein interactions by invoking a simple analytic model of the MscL channel and the surrounding lipid bilayer. In this paper, we focus on improving and expanding this analytic framework for studying lipid-protein interactions with special attention to MscL. Our goal is to generate simple scaling relations which can be used to provide qualitative understanding of the role of membrane mechanics in protein function and to quantitatively interpret experimental results. For the MscL channel, we find that the free energies induced by lipid-protein interaction are of the same order as the free energy differences between conductance states measured by Sukharev et al. (1999). We therefore conclude that the mechanics of the bilayer plays an essential role in determining the conformation and function of the channel. Finally, we compare the predictions of our model to experimental results from the recent investigations of the MscL channel by Perozo et al. (2002), Powl et al. (2003), Yoshimura et al. (2004), and others and suggest a suite of new experiments.
q-bio.BM:The conjunction of insights from structural biology, solution biochemistry, genetics and single molecule biophysics has provided a renewed impetus for the construction of quantitative models of biological processes. One area that has been a beneficiary of these experimental techniques is the study of viruses. In this paper we describe how the insights obtained from such experiments can be utilized to construct physical models of processes in the viral life cycle. We focus on dsDNA bacteriophages and show that the bending elasticity of DNA and its electrostatics in solution can be combined to determine the forces experienced during packaging and ejection of the viral genome. Furthermore, we quantitatively analyze the effect of fluid viscosity and capsid expansion on the forces experienced during packaging. Finally, we present a model for DNA ejection from bacteriophages based on the hypothesis that the energy stored in the tightly packed genome within the capsid leads to its forceful ejection. The predictions of our model can be tested through experiments in vitro where DNA ejection is inhibited by the application of external osmotic pressure.
q-bio.BM:Amyloid fibers are aggregates of proteins. They are built out of a peptide called $\beta$--amyloid (A$\beta$) containing between 41 and 43 residues, produced by the action of an enzyme which cleaves a much larger protein known as the Amyloid Precursor Protein (APP). X-ray diffraction experiments have shown that these fibrils are rich in $\beta$--structures, whereas the shape of the peptide displays an $\alpha$--helix structure within the APP in its biologically active conformation. A realistic model of fibril formation is developed based on the seventeen residues A$\beta$12--28 amyloid peptide, which has been shown to form fibrils structurally similar to those of the whole A$\beta$ peptide. With the help of physical arguments and in keeping with experimental findings, the A$\beta$12--28 monomer is assumed to be in four possible states (i.e., native helix conformation, $\beta$--hairpin, globular low--energy state and unfolded state). Making use of these monomeric states, oligomers (dimers, tertramers and octamers) were constructed. With the help of short, detailed Molecular Dynamics (MD) calculations of the three monomers and of a variety of oligomers, energies for these structures were obtained. Making use of these results within the framework of a simple yet realistic model to describe the entropic terms associated with the variety of amyloid conformations, a phase diagram can be calculated of the whole many--body system, leading to a thermodynamical picture in overall agreement with the experimental findings. In particular, the existence of micellar metastable states seem to be a key issue to determine the thermodynamical properties of the system.
q-bio.BM:The possibility of deriving the contact potentials between amino acids from their frequencies of occurence in proteins is discussed in evolutionary terms. This approach allows the use of traditional thermodynamics to describe such frequencies and, consequently, to develop a strategy to include in the calculations correlations due to the spatial proximity of the amino acids and to their overall tendency of being conserved in proteins. Making use of a lattice model to describe protein chains and defining a "true" potential, we test these strategies by selecting a database of folding model sequences, deriving the contact potentials from such sequences and comparing them with the "true" potential. Taking into account correlations allows for a markedly better prediction of the interaction potentials.
q-bio.BM:While all the information required for the folding of a protein is contained in its amino acid sequence, one has not yet learned how to extract this information to predict the three--dimensional, biologically active, native conformation of a protein whose sequence is known. Using insight obtained from simple model simulations of the folding of proteins, in particular of the fact that this phenomenon is essentially controlled by conserved (native) contacts among (few) strongly interacting ("hot"), as a rule hydrophobic, amino acids, which also stabilize local elementary structures (LES, hidden, incipient secondary structures like $\alpha$--helices and $\beta$--sheets) formed early in the folding process and leading to the postcritical folding nucleus (i.e., the minimum set of native contacts which bring the system pass beyond the highest free--energy barrier found in the whole folding process) it is possible to work out a succesful strategy for reading the native structure of designed proteins from the knowledge of only their amino acid sequence and of the contact energies among the amino acids. Because LES have undergone millions of years of evolution to selectively dock to their complementary structures, small peptides made out of the same amino acids as the LES are expected to selectively attach to the newly expressed (unfolded) protein and inhibit its folding, or to the native (fluctuating) native conformation and denaturate it. These peptides, or their mimetic molecules, can thus be used as effective non--conventional drugs to those already existing (and directed at neutralizing the active site of enzymes), displaying the advantage of not suffering from the uprise of resistance.
q-bio.BM:Methods for alignment of protein sequences typically measure similarity by using substitution matrix with scores for all possible exchanges of one amino acid with another. Although widely used, the matrices derived from homologous sequence segments, such as Dayhoff's PAM matrices and Henikoff's BLOSUM matrices, are not specific for protein conformation identification. Using a different approach, we got many amino acid segment blocks. For each of them, the protein secondary structure is identical. Based on these blocks, we have derived new amino acid substitution matrices. The application of these matrices led to marked improvements in conformation segment search and homologues detection in twilight zone.
q-bio.BM:The advent of new experimental genomic technologies and the massive increase of DNA sequence information is helping researchers better understand how our genes work. Recently, experiments on mRNA abundance (gene expression) have revealed that gene expression shows a stationary organization described by a power-law distribution (scale-free organization) (i.e., gene expression $k$ decays as $k^{-\gamma}$), which is highly conserved in all the major five kingdoms of life, from Bacteria to Human. An underlying gene expression dynamics "rich-travel-more" was suggested to recover that evolutional conservation of transcriptional organization. Here we propose a constructive approach to gene expression dynamics with larger scope. Our gene expression construction restores the stationary state, predicts the power-law exponent for different organisms with natural explanation for small correction at high and low expression levels, describes the intermediate state dynamics (time finite) and elucidates the gene expression stability. This approach requires only one assumption: Markov property.
q-bio.BM:Proteins are minimally frustrated polymers. However, for realistic protein models non-native interactions must be taken into account. In this paper we analyze the effect of non-native interactions on the folding rate and on the folding free energy barrier. We present an analytic theory to account for the modification on the free energy landscape upon introduction of non-native contacts, added as a perturbation to the strong native interactions driving folding. Our theory predicts a rate-enhancement regime at fixed temperature, under the introduction of weak, non-native interactions. We have thoroughly tested this theoretical prediction with simulations of a coarse-grained protein model, by employing an off-lattice $C_\alpha$ model of the src-SH3 domain. The strong agreement between results from simulations and theory confirm the non trivial result that a relatively small amount of non-native interaction energy can actually assist the folding to the native structure.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. For simplified models of proteins where coordinates of only $C_\alpha$ atoms need to be specified, an accurate potential function is important. Such a simplified model is essential for efficient search of conformational space. In this work, we present a formulation of potential function for simplified representations of protein structures. It is based on the combination of descriptors derived from residue-residue contact and sequence-dependent local geometry. The optimal weight coefficients for contact and local geometry is obtained through optimization by maximizing margins among native and decoy structures. The latter are generated by chain growth and by gapless threading. The performance of the potential function in blind test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. This potential function have comparable or better performance than several residue-based potential functions that require in addition coordinates of side chain centers or coordinates of all side chain atoms.
q-bio.BM:Circular permutation connects the N and C termini of a protein and concurrently cleaves elsewhere in the chain, providing an important mechanism for generating novel protein fold and functions. However, their in genomes is unknown because current detection methods can miss many occurances, mistaking random repeats as circular permutation. Here we develop a method for detecting circularly permuted proteins from structural comparison. Sequence order independent alignment of protein structures can be regarded as a special case of the maximum-weight independent set problem, which is known to be computationally hard. We develop an efficient approximation algorithm by repeatedly solving relaxations of an appropriate intermediate integer programming formulation, we show that the approximation ratio is much better then the theoretical worst case ratio of $r = 1/4$. Circularly permuted proteins reported in literature can be identified rapidly with our method, while they escape the detection by publicly available servers for structural alignment.
q-bio.BM:Motivation. Protein design aims to identify sequences compatible with a given protein fold but incompatible to any alternative folds. To select the correct sequences and to guide the search process, a design scoring function is critically important. Such a scoring function should be able to characterize the global fitness landscape of many proteins simultaneously.   Results. To find optimal design scoring functions, we introduce two geometric views and propose a formulation using mixture of nonlinear Gaussian kernel functions. We aim to solve a simplified protein sequence design problem. Our goal is to distinguish each native sequence for a major portion of representative protein structures from a large number of alternative decoy sequences, each a fragment from proteins of different fold. Our scoring function discriminate perfectly a set of 440 native proteins from 14 million sequence decoys. We show that no linear scoring function can succeed in this task. In a blind test of unrelated proteins, our scoring function misclassfies only 13 native proteins out of 194. This compares favorably with about 3-4 times more misclassifications when optimal linear functions reported in literature are used. We also discuss how to develop protein folding scoring function.
q-bio.BM:Being HIV-1-PR an essential enzyme in the viral life cycle, its inhibition can control AIDS. Because the folding of single domain proteins, like HIV-1-PR is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved amino acids) which have evolved over myriads of generations to recognize and strongly attract each other so as to make the protein fold fast, we suggest a novel type of HIV-1-PR inhibitors which interfere with the folding of the protein: short peptides displaying the same amino acid sequence of that of LES. Theoretical and experimental evidence for the specificity and efficiency of such inhibitors are presented.
q-bio.BM:Vibrational energy relaxation (VER) of a selected mode in cytochrome c (hemeprotein) in vacuum is studied using two theoretical approaches: One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes coupled with nonlinear coupling elements. Both methods result in estimates of VER time (sub ps) for a CD stretching mode in the protein at room temperature, that are in accord with the experimental data of Romesberg's group. The applicability of the two methods is examined through a discussion of the validity of Fermi's golden rule on which the two methods are based.
q-bio.BM:The 16-22 amino acid fragment of the beta-amyloid peptide associated with the Alzheimer's disease, Abeta, is capable of forming amyloid fibrils. Here we study the aggregation mechanism of Abeta(16-22) peptides by unbiased thermodynamic simulations at the atomic level for systems of one, three and six Abeta(16-22) peptides. We find that the isolated Abeta(16-22) peptide is mainly a random coil in the sense that both the alpha-helix and beta-strand contents are low, whereas the three- and six-chain systems form aggregated structures with a high beta-sheet content. Furthermore, in agreement with experiments on Abeta(16-22) fibrils, we find that large parallel beta-sheets are unlikely to form. For the six-chain system, the aggregated structures can have many different shapes, but certain particularly stable shapes can be identified.
q-bio.BM:We identified latent periodicity in catalytic domains of approximately 85% of serine/threonine and tyrosine protein kinases. Similar results were obtained for other 22 protein domains. We also designed the method of noise decomposition, which is aimed to distinguish between different periodicity types of the same period length. The method is to be used in conjunction with the cyclic profile alignment, and this combination is able to reveal structure-related or function-related patterns of latent periodicity. Possible origins of the periodic structure of protein kinase active sites are discussed. Summarizing, we presume that latent periodicity is the common property of many catalytic protein domains.
q-bio.BM:We found latent periodicity of 150 protein families now. We suppose that latent periodicity can determine a spectrum of resonance oscillations in proteins.
q-bio.BM:Proteins have regular tertiary structures but irregular amino acid sequences. This made it very difficult to decode the structural information in the protein sequences. Here we demonstrate that many small alpha protein domains have hidden sequence symmetries characteristic of their pseudo-symmetric tertiary structures. We also present a modified method of recurrent plot to reveal this kind of the hidden sequence symmetry. The results may enable us understand parts of the relations between protein sequences and their tertiary structures, i.e, how the primary sequence of a protein determines its tertiary structure.
q-bio.BM:Summary: The F2CS server provides access to the software, F2CS2.00, that implements an automated prediction method of SCOP and CATH classifications of proteins, based on their FSSP Z-scores (Getz et al., 2002), Availability: Free, at http://www.weizmann.ac.il/physics/complex/compphys/f2cs/. Contact: eytan.domany@weizmann.ac.il Supplementary information: The site contains links to additional figures and tables.
q-bio.BM:Activated processes such as protein unfolding are highly sensitive to heterogeneity in the environment. We study a highly simplified model of a protein in a random heterogeneous environment, a model of the in vivo environment. It is found that if the heterogeneity is sufficiently large the total rate of the process is essentially a random variable; this may be the cause of the species-to-species variability in the rate of prion protein conversion found by Deleault et al. [Nature, 425 (2003) 717].
q-bio.BM:Water molecules and molecular chaperones efficiently help the protein folding process. Here we describe their action in the context of the energy and topological networks of proteins. In energy terms water and chaperones were suggested to decrease the activation energy between various local energy minima smoothing the energy landscape, rescuing misfolded proteins from conformational traps and stabilizing their native structure. In kinetic terms water and chaperones may make the punctuated equilibrium of conformational changes less punctuated and help protein relaxation. Finally, water and chaperones may help the convergence of multiple energy landscapes during protein-macromolecule interactions. We also discuss the possibility of the introduction of protein games to narrow the multitude of the energy landscapes when a protein binds to another macromolecule. Both water and chaperones provide a diffuse set of rapidly fluctuating weak links (low affinity and low probability interactions), which allow the generalization of all these statements to a multitude of networks.
q-bio.BM:Nucleosomes organize the folding of DNA into chromatin and significantly influence transcription, replication, regulation and repair. All atom molecular dynamics simulations of a nucleosome and of its 146 basepairs of DNA free in solution have been conducted. DNA helical parameters are extracted from each trajectory to compare the conformation, effective force constants, persistence length measures, and fluctuations of nucleosomal DNA to free DNA. A method for disassembling and reconstructing the conformation and dynamics of the nucleosome using Fourier analysis is presented. Results indicate that the superhelical path of DNA in the nucleosome is irregular. Long length variations in the conformation of nucleosomal DNA are identified other than those associated with helix repeat. These variations are required to create a proposed tetrasome conformation or to qualitatively reconstruct the 1.75 turns of the nuclesomal superhelix. Free DNA achieves enough bend and shear in solution to create an ideal nucleosome superhelix, but these deformations are not organized so the conformation is essentially linear. Reconstruction of free DNA using selected long wavelength variations in conformation can produce either a left-handed or a right-handed superhelix. DNA is less flexible in the nucleosome than when free in solution, however such measures are length scale dependent.
q-bio.BM:Gene expression analysis by means of microarrays is based on the sequence specific binding of mRNA to DNA oligonucleotide probes and its measurement using fluorescent labels. The binding of RNA fragments involving other sequences than the intended target is problematic because it adds a "chemical background" to the signal, which is not related to the expression degree of the target gene. The paper presents a molecular signature of specific and non specific hybridization with potential consequences for gene expression analysis. We analyzed the signal intensities of perfect match (PM) and mismatch (MM) probes of GeneChip microarrays to specify the effect of specific and non specific hybridization. We found that these events give rise to different relations between the PM and MM intensities as function of the middle base of the PMs, namely a triplet- (C>G=T>A>0) and a duplet-like (C=T>0>G=A) pattern of the PM-MM log-intensity difference upon binding of specific and non specific RNA fragments, respectively. The systematic behaviour of the intensity difference can be rationalized on the level of base pairings of DNA/RNA oligonucleotide duplexes in the middle of the probe sequence. Non-specific binding is characterized by the reversal of the central Watson Crick (WC) pairing for each PM/MM probe pair, whereas specific binding refers to the combination of a WC and a self complementary (SC) pairing in PM and MM probes, respectively. The intensity of complementary MM introduces a systematic source of variation which decreases the precision of expression measures based on the MM intensities.
q-bio.BM:We implement the replica exchange molecular dynamics algorithm to study the interactions of a model peptide (WALP-16) with an explicitly represented DPPC membrane bilayer. We observe the spontaneous, unbiased insertion of WALP-16 into the DPPC bilayer and its folding into an a-helix with a trans-bilayer orientation. We observe that the insertion of the peptide into the DPPC bilayer precedes secondary structure formation. Although the peptide has some propensity to form a partially helical structure in the interfacial region of the DPPC/water system, this state is not a productive intermediate but rather an off-pathway trap for WALP-16 insertion. Equilibrium simulations show that the observed insertion/folding pathway mirrors the potential of mean force (PMF). Calculation of the enthalpic and entropic contributions to this PMF show that the surface bound conformation of WALP-16 is significantly lower in energy than other conformations, and that the insertion of WALP-16 into the bilayer without regular secondary structure is enthalpically unfavorable by 5-10 kcal/mol/residue. The observed insertion/folding pathway disagrees with the dominant conceptual model, which is that a surface bound helix is an obligatory intermediate for the insertion of a-helical peptides into lipid bilayers. In our simulations, the observed insertion/folding pathway is favored because of a large (> 100 kcal/mol) increase in system entropy that occurs when the unstructured WALP-16 peptide enters the lipid bilayer interior. The insertion/folding pathway that is lowest in free energy depends sensitively on the near cancellation of large enthalpic and entropic terms. This suggests that intrinsic membrane peptides may have a diversity of insertion/folding behaviors depending on the exact system of peptide and lipid under consideration.
q-bio.BM:Analysis of data from an Affymetrix Latin Square spike-in experiment indicates that measured fluorescence intensities of features on an oligonucleotide microarray are related to spike-in RNA target concentrations via a hyperbolic response function, generally identified as a Langmuir adsorption isotherm. Furthermore the asymptotic signal at high spike-in concentrations is almost invariably lower for a mismatch feature than for its partner perfect match feature. We survey a number of theoretical adsorption models of hybridization at the microarray surface and find that in general they are unable to explain the differing saturation responses of perfect and mismatch features. On the other hand, we find that a simple and consistent explanation can be found in a model in which equilibrium hybridization followed by partial dissociation of duplexes during the post-hybridization washing phase.
q-bio.BM:A model for the processive movement of dynein is presented based on experimental observations available. In the model, the change from strong microtubule-binding to weak binding of dynein is determined naturally by the variation of the relative orientation between the two interacting surfaces of the stalk tip and the microtubule as the stalk rotates from the ADP.Vi-state orientation to the apo-state orientation. This means that the puzzling communication from the ATP binding site in the globular head to the MT-binding site in the tip of the stalk, which is prerequisite in the conventional model, is not required. Using the present model, the previous experimental results, such as (i) the step size of a dynein being an integer times of the period of the MT lattice, (ii) the dependence of the step size on load, i.e., the step size decreasing with the increase of load, and (iii) the stall force being proportional to [ATP] at low [ATP] and becoming saturated at high [ATP], are well explained.
q-bio.BM:We address the controversial hot question concerning the validity of the loose coupling versus the lever-arm theories in the actomyosin dynamics by re-interpreting and extending the phenomenological washboard potential model proposed by some of us in a previous paper. In this new model a Brownian motion harnessing thermal energy is assumed to co-exist with the deterministic swing of the lever-arm, to yield an excellent fit of the set of data obtained by some of us on the sliding of Myosin II heads on immobilized actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of the model is tested via different choices of parameters and potential profiles.
q-bio.BM:In this paper we investigate the role of native geometry on the kinetics of protein folding based on simple lattice models and Monte Carlo simulations. Results obtained within the scope of the Miyazawa-Jernigan indicate the existence of two dynamical folding regimes depending on the protein chain length. For chains larger than 80 amino acids the folding performance is sensitive to the native state's conformation. Smaller chains, with less than 80 amino acids, fold via two-state kinetics and exhibit a significant correlation between the contact order parameter and the logarithmic folding times. In particular, chains with N=48 amino acids were found to belong to two broad classes of folding, characterized by different cooperativity, depending on the contact order parameter. Preliminary results based on the G\={o} model show that the effect of long range contact interaction strength in the folding kinetics is largely dependent on the native state's geometry.
q-bio.BM:Monte Carlo simulations show that long-range interactions play a major role in determining the folding rates of 48-mer three-dimensional lattice polymers modelled by the Go potential. For three target structures with different native geometries we found a sharp increase in the folding time when the relative contribution of the long-range interactions to the native state's energy is decreased from ~50% towards zero. However, the dispersion of the simulated folding times depends strongly on the native geometry and Go polymers folding to one of the target structures exhibit folding times spanning three orders of magnitude. We have also found that, depending on the target geometry, a strong geometric coupling may exist between local and long-range contacts meaning that, when this coupling exists, the formation of long-range contacts is forced by the previous formation of local contacts. The absence of a strong geometric coupling leads to kinetics that are more sensitive to the interaction energy parameters; in this case the formation of local contacts is not sufficient to promote the establishment of long-range ones when these are strongly penalized energetically, leading to longer folding times.
q-bio.BM:A simplified interaction potential for protein folding studies at the atomic level is discussed and tested on a set of peptides with about 20 residues each. The test set contains both alpha-helical (Trp cage, Fs) and beta-sheet (GB1p, GB1m2, GB1m3, Betanova, LLM) peptides. The model, which is entirely sequence-based, is able to fold these different peptides for one and the same choice of model parameters. Furthermore, the melting behavior of the peptides is in good quantitative agreement with experimental data. Apparent folded populations obtained using different observables are compared, and are found to be very different for some of the peptides (e.g., Betanova). In other cases (in particular, GB1m2 and GB1m3), the different estimates agree reasonably well, indicating a more two-state-like melting behavior.
q-bio.BM:Single molecule FRET (fluorescence resonance energy transfer) is a powerful technique for detecting real-time conformational changes and molecular interactions during biological reactions. In this review, we examine different techniques of extending observation times via immobilization and illustrate how useful biological information can be obtained from single molecule FRET time trajectories with or without absolute distance information.
q-bio.BM:An ensemble of directed macromolecules on a lattice is considered, where the constituting molecules are chosen as a random sequence of N different types. The same type of molecules experiences a hard-core (exclusion) interaction. We study the robustness of the macromolecules with respect to breaking and substituting individual molecules, using a 1/N expansion. The properties depend strongly on the density of macromolecules. In particular, the macromolecules are robust against breaking and substituting at high densities.
q-bio.BM:The vibrational dynamics of a DNA molecule with counterions neutralizing the charged phosphate groups have been studied. With the help of elaborated model the conformational vibrations of the DNA double helix with alkaline metal ions have been described both qualitatively and quantitatively. For the complexes of DNA with counterions Li+, Na+, K+, Rb+ and Cs+ the normal modes have been found, and a mode characterized by the most notable ion displacements with respect to the DNA backbone has been determined. The frequency of counterion vibrations has been established to decrease as the ion mass increases. The results of theoretical calculation have been showed to be in good agreement with the experimental data of Raman spectroscopy.
q-bio.BM:To understand the mechanism of TATA-box conformational transformations we model structure mobility and find the types of conformational excitations of DNA macromolecule in heteronomous conformation. We have constructed the two-component model for describing DNA conformational transformation with simultaneous transitions in the furanos rings of the monomer link. Internal component describes the change of the base pair position in the double helix. External component describes the displacement of mass center of the monomer link. Nonlinearity of the system is accounted with a form of potential energy describing C3'-C2' and C2'-C3' sugars transitions in monomer link, and interrelation between monomer conformational transition and macromolecule deformation. The comparison of our results with experimental data allows to confirm that the localized conformational excitations may realise in DNA TATA-box. These excitations cause the deformation of the macromolecule fragment.
q-bio.BM:A simple approach is proposed to investigate the protein structure. Using a low complexity model, a simple pairwise interaction and the concept of global optimization, we are able to calculate ground states of proteins, which are in agreement with experimental data. All possible model structures of small proteins are available below a certain energy threshold. The exact lowenergy landscapes for the trp cage protein (1L2Y) is presented showing the connectivity of all states and energy barriers.
q-bio.BM:The effective DNA-DNA interaction force is calculated by computer simulations with explicit tetravalent counterions and monovalent salt. For overcharged DNA molecules, the interaction force shows a double-minimum structure. The positions and depths of these minima are regulated by the counterion density in the bulk. Using two-dimensional lattice sum and free energy perturbation theories, the coexisting phases for DNA bundles are calculated. A DNA-condensation and redissolution transition and a stable mesocrystal with an intermediate lattice constant for high counterion concentration are obtained.
q-bio.BM:By using a mixture model for the density distribution of the three pseudobond angles formed by $C_\alpha$ atoms of four consecutive residues, the local structural states are discretized as 17 conformational letters of a protein structural alphabet. This coarse-graining procedure converts a 3D structure to a 1D code sequence. A substitution matrix between these letters is constructed based on the structural alignments of the FSSP database.
q-bio.BM:An overview of theories related to vibrational energy relaxation (VER) in proteins is presented. VER of a selected mode in cytochrome c is studied using two theoretical approaches. One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes interacting through nonlinear coupling elements. Both methods result in estimates of the VER time (sub ps) for a CD stretching mode in the protein at room temperature. The theoretical predictions are in accord with the experimental data of Romesberg's group. A perspective on future directions for the detailed study of time scales and mechanisms for VER in proteins is presented.
q-bio.BM:Protein one-dimensional (1D) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional (3D) structure of a protein is encoded in the amino acid sequence. However, it has not been clear whether a given set of 1D structures contains sufficient information for recovering the underlying 3D structure. Here we show that the 3D structure of a protein can be recovered from a set of three types of 1D structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. Using simulated annealing molecular dynamics simulations, the structures satisfying the given native 1D structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. By selecting the structures best satisfying the restraints, all the proteins showed a coordinate RMS deviation of less than 4\AA{} from the native structure, and for most of them, the deviation was even less than 2\AA{}. The present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship.
q-bio.BM:The lack of specificity in microarray experiments due to non-specific hybridization raises a serious problem for the analysis of microarray data because the residual chemical background intensity is not related to the expression degree of the gene of interest. We analyzed the concentration dependence of the signal intensity of perfect match (PM) and mismatch (MM) probes in terms using a microscopic binding model using a combination of mean hybridization isotherms and single base related affinity terms. The signal intensities of the PM and MM probes and their difference are assessed with regard to their sensitivity, specificity and resolution for gene expression measures. The presented theory implies the refinement of existing algorithms of probe level analysis to correct microarray data for non-specific background intensities.
q-bio.BM:Residue-wise contact order (RWCO) is a new kind of one-dimensional protein structures which represents the extent of long-range contacts. We have recently shown that a set of three types of one-dimensional structures (secondary structure, contact number, and RWCO) contains sufficient information for reconstructing the three-dimensional structure of proteins. Currently, there exist prediction methods for secondary structure and contact number from amino acid sequence, but none exists for RWCO. Also, the properties of amino acids that affect RWCO is not clearly understood. Here, we present a linear regression-based method to predict RWCO from amino acid sequence, and analyze the regression parameters to identify the properties that correlates with the RWCO. The present method achieves the significant correlation of 0.59 between the native and predicted RWCOs on average. An unusual feature of the RWCO prediction is the remarkably large optimal half window size of 26 residues. The regression parameters for the central and near-central residues of the local sequence segment highly correlate with those of the contact number prediction, and hence with hydrophobicity.
q-bio.BM:In this paper the heat signaling in microtubules (MT) is investigated. It is argued that for the description of the heat signaling phenomena in MT, the hyperbolic heat transport (HHT) equation must be used. It is shown that HHT is the Klein-Gordon (K-G) equation. The general solution for the K-G equation for MT is obtained. For the undistorted signal propagation in MT the Heisenberg uncertainty principle is formulated and discussed.   Key words: Microtubules; Heat signaling; Klein-Gordon equation; Heisenberg principle.
q-bio.BM:In the last years, tens of thousands gene expression profiles for cells of several organisms have been monitored. Gene expression is a complex transcriptional process where mRNA molecules are translated into proteins, which control most of the cell functions. In this process, the correlation among genes is crucial to determine the specific functions of genes. Here, we propose a novel multi-dimensional stochastic approach to deal with the gene correlation phenomena. Interestingly, our stochastic framework suggests that the study of the gene correlation requires only one theoretical assumption -Markov property- and the experimental transition probability, which characterizes the gene correlation system. Finally, a gene expression experiment is proposed for future applications of the model.
q-bio.BM:The importance of understanding the mechanism of protein aggregation into insoluble amyloid fibrils relies not only on its medical consequences, but also on its more basic properties of self--organization. The discovery that a large number of uncorrelated proteins can form, under proper conditions, structurally similar fibrils has suggested that the underlying mechanism is a general feature of polypeptide chains. In the present work, we address the early events preceeding amyloid fibril formation in solutions of zinc--free human insulin incubated at low pH and high temperature. Aside from being a easy--to--handle model for protein fibrillation, subcutaneous aggregation of insulin after injection is a nuisance which affects patients with diabetes. Here, we show by time--lapse atomic force microscopy (AFM) that a steady-state distribution of protein oligomers with an exponential tail is reached within few minutes after heating. This metastable phase lasts for few hours until aggregation into fibrils suddenly occurs. A theoretical explanation of the oligomer pre--fibrillar distribution is given in terms of a simple coagulation--evaporation kinetic model, in which concentration plays the role of a critical parameter. Due to high resolution and sensitivity of AFM technique, the observation of a long-lasting latency time should be considered an actual feature of the aggregation process, and not simply ascribed to instrumental inefficency. These experimental facts, along with the kinetic model used, claim for a critical role of thermal concentration fluctuations in the process of fibril nucleation.
q-bio.BM:Identical objects, regularly assembled, form a helix, which is the principal motif of nucleic acids, proteins, and viral capsids.
q-bio.BM:The ambitious and ultimate research purpose in Systems Biology is the understanding and modelling of the cell's system. Although a vast number of models have been developed in order to extract biological knowledge from complex systems composed of basic elements as proteins, genes and chemical compounds, a need remains for improving our understanding of dynamical features of the systems (i.e., temporal-dependence).   In this article, we analyze the gene expression dynamics (i.e., how the genes expression fluctuates in time) by using a new constructive approach. This approach is based on only two fundamental ingredients: symmetry and the Markov property of dynamics. First, by using experimental data of human and yeast gene expression time series, we found a symmetry in short-time transition probability from time $t$ to time $t+1$. We call it self-similarity symmetry (i.e., surprisingly, the gene expression short-time fluctuations contain a repeating pattern of smaller and smaller parts that are like the whole, but different in size). Secondly, the Markov property of dynamics reflects that the short-time fluctuation governs the full-time behaviour of the system. Here, we succeed in reconstructing naturally the global behavior of the observed distribution of gene expression (i.e., scaling-law) and the local behaviour of the power-law tail of this distribution, by using only these two ingredients: symmetry and the Markov property of dynamics. This approach may represent a step forward toward an integrated image of the basic elements of the whole cell.
q-bio.BM:In this work, the dynamics of fluctuations in gene expression time series is investigated. By using collected data of gene expression from yeast and human organisms, we found that the fluctuations of gene expression level and its average value over time are strongly correlated and obey a scaling law. As this feature is found in yeast and human organisms, it suggests that probably this coupling is a common dynamical organizing property of all living systems. To understand these observations, we propose a stochastic model which can explain these collective fluctuations, and predict the scaling exponent. Interestingly, our results indicate that the observed scaling law emerges from the self-similarity symmetry embedded in gene expression fluctuations.
q-bio.BM:We study the mechanism underlying the attraction between nucleosomes, the fundamental packaging units of DNA inside the chromatin complex. We introduce a simple model of the nucleosome, the eight-tail colloid, consisting of a charged sphere with eight oppositely charged, flexible, grafted chains that represent the terminal histone tails. We demonstrate that our complexes are attracted via the formation of chain bridges and that this attraction can be tuned by changing the fraction of charged monomers on the tails. This suggests a physical mechanism of chromatin compaction where the degree of DNA condensation can be controlled via biochemical means, namely the acetylation and deacetylation of lysines in the histone tails.
q-bio.BM:The effects of monovalent (Na+, K+) and divalent (Mg2+, Ca2+, Mn2+) ions on the interaction between DNA and histone are studied using the molecular combing technique. Lamda-DNA molecules and DNA-histone complexes incubated with metal cations (Na+, K+, Mg2+, Ca2+, Mn2+) are stretched on hydrophobic surfaces, and directly observed by fluorescence microscopy. The results indicate that when these cations are added into the DNA solution, the fluorescence intensities of the stained DNA are reduced differently. The monovalent cations (Na+, K+) inhibit binding of histone to DNA. The divalent cations (Mg2+, Ca2+, Mn2+) enhance significantly the binding of histone to DNA and the binding of the DNA-histone complex to the hydrophobic surface. Mn2+ also induces condensation and aggregation of the DNA-histone complex.
q-bio.BM:PDZ (Post-synaptic density-95/discs large/zonula occludens-1) domains are relatively small (80 to 120 residues) protein binding modules central in the organization of receptor clusters and in the association of cellular proteins. Their main function is to bind C-terminals of selected proteins that are recognized through specific amino-acids in their carboxyl end. Binding is associated with a deformation of the PDZ native structure and is responsible for dynamical changes in regions not in direct contact with the target. We investigate how this deformation is related to the harmonic dynamics of the PDZ structure and show that one low-frequency collective normal mode, characterized by the concerted movements of different secondary structures, is involved in the binding process. Our results suggest that even minimal structural changes are responsible of communication between distant regions of the protein, in agreement with recent Nuclear Magnetic Resonance (NMR) experiments. Thus PDZ domains are a very clear example of how collective normal modes are able to characterize the relation between function and dynamics of proteins, and to provide indications on the precursors of binding/unbonding events.
q-bio.BM:We introduce a new measure of antigenic distance between influenza A vaccine and circulating strains. The measure correlates well with efficacies of the H3N2 influenza A component of the annual vaccine between 1971 and 2004, as do results of a theory of the immune response to influenza following vaccination. This new measure of antigenic distance is correlated with vaccine efficacy to a greater degree than are current state-of-the-art phylogenetic sequence analyzes or ferret antisera inhibition assays. We suggest that this new measure of antigenic distance be used in the design of the annual influenza vaccine and in the interpretation of vaccine efficacy monitoring.
q-bio.BM:Prediction of one-dimensional protein structures such as secondary structures and contact numbers is useful for the three-dimensional structure prediction and important for the understanding of sequence-structure relationship. Here we present a new machine-learning method, critical random networks (CRNs), for predicting one-dimensional structures, and apply it, with position-specific scoring matrices, to the prediction of secondary structures (SS), contact numbers (CN), and residue-wise contact orders (RWCO). The present method achieves, on average, $Q_3$ accuracy of 77.8% for SS, correlation coefficients of 0.726 and 0.601 for CN and RWCO, respectively. The accuracy of the SS prediction is comparable to other state-of-the-art methods, and that of the CN prediction is a significant improvement over previous methods. We give a detailed formulation of critical random networks-based prediction scheme, and examine the context-dependence of prediction accuracies. In order to study the nonlinear and multi-body effects, we compare the CRNs-based method with a purely linear method based on position-specific scoring matrices. Although not superior to the CRNs-based method, the surprisingly good accuracy achieved by the linear method highlights the difficulty in extracting structural features of higher order from amino acid sequence beyond that provided by the position-specific scoring matrices.
q-bio.BM:We describe the results obtained from an improved model for protein folding. We find that a good agreement with the native structure of a 46 residue long, five-letter protein segment is obtained by carefully tuning the parameters of the self-avoiding energy. In particular we find an improved free-energy profile. We also compare the efficiency of the multidimensional replica exchange method with the widely used parallel tempering.
q-bio.BM:We use single-particle tracking to study the elastic properties of single microtubules grafted to a substrate. Thermal fluctuations of the free microtubule's end are recorded, in order to measure position distribution functions from which we calculate the persistence length of microtubules with contour lengths between 2.6 and 48 micrometers. We find the persistence length to vary by more than a factor of 20 over the total range of contour lengths. Our results support the hypothesis that shearing between protofilaments contributes significantly to the mechanics of microtubules.
q-bio.BM:Optimal structure of proteins is described by linear stochastic differential equation with mean decrease of free energy and volatility. Structure determining strategy is given by a twin of stochastic variables for which empirical conditions are not postulated. Optimal structure determination will be deformed to be adoptive to trading strategy employing martingale property where stochastic integral w.r.t. analytical solution of stochastic differential equation will be employed.
q-bio.BM:One of the main problems of drug design is that of optimizing the drug--target interaction. In the case in which the target is a viral protein displaying a high mutation rate, a second problem arises, namely the eventual development of resistance. We wish to suggest a scheme for the design of non--conventional drugs which do not face any of these problems and apply it to the case of HIV--1 protease. It is based on the knowledge that the folding of single--domain proteins, like e.g. each of the monomers forming the HIV--1--PR homodimer, is controlled by local elementary structures (LES), stabilized by local contacts among hydrophobic, strongly interacting and highly conserved amino acids which play a central role in the folding process. Because LES have evolved over myriads of generations to recognize and strongly interact with each other so as to make the protein fold fast as well as to avoid aggregation with other proteins, highly specific (and thus little toxic) as well as effective folding--inhibitor drugs suggest themselves: short peptides (or eventually their mimetic molecules), displaying the same amino acid sequence of that of LES (p--LES). Aside from being specific and efficient, these inhibitors are expected not to induce resistance: in fact, mutations which successfully avoid their action imply the destabilization of one or more LES and thus should lead to protein denaturation. Making use of Monte Carlo simulations within the framework of a simple although not oversimplified model, which is able to reproduce the main thermodynamic as well as dynamic properties of monoglobular proteins, we first identify the LES of the HIV--1--PR and then show that the corresponding p--LES peptides act as effective inhibitors of the folding of the protease which do not create resistance.
q-bio.BM:Protein structure is generally conceptualized as the global arrangement or of smaller, local motifs of helices, sheets, and loops. These regular, recurring secondary structural elements have well-understood and standardized definitions in terms of amino acid backbone geometry and the manner in which hydrogen bonding requirements are satisfied. Recently, "tube" models have been proposed to explain protein secondary structure in terms of the geometrically optimal packing of a featureless cylinder. However, atomically detailed simulations demonstrate that such packing considerations alone are insufficient for defining secondary structure; both excluded volume and hydrogen bonding must be explicitly modeled for helix formation. These results have fundamental implications for the construction and interpretation of realistic and meaningful biomacromolecular models.
q-bio.BM:Structure predictions of helical membrane proteins have been designed to take advantage of the structural autonomy of secondary structure elements, as postulated by the two-stage model of Engelman and Popot. In this context, we investigate structure calculation strategies for two membrane proteins with different functions, sizes, aminoacid compositions, and topologies: the glycophorin A homodimer (a paradigm for close inter-helical packing in membrane proteins) and aquaporin (a channel protein). Our structure calculations are based on two alternative folding schemes: a one-step simulated annealing from an extended chain conformation, and a two-step procedure inspired by the grid-search methods traditionally used in membrane protein predictions. In this framework, we investigate rationales for the utilization of sparse NMR data such as distance-based restraints and residual dipolar couplings in structure calculations of helical membrane proteins.
q-bio.BM:Proteins created by combinatorial methods in vitro are an important source of information for understanding sequence-structure-function relationships. Alignments of folded proteins from combinatorial libraries can be analyzed using methods developed for naturally occurring proteins, but this neglects the information contained in the unfolded sequences of the library. We introduce two algorithms, logistic regression and excess information analysis, that use both the folded and unfolded sequences and compare them against contingency table and statistical coupling analysis, which only use the former. The test set for this benchmark study is a library of fictitious proteins that fold according to a hypothetical energy model. Of the four methods studied, only logistic regression is able to correctly recapitulate the energy model from the sequence alignment. The other algorithms predict spurious interactions between alignment positions with strong but individual influences on protein stability. When present in the same protein, stabilizing amino acids tend to lower the energy below the threshold needed for folding. As a result, their frequencies in the alignment can be correlated even if the positions do not interact. We believe any algorithm that neglects the nonlinear relationship between folding and energy is susceptible to this error.
q-bio.BM:In order to extend the results obtained with minimal lattice models to more realistic systems, we study a model where proteins are described as a chain of 20 kinds of structureless amino acids moving in a continuum space and interacting through a contact potential controlled by a 20x20 quenched random matrix. The goal of the present work is to design and characterize amino acid sequences folding to the SH3 conformation, a 60-residues recognition domain common to many regulatory proteins. We show that a number of sequences can fold, starting from a random conformation, to within a distance root mean square deviation (dRMSD) of 2.6A from the native state. Good folders are those sequences displaying in the native conformation an energy lower than a sequence--independent threshold energy.
q-bio.BM:We recently introduced a physical model [Hoang et al., P. Natl. Acad. Sci. USA (2004), Banavar et al., Phys. Rev. E (2004)] for proteins which incorporates, in an approximate manner, several key features such as the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and the role played by hydrophobicity. Within this framework, marginally compact conformations resembling the native state folds of proteins emerge as broad competing minima in the free energy landscape even for a homopolymer. Here we show how the introduction of sequence heterogeneity using a simple scheme of just two types of amino acids, hydrophobic (H) and polar (P), and sequence design allows a selected putative native fold to become the free energy minimum at low temperature. The folding transition exhibits thermodynamic cooperativity, if one neglects the degeneracy between two different low energy conformations sharing the same fold topology.
q-bio.BM:In eukaryote nucleosome, DNA wraps around a histone octamer in a left-handed way. We study the process of chirality formation of nucleosome with Brownian dynamics simulation. We model the histone octamer with a quantitatively adjustable chirality: left-handed, right-handed or non-chiral, and simulate the dynamical wrapping process of a DNA molecule on it. We find that the chirality of a nucleosome formed is strongly dependent on that of the histone octamer, and different chiralities of the histone octamer induce its different rotation directions in the wrapping process of DNA. In addition, a very weak chirality of the histone octamer is quite enough for sustaining the correct chirality of the nucleosome formed. We also show that the chirality of a nucleosome may be broken at elevated temperature.
q-bio.BM:Trypsin and chymotrypsin are both serine proteases with high sequence and structural similarities, but with different substrate specificity. Previous experiments have demonstrated the critical role of the two loops outside the binding pocket in controlling the specificity of the two enzymes. To understand the mechanism of such a control of specificity by distant loops, we have used the Gaussian Network Model to study the dynamic properties of trypsin and chymotrypsin and the roles played by the two loops. A clustering method was introduced to analyze the correlated motions of residues. We have found that trypsin and chymotrypsin have distinct dynamic signatures in the two loop regions which are in turn highly correlated with motions of certain residues in the binding pockets. Interestingly, replacing the two loops of trypsin with those of chymotrypsin changes the motion style of trypsin to chymotrypsin-like, whereas the same experimental replacement was shown necessary to make trypsin have chymotrypsin's enzyme specificity and activity. These results suggest that the cooperative motions of the two loops and the substrate-binding sites contribute to the activity and substrate specificity of trypsin and chymotrypsin.
q-bio.BM:The emergence and spreading of chirality on the early Earth is considered by studying a set of reaction-diffusion equations based on a polymerization model. It is found that effective mixing of the early oceans is necessary to reach the present homochiral state. The possibility of introducing mass extinctions and modifying the emergence rate of life is discussed.
q-bio.BM:The differences between uni-directional and bi-directional polymerization are considered. The uni-directional case is discussed in the framework of the RNA world. Similar to earlier models of this type, where polymerization was assumed to proceed in a bi-directional fashion (presumed to be relevant to peptide nucleic acids), left-handed and right-handed monomers are produced via an autocatalysis from an achiral substrate. The details of the bifurcation from a racemic solution to a homochiral state of either handedness is shown to be remarkably independent of whether the polymerization in uni-directional or bi-directional. Slightly larger differences are seen when dissociation is allowed and the dissociation fragments are being recycled into the achiral substrate.
q-bio.BM:A variety of viruses tightly pack their genetic material into protein capsids that are barely large enough to enclose the genome. In particular, in bacteriophages, forces as high as 60 pN are encountered during packaging and ejection, produced by DNA bending elasticity and self-interactions. The high forces are believed to be important for the ejection process, though the extent of their involvement is not yet clear. As a result, there is a need for quantitative models and experiments that reveal the nature of the forces relevant to DNA ejection. Here we report measurements of the ejection forces for two different mutants of bacteriophage lambda, lambda b221cI26 and lambda cI60, which differ in genome length by ~30%. As expected for a force-driven ejection mechanism, the osmotic pressure at which DNA release is completely inhibited varies with the genome length: we find inhibition pressures of 15 atm and 25 atm, respectively, values that are in agreement with our theoretical calculations.
q-bio.BM:The correlations of primary and secondary structures were analyzed using proteins with known structure from Protein Data Bank. The correlation values of amino acid type and the eight secondary structure types at distant position were calculated for distances between -25 and 25. Shapes of the diagrams indicate that amino acids polarity and capability for hydrogen bonding have influence on the secondary structure at some distances. Clear preference of most of the amino acids towards certain secondary structure type classifies amino acids into four groups: alpha-helix admirers, strand admirers, turn and bend admirers and the others. Group four consists of His and Cis, the amino acids that do not show clear preference for any secondary structure. Amino acids from a group have similar physicochemical properties, and the same structural characteristics. The results suggest that amino acid preference for secondary structure type is based on the structural characteristics at Cb and Cg atoms of amino acid. alpha-helix admirers do not have polar heteroatoms on Cb and Cg atoms, nor branching or aromatic group on Cb atom. Amino acids that have aromatic groups or branching on Cb atom are strand admirers. Turn and bend admirers have polar heteroatom on Cb or Cg atoms or do not have Cb atom at all. Our results indicate that polarity and capability for hydrogen bonding have influence on the secondary structure at some distance, and that amino acid preference for secondary structure is caused by structural properties at Cb or Cg atoms.
q-bio.BM:The functionality of proteins is related to their structure in the native state. Protein structures are made up of emergent building blocks of helices and almost planar sheets. A simple coarse-grained geometrical model of a flexible tube barely subject to compaction provides a unified framework for understanding the common character of globular proteins.We argue that a recent critique of the tube idea is not well founded.
q-bio.BM:To gain a deeper insight into cellular processes such as transcription and translation, one needs to uncover the mechanisms controlling the configurational changes of nucleic acids. As a step toward this aim, we present here a novel mesoscopic-level computational model that provides a new window into nucleic acid dynamics. We model a single-stranded nucleic as a polymer chain whose monomers are the nucleosides. Each monomer comprises a bead representing the sugar molecule and a pin representing the base. The bead-pin complex can rotate about the backbone of the chain. We consider pairwise stacking and hydrogen-bonding interactions. We use a modified Monte Carlo dynamics that splits the dynamics into translational bead motion and rotational pin motion. By performing a number of tests we first show that our model is physically sound. We then focus on the study of a the kinetics of a DNA hairpin--a single-stranded molecule comprising two complementary segments joined by a non-complementary loop--studied experimentally. We find that results from our simulations agree with experimental observations, demonstrating that our model is a suitable tool for the investigation of the hybridization of single strands.
q-bio.BM:We investigate the folding behavior of protein sequences by numerically studying all sequences with maximally compact lattice model through exhaustive enumeration. We get the prion-like behavior of protein folding. Individual proteins remaining stable in the isolated native state may change their conformations when they aggregate. We observe the folding properties as the interfacial interaction strength changes, and find that the strength must be strong enough before the propagation of the most stable structures happens.
q-bio.BM:We review some of our recent results obtained within the scope of simple lattice models and Monte Carlo simulations that illustrate the role of native geometry in the folding kinetics of two state folders.
q-bio.BM:Ion channels are proteins with a hole down the middle embedded in cell membranes. Membranes form insulating structures and the channels through them allow and control the movement of charged particles, spherical ions, mostly Na+, K+, Ca++, and Cl-. Membranes contain hundreds or thousands of types of channels, fluctuating between open conducting, and closed insulating states. Channels control an enormous range of biological function by opening and closing in response to specific stimuli using mechanisms that are not yet understood in physical language. Open channels conduct current of charged particles following laws of Brownian movement of charged spheres rather like the laws of electrodiffusion of quasi-particles in semiconductors. Open channels select between similar ions using a combination of electrostatic and 'crowded charge' (Lennard-Jones) forces. The specific location of atoms and the exact atomic structure of the channel protein seems much less important than certain properties of the structure, namely the volume accessible to ions and the effective density of fixed and polarization charge. There is no sign of other chemical effects like delocalization of electron orbitals between ions and the channel protein. Channels play a role in biology as important as transistors in computers, and they use rather similar physics to perform part of that role. Understanding their fluctuations awaits physical insight into the source of the variance and mathematical analysis of the coupling of the fluctuations to the other components and forces of the system.
q-bio.BM:We propose a criterion for optimal parameter selection in coarse-grained models of proteins, and develop a refined elastic network model (ENM) of bovine trypsinogen. The unimodal density-of-states distribution of the trypsinogen ENM disagrees with the bimodal distribution obtained from an all-atom model; however, the bimodal distribution is recovered by strengthening interactions between atoms that are backbone neighbors. We use the backbone-enhanced model to analyze allosteric mechanisms of trypsinogen, and find relatively strong communication between the regulatory and active sites.
q-bio.BM:The protonation of N2 bound to the active center of nitrogenase has been investigated using state-of-the-art DFT calculations. Dinitrogen in the bridging mode is activated by forming two bonds to Fe sites, which results in a reduction of the energy for the first hydrogen transfer by 123 kJ/mol. The axial binding mode with open sulfur bridge is less reactive by 30 kJ/mol and the energetic ordering of the axial and bridged binding mode is reversed in favor of the bridging dinitrogen during the first protonation. Protonation of the central ligand is thermodynamically favorable but kinetically hindered. If the central ligand is protonated, the proton is transferred to dinitrogen following the second protonation. Protonation of dinitrogen at the Mo site does not lead to low-energy intermediates.
q-bio.BM:The ejection of DNA from a bacterial virus (``phage'') into its host cell is a biologically important example of the translocation of a macromolecular chain along its length through a membrane. The simplest mechanism for this motion is diffusion, but in the case of phage ejection a significant driving force derives from the high degree of stress to which the DNA is subjected in the viral capsid. The translocation is further sped up by the ratcheting and entropic forces associated with proteins that bind to the viral DNA in the host cell cytoplasm. We formulate a generalized diffusion equation that includes these various pushing and pulling effects and make estimates of the corresponding speed-ups in the overall translocation process. Stress in the capsid is the dominant factor throughout early ejection, with the pull due to binding particles taking over at later stages. Confinement effects are also investigated, in the case where the phage injects its DNA into a volume comparable to the capsid size. Our results suggest a series of in vitro experiments involving the ejection of DNA into vesicles filled with varying amounts of binding proteins from phage whose state of stress is controlled by ambient salt conditions or by tuning genome length.
q-bio.BM:We present a base-pairing model of oligonuleotide duplex formation and show in detail its equivalence to the Nearest-Neighbour dimer methods from fits to free energy of duplex formation data for short DNA-DNA and DNA-RNA hybrids containing only Watson Crick pairs. In this approach the connection between rank-deficient polymer and rank-determinant oligonucleotide parameter, sets for DNA duplexes is transparent. The method is generalised to include RNA/DNA hybrids where the rank-deficient model with 11 dimer parameters in fact provides marginally improved predictions relative to the standard method with 16 independent dimer parameters ($\Delta G$ mean errors of 4.5 and 5.4 % respectively).
q-bio.BM:A formalism is developed which allows to determine the locations of all local symmetry axes of three-dimensional particles with overall icosahedral symmetry. It relies on the fact that the root system of the non-crystallographic Coxeter group H_3 encodes the locations of the planes of reflection that generate the discrete rotational symmetries of the particles. Via an appropriate extension of the root system, new planes of reflection are introduced which determine local axes of rotational symmetry. An easy-to-implement formalism is derived that allows to compute the surface structure of any three-dimensional icosahedral particle with local symmetries. It can be used also for particles with overall octahedral and tetrahedral symmetry in conjunction with the root systems of the corresponding reflection groups.   Applications to viruses are discussed explicitly. It is shown that the concept of quasi-equivalence in Caspar-Klug Theory corresponds to the special case of local six-fold symmetry axes contained in the theory developed here, and the corresponding geometries can hence be obtained with this formalism based on the root system of H_3.   Moreover, as a by-product, the theory answers the long-standing open question why only certain types of capsomeres, i.e. clusters of protein subunits, are observed in the surface structures of viruses. Since the types of the capsomeres are determined by the orders of the local symmetry axes on which they are located, the possible types of capsomeres are restricted by the spectrum of local symmetry axes allowed by the theory. Based on this we determine the spectrum of all capsomere types that may occur in viral capsids and give explicit examples for the lower-order cases.
q-bio.BM:The tethered-particle method is a single-molecule technique that has been used to explore the dynamics of a variety of macromolecules of biological interest. We give a theoretical analysis of the particle motions in such experiments. Our analysis reveals that the proximity of the tethered bead to a nearby surface (the microscope slide) gives rise to a volume-exclusion effect, resulting in an entropic force on the molecule. This force stretches the molecule, changing its statistical properties. In particular, the proximity of bead and surface brings about intriguing scaling relations between key observables (statistical moments of the bead) and parameters such as the bead size and contour length of the molecule. We present both approximate analytic solutions and numerical results for these effects in both flexible and semiflexible tethers. Finally, our results give a precise, experimentally-testable prediction for the probability distribution of the distance between the polymer attachment point and the center of the mobile bead.
q-bio.BM:The distribution of inequivalent geometries occurring during self-assembly of the major capsid protein in thermodynamic equilibrium is determined based on a master equation approach. These results are implemented to characterize the assembly of SV40 virus and to obtain information on the putative pathways controlling the progressive build-up of the SV40 capsid. The experimental testability of the predictions is assessed and an analysis of the geometries of the assembly intermediates on the dominant pathways is used to identify targets for antiviral drug design.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Assembly models are developed for viral capsids built from protein building blocks that can assume different local bonding structures in the capsid. This situation occurs, for example, for viruses in the family of Papovaviridae, which are linked to cancer and are hence of particular interest for the health sector. More specifically, the viral capsids of the (pseudo-) T=7 particles in this family consist of pentamers that exhibit two different types of bonding structures. While this scenario cannot be described mathematically in terms of Caspar-Klug Theory (Caspar and Klug 1962), it can be modelled via tiling theory (Twarock 2004). The latter is used to encode the local bonding environment of the building blocks in a combinatorial structure, called the assembly tree, which is a basic ingredient in the derivation of assembly models for Papovaviridae along the lines of the equilibrium approach of Zlotnick (Zlotnick 1994). A phase space formalism is introduced to characterize the changes in the assembly pathways and intermediates triggered by the variations in the association energies characterizing the bonds between the building blocks in the capsid. Furthermore, the assembly pathways and concentrations of the statistically dominant assembly intermediates are determined. The example of Simian Virus 40 is discussed in detail.
q-bio.BM:We calculate the equation of state of DNA under tension for the case that the DNA features loops. Such loops occur transiently during DNA condensation in the presence of multivalent ions or sliding cationic protein linkers. The force-extension relation of such looped DNA modelled as a wormlike chain is calculated via path integration in the semiclassical limit. This allows us to determine rigorously the high stretching asymptotics. Notably the functional form of the force-extension curve resembles that of straight DNA, yet with a strongly renormalized apparent persistence length. That means that the experimentally extracted single molecule elasticity does not necessarily reflect the bare DNA stiffness only, but can also contain additional contributions that depend on the overall chain conformation and length.
q-bio.BM:A generalized computational method for folding proteins with a fully transferable potential and geometrically realistic all-atom model is presented and tested on seven different helix bundle proteins. The protocol, which includes graph-theoretical analysis of the ensemble of resulting folded conformations, was systematically applied and consistently produced structure predictions of approximately 3 Angstroms without any knowledge of the native state. To measure and understand the significance of the results, extensive control simulations were conducted. Graph theoretic analysis provides a means for systematically identifying the native fold and provides physical insight, conceptually linking the results to modern theoretical views of protein folding. In addition to presenting a method for prediction of structure and folding mechanism, our model suggests that a accurate all-atom amino acid representation coupled with a physically reasonable atomic interaction potential (that does not require optimization to the test set) and hydrogen bonding are essential features for a realistic protein model.
q-bio.BM:Being the HIV-1 Protease (HIV-1-PR) an essential enzyme in the viral life cycle, its inhibition can control AIDS. The folding of single domain proteins, like each of the monomers forming the HIV-1-PR homodimer, is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved, as a rule hydrophobic, amino acids). These LES have evolved over myriad of generations to recognize and strongly attract each other, so as to make the protein fold fast and be stable in its native conformation. Consequently, peptides displaying a sequence identical to those segments of the monomers associated with LES are expected to act as competitive inhibitors and thus destabilize the native structure of the enzyme. These inhibitors are unlikely to lead to escape mutants as they bind to the protease monomers through highly conserved amino acids which play an essential role in the folding process. The properties of one of the most promising inhibitors of the folding of the HIV-1-PR monomers found among these peptides is demonstrated with the help of spectrophotometric assays and CD spectroscopy.
q-bio.BM:It is shown that a small subset of modes which are likely to be involved in protein functional motions of large amplitude can be determined by retaining the most robust normal modes obtained using different protein models. This result should prove helpful in the context of several applications proposed recently, like for solving difficult molecular replacement problems or for fitting atomic structures into low-resolution electron density maps. Moreover, it may also pave the way for the development of methods allowing to predict such motions accurately.
q-bio.BM:A new formalism for calculation of the partition function of single stranded nucleic acids is presented. Secondary structures and the topology of structure elements are the level of resolution that is used. The folding model deals with matches, mismatches, symmetric and asymmetric interior loops, stacked pairs in loop and dangling end regions, multi-branched loops, bulges and single base stacking that might exist at duplex ends or at the ends of helices. Calculations on short and long sequences show, that for short oligonucleotides, a duplex formation often displays a two-state transition. However, for longer oligonucleotides, the thermodynamic properties of the single self-folding transition affects the transition nature of the duplex formation, resulting in a population of intermediate hairpin species in the solution. The role of intermediate hairpin species is analyzed in the case when a short oligonucleotides (molecular beacons) have to reliably identify and hybridize to accessible nucleotides within their targeted mRNA sequences. It is shown that the enhanced specificity of the molecular beacons is a result of their constrained conformational flexibility and the all-or-none mechanism of their hybridization to the target sequence.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Viral capsids are usually spherical, and for a significant number of viruses exhibit overall icosahedral symmetry. The corresponding surface lattices, that encode the locations of the capsid proteins and intersubunit bonds, can be modelled by Viral Tiling Theory.   It has been shown in vitro that under a variation of the experimental boundary conditions, such as the pH value and salt concentration, tubular particles may appear instead of, or in addition to, spherical ones. In order to develop models that describe the simultaneous assembly of both spherical and tubular variants, and hence study the possibility of triggering tubular malformations as a means of interference with the replication mechanism, Viral Tiling Theory has to be extended to include tubular lattices with end caps. This is done here for the case of Papovaviridae, which play a distinguished role from the viral structural point of view as they correspond to all pentamer lattices, i.e. lattices formed from clusters of five protein subunits throughout. These results pave the way for a generalisation of recently developed assembly models.
q-bio.BM:Experimental investigations of the biosynthesis of a number of proteins have pointed out that part of the native structure can be acquired already during translation. We carried out a comprehensive statistical analysis of some average structural properties of proteins that have been put forward as possible signatures of this progressive buildup process. Contrary to a widespread belief, it is found that there is no major propensity of the amino acids to form contacts with residues that are closer to the N terminus. Moreover, it is found that the C terminus is significantly more compact and locally-organized than the N one. Also this bias, though, is unlikely to be related to vectorial effects, since it correlates with subtle differences in the primary sequence. These findings indicate that even if proteins aquire their structure vectorially no signature of this seems to be detectable in their average structural properties.
q-bio.BM:The aim of this article is to present a developed method that decomposes the autofluorescence spectrum into the spectra of naturally occurring biochemical components of biotissue. It requires knowledge of detailed spectrum behaviour of different endogenous fluorophores. We have studied the main bio-markers in human tissue and proposed a simple modelling algorithm for their spectra shapes. The empirical method was tested theoretically by quantum-mechanical calculations of the spectra in the unharmonic Morse potential approach.
q-bio.BM:Experimental evidence suggests that the folding and aggregation of the amyloid $\beta$-protein (A$\beta$) into oligomers is a key pathogenetic event in Alzheimer's disease (AD). Inhibiting the pathologic folding and oligomerization of A$\beta$ could be effective in the prevention and treatment of AD. Here, using all-atom molecular dynamics simulations in explicit solvent, we probe the initial stages of folding of a decapeptide segment of A$\beta$, A$\beta_{21-30}$, shown experimentally to nucleate the folding process. In addition, we examine the folding of a homologous decapeptide containing an amino acid substitution linked to hereditary cerebral hemorrhage with amyloidosis--Dutch type, [Gln22]A$\beta_{21-30}$. We find that: (i) when the decapeptide is in water, hydrophobic interactions and transient salt bridges between Lys28 and either Glu22 or Asp23 are important in the formation of a loop in the Val24--Lys28 region of the wild type decapeptide; (ii) in the presence of salt ions, salt bridges play a more prominent role in the stabilization of the loop; (iii) in water with a reduced density, the decapeptide forms a helix, indicating the sensitivity of folding to different aqueous environments; (iv) the ``Dutch'' peptide in water, in contrast to the wild type peptide, fails to form a long-lived Val24--Lys28 loop, suggesting that loop stability is a critical factor in determining whether A$\beta$ folds into pathologic structures. Our results are relevant to understand the mechanism of A$\beta$ peptide folding in different environments, such as intra- and extracellular milieus or cell membranes, and how amino acid substitutions linked to familial forms of amyloidosis cause disease.
q-bio.BM:This paper was withdrawn by the authors.
q-bio.BM:We develop a class of models with which we simulate the assembly of particles into T1 capsid-like objects using Newtonian dynamics. By simulating assembly for many different values of system parameters, we vary the forces that drive assembly. For some ranges of parameters, assembly is facile, while for others, assembly is dynamically frustrated by kinetic traps corresponding to malformed or incompletely formed capsids. Our simulations sample many independent trajectories at various capsomer concentrations, allowing for statistically meaningful conclusions. Depending on subunit (i.e., capsomer) geometries, successful assembly proceeds by several mechanisms involving binding of intermediates of various sizes. We discuss the relationship between these mechanisms and experimental evaluations of capsid assembly processes.
q-bio.BM:In this paper the heat transport in microtubules (MT) is investigated. When the dimension of the structure is of the order of the de Broglie wave length the transport phenomena must be analyzed within quantum mechanics. In this paper we developed the Dirac type thermal equation for MT .The solution of the equation-the temperature fields for electrons can be wave type or diffusion type depending on the dynamics of the scattering. Key words: Microtubules ultrashort laser pulses, Dirac thermal equation, temperature fields.
q-bio.BM:Mouse prion protein PrP106-126 is a peptide corresponding to the residues 107-127 of human prion protein. It has been shown that PrP106-126 can reproduce the main neuropathological features of prionrelated transmissible spongiform encephalopathies and can form amyloid-like fibrils in vitro. The conformational characteristics of PrP106-126 fibril have been investigated by electron microscopy, CD spectroscopy, NMR and molecular dynamics simulations. Recent researches have found out that PrP106-126 in water assumes a stable structure consisting of two parallel beta-sheets that are tightly packed against each other. In this work we perform molecular dynamics simulation to reveal the elongation mechanism of PrP106-126 fibril. Influenced by the edge strands of the fibril which already adopt beta-sheets conformation, single PrP106-126 peptide forms beta-structure and becomes a new element of the fibril. Under acidic condition, single PrP106-126 peptide adopts a much larger variety of conformations than it does under neural condition, which makes a peptide easier to be influenced by the edge strands of the fibril. However, acidic condition dose not largely affect the stability of PrP106-126 peptide fibril. Thus, the speed of fibril elongation can be dramatically increased by lowering the pH value of the solution. The pH value was adjusted by either changing the protonation state of the residues or adding hydronium ions (acidic solution) or hydroxyl ions (alkaline solution). The differences between these two approaches are analyzed here.
q-bio.BM:The thermodynamics of the small SH3 protein domain is studied by means of a simplified model where each bead-like amino acid interacts with the others through a contact potential controlled by a 20x20 random matrix. Good folding sequences, characterized by a low native energy, display three main thermodynamical phases, namely a coil-like phase, an unfolded globule and a folded phase (plus other two phases, namely frozen and random coil, populated only at extremes temperatures). Interestingly, the unfolded globule has some regions already structured. Poorly designed sequences, on the other hand, display a wide transition from the random coil to a frozen state. The comparison with the analytic theory of heteropolymers is discussed.
q-bio.BM:In a seminal paper Caspar and Klug established a theory that provides a family of polyhedra as blueprints for the structural organisation of viral capsids. In particular, they encode the locations of the proteins in the shells that encapsulate, and hence provide protection for, the viral genome. Despite of its huge success and numerous applications in virology experimental results have provided evidence for the fact that the theory is too restrictive to describe all known viruses. Especially, the family of Papovaviridae, which contains cancer-causing viruses, falls out of the scope of this theory.   In a recent paper we have shown that certain members of the family of Papovaviridae can be described via tilings. In this paper, we develop a comprehensive mathematical framework for the derivation of all surface structures of viral particles in this family. We show that this formalism fixes the structure and relative sizes of all particles collectively so that there exists only one scaling factor that relates the sizes of all particles with their biological counterparts.   The series of polyhedra derived here complements the Caspar-Klug family of polyhedra. It is the first mathematical result that provides a common organisational principle for different types of viral particles in the family of Papovaviridae and paves the way for an understanding of Papovaviridae polymorphism. Moreover, it provides crucial input for the construction of assembly models.
q-bio.BM:The amino acid sequences of proteins provide rich information for inferring distant phylogenetic relationships and for predicting protein functions. Estimating the rate matrix of residue substitutions from amino acid sequences is also important because the rate matrix can be used to develop scoring matrices for sequence alignment. Here we use a continuous time Markov process to model the substitution rates of residues and develop a Bayesian Markov chain Monte Carlo method for rate estimation. We validate our method using simulated artificial protein sequences. Because different local regions such as binding surfaces and the protein interior core experience different selection pressures due to functional or stability constraints, we use our method to estimate the substitution rates of local regions. Our results show that the substitution rates are very different for residues in the buried core and residues on the solvent exposed surfaces. In addition, the rest of the proteins on the binding surfaces also have very different substitution rates from residues. Based on these findings, we further develop a method for protein function prediction by surface matching using scoring matrices derived from estimated substitution rates for residues located on the binding surfaces. We show with examples that our method is effective in identifying functionally related proteins that have overall low sequence identity, a task known to be very challenging.
q-bio.BM:This chapter discusses geometric models of biomolecules and geometric constructs, including the union of ball model, the weigthed Voronoi diagram, the weighted Delaunay triangulation, and the alpha shapes. These geometric constructs enable fast and analytical computaton of shapes of biomoleculres (including features such as voids and pockets) and metric properties (such as area and volume). The algorithms of Delaunay triangulation, computation of voids and pockets, as well volume/area computation are also described. In addition, applications in packing analysis of protein structures and protein function prediction are also discussed.
q-bio.BM:This chapter discusses theoretical framework and methods for developing knowledge-based potential functions essential for protein structure prediction, protein-protein interaction, and protein sequence design. We discuss in some details about the Miyazawa-Jernigan contact statistical potential, distance-dependent statistical potentials, as well as geometric statistical potentials. We also describe a geometric model for developing both linear and non-linear potential functions by optimization. Applications of knowledge-based potential functions in protein-decoy discrimination, in protein-protein interactions, and in protein design are then described. Several issues of knowledge-based potential functions are finally discussed.
q-bio.BM:$\beta$-barrel membrane proteins are found in the outer membrane of gram-negative bacteria, mitochondria, and chloroplasts. We have developed probabilistic models to quantify propensities of residues for different spatial locations and for interstrand pairwise contact interactions involving strong H-bonds, side-chain interactions, and weak H-bonds. The propensity values and p-values measuring statistical significance are calculated exactly by analytical formulae we have developed. Contrary to the ``positive-inside'' rule for helical membrane proteins, $\beta$-barrel membrane proteins follow a significant albeit weaker ``positive-outside'' rule, in that the basic residues Arg and Lys are disproportionately favored in the extracellular cap region and disfavored in the periplasmic cap region. Different residue pairs prefer strong backbone H-bonded interstrand pairings (e.g. Gly-Aromatic) or non-H-bonded pairings (e.g. Aromatic-Aromatic). In addition, Tyr and Phe participate in aromatic rescue by shielding Gly from polar environments. These propensities can be used to predict the registration of strand pairs, an important task for the structure prediction of $\beta$-barrel membrane proteins. Our accuracy of 44% is considerably better than random (7%) and other studies. Our results imply several experiments that can help to elucidate the mechanisms of in vitro and in vivo folding of $\beta$-barrel membrane proteins. See supplementary material after the bibliography for detailed techniques.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. Simplified protein models such as those requiring only $C_\alpha$ or backbone atoms are attractive because they enable efficient search of the conformational space. We show residue specific reduced discrete state models can represent the backbone conformations of proteins with small RMSD values. However, no potential functions exist that are designed for such simplified protein models. In this study, we develop optimal potential functions by combining contact interaction descriptors and local sequence-structure descriptors. The form of the potential function is a weighted linear sum of all descriptors, and the optimal weight coefficients are obtained through optimization using both native and decoy structures. The performance of the potential function in test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. Our potential function requiring only backbone atoms or $C_\alpha$ atoms have comparable or better performance than several residue-based potential functions that require additional coordinates of side chain centers or coordinates of all side chain atoms. By reducing the residue alphabets down to size 5 for local structure-sequence relationship, the performance of the potential function can be further improved. Our results also suggest that local sequence-structure correlation may play important role in reducing the entropic cost of protein folding.
q-bio.BM:Without invoking the Markov approximation, we derive formulas for vibrational energy relaxation (VER) and dephasing for an anharmonic system oscillator using a time-dependent perturbation theory. The system-bath Hamiltonian contains more than the third order coupling terms since we take a normal mode picture as a zeroth order approximation. When we invoke the Markov approximation, our theory reduces to the Maradudin-Fein formula which is used to describe VER properties of glass and proteins. When the system anharmonicity and the renormalization effect due to the environment vanishes, our formulas reduce to those derived by Mikami and Okazaki invoking the path-integral influence functional method [J. Chem. Phys. 121 (2004) 10052]. We apply our formulas to VER of the amide I mode of a small amino-acide like molecule, N-methylacetamide, in heavy water.
q-bio.BM:Simplified Go models, where only native contacts interact favorably, have proven useful to characterize some aspects of the folding of small proteins. The success of these models is limited by the fact that all residues interact in the same way, so that the folding features of a protein are determined only by the geometry of its native conformation. We present an extended version of a C-alpha based Go model where different residues interact with different energies. The model is used to calculate the thermodynamics of three small proteins (Protein G, SrcSH3 and CI2) and the effect of mutations on the wildtype sequence. The model allows to investigate some of the most controversial areas in protein folding such as its earliest stages, a subject which has lately received particular attention. The picture which emerges for the three proteins under study is that of a hierarchical process, where local elementary structures (LES) (not necessarily coincident with elements of secondary structure) are formed at the early stages of the folding and drive the protein, through the transition state and the postcritical folding nucleus (FN), resulting from the docking of the LES, to the native conformation.
q-bio.BM:The chiral nature of DNA plays a crucial role in cellular processes. Here we use magnetic tweezers to explore one of the signatures of this chirality, the coupling between stretch and twist deformations. We show that the extension of a stretched DNA molecule increases linearly by 0.42 nm per excess turn applied to the double helix. This result contradicts the intuition that DNA should lengthen as it is unwound and get shorter with overwinding. We then present numerical results of energy minimizations of torsionally restrained DNA that display a behaviour similar to the experimental data and shed light on the molecular details of this surprising effect.
q-bio.BM:PCR (Polymerase Chain Reaction), a method which replicates a selected sequence of DNA, has revolutionized the study of genomic material, but mathematical study of the process has been limited to simple deterministic models or descriptions relying on stochastic processes. In this paper we develop a suite of deterministic models for the reactions of quantitative PCR (Polymerase Chain Reaction) based on the law of mass action. Maps are created from DNA copy number in one cycle to the next, with ordinary differential equations describing the evolution of difference molecular species during each cycle. Qualitative analysis is preformed at each stage and parameters are estimated by fitting each model to data from Roche LightCycler (TM) runs.
q-bio.BM:Kinetics of folding of a protein held in a force-clamp are compared to an unconstrained folding. The comparison is made within a simple topology-based dynamical model of ubiquitin. We demonstrate that the experimentally observed variations in the end-to-end distance reflect microscopic events during folding. However, the folding scenarios in and out of the force-clamp are distinct.
q-bio.BM:In the template-assistance model, normal prion protein (PrPC), the pathogenic cause of prion diseases such as Creutzfeldt-Jakob (CJD) in human, Bovine Spongiform Encephalopathy (BSE) in cow, and scrapie in sheep, converts to infectious prion (PrPSc) through an autocatalytic process triggered by a transient interaction between PrPC and PrPSc. Conventional studies suggest the S1-H1-S2 region in PrPC to be the template of S1-S2 $\beta$-sheet in PrPSc, and the conformational conversion of PrPC into PrPSc may involve an unfolding of H1 in PrPC and its refolding into the $\beta$-sheet in PrPSc. Here we conduct a series of simulation experiments to test the idea of transient interaction of the template-assistance model. We find that the integrity of H1 in PrPC is vulnerable to a transient interaction that alters the native dihedral angles at residue Asn$^{143}$, which connects the S1 flank to H1, but not to interactions that alter the internal structure of the S1 flank, nor to those that alter the relative orientation between H1 and the S2 flank.
q-bio.BM:The Yakushevich model of DNA torsion dynamics supports soliton solutions, which are supposed to be of special interest for DNA transcription. In the discussion of the model, one usually adopts the approximation $\ell_0 \to 0$, where $\ell_0$ is a parameter related to the equilibrium distance between bases in a Watson-Crick pair. Here we analyze the Yakushevich model without $\ell_0 \to 0$. The model still supports soliton solutions indexed by two winding numbers $(n,m)$; we discuss in detail the fundamental solitons, corresponding to winding numbers (1,0) and (0,1) respectively.
q-bio.BM:The Yakushevich (Y) model provides a very simple pictures of DNA torsion dynamics, yet yields remarkably correct predictions on certain physical characteristics of the dynamics. In the standard Y model, the interaction between bases of a pair is modelled by a harmonic potential, which becomes anharmonic when described in terms of the rotation angles; here we substitute to this different types of improved potentials, providing a more physical description of the H-bond mediated interactions between the bases. We focus in particular on soliton solutions; the Y model predicts the correct size of the nonlinear excitations supposed to model the ``transcription bubbles'', and this is essentially unchanged with the improved potential. Other features of soliton dynamics, in particular curvature of soliton field configurations and the Peierls-Nabarro barrier, are instead significantly changed.
q-bio.BM:Simple coarse-grained models, such as the Gaussian Network Model, have been shown to capture some of the features of equilibrium protein dynamics. We extend this model by using atomic contacts to define residue interactions and introducing more than one interaction parameter between residues. We use B-factors from 98 ultra-high resolution X-ray crystal structures to optimize the interaction parameters. The average correlation between GNM fluctuation predictions and the B-factors is 0.64 for the data set, consistent with a previous large-scale study. By separating residue interactions into covalent and noncovalent, we achieve an average correlation of 0.74, and addition of ligands and cofactors further improves the correlation to 0.75. However, further separating the noncovalent interactions into nonpolar, polar, and mixed yields no significant improvement. The addition of simple chemical information results in better prediction quality without increasing the size of the coarse-grained model.
q-bio.BM:Background: One-dimensional protein structures such as secondary structures or contact numbers are useful for three-dimensional structure prediction and helpful for intuitive understanding of the sequence-structure relationship. Accurate prediction methods will serve as a basis for these and other purposes. Results: We implemented a program CRNPRED which predicts secondary structures, contact numbers and residue-wise contact orders. This program is based on a novel machine learning scheme called critical random networks. Unlike most conventional one-dimensional structure prediction methods which are based on local windows of an amino acid sequence, CRNPRED takes into account the whole sequence. CRNPRED achieves, on average per chain, Q3 = 81% for secondary structure prediction, and correlation coefficients of 0.75 and 0.61 for contact number and residue-wise contact order predictions, respectively. Conclusion: CRNPRED will be a useful tool for computational as well as experimental biologists who need accurate one-dimensional protein structure predictions.
q-bio.BM:To confer high specificity and affinity in binding, contacts at interfaces between two interacting macromolecules are expected to exhibit pair preferences for types of atoms or residues. Here we quantify these preferences by measuring the mutual information of contacts for 895 protein-protein interfaces. The information content is significant and is highest at the atomic resolution. A simple phenomenological theory reveals a connection between information at interfaces and the free energy spectrum of association. The connection is presented in the form of a relation between mutual information and the energy gap of the native bound state to off-target bound states. Measurement of information content in designed lattice interfaces show the predicted scaling behavior to the energy gap. Our theory also suggests that mutual information in contacts emerges by a selection mechanism, and that strong selection, or high conservation, of residues should lead to correspondingly high mutual information. Amino acids which contribute more heavily to information content are then expected to be more conserved. We verify this by showing a statistically significant correlation between the conservation of each of the twenty amino acids and their individual contribution to the information content at protein-protein interfaces
q-bio.BM:It was first suggested by Englander et al to model the nonlinear dynamics of DNA relevant to the transcription process in terms of a chain of coupled pendulums. In a related paper [q-bio.BM/0604014] we argued for the advantages of an extension of this approach based on considering a chain of double pendulums with certain characteristics. Here we study a simplified model of this kind, focusing on its general features and nonlinear travelling wave excitations; in particular, we show that some of the degrees of freedom are actually slaved to others, allowing for an effective reduction of the relevant equations.
q-bio.BM:The Fast Fourier Transform (FFT) correlation approach to protein-protein docking can evaluate the energies of billions of docked conformations on a grid if the energy is described in the form of a correlation function. Here, this restriction is removed, and the approach is efficiently used with pairwise interactions potentials that substantially improve the docking results. The basic idea is approximating the interaction matrix by its eigenvectors corresponding to the few dominant eigenvalues, resulting in an energy expression written as the sum of a few correlation functions, and solving the problem by repeated FFT calculations. In addition to describing how the method is implemented, we present a novel class of structure based pairwise intermolecular potentials. The DARS (Decoys As the Reference State) potentials are extracted from structures of protein-protein complexes and use large sets of docked conformations as decoys to derive atom pair distributions in the reference state. The current version of the DARS potential works well for enzyme-inhibitor complexes. With the new FFT-based program, DARS provides much better docking results than the earlier approaches, in many cases generating 50\% more near-native docked conformations. Although the potential is far from optimal for antibody-antigen pairs, the results are still slightly better than those given by an earlier FFT method. The docking program PIPER is freely available for non-commercial applications.
q-bio.BM:We investigated the structural relaxation of myosin motor domain from the pre-power stroke state to the near-rigor state using molecular dynamics simulation of a coarse-grained protein model. To describe the structural change, we propose a "dual Go-model," a variant of the Go-like model that has two reference structures. The nucleotide dissociation process is also studied by introducing a coarse-grained nucleotide in the simulation. We found that the myosin structural relaxation toward the near-rigor conformation cannot be completed before the nucleotide dissociation. Moreover, the relaxation and the dissociation occurred cooperatively when the nucleotide was tightly bound to the myosin head. The result suggested that the primary role of the nucleotide is to suppress the structural relaxation.
q-bio.BM:Self-similar properties of the ribosome in terms of the mass fractal dimension are investigated. We find that both the 30S subunit and the 16S rRNA have fractal dimensions of 2.58 and 2.82, respectively; while the 50S subunit as well as the 23S rRNA has the mass fractal dimension close to 3, implying a compact three dimensional macromolecule. This finding supports the dynamic and active role of the 30S subunit in the protein synthesis, in contrast to the pass role of the 50S subunit.
q-bio.BM:We present an extremely simplified model of multiple-domains polymer stretching in an atomic force microscopy experiment. We portray each module as a binary set of contacts and decompose the system energy into a harmonic term (the cantilever) and long-range interactions terms inside each domain. Exact equilibrium computations and Monte Carlo simulations qualitatively reproduce the experimental saw-tooth pattern of force-extension profiles, corresponding (in our model) to first-order phase transitions. We study the influence of the coupling induced by the cantilever and the pulling speed on the relative heights of the force peaks. The results suggest that the increasing height of the critical force for subsequent unfolding events is an out-of-equilibrium effect due to a finite pulling speed. The dependence of the average unfolding force on the pulling speed is shown to reproduce the experimental logarithmic law.
q-bio.BM:Phi-values are experimental measures of the effects of mutations on the folding kinetics of a protein. A central question is which structural information Phi-values contain about the transition state of folding. Traditionally, a Phi-value is interpreted as the 'nativeness' of a mutated residue in the transition state. However, this interpretation is often problematic because it assumes a linear relation between the nativeness of the residue and its free-energy contribution. We present here a better structural interpretation of Phi-values for mutations within a given helix. Our interpretation is based on a simple physical model that distinguishes between secondary and tertiary free-energy contributions of helical residues. From a linear fit of our model to the experimental data, we obtain two structural parameters: the extent of helix formation in the transition state, and the nativeness of tertiary interactions in the transition state. We apply our model to all proteins with well-characterized helices for which more than 10 Phi-values are available: protein A, CI2, and protein L. The model captures nonclassical Phi-values <0 or >1 in these helices, and explains how different mutations at a given site can lead to different Phi-values.
q-bio.BM:A model for the unidirectional movement of dynein is presented based on structural observations and biochemical experimental results available. In this model, the binding affinity of dynein for microtubule is independent of its nucleotide state and the change between strong and weak microtubule-binding is determined naturally by the variation of relative orientation between the stalk and microtubule as the stalk rotates following nucleotide-state transition. Thus the enigmatic communication from the ATP binding site in the globular domain to the far MT-binding site in the tip of the stalk, which is prerequisite in conventional models, is not required. Using the present model, the previous experimental results such as the effect of ATP and ADP bindings on dissociation of dynein from microtubule, the processive movement of single-headed axonemal dyneins at saturating ATP concentration, the load dependence of step size for the processive movement of two-headed cytoplasmic dyneins and the dependence of stall force on ATP concentration can be well explained.
q-bio.BM:Over the last 10-15 years a general understanding of the chemical reaction of protein folding has emerged from statistical mechanics. The lessons learned from protein folding kinetics based on energy landscape ideas have benefited protein structure prediction, in particular the development of coarse grained models. We survey results from blind structure prediction. We explore how second generation prediction energy functions can be developed by introducing information from an ensemble of previously simulated structures. This procedure relies on the assumption of a funnelled energy landscape keeping with the principle of minimal frustration. First generation simulated structures provide an improved input for associative memory energy functions in comparison to the experimental protein structures chosen on the basis of sequence alignment.
q-bio.BM:The precise details of how myosin-V coordinates the biochemical reactions and mechanical motions of its two head elements to engineer effective processive molecular motion along actin filaments remain unresolved. We compare a quantitative kinetic model of the myosin-V walk, consisting of five basic states augmented by two further states to allow for futile hydrolysis and detachments, with experimental results for run lengths, velocities, and dwell times and their dependence on bulk nucleotide concentrations and external loads in both directions. The model reveals how myosin-V can use the internal strain in the molecule to synchronise the motion of the head elements. Estimates for the rate constants in the reaction cycle and the internal strain energy are obtained by a computational comparison scheme involving an extensive exploration of the large parameter space. This scheme exploits the fact that we have obtained analytic results for our reaction network, e.g. for the velocity but also the run length, diffusion constant and fraction of backward steps. The agreement with experiment is often reasonable but some open problems are highlighted, in particular the inability of such a general model to reproduce the reported dependence of run length on ADP. The novel way that our approach explores parameter space means that any confirmed discrepancies should give new insights into the reaction network model.
q-bio.BM:The prion protein (PrP) binds Cu2+ ions in the octarepeat domain of the N-terminal tail up to full occupancy at pH=7.4. Recent experiments show that the HGGG octarepeat subdomain is responsible for holding the metal bound in a square planar coordination. By using first principle ab initio molecular dynamics simulations of the Car-Parrinello type, the Cu coordination mode to the binding sites of the PrP octarepeat region is investigated. Simulations are carried out for a number of structured binding sites. Results for the complexes Cu(HGGGW)+(wat), Cu(HGGG) and the 2[Cu(HGGG)] dimer are presented. While the presence of a Trp residue and a H2O molecule does not seem to affect the nature of the Cu coordination, high stability of the bond between Cu and the amide Nitrogens of deprotonated Gly's is confirmed in the case of the Cu(HGGG) system. For the more interesting 2[Cu(HGGG)] dimer a dynamically entangled arrangement of the two monomers, with intertwined N-Cu bonds, emerges. This observation is consistent with the highly packed structure seen in experiments at full Cu occupancy.
q-bio.BM:We formulate a simple solvation potential based on a coarsed-grain representation of amino acids with two spheres modeling the $C_\alpha$ atom and an effective side-chain centroid. The potential relies on a new method for estimating the buried area of residues, based on counting the effective number of burying neighbours in a suitable way. This latter quantity shows a good correlation with the buried area of residues computed from all atom crystallographic structures. We check the discriminatory power of the solvation potential alone to identify the native fold of a protein from a set of decoys and show the potential to be considerably selective.
q-bio.BM:The aim of this work is to elucidate how physical principles of protein design are reflected in natural sequences that evolved in response to the thermal conditions of the environment. Using an exactly solvable lattice model, we design sequences with selected thermal properties. Compositional analysis of designed model sequences and natural proteomes reveals a specific trend in amino acid compositions in response to the requirement of stability at elevated environmental temperature, i.e. the increase of fractions of hydrophobic and charged amino acid residues at the expense of polar ones. We show that this from both ends of hydrophobicity scale trend is due to positive (to stabilize the native state) and negative (to destabilize misfolded states) components of protein design. Negative design strengthens specific repulsive nonnative interactions that appear in misfolded structures. A pressure to preserve specific repulsive interactions in non-native conformations may result in correlated mutations between amino acids which are far apart in the native state but may be in contact in misfolded conformations. Such correlated mutations are indeed found in TIM barrel and other proteins.
q-bio.BM:F-actin bundles constitute principal components of a multitude of cytoskeletal processes including stereocilia, filopodia, microvilli, neurosensory bristles, cytoskeletal stress fibers, and the sperm acrosome. The bending, buckling, and stretching behaviors of these processes play key roles in cellular functions ranging from locomotion to mechanotransduction and fertilization. Despite their central importance to cellular function, F-actin bundle mechanics remain poorly understood. Here, we demonstrate that bundle bending stiffness is a state-dependent quantity with three distinct regimes that are mediated by bundle dimensions in addition to crosslink properties. We calculate the complete state-dependence of the bending stiffness and elucidate the mechanical origin of each. A generic set of design parameters delineating the regimes in state-space is derived and used to predict the bending stiffness of a variety of F-actin bundles found in cells. Finally, the broad and direct implications that the isolated state-dependence of F-actin bundle stiffness has on the interpretation of the bending, buckling, and stretching behavior of cytoskeletal bundles is addressed.
q-bio.BM:In this work we develop a theory of interaction of randomly patterned surfaces as a generic prototype model of protein-protein interactions. The theory predicts that pairs of randomly superimposed identical (homodimeric) random patterns have always twice as large magnitude of the energy fluctuations with respect to their mutual orientation, as compared with pairs of different (heterodimeric) random patterns. The amplitude of the energy fluctuations is proportional to the square of the average pattern density, to the square of the amplitude of the potential and its characteristic length, and scales linearly with the area of surfaces. The greater dispersion of interaction energies in the ensemble of homodimers implies that strongly attractive complexes of random surfaces are much more likely to be homodimers, rather than heterodimers. Our findings suggest a plausible physical reason for the anomalously high fraction of homodimers observed in real protein interaction networks.
q-bio.BM:We extend our previously developed general approach (1) to study a phenomenological model in which the simulated packing of hard, attractive spheres on a prolate spheroid surface with convexity constraints produces structures identical to those of prolate virus capsid structures. Our simulation approach combines the traditional Monte Carlo method with the method of random sampling on an ellipsoidal surface and a convex hull searching algorithm. Using this approach we study the assembly and structural origin of non-icosahedral, elongated virus capsids, such as two aberrant flock house virus (FHV) particles and the prolate prohead of bacteriophage phi29, and discuss the implication of our simulation results in the context of recent experimental findings.
q-bio.BM:The need to understand the assembly kinetics of fibril formation has become urgent because of the realization that soluble oligomers of amyloidogenic peptides may be even more neurotoxic than the end product, namely, the amyloid fibrils. In order to fully understand the routes to fibril formation one has to characterize the major species in the assembly pathways. The characterization of the energetics and dynamics of oligomers (dimers, trimers etc) is difficult using experiments alone because they undergo large conformational fluctuations. In this context, carefully planned molecular dynamics simulation studies, computations using coarse-grained models, and bioinformatic analysis have given considerable insights into the early events in the route to fibril formation. Here, we describe progress along this direction using examples taken largely from our own work. In this chapter, we focus on aspects of protein aggregation using Abeta-peptides and prion proteins as examples.
q-bio.BM:The native three dimensional structure of a single protein is determined by the physico chemical nature of its constituent amino acids. The twenty different types of amino acids, depending on their physico chemical properties, can be grouped into three major classes - hydrophobic, hydrophilic and charged. We have studied the anatomy of the weighted and unweighted networks of hydrophobic, hydrophilic and charged residues separately for a large number of proteins. Our results show that the average degree of the hydrophobic networks has significantly larger value than that of hydrophilic and charged networks. The average degree of the hydrophilic networks is slightly higher than that of charged networks. The average strength of the nodes of hydrophobic networks is nearly equal to that of the charged network; whereas that of hydrophilic networks has smaller value than that of hydrophobic and charged networks. The average strength for each of the three types of networks varies with its degree. The average strength of a node in charged networks increases more sharply than that of the hydrophobic and hydrophilic networks. Each of the three types of networks exhibits the 'small-world' property. Our results further indicate that the all amino acids' networks and hydrophobic networks are of assortative type. While maximum of the hydrophilic and charged networks are of assortative type, few others have the characteristics of disassortative mixing of the nodes. We have further observed that all amino acids' networks and hydrophobic networks bear the signature of hierarchy; whereas the hydrophilic and charged networks do not have any hierarchical signature.
q-bio.BM:We study statistical properties of interacting protein-like surfaces and predict two strong, related effects: (i) statistically enhanced self-attraction of proteins; (ii) statistically enhanced attraction of proteins with similar structures. The effects originate in the fact that the probability to find a pattern self-match between two identical, even randomly organized interacting protein surfaces is always higher compared with the probability for a pattern match between two different, promiscuous protein surfaces. This theoretical finding explains statistical prevalence of homodimers in protein-protein interaction networks reported earlier. Further, our findings are confirmed by the analysis of curated database of protein complexes that showed highly statistically significant overrepresentation of dimers formed by structurally similar proteins with highly divergent sequences (superfamily heterodimers). We predict that significant fraction of heterodimers evolved from homodimers with the negative design evolutionary pressure applied against promiscuous homodimer formation. This is achieved through the formation of highly specific contacts formed by charged residues as demonstrated both in model and real superfamily heterodimers
q-bio.BM:Stretching of a protein by a fluid flow is compared to that in a force-clamp apparatus. The comparison is made within a simple topology-based dynamical model of a protein in which the effects of the flow are implemented using Langevin dynamics. We demonstrate that unfolding induced by a uniform flow shows a richer behavior than that in the force clamp. The dynamics of unfolding is found to depend strongly on the selection of the amino acid, usually one of the termini, which is anchored. These features offer potentially wider diagnostic tools to investigate structure of proteins compared to experiments based on the atomic force microscopy.
q-bio.BM:Secretion and role of autotaxin and lysophosphatidic acid in adipose tissue In obesity, adipocyte hypertrophy is often associated with recrutement of new fat cells (adipogenesis) under the control of circulating and local regulatory factors. Among the different lipids released in the extracellular compartment of adipocytes, our group found the presence of lysophosphatidic acid (LPA). LPA is a bioactive phospholipid able to regulate several cell responses via the activation of specific G-protein coupled membrane receptors. Our group found that LPA increases preadipocyte proliferation and inhibits adipogenesis via the activation of LPA1 receptor subtype. Extracellular LPA-synthesis is catalyzed by a lysophospholipase D secreted by adipocytes : autotaxin (ATX). Adipocyte ATX expression strongly increases with adipogenesis as well as in individuals exhibiting type 2 diabetes associated with massive obesity. A possible contribution of ATX and LPA as paracrine regulators of adipogenesis and obesity associated diabetes is proposed.
q-bio.BM:A recently proposed model of non-autocatalytic reactions in dipeptide reactions leading to spontaneous symmetry breaking and homochirality is examined. The model is governed by activation, polymerization, epimerization and depolymerization of amino acids. Symmetry breaking is primarily a consequence of the fact that the rates of reactions involving homodimers and heterodimers are different, i.e., stereoselective, and on the fact that epimerization can only occur on the N-terminal residue and not on the Cterminal residue. This corresponds to an auto-inductive cyclic process that works only in one sense. It is argued that epimerization mimics both autocatalytic behavior as well as mutual antagonism - both of which were known to be crucial for producing full homochirality.
q-bio.BM:The structural organisation of the viral genome within its protein container, called the viral capsid, is an important aspect of virus architecture. Many single-stranded (ss) RNA viruses organise a significant part of their genome in a dodecahedral cage as a RNA duplex structure that mirrors the symmetry of the capsid. Bruinsma and Rudnick have suggested a model for the structural organisation of the RNA in these cages. It is the purpose of this paper to further develop their approach based on results from the areas of graph theory and DNA network engineering. We start by suggesting a scenario for pariacoto virus, a representative of this class of viruses, that is energetically more favorable than those derived previously. We then show that it is a representative of a whole family of cage structures that abide to the same construction principle, and then derive the energetically optimal configuration for a second family of cage structures along similar lines. Finally, we give reasons for the conjecture that these two families are more likely to occur in nature than other scenarios.
q-bio.BM:The sequence-dependent elasticity of double-helical DNA on a nm length scale can be captured by the rigid base-pair model, whose strains are the relative position and orientation of adjacent base-pairs. Corresponding elastic potentials have been obtained from all-atom MD simulation and from high-resolution structural data. On the scale of a hundred nm, DNA is successfully described by a continuous worm-like chain model with homogeneous elastic properties characterized by a set of four elastic constants, which have been directly measured in single-molecule experiments. We present here a theory that links these experiments on different scales, by systematically coarse-graining the rigid base-pair model for random sequence DNA to an effective worm-like chain description. The average helical geometry of the molecule is exactly taken into account in our approach. We find that the available microscopic parameters sets predict qualitatively similar mesoscopic parameters. The thermal bending and twisting persistence lengths computed from MD data are 42 and 48 nm, respectively. The static persistence lengths are generally much higher, in agreement with cyclization experiments. All microscopic parameter sets predict negative twist-stretch coupling. The variability and anisotropy of bending stiffness in short random chains lead to non-Gaussian bend angle distributions, but become unimportant after two helical turns.
q-bio.BM:Processive molecular motors take more-or-less uniformly sized steps, along spatially periodic tracks, mostly forwards but increasingly backwards under loads. Experimentally, the major steps can be resolved clearly within the noise but one knows biochemically that one or more mechanochemical substeps remain hidden in each enzymatic cycle. In order to properly interpret experimental data for back/forward step ratios, mean conditional step-to-step dwell times, etc., a first-passage analysis has been developed that takes account of hidden substeps in $N$-state sequential models. The explicit, general results differ significantly from previous treatments that identify the observed steps with complete mechanochemical cycles; e.g., the mean dwell times $\tau_+$ and $\tau_-$ prior to forward and back steps, respectively, are normally {\it unequal} although the dwell times $\tau_{++}$ and $\tau_{--}$ between {\it successive} forward and back steps are equal. Illustrative (N=2)-state examples display a wide range of behavior. The formulation extends to the case of two or more detectable transitions in a multistate cycle with hidden substeps.
q-bio.BM:For the vast majority of naturally occurring, small, single domain proteins folding is often described as a two-state process that lacks detectable intermediates. This observation has often been rationalized on the basis of a nucleation mechanism for protein folding whose basic premise is the idea that after completion of a specific set of contacts forming the so-called folding nucleus the native state is achieved promptly. Here we propose a methodology to identify folding nuclei in small lattice polymers and apply it to the study of protein molecules with chain length N=48. To investigate the extent to which protein topology is a robust determinant of the nucleation mechanism we compare the nucleation scenario of a native-centric model with that of a sequence specific model sharing the same native fold. To evaluate the impact of the sequence's finner details in the nucleation mechanism we consider the folding of two non- homologous sequences. We conclude that in a sequence-specific model the folding nucleus is, to some extent, formed by the most stable contacts in the protein and that the less stable linkages in the folding nucleus are solely determined by the fold's topology. We have also found that independently of protein sequence the folding nucleus performs the same `topological' function. This unifying feature of the nucleation mechanism results from the residues forming the folding nucleus being distributed along the protein chain in a similar and well-defined manner that is determined by the fold's topological features.
q-bio.BM:The folding of naturally occurring, single domain proteins is usually well-described as a simple, single exponential process lacking significant trapped states. Here we further explore the hypothesis that the smooth energy landscape this implies, and the rapid kinetics it engenders, arises due to the extraordinary thermodynamic cooperativity of protein folding. Studying Miyazawa-Jernigan lattice polymers we find that, even under conditions where the folding energy landscape is relatively optimized (designed sequences folding at their temperature of maximum folding rate), the folding of protein-like heteropolymers is accelerated when their thermodynamic cooperativity enhanced by enhancing the non-additivity of their energy potentials. At lower temperatures, where kinetic traps presumably play a more significant role in defining folding rates, we observe still greater cooperativity-induced acceleration. Consistent with these observations, we find that the folding kinetics of our computational models more closely approximate single-exponential behavior as their cooperativity approaches optimal levels. These observations suggest that the rapid folding of naturally occurring proteins is, at least in part, consequences of their remarkably cooperative folding.
q-bio.BM:An increasing number of proteins are being discovered with a remarkable and somewhat surprising feature, a knot in their native structures. How the polypeptide chain is able to knot itself during the folding process to form these highly intricate protein topologies is not known. Here, we perform a computational study on the 160-amino acid homodimeric protein YibK which, like other proteins in the SpoU family of MTases, contains a deep trefoil knot in its C-terminal region. In this study, we use a coarse-grained C-alpha-chain representation and Langevin dynamics to study folding kinetics. We find that specific, attractive nonnative interactions are critical for knot formation. In the absence of these interactions, i.e. in an energetics driven entirely by native interactions, knot formation is exceedingly unlikely. Further, we find, in concert with recent experimental data on YibK, two parallel folding pathways which we attribute to an early and a late formation of the trefoil knot, respectively. For both pathways, knot formation occurs before dimerization. A bioinformatics analysis of the SpoU family of proteins reveals further that the critical nonnative interactions may originate from evolutionary conserved hydrophobic segments around the knotted region.
q-bio.BM:The structure of the self-cleaving hairpin ribozyme is well characterized, and its folding has been examined in bulk and by single-molecule fluorescence, establishing the importance of cations, especially magnesium in the stability of the native fold. Here we describe the first all-atom folding simulations of the hairpin ribozyme, using a version of a Go potential with separate secondary and tertiary structure energetic contributions. The ratio of tertiary/secondary interaction energies serves as a proxy for non-specific cation binding: a high ratio corresponds to a high concentration, while a low one mimics low concentration. By studying the unfolding behavior of the RNA over a range of temperature and tertiary/secondary energies, a three-state phase diagram emerges, with folded, unfolded (coil) and transient folding/unfolding tertiary structure species. The thermodynamics were verified by paired folding simulations in each region of the phase diagram. The three phase behaviors correspond with experimentally observed states, so this simple model captures the essential aspect of thermodynamics in RNA folding.
q-bio.BM:The refolding from stretched initial conformations of ubiquitin (PDB ID: 1ubq) under the quenched force is studied using the Go model and the Langevin dynamics. It is shown that the refolding decouples the collapse and folding kinetics. The force quench refolding times scale as tau_F ~ exp(f_q*x_F/k_B*T), where f_q is the quench force and x_F = 0.96 nm is the location of the average transition state along the reaction coordinate given by the end-to-end distance. This value is close to x_F = 0.8 nm obtained from the force-clamp experiments. The mechanical and thermal unfolding pathways are studied and compared with the experimental and all-atom simulation results in detail. The sequencing of thermal unfolding was found to be markedly different from the mechanical one. It is found that fixing the N-terminus of ubiquitin changes its mechanical unfolding pathways much more drastically compared to the case when the C-end is anchored. We obtained the distance between the native state and the transition state x_UF=0.24 nm which is in reasonable agreement with the experimental data.
q-bio.BM:Natural proteins fold to a unique, thermodynamically dominant state. Modeling of the folding process and prediction of the native fold of proteins are two major unsolved problems in biophysics. Here, we show successful all-atom ab initio folding of a representative diverse set of proteins, using a minimalist transferable energy model that consists of two-body atom-atom interactions, hydrogen-bonding, and a local sequence energy term that models sequence-specific chain stiffness. Starting from a random coil, the native-like structure was observed during replica exchange Monte Carlo (REMC) simulation for most proteins regardless of their structural classes; the lowest energy structure was close to native- in the range of 2-6 A root-mean-square deviation (RMSD). Our results demonstrate that the successful all-atom folding of a protein chain to its native state is governed by only a few crucial energetic terms.
q-bio.BM:Protein-DNA interactions are vital for many processes in living cells, especially transcriptional regulation and DNA modification. To further our understanding of these important processes on the microscopic level, it is necessary that theoretical models describe the macromolecular interaction energetics accurately. While several methods have been proposed, there has not been a careful comparison of how well the different methods are able to predict biologically important quantities such as the correct DNA binding sequence, total binding free energy, and free energy changes caused by DNA mutation. In addition to carrying out the comparison, we present two important theoretical models developed initially in protein folding that have not yet been tried on protein-DNA interactions. In the process, we find that the results of these knowledge-based potentials show a strong dependence on the interaction distance and the derivation method. Finally, we present a knowledge-based potential that gives comparable or superior results to the best of the other methods, including the molecular mechanics force field AMBER99.
q-bio.BM:The folding of the alpha-helice domain hbSBD of the mammalian mitochondrial branched-chain alpha-ketoacid dehydrogenase (BCKD) complex is studied by the circular dichroism technique in absence of urea. Thermal denaturation is used to evaluate various thermodynamic parameters defining the equilibrium unfolding, which is well described by the two-state model with the folding temperature T_f = 317.8 K and the enthalpy change Delta H_g = 19.67 kcal/mol. The folding is also studied numerically using the off-lattice coarse-grained Go model and the Langevin dynamics. The obtained results, including the population of the native basin, the free energy landscape as a function of the number of native contacts and the folding kinetics, also suggest that the hbSBD domain is a two-state folder. These results are consistent with the biological function of hbSBD in BCKD.
q-bio.BM:We analyze the dependence of cooperativity of the thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, $N$, using lattice models with side chains,off-lattice Go models and the available experimental data. A dimensionless measure of cooperativity, $\Omega_c$ ($0 < \Omega_c < \infty$), scales as $\Omega_c \sim N^{\zeta}$. The results of simulations and the analysis of experimental data further confirm the earlier prediction that $\zeta$ is universal with $\zeta = 1 +\gamma$, where exponent $\gamma$ characterizes the susceptibility of a self-avoiding walk. This finding suggests that the structural characteristics in the denaturated state are manifested in the folding cooperativity at the transition temperature. The folding rates $k_F$ for the Go models and a dataset of 69 proteins can be fit using $k_F = k_F^0 \exp(-cN^\beta)$. Both $\beta = 1/2$ and 2/3 provide a good fit of the data. We find that $k_F = k_F^0 \exp(-cN^{{1/2}})$, with the average (over the dataset of proteins) $k_F^0 \approx (0.2\mu s)^{-1}$ and $c \approx 1.1$, can be used to estimate folding rates to within an order of magnitude in most cases. The minimal models give identical $N$ dependence with $c \approx 1$. The prefactor for off-lattice Go models is nearly four orders of magnitude larger than the experimental value.
q-bio.BM:The didemnins represent a versatile class of depsipeptides of marine origin and hold a great deal of potential for biomedical application. The biological and geographical origins of the didemnins are reviewed in addition to the chemical structures of the major didemnins. The biological mechanisms behind the antiviral and anticancer effects of selected didemnins are summarized and the special case of dehydrodidemnin B (Aplidin) is expounded upon including structural characteristics, synthesis, pharmacological mechanism and a discussion of its current clinical trials as an anticancer agent.
q-bio.BM:TThe paper had many errors.
q-bio.BM:Pathological folding and oligomer formation of the amyloid beta-protein (Abeta) are widely perceived as central to Alzheimer's disease (AD). Experimental approaches to study Abeta self-assembly are problematic, because most relevant aggregates are quasi-stable and inhomogeneous. We apply a discrete molecular dynamics (DMD) approach combined with a four-bead protein model to study oligomer formation of the amyloid beta-protein (Abeta). We address the differences between the two most common Abeta alloforms, Abeta40 and Abeta42, which oligomerize differently in vitro. We study how the presence of electrostatic interactions (EIs) between pairs of charged amino acids affects Abeta40 and Abeta42 oligomer formation. Our results indicate that EIs promote formation of larger oligomers in both Abeta40 and Abeta42. The Abeta40 size distribution remains unimodal, whereas the Abeta42 distribution is trimodal, as observed experimentally. Abeta42 folded structure is characterized by a turn in the C-terminus that is not present in Abeta40. We show that the same C-terminal region is also responsible for the strongest intermolecular contacts in Abeta42 pentamers and larger oligomers. Our results suggest that this C-terminal region plays a key role in the formation of Abeta42 oligomers and the relative importance of this region increases in the presence of EIs. These results suggest that inhibitors targeting the C-terminal region of Abeta42 oligomers may be able to prevent oligomer formation or structurally modify the assemblies to reduce their toxicity.
q-bio.BM:The results of Brownian dynamics simulations of a single DNA molecule in shear flow are presented taking into account the effect of internal viscosity. The dissipative mechanism of internal viscosity is proved necessary in the research of DNA dynamics. A stochastic model is derived on the basis of the balance equation for forces acting on the chain. The Euler method is applied to the solution of the model. The extensions of DNA molecules for different Weissenberg numbers are analyzed. Comparison with the experimental results available in the literature is carried out to estimate the contribution of the effect of internal viscosity.
q-bio.BM:The network paradigm is increasingly used to describe the topology and dynamics of complex systems. Here we review the results of the topological analysis of protein structures as molecular networks describing their small-world character, and the role of hubs and central network elements in governing enzyme activity, allosteric regulation, protein motor function, signal transduction and protein stability. We summarize available data how central network elements are enriched in active centers and ligand binding sites directing the dynamics of the entire protein. We assess the feasibility of conformational and energy networks to simplify the vast complexity of rugged energy landscapes and to predict protein folding and dynamics. Finally, we suggest that modular analysis, novel centrality measures, hierarchical representation of networks and the analysis of network dynamics will soon lead to an expansion of this field.
q-bio.BM:Annealed importance sampling is a means to assign equilibrium weights to a nonequilibrium sample that was generated by a simulated annealing protocol. The weights may then be used to calculate equilibrium averages, and also serve as an ``adiabatic signature'' of the chosen cooling schedule. In this paper we demonstrate the method on the 50-atom dileucine peptide, showing that equilibrium distributions are attained for manageable cooling schedules. For this system, as naively implemented here, the method is modestly more efficient than constant temperature simulation. However, the method is worth considering whenever any simulated heating or cooling is performed (as is often done at the beginning of a simulation project, or during an NMR structure calculation), as it is simple to implement and requires minimal additional CPU expense. Furthermore, the naive implementation presented here can be improved.
q-bio.BM:Conformational transitions in macromolecular complexes often involve the reorientation of lever-like structures. Using a simple theoretical model, we show that the rate of such transitions is drastically enhanced if the lever is bendable, e.g. at a localized "hinge''. Surprisingly, the transition is fastest with an intermediate flexibility of the hinge. In this intermediate regime, the transition rate is also least sensitive to the amount of "cargo'' attached to the lever arm, which could be exploited by molecular motors. To explain this effect, we generalize the Kramers-Langer theory for multi-dimensional barrier crossing to configuration dependent mobility matrices.
q-bio.BM:We report 10 successfully folding events of trpzip2 by molecular dynamics simulation. It is found that the trizip2 can fold into its native state through different zipper pathways, depending on the ways of forming hydrophobic core. We also find a very fast non-zipper pathway. This indicates that there may be no inconsistencies in the current pictures of beta-hairpin folding mechanisms. These pathways occur with different probabilities. zip-out is the most probable one. This may explain the recent experiment that the turn formation is the rate-limiting step for beta-hairpin folding.
q-bio.BM:Structural fluctuations in the thermal equilibrium of the kinesin motor domain are studied using a lattice protein model with Go interactions. By means of the multi-self-overlap ensemble (MSOE) Monte Carlo method and the principal component analysis (PCA), the free-energy landscape is obtained. It is shown that kinesins have two subdomains that exhibit partial folding/unfolding at functionally important regions: one is located around the nucleotide binding site and the other includes the main microtubule binding site. These subdomains are consistent with structural variability that was reported recently based on experimentally-obtained structures. On the other hand, such large structural fluctuations have not been captured by B-factor or normal mode analyses. Thus, they are beyond the elastic regime, and it is essential to take into account chain connectivity for studying the function of kinesins.
q-bio.BM:We introduce a topology-based nonlinear network model of protein dynamics with the aim of investigating the interplay of spatial disorder and nonlinearity. We show that spontaneous localization of energy occurs generically and is a site-dependent process. Localized modes of nonlinear origin form spontaneously in the stiffest parts of the structure and display site-dependent activation energies. Our results provide a straightforward way for understanding the recently discovered link between protein local stiffness and enzymatic activity. They strongly suggest that nonlinear phenomena may play an important role in enzyme function, allowing for energy storage during the catalytic process.
q-bio.BM:We incorporate hydrodynamic interactions in a structure-based model of ubiquitin and demonstrate that the hydrodynamic coupling may reduce the peak force when stretching the protein at constant speed, especially at larger speeds. Hydrodynamic interactions are also shown to facilitate unfolding at constant force and inhibit stretching by fluid flows.
q-bio.BM:We demonstrate a new algorithm for finding protein conformations that minimize a non-bonded energy function. The new algorithm, called the difference map, seeks to find an atomic configuration that is simultaneously in two constraint spaces. The first constraint space is the space of atomic configurations that have a valid peptide geometry, while the second is the space of configurations that have a non-bonded energy below a given target. These two constraint spaces are used to define a deterministic dynamical system, whose fixed points produce atomic configurations in the intersection of the two constraint spaces. The rate at which the difference map produces low energy protein conformations is compared with that of a contemporary search algorithm, parallel tempering. The results indicate the difference map finds low energy protein conformations at a significantly higher rate then parallel tempering.
q-bio.BM:Vibrational energy transfer of the amide I mode of N-methylacetamide (NMA) is studied theoretically using the vibrational configuration interaction method. A quartic force field of NMA is constructed at the B3LYP/6-31G+(d) level of theory and its accuarcy is checked by comparing the resulting anharmonic frequencies with available theoretical and experimental values. Quantum dynamics calculations for the amide I mode excitation clarify the dominant energy transfer pathways, which sensitively depend on the anharmonic couplings among vibrational modes. A ratio of the anharmonic coupling to the frequency mismatch is employed to predict and interpret the dominant energy flow pathways.
q-bio.BM:It previously has been discovered that visible light irradiation of crystalline substrates can lead to enhancement of subsequent enzymatic reaction rates as sharply peaked oscillatory functions of irradiation time. The particular activating irradiation times can vary with source of a given enzyme and thus, presumably, its molecular structure. The experiments reported here demonstrate that the potential for this anomalous enzyme reaction rate enhancement can be transferred from one bacterial species to another coincident with transfer of the genetic determinant for the relevant enzyme. In particular, the effect of crystal-irradiated chloramphenicol on growth of bacterial strains in which a transferable R-factor DNA plasmid coding for chloramphenicol resistance was or was not present (S. panama R+, E. coli R+, and E. coli R-) was determined. Chloramphenicol samples irradiated 10, 35 and 60 sec produced increased growth rates (diminished inhibition) for the resistant S. panama and E. coli strains, while having no such effect on growth rate of the sensitive E. coli strain. Consistent with past findings, chloramphenicol samples irradiated 5, 30 and 55 sec produced decreased growth rates (increased inhibition) for all three strains.
q-bio.BM:Inherent structure theory is used to discover strong connections between simple characteristics of protein structure and the energy landscape of a Go model. The potential energies and vibrational free energies of inherent structures are highly correlated, and both reflect simple measures of networks of native contacts. These connections have important consequences for models of protein dynamics and thermodynamics.
q-bio.BM:The free-energy landscape of the alpha-helix of protein G is studied by means of metadynamics coupled with a solute tempering algorithm. Metadynamics allows to overcome large energy barriers, whereas solute tempering improves the sampling with an affordable computational effort. From the sampled free-energy surface we are able to reproduce a number of experimental observations, such as the fact that the lowest minimum corresponds to a globular conformation displaying some degree of beta-structure, that the helical state is metastable and involves only 65% of the chain. The calculations also show that the system populates consistently a pi-helix state and that the hydrophobic staple motif is present only in the free-energy minimum associated with the helices, and contributes to their stabilization. The use of metadynamics coupled with solute tempering results then particularly suitable to provide the thermodynamics of a short peptide, and its computational efficiency is promising to deal with larger proteins.
q-bio.BM:Using a time-dependent perturbation theory, vibrational energy relaxation (VER) of isotopically labeled amide I modes in cytochrome c solvated with water is investigated. Contributions to the VER are decomposed into two contributions from the protein and water. The VER pathways are visualized using radial and angular excitation functions for resonant normal modes. Key differences of VER among different amide I modes are demonstrated, leading to a detailed picture of the spatial anisotropy of the VER. The results support the experimental observation that amide I modes in proteins relax with sub picosecond timescales, while the relaxation mechanism turns out to be sensitive to the environment of the amide I mode.
q-bio.BM:Local minima and the saddle points separating them in the energy landscape are known to dominate the dynamics of biopolymer folding. Here we introduce a notion of a "folding funnel" that is concisely defined in terms of energy minima and saddle points, while at the same time conforming to a notion of a "folding funnel" as it is discussed in the protein folding literature.
q-bio.BM:Using magnetic tweezers to investigate the mechanical response of single chromatin fibers, we show that fibers submitted to large positive torsion transiently trap positive turns, at a rate of one turn per nucleosome. A comparison with the response of fibers of tetrasomes (the (H3-H4)2 tetramer bound with ~50 bp of DNA) obtained by depletion of H2A-H2B dimers, suggests that the trapping reflects a nucleosome chiral transition to a metastable form built on the previously documented righthanded tetrasome. In view of its low energy, <8 kT, we propose this transition is physiologically relevant and serves to break the docking of the dimers on the tetramer which in the absence of other factors exerts a strong block against elongation of transcription by the main RNA polymerase.
q-bio.BM:We perform extensive Monte Carlo simulations of a lattice model and the Go potential to investigate the existence of folding pathways at the level of contact cluster formation for two native structures with markedly different geometries. Our analysis of folding pathways revealed a common underlying folding mechanism, based on nucleation phenomena, for both protein models. However, folding to the more complex geometry (i.e. that with more non-local contacts) is driven by a folding nucleus whose geometric traits more closely resemble those of the native fold. For this geometry folding is clearly a more cooperative process.
q-bio.BM:In this study we evaluate, at full atomic detail, the folding processes of two small helical proteins, the B domain of protein A and the Villin headpiece. Folding kinetics are studied by performing a large number of ab initio Monte Carlo folding simulations using a single transferable all-atom potential. Using these trajectories, we examine the relaxation behavior, secondary structure formation, and transition-state ensembles (TSEs) of the two proteins and compare our results with experimental data and previous computational studies. To obtain a detailed structural information on the folding dynamics viewed as an ensemble process, we perform a clustering analysis procedure based on graph theory. Moreover, rigorous pfold analysis is used to obtain representative samples of the TSEs and a good quantitative agreement between experimental and simulated Fi-values is obtained for protein A. Fi-values for Villin are also obtained and left as predictions to be tested by future experiments. Our analysis shows that two-helix hairpin is a common partially stable structural motif that gets formed prior to entering the TSE in the studied proteins. These results together with our earlier study of Engrailed Homeodomain and recent experimental studies provide a comprehensive, atomic-level picture of folding mechanics of three-helix bundle proteins.
q-bio.BM:Strong experimental and theoretical evidence shows that transcription factors and other specific DNA-binding proteins find their sites using a two-mode search: alternating between 3D diffusion through the cell and 1D sliding along the DNA. We consider the role spatial effects in the mechanism on two different scales. First, we reconcile recent experimental findings by showing that the 3D diffusion of the transcription factor is often local, i.e. the transcription factor lands quite near its dissociation site. Second, we discriminate between two types of searches: global searches and local searches. We show that these searches differ significantly in average search time and the variability of search time. Using experimentally measured parameter values, we also show that 1D and 3D search is not optimally balanced, leading to much larger estimates of search time. Together, these results lead to a number of biological implications including suggestions of how prokaryotes and eukaryotes achieve rapid gene regulation and the relationship between the search mechanism and noise in gene expression.
q-bio.BM:Thermal shape fluctuations of grafted microtubules were studied using high resolution particle tracking of attached fluorescent beads. First mode relaxation times were extracted from the mean square displacement in the transverse coordinate. For microtubules shorter than 10 um, the relaxation times were found to follow an L^2 dependence instead of L^4 as expected from the standard wormlike chain model. This length dependence is shown to result from a complex length dependence of the bending stiffness which can be understood as a result of the molecular architecture of microtubules. For microtubules shorter than 5 um, high drag coefficients indicate contributions from internal friction to the fluctuation dynamics.
q-bio.BM:E. Coli. dihydrofolate reductase (DHFR) undergoes conformational transitions between the closed (CS) and occluded (OS) states which, respectively, describe whether the active site is closed or occluded by the Met20 loop. A sequence-based approach is used to identify a network of residues that represents the allostery wiring diagram. We also use a self-organized polymer model to monitor the kinetics of the CS->OS and the reverse transitions. a sliding motion of Met20 loop is observed. The residues that facilitate the Met20 loop motion are part of the network of residues that transmit allosteric signals during the CS->OS transition.
q-bio.BM:A model is presented to describe the nucleotide and repeat addition processivity by the telomerase. In the model, the processive nucleotide addition is implemented on the basis of two requirements: One is that stem IV loop stimulates the chemical reaction of nucleotide incorporation, and the other one is the existence of an ssRNA-binding site adjacent to the polymerase site that has a high affinity for the unpaired base of the template. The unpairing of DNA:RNA hybrid after the incorporation of the nucleotide paired with the last base on the template, which is the prerequisite for repeat addition processivity, is caused by a force acting on the primer. The force is resulted from the unfolding of stem III pseudoknot that is induced by the swinging of stem IV loop towards the nucleotide-bound polymerase site. Based on the model, the dynamics of processive nucleotide and repeat additions by Tetrahymena telomerase are quantitatively studied, which give good explanations to the previous experimental results. Moreover, some predictions are presented. In particular, it is predicted that the repeat addition processivity is mainly determined by the difference between the free energy required to disrupt the DNA:RNA hybrid and that required to unfold the stem III pseudoknot, with the large difference corresponding to a low repeat addition processivity while the small one corresponding to a high repeat addition processivity.
q-bio.BM:Small single-domain proteins often exhibit only a single free-energy barrier, or transition state, between the denatured and the native state. The folding kinetics of these proteins is usually explored via mutational analysis. A central question is which structural information on the transition state can be derived from the mutational data. In this article, we model and structurally interpret mutational Phi-values for two small beta-sheet proteins, the PIN and the FBP WW domain. The native structure of these WW domains comprises two beta-hairpins that form a three-stranded beta-sheet. In our model, we assume that the transition state consists of two conformations in which either one of the hairpins is formed. Such a transition state has been recently observed in Molecular Dynamics folding-unfolding simulations of a small designed three-stranded beta-sheet protein. We obtain good agreement with the experimental data (i) by splitting up the mutation-induced free-energy changes into terms for the two hairpins and for the small hydrophobic core of the proteins, and (ii) by fitting a single parameter, the relative degree to which hairpin 1 and 2 are formed in the transition state. The model helps to understand how mutations affect the folding kinetics of WW domains, and captures also negative Phi-values that have been difficult to interpret.
q-bio.BM:Simple theoretical concepts and models have been helpful to understand the folding rates and routes of single-domain proteins. As reviewed in this article, a physical principle that appears to underly these models is loop closure.
q-bio.BM:The isotopic composition, for example, 14C/12C, 13C/12C, 2H/1H, 15N/14N and 18O/16O, of the elements of matter is heterogeneous. It is ruled by physical, chemical and biological mechanisms. Isotopes can be employed to follow the fate of mineral and organic compounds during biogeochemical transformations. The determination of the isotopic composition of organic substances occurring at trace level in very complex mixtures such as sediments, soils and blood, has been made possible during the last 20 years due to the rapid development of molecular level isotopic techniques. After a brief glance at pioneering studies revealing isotopic breakthroughs at the molecular and intramolecular levels, this paper reviews selected applications of compound-specific isotope analysis in various scientific fields.
q-bio.BM:The conformational dynamics of a single protein molecule in a shear flow is investigated using Brownian dynamics simulations. A structure-based coarse grained model of a protein is used. We consider two proteins, ubiquitin and integrin, and find that at moderate shear rates they unfold through a sequence of metastable states - a pattern which is distinct from a smooth unraveling found in homopolymers. Full unfolding occurs only at very large shear rates. Furthermore, the hydrodynamic interactions between the amino acids are shown to hinder the shear flow unfolding. The characteristics of the unfolding process depend on whether a protein is anchored or not, and if it is, on the choice of an anchoring point.
q-bio.BM:We demonstrate that a common-line method can assemble a 3D oversampled diffracted intensity distribution suitable for high-resolution structure solution from a set of measured 2D diffraction patterns, as proposed in experiments with an X-ray free electron laser (XFEL) (Neutze {\it et al.}, 2000). Even for a flat Ewald sphere, we show how the ambiguities due to Friedel's Law may be overcome. The method breaks down for photon counts below about 10 per detector pixel, almost 3 orders of magnitude higher than expected for scattering by a 500 kDa protein with an XFEL beam focused to a 0.1 micron diameter spot. Even if 10**3 orientationally similar diffraction patterns could be identified and added to reach the requisite photon count per pixel, the need for about 10**6 orientational classes for high-resolution structure determination suggests that about ~ 10**9 diffraction patterns must be recorded. Assuming pulse and read-out rates of 100 Hz, such measurements would require ~ 10**7 seconds, i.e. several months of continuous beam time.
q-bio.BM:Background. All-atom crystallographic refinement of proteins is a laborious manually driven procedure, as a result of which, alternative and multiconformer interpretations are not routinely investigated.   Results. We describe efficient loop sampling procedures in Rappertk and demonstrate that single loops in proteins can be automatically and accurately modelled with few positional restraints. Loops constructed with a composite CNS/Rappertk protocol consistently have better Rfree than those with CNS alone. This approach is extended to a more realistic scenario where there are often large positional uncertainties in loops along with small imperfections in the secondary structural framework. Both ensemble and collection methods are used to estimate the structural heterogeneity of loop regions.   Conclusion. Apart from benchmarking Rappertk for the all-atom protein refinement task, this work also demonstrates its utility in both aspects of loop modelling - building a single conformer and estimating structural heterogeneity the loops can exhibit.
q-bio.BM:Background. Dramatic increases in RNA structural data have made it possible to recognize its conformational preferences much better than a decade ago. This has created an opportunity to use discrete restraint-based conformational sampling for modelling RNA and automating its crystallographic refinement. Results. All-atom sampling of entire RNA chains, termini and loops is achieved using the Richardson RNA backbone rotamer library and an unbiased distribution for glycosidic dihedral angle. Sampling behaviour of Rappertk on a diverse dataset of RNA chains under varying spatial restraints is benchmarked. The iterative composite crystallographic refinement protocol developed here is demonstrated to outperform CNS-only refinement on parts of tRNA(Asp) structure. Conclusion. This work opens exciting possibilities for further work in RNA modelling and crystallography.
q-bio.BM:DNA torsion dynamics is essential in the transcription process; simple models for it have been proposed by several authors, in particular Yakushevich (Y model). These are strongly related to models of DNA separation dynamics such as the one first proposed by Peyrard and Bishop (and developed by Dauxois, Barbi, Cocco and Monasson among others), but support topological solitons. We recently developed a ``composite'' version of the Y model, in which the sugar-phosphate group and the base are described by separate degrees of freedom. This at the same time fits experimental data better than the simple Y model, and shows dynamical phenomena, which are of interest beyond DNA dynamics. Of particular relevance are the mechanism for selecting the speed of solitons by tuning the physical parameters of the non linear medium and the hierarchal separation of the relevant degrees of freedom in ``master'' and ``slave''. These mechanisms apply not only do DNA, but also to more general macromolecules, as we show concretely by considering polyethylene.
q-bio.BM:The Caspar-Klug classification of viruses whose protein shell, called viral capsid, exhibits icosahedral symmetry, has recently been extended to incorporate viruses whose capsid proteins are exclusively organised in pentamers. The approach, named `Viral Tiling Theory', is inspired by the theory of quasicrystals, where aperiodic Penrose tilings enjoy 5-fold and 10-fold local symmetries. This paper analyzes the extent to which this classification approach informs dynamical properties of the viral capsids, in particular the pattern of Raman active modes of vibrations, which can be observed experimentally.
q-bio.BM:Position-specific scoring matrices (PSSMs) are useful for detecting weak homology in protein sequence analysis, and they are thought to contain some essential signatures of the protein families. In order to elucidate what kind of ingredients constitute such family-specific signatures, we apply singular value decomposition to a set of PSSMs and examine the properties of dominant right and left singular vectors. The first right singular vectors were correlated with various amino acid indices including relative mutability, amino acid composition in protein interior, hydropathy, or turn propensity, depending on proteins. A significant correlation between the first left singular vector and a measure of site conservation was observed. It is shown that the contribution of the first singular component to the PSSMs act to disfavor potentially but falsely functionally important residues at conserved sites. The second right singular vectors were highly correlated with hydrophobicity scales, and the corresponding left singular vectors with contact numbers of protein structures. It is suggested that sequence alignment with a PSSM is essentially equivalent to threading supplemented with functional information. The presented method may be used to separate functionally important sites from structurally important ones, and thus it may be a useful tool for predicting protein functions.
q-bio.BM:A method to search for local structural similarities in proteins at atomic resolution is presented. It is demonstrated that a huge amount of structural data can be handled within a reasonable CPU time by using a conventional relational database management system with appropriate indexing of geometric data. This method, which we call geometric indexing, can enumerate ligand binding sites that are structurally similar to sub-structures of a query protein among more than 160,000 possible candidates within a few hours of CPU time on an ordinary desktop computer. After detecting a set of high scoring ligand binding sites by the geometric indexing search, structural alignments at atomic resolution are constructed by iteratively applying the Hungarian algorithm, and the statistical significance of the final score is estimated from an empirical model based on a gamma distribution. Applications of this method to several protein structures clearly shows that significant similarities can be detected between local structures of non-homologous as well as homologous proteins.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum stack-length. We show that the numbers of $k$-noncrossing structures without isolated base pairs are significantly smaller than the number of all $k$-noncrossing structures. In particular we prove that the number of 3- and 4-noncrossing RNA structures with stack-length $\ge 2$ is for large $n$ given by $311.2470 \frac{4!}{n(n-1)...(n-4)}2.5881^n$ and $1.217\cdot 10^{7} n^{-{21/2}} 3.0382^n$, respectively. We furthermore show that for $k$-noncrossing RNA structures the drop in exponential growth rates between the number of all structures and the number of all structures with stack-size $\ge 2$ increases significantly. Our results are of importance for prediction algorithms for pseudoknot-RNA and provide evidence that there exist neutral networks of RNA pseudoknot structures.
q-bio.BM:Network science is already making an impact on the study of complex systems and offers a promising variety of tools to understand their formation and evolution (1-4) in many disparate fields from large communication networks (5,6), transportation infrastructures (7) and social communities (8,9) to biological systems (1,10,11). Even though new highthroughput technologies have rapidly been generating large amounts of genomic data, drug design has not followed the same development, and it is still complicated and expensive to develop new single-target drugs. Nevertheless, recent approaches suggest that multi-target drug design combined with a network-dependent approach and large-scale systems-oriented strategies (12-14) create a promising framework to combat complex multigenetic disorders like cancer or diabetes. Here, we investigate the human network corresponding to the interactions between all US approved drugs and human therapies, defined by known drug-therapy relationships. Our results show that the key paths in this network are shorter than three steps, indicating that distant therapies are separated by a surprisingly low number of chemical compounds. We also identify a sub-network composed by drugs with high centrality measures (15), which represent the structural back-bone of the drug-therapy system and act as hubs routing information between distant parts of the network. These findings provide for the first time a global map of the largescale organization of all known drugs and associated therapies, bringing new insights on possible strategies for future drug development. Special attention should be given to drugs which combine the two properties of (a) having a high centrality value and (b) acting on multiple targets.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum arc- and stack-length. That is, we study the numbers of RNA pseudoknot structures with arc-length $\ge 3$, stack-length $\ge \sigma$ and in which there are at most $k-1$ mutually crossing bonds, denoted by ${\sf T}_{k,\sigma}^{[3]}(n)$. In particular we prove that the numbers of 3, 4 and 5-noncrossing RNA structures with arc-length $\ge 3$ and stack-length $\ge 2$ satisfy ${\sf T}_{3,2}^{[3]}(n)^{}\sim K_3 n^{-5} 2.5723^n$, ${\sf T}^{[3]}_{4,2}(n)\sim K_4 n^{-{21/2}} 3.0306^n$, and ${\sf T}^{[3]}_{5,2}(n)\sim K_5 n^{-18} 3.4092^n$, respectively, where $K_3,K_4,K_5$ are constants. Our results are of importance for prediction algorithms for RNA pseudoknot structures.
q-bio.BM:We have developed a new extended replica exchange method to study thermodynamics of a system in the presence of external force. Our idea is based on the exchange between different force replicas to accelerate the equilibrium process. We have shown that the refolding pathways of single ubiquitin depend on which terminus is fixed. If the N-end is fixed then the folding pathways are different compared to the case when both termini are free, but fixing the C-terminal does not change them. Surprisingly, we have found that the anchoring terminal does not affect the pathways of individual secondary structures of three-domain ubiquitin, indicating the important role of the multi-domain construction. Therefore, force-clamp experiments, in which one end of a protein is kept fixed, can probe the refolding pathways of a single free-end ubiquitin if one uses either the poly-ubiquitin or a single domain with the C-terminus anchored. However, it is shown that anchoring one end does not affect refolding pathways of the titin domain I27, and the force-clamp spectroscopy is always capable to predict folding sequencing of this protein. We have obtained the reasonable estimate for unfolding barrier of ubiqutin. The linkage between residue Lys48 and the C-terminal of ubiquitin is found to have the dramatic effect on the location of the transition state along the end-to-end distance reaction coordinate, but the multi-domain construction leaves the transition state almost unchanged. We have found that the maximum force in the force-extension profile from constant velocity force pulling simulations depends on temperature nonlinearly. However, for some narrow temperature interval this dependence becomes linear, as have been observed in recent experiments.
q-bio.BM:A construction method for duplex cage structures with icosahedral sym- metry made out of single-stranded DNA molecules is presented and applied to an icosidodecahedral cage. It is shown via a mixture of analytic and computer techniques that there exist realisations of this graph in terms of two circular DNA molecules. These blueprints for the organisation of a cage structure with a noncrystallographic symmetry may assist in the design of containers made from DNA for applications in nanotechnology.
q-bio.BM:We analyzed folding routes predicted by a variational model in terms of a generalized formalism of the capillarity scaling theory for 28 two-state proteins. The scaling exponent ranged from 0.2 to 0.45 with an average of 0.33. This average value corresponds to packing of rigid objects.That is, on average the folded core of the nucleus is found to be relatively diffuse. We also studied the growth of the folding nucleus and interface along the folding route in terms of the density or packing fraction. The evolution of the folded core and interface regions can be classified into three patterns of growth depending on how the growth of the folded core is balanced by changes in density of the interface. Finally, we quantified the diffuse versus polarized structure of the critical nucleus through direct calculation of the packing fraction of the folded core and interface regions. Our results support the general picture of describing protein folding as the capillarity-like growth of folding nuclei.
q-bio.BM:Biomolecular structures are assemblies of emergent anisotropic building modules such as uniaxial helices or biaxial strands. We provide an approach to understanding a marginally compact phase of matter that is occupied by proteins and DNA. This phase, which is in some respects analogous to the liquid crystal phase for chain molecules, stabilizes a range of shapes that can be obtained by sequence-independent interactions occurring intra- and intermolecularly between polymeric molecules. We present a singularityfree self-interaction for a tube in the continuum limit and show that this results in the tube being positioned in the marginally compact phase. Our work provides a unified framework for understanding the building blocks of biomolecules.
q-bio.BM:The mean time required by a transcription factor (TF) or an enzyme to find a target in the nucleus is of prime importance for the initialization of transcription, gene activation or the start of DNA repair. We obtain new estimates for the mean search time when the TF or enzyme, confined to the cell nucleus, can switch from a one dimensional motion along the DNA and a free Brownian regime inside the crowded nucleus. We give analytical expressions for the mean time the particle stays bound to the DNA, $\tau_{DNA}$, and the mean time it diffuses freely, $\tau_{free}$. Contrary to previous results but in agreement with experimental data, we find a factor $\tau_{DNA} \approx 3.7 \tau_{free}$ for the Lac-I TF. The formula obtained for the time required to bind to a target site is found to be coherent with observed data. We also conclude that a higher DNA density leads to a more efficient search process.
q-bio.BM:We develop coarse-grained models that describe the dynamic encapsidation of functionalized nanoparticles by viral capsid proteins. We find that some forms of cooperative interactions between protein subunits and nanoparticles can dramatically enhance rates and robustness of assembly, as compared to the spontaneous assembly of subunits into empty capsids. For large core-subunit interactions, subunits adsorb onto core surfaces en masse in a disordered manner, and then undergo a cooperative rearrangement into an ordered capsid structure. These assembly pathways are unlike any identified for empty capsid formation. Our models can be directly applied to recent experiments in which viral capsid proteins assemble around the functionalized inorganic nanoparticles [Sun et al., Proc. Natl. Acad. Sci (2007) 104, 1354]. In addition, we discuss broader implications for understanding the dynamic encapsidation of single-stranded genomic molecules during viral replication and for developing multicomponent nanostructured materials.
q-bio.BM:The total conformational energy is assumed to consist of pairwise interaction energies between atoms or residues, each of which is expressed as a product of a conformation-dependent function (an element of a contact matrix, C-matrix) and a sequence-dependent energy parameter (an element of a contact energy matrix, E-matrix). Such pairwise interactions in proteins force native C-matrices to be in a relationship as if the interactions are a Go-like potential [N. Go, Annu. Rev. Biophys. Bioeng. 12. 183 (1983)] for the native C-matrix, because the lowest bound of the total energy function is equal to the total energy of the native conformation interacting in a Go-like pairwise potential. This relationship between C- and E-matrices corresponds to (a) a parallel relationship between the eigenvectors of the C- and E-matrices and a linear relationship between their eigenvalues, and (b) a parallel relationship between a contact number vector and the principal eigenvectors of the C- and E-matrices; the E-matrix is expanded in a series of eigenspaces with an additional constant term, which corresponds to a threshold of contact energy that approximately separates native contacts from non-native ones. These relationships are confirmed in 182 representatives from each family of the SCOP database by examining inner products between the principal eigenvector of the C-matrix, that of the E-matrix evaluated with a statistical contact potential, and a contact number vector. In addition, the spectral representation of C- and E-matrices reveals that pairwise residue-residue interactions, which depends only on the types of interacting amino acids but not on other residues in a protein, are insufficient and other interactions including residue connectivities and steric hindrance are needed to make native structures the unique lowest energy conformations.
q-bio.BM:We consider an elastic rod model for twisted DNA in the plectonemic regime. The molecule is treated as an impenetrable tube with an effective, adjustable radius. The model is solved analytically and we derive formulas for the contact pressure, twisting moment and geometrical parameters of the supercoiled region. We apply our model to magnetic tweezer experiments of a DNA molecule subjected to a tensile force and a torque, and extract mechanical and geometrical quantities from the linear part of the experimental response curve. These reconstructed values are derived in a self-contained manner, and are found to be consistent with those available in the literature.
q-bio.BM:Nicodemi and Prisco recently proposed a model for X-chromosome inactivation in mammals, explaining this phenomenon in terms of a spontaneous symmetry-breaking mechanism [{\it Phys. Rev. Lett.} 99 (2007), 108104]. Here we provide a mean-field version of their model.
q-bio.BM:We present a theoretical investigation on possible selection of olfactory receptors (ORs) as sensing components of nanobiosensors. Accordingly, we generate the impedance spectra of the rat OR I7 in the native and activated state and analyze their differences. In this way, we connect the protein morphological transformation, caused by the sensing action, with its change of electrical impedance. The results are compared with those obtained by studying the best known protein of the GPCR family: bovine rhodopsin. Our investigations indicate that a change in morphology goes with a change in impedance spectrum mostly associated with a decrease of the static impedance up to about 60 % of the initial value, in qualitative agreement with existing experiments on rat OR I7. The predictiveness of the model is tested successfully for the case of recent experiments on bacteriorhodopsin. The present results point to a promising development of a new class of nanobiosensors based on the electrical properties of GPCR and other sensing proteins.
q-bio.BM:How molecular motors like Kinesin regulates the affinity to the rail protein in the process of ATP hydrolysis remains to be uncovered. To understand the regulation mechanism, we investigate the structural fluctuation of KIF1A in different nucleotide states that are realized in the ATP hydrolysis process by molecular dynamics simulations of Go-like model. We found that "alpha4 helix", which is a part of the microtubule (MT) binding site, changes its fluctuation systematically according to the nucleotide states. In particular, the frequency of large fluctuations of alpha4 strongly correlates with the affinity of KIF1A for microtubule. We also show how the strength of the thermal fluctuation and the interaction with the nucleotide affect the dynamics of microtubule binding site. These results suggest that KIF1A regulates the affinity to MT by changing the flexibility of alpha4 helix according to the nucleotide states.
q-bio.BM:A theoretical framework is developed to study the dynamics of protein folding. The key insight is that the search for the native protein conformation is influenced by the rate r at which external parameters, such as temperature, chemical denaturant or pH, are adjusted to induce folding. A theory based on this insight predicts that (1) proteins with non-funneled energy landscapes can fold reliably to their native state, (2) reliable folding can occur as an equilibrium or out-of-equilibrium process, and (3) reliable folding only occurs when the rate r is below a limiting value, which can be calculated from measurements of the free energy. We test these predictions against numerical simulations of model proteins with a single energy scale.
q-bio.BM:Despite the spontaneity of some in vitro protein folding reactions, native folding in vivo often requires the participation of barrel-shaped multimeric complexes known as chaperonins. Although it has long been known that chaperonin substrates fold upon sequestration inside the chaperonin barrel, the precise mechanism by which confinement within this space facilitates folding remains unknown. In this study, we examine the possibility that the chaperonin mediates a favorable reorganization of the solvent for the folding reaction. We begin by discussing the effect of electrostatic charge on solvent-mediated hydrophobic forces in an aqueous environment. Based on these initial physical arguments, we construct a simple, phenomenological theory for the thermodynamics of density and hydrogen bond order fluctuations in liquid water. Within the framework of this model, we investigate the effect of confinement within a chaperonin-like cavity on the configurational free energy of water by calculating solvent free energies for cavities corresponding to the different conformational states in the ATP- driven catalytic cycle of the prokaryotic chaperonin GroEL. Our findings suggest that one function of chaperonins may be to trap unfolded proteins and subsequently expose them to a micro-environment in which the hydrophobic effect, a crucial thermodynamic driving force for folding, is enhanced.
q-bio.BM:We present a top-down approach to the study of the dynamics of icosahedral virus capsids, in which each protein is approximated by a point mass. Although this represents a rather crude coarse-graining, we argue that it highlights several generic features of vibrational spectra which have been overlooked so far. We furthermore discuss the consequences of approximate inversion symmetry as well as the role played by Viral Tiling Theory in the study of virus capsid vibrations.
q-bio.BM:We study the coupled dynamics of primary and secondary structure formation (i.e. slow genetic sequence selection and fast folding) in the context of a solvable microscopic model that includes both short-range steric forces and and long-range polarity-driven forces. Our solution is based on the diagonalization of replicated transfer matrices, and leads in the thermodynamic limit to explicit predictions regarding phase transitions and phase diagrams at genetic equilibrium. The predicted phenomenology allows for natural physical interpretations, and finds satisfactory support in numerical simulations.
q-bio.BM:We analyze the thermodynamic properties of a simplified model for folded RNA molecules recently studied by G. Vernizzi, H. Orland, A. Zee (in {\it Phys. Rev. Lett.} {\bf 94} (2005) 168103). The model consists of a chain of one-flavor base molecules with a flexible backbone and all possible pairing interactions equally allowed. The spatial pseudoknot structure of the model can be efficiently studied by introducing a $N \times N$ hermitian random matrix model at each chain site, and associating Feynman diagrams of these models to spatial configurations of the molecules. We obtain an exact expression for the topological expansion of the partition function of the system. We calculate exact and asymptotic expressions for the free energy, specific heat, entanglement and chemical potential and study their behavior as a function of temperature. Our results are consistent with the interpretation of $1/N$ as being a measure of the concentration of $\rm{Mg}^{++}$ in solution.
q-bio.BM:Blueprints of polyhedral cages with icosahedral symmetry made of circular DNA molecules are provided. The basic rule is that every edge of the cage is met twice in opposite directions by the DNA strand, and vertex junctions are realised by a set of admissible junction types. As nanocontainers for cargo storage and delivery, the icosidodecahedral cages are of special interest as they have the largest volume per surface ratio of all cages discussed here.
q-bio.BM:Group theoretical arguments combined with normal mode analysis techniques are applied to a coarse-grained approximation of icosahedral viral capsids which incorporates areas of variable flexibility. This highlights a remarkable structure of the low-frequency spectrum in this approximation, namely the existence of a plateau of 24 near zero-modes with universal group theory content.
q-bio.BM:The low-frequency Raman spectra of Na-and Cs-DNA water solutions have been studied to determine the mode of counterion vibrations with respect to phosphate groups of the DNA double helix. The obtained spectra are characterized by the water band near 180 cm-1 and by several DNA bands near 100 cm-1. The main difference between Na- and Cs-DNA spectra is observed in case of the band 100 cm-1. In Cs-DNA spectra this band has about twice higher intensity than in Na-DNA spectra. The comparison of obtained spectra with the calculated frequencies of Na- and Cs-DNA conformational vibrations [Perepelytsya S.M., Volkov S.N. Eur. Phys. J. E. 24, 261 (2007)] show that the band 100 cm-1 in the spectra of Cs-DNA is formed by the modes of both H-bond stretching vibrations and vibrations of caesium counterions, while in Na-DNA spectra the band 100 cm-1 is formed by the mode of H-bond stretching vibrations only. The modes of sodium counterion vibrations have a frequency 180 cm-1, and they do not rise above the water band. Thus, the increase in intensity of the band 100 cm-1 in the spectra of Cs-DNA as compared with Na-DNA is caused by the mode of ion-phosphate vibrations.
q-bio.BM:The Ca-sensitive regulatory switch of cardiac muscle is a paradigmatic example of protein assemblies that communicate ligand binding through allosteric change. The switch is a dimeric complex of troponin C (TnC), an allosteric sensor for Ca, and troponin I (TnI), an allosteric reporter. Time-resolved equilibrium FRET measurements suggest that the switch activates in two steps: a TnI-independent Ca-priming step followed by TnI-dependent opening. To resolve the mechanistic role of TnI in activation we performed stopped-flow FRET measurements of activation following rapid addition of a lacking component (Ca or TnI) and deactivation following rapid chelation of Ca. The time-resolved measurements, stopped-flow measurements, and Ca-titration measurements were globally analyzed in terms of a new quantitative dynamic model of TnC-TnI allostery. The analysis provided a mesoscopic parameterization of distance changes, free energy changes, and transition rates among the accessible coarse-grained states of the system. The results reveal (i) the Ca-induced priming step, which precedes opening, is the rate limiting step in activation, (ii) closing is the rate limiting step in deactivation, (iii) TnI induces opening, (iv) an incompletely deactivated population when regulatory Ca is not bound, which generates an accessory pathway of activation, and (v) incomplete activation by Ca--when regulatory Ca is bound, a 3:2 mixture of dynamically inter-converting open (active) and primed-closed (partially active) conformers is observed (15 C). Temperature-dependent stopped-flow FRET experiments provide a near complete thermo-kinetic parametrization of opening. <Abstract Truncated>
q-bio.BM:We explore the use of a top-down approach to analyse the dynamics of icosahedral virus capsids and complement the information obtained from bottom-up studies of viral vibrations available in the literature. A normal mode analysis based on protein association energies is used to study the frequency spectrum, in which we reveal a universal plateau of low-frequency modes shared by a large class of Caspar-Klug capsids. These modes break icosahedral symmetry and are potentially relevant to the genome release mechanism. We comment on the role of viral tiling theory in such dynamical considerations.
q-bio.BM:In many cases, transcriptional regulation involves the binding of transcription factors at sites on the DNA that are not immediately adjacent to the promoter of interest. This action at a distance is often mediated by the formation of DNA loops: Binding at two or more sites on the DNA results in the formation of a loop, which can bring the transcription factor into the immediate neighborhood of the relevant promoter. Though there have been a variety of insights into the combinatorial aspects of transcriptional control, the mechanism of DNA looping as an agent of combinatorial control in both prokaryotes and eukaryotes remains unclear. We use single-molecule techniques to dissect DNA looping in the lac operon. In particular, we measure the propensity for DNA looping by the Lac repressor as a function of the concentration of repressor protein and as a function of the distance between repressor binding sites. As with earlier single-molecule studies, we find (at least) two distinct looped states and demonstrate that the presence of these two states depends both upon the concentration of repressor protein and the distance between the two repressor binding sites. We find that loops form even at interoperator spacings considerably shorter than the DNA persistence length, without the intervention of any other proteins to prebend the DNA. The concentration measurements also permit us to use a simple statistical mechanical model of DNA loop formation to determine the free energy of DNA looping, or equivalently, the J-factor for looping.
q-bio.BM:We apply a simulational proxy of the phi-value analysis and perform extensive mutagenesis experiments to identify the nucleating residues in the folding reactions of two small lattice Go polymers with different native geometries. These results are compared with those obtained from an accurate analysis based on the reaction coordinate folding probability Pfold, and on structural clustering methods. For both protein models, the transition state ensemble is rather heterogeneous and splits-up into structurally different populations. For the more complex geometry the identified subpopulations are actually structurally disjoint. For the less complex native geometry we found a broad transition state with microscopic heterogeneity. For both geometries, the identification of the folding nucleus via the Pfold analysis agrees with the identification of the folding nucleus carried out with the phi-value analysis. For the most complex geometry, however, the apllied methodologies give more consistent results than for the more local geometry. The study of the transition state' structure reveals that the nucleus residues are not necessarily fully native in the transition state. Indeed, it is only for the more complex geometry that two of the five critical residues show a considerably high probability of having all its native bonds formed in the transition state. Therefore, one concludes that in general the phi-value correlates with the acceleration/deceleration of folding induced by mutation, rather than with the degree of nativeness of the transition state, and that the traditional interpretation of phi-values may provide a more realistic picture of the structure of the transition state only for more complex native geometries.
q-bio.BM:Associative memory Hamiltonian structure prediction potentials are not overly rugged, thereby suggesting their landscapes are like those of actual proteins. In the present contribution we show how basin-hopping global optimization can identify low-lying minima for the corresponding mildly frustrated energy landscapes. For small systems the basin-hopping algorithm succeeds in locating both lower minima and conformations closer to the experimental structure than does molecular dynamics with simulated annealing. For large systems the efficiency of basin-hopping decreases for our initial implementation, where the steps consist of random perturbations to the Cartesian coordinates. We implemented umbrella sampling using basin-hopping to further confirm when the global minima are reached. We have also improved the energy surface by employing bioinformatic techniques for reducing the roughness or variance of the energy surface. Finally, the basin-hopping calculations have guided improvements in the excluded volume of the Hamiltonian, producing better structures. These results suggest a novel and transferable optimization scheme for future energy function development.
q-bio.BM:The performance of single folding predictors and combination scores is critically evaluated. We test mean packing, mean pairwise energy and the new index gVSL2 on a dataset of 743 folded proteins and 81 natively unfolded proteins. These predictors have an individual performance comparable or even better than other proposed methods. We introduce here a strictly unanimous score S_{SU} that combines them but leaves undecided those sequences differently classified by two single predictors. The performance of the single predictors on a dataset purged from the proteins left unclassified by S_{SU}, significantly increases, indicating that unclassified proteins are mainly false predictions. Amino acid composition is the main determinant considered by these predictors, therefore unclassified proteins have a composition compatible with both folded and unfolded status. This is why purging a dataset from these ambiguous proteins increases the performance of single predictors. The percentage of proteins predicted as natively unfolded by S_{SU} in the three kingdoms are: 4.1% for Bacteria, 1.0% for Archaea and 20.0% for Eukarya; compatible with previous determinations. Evidence is given of a scaling law relating the number of natively unfolded proteins with the total number of proteins in a genome; a first estimate of the critical exponent is 1.95 +- 0.21
q-bio.BM:The sequence-dependent structural variability and conformational dynamics of DNA play pivotal roles in many biological milieus, such as in the site-specific binding of transcription factors to target regulatory elements. To better understand DNA structure, function, and dynamics in general, and protein-DNA recognition in the 'kB' family of genetic regulatory elements in particular, we performed molecular dynamics simulations of a 20-base pair DNA encompassing a cognate kB site recognized by the proto-oncogenic 'c-Rel' subfamily of NF-kB transcription factors. Simulations of the kB DNA in explicit water were extended to microsecond duration, providing a broad, atomically-detailed glimpse into the structural and dynamical behavior of double helical DNA over many timescales. Of particular note, novel (and structurally plausible) conformations of DNA developed only at the long times sampled in this simulation -- including a peculiar state arising at ~ 0.7 us and characterized by cross-strand intercalative stacking of nucleotides within a longitudinally-sheared base pair, followed (at ~ 1 us) by spontaneous base flipping of a neighboring thymine within the A-rich duplex. Results and predictions from the us-scale simulation include implications for a dynamical NF-kB recognition motif, and are amenable to testing and further exploration via specific experimental approaches that are suggested herein.
q-bio.BM:The interaction cutoff contribution to the ruggedness of protein-protein energy landscape (the artificial ruggedness) is studied in terms of relative energy fluctuations for 1/r^n potentials based on a simplistic model of a protein complex. Contradicting the principle of minimal frustration, the artificial ruggedness exists for short cutoffs and gradually disappears with the cutoff increase. The critical values of the cutoff were calculated for each of eleven popular power-type potentials with n=0-9, 12 and for two thresholds of 5% and 10%. The artificial ruggedness decreases to tolerable thresholds for cutoffs longer than the critical ones. The results showed that for both thresholds the critical cutoff is a non-monotonic function of the potential power n. The functions reach the maximum at n=3-4 and then decrease with the increase of the potential power. The difference between two cutoffs for 5% and 10% artificial ruggedness becomes negligible for potentials decreasing faster than 1/r^12. The results suggest that cutoffs longer than critical ones can be recommended for protein-protein potentials.
q-bio.BM:Mechanical characterization of protein molecules has played a role on gaining insight into the biological functions of proteins, since some proteins perform the mechanical function. Here, we present the mesoscopic model of biological protein materials composed of protein crystals prescribed by Go potential for characterization of elastic behavior of protein materials. Specifically, we consider the representative volume element (RVE) containing the protein crystals represented by alpha-carbon atoms, prescribed by Go potential, with application of constant normal strain to RVE. The stress-strain relationship computed from virial stress theory provides the nonlinear elastic behavior of protein materials and their mechanical properties such as Young's modulus, quantitatively and/or qualitatively comparable to mechanical properties of biological protein materials obtained from experiments and/or atomistic simulations. Further, we discuss the role of native topology on the mechanical properties of protein crystals. It is shown that parallel strands (hydrogen bonds in parallel) enhances the mechanical resilience of protein materials.
q-bio.BM:Unfolded proteins may contain native or non-native residual structure, which has important implications for the thermodynamics and kinetics of folding as well as for misfolding and aggregation diseases. However, it has been universally accepted that residual structure should not affect the global size scaling of the denatured chain, which obeys the statistics of random coil polymers. Here we use a single-molecule optical technique, fluorescence correlation spectroscopy, to probe the denatured state of set of repeat proteins containing an increasing number of identical domains, from two to twenty. The availability of this set allows us to obtain the scaling law for the unfolded state of these proteins, which turns out to be unusually compact, strongly deviating from random-coil statistics. The origin of this unexpected behavior is traced to the presence of extensive non-native polyproline II helical structure, which we localize to specific segments of the polypeptide chain. We show that the experimentally observed effects of PPII on the size scaling of the denatured state can be well-described by simple polymer models. Our findings suggest an hitherto unforeseen potential of non-native structure to induce significant compaction of denatured proteins, affecting significantly folding pathways and kinetics.
q-bio.BM:Renaturation and hybridization reactions lead to the pairing of complementary single-stranded nucleic acids. We present here a theoretical investigation of the mechanism of these reactions in vitro under thermal conditions (dilute solutions of single-stranded chains, in the presence of molar concentrations of monovalent salts and at elevated temperatures). The mechanism follows a Kramers' process, whereby the complementary chains overcome a potential barrier through Brownian motion. The barrier originates from a single rate-limiting nucleation event in which the first complementary base pairs are formed. The reaction then proceeds through a fast growth of the double helix. For the DNA of bacteriophages T7, T4 and $\phi$X174 as well as for Escherichia coli DNA, the bimolecular rate $k_2$ of the reaction increases as a power law of the average degree of polymerization $<N>$ of the reacting single- strands: $k_2 \prop <N>^\alpha$. This relationship holds for $100 \leq <N> \leq 50 000$ with an experimentally determined exponent $\alpha = 0.51 \pm 0.01$. The length dependence results from a thermodynamic excluded-volume effect. The reacting single-stranded chains are predicted to be in universal good solvent conditions, and the scaling law is determined by the relevant equilibrium monomer contact probability. The value theoretically predicted for the exponent is $\alpha = 1-\nu \theta_2$, where $\nu$ is Flory's swelling exponent ($nu approx 0.588$) and $\theta_2$ is a critical exponent introduced by des Cloizeaux ($\theta_2 \approx 0.82$), yielding $\alpha = 0.52 \pm 0.01$, in agreement with the experimental results.
q-bio.BM:This article is interested in the origin of the genetic code, it puts forward a scenario of a simultaneous selection of the bases and amino acids and setting up of a correlation between them. Each amino acid is associated with a pair of its own kind, called the binding pair and each binding pair is associated with the codon(s) corresponding to the same amino acid. An explanation is also proposed about the origin of the start and stop codons.
q-bio.BM:We develop equilibrium and kinetic theories that describe the assembly of viral capsid proteins on a charged central core, as seen in recent experiments in which brome mosaic virus (BMV) capsids assemble around nanoparticles functionalized with polyelectrolyte. We model interactions between capsid proteins and nanoparticle surfaces as the interaction of polyelectrolyte brushes with opposite charge, using the nonlinear Poisson Boltzmann equation. The models predict that there is a threshold density of functionalized charge, above which capsids efficiently assemble around nanoparticles, and that light scatter intensity increases rapidly at early times, without the lag phase characteristic of empty capsid assembly. These predictions are consistent with, and enable interpretation of, preliminary experimental data. However, the models predict a stronger dependence of nanoparticle incorporation efficiency on functionalized charge density than measured in experiments, and do not completely capture a logarithmic growth phase seen in experimental light scatter. These discrepancies may suggest the presence of metastable disordered states in the experimental system. In addition to discussing future experiments for nanoparticle-capsid systems, we discuss broader implications for understanding assembly around charged cores such as nucleic acids.
q-bio.BM:The binding of a ligand molecule to a protein is often accompanied by conformational changes of the protein. A central question is whether the ligand induces the conformational change (induced-fit), or rather selects and stabilizes a complementary conformation from a pre-existing equilibrium of ground and excited states of the protein (selected-fit). We consider here the binding kinetics in a simple four-state model of ligand-protein binding. In this model, the protein has two conformations, which can both bind the ligand. The first conformation is the ground state of the protein when the ligand is off, and the second conformation is the ground state when the ligand is bound. The induced-fit mechanism corresponds to ligand binding in the unbound ground state, and the selected-fit mechanism to ligand binding in the excited state. We find a simple, characteristic difference between the on- and off-rates in the two mechanisms if the conformational relaxation into the ground states is fast. In the case of selected-fit binding, the on-rate depends on the conformational equilibrium constant, while the off-rate is independent. In the case of induced-fit binding, in contrast, the off-rate depends on the conformational equilibrium, while the on-rate is independent. Whether a protein binds a ligand via selected-fit or induced-fit thus may be revealed by mutations far from the protein's binding pocket, or other "perturbations" that only affect the conformational equilibrium. In the case of selected-fit, such mutations will only change the on-rate, and in the case of induced-fit, only the off-rate.
q-bio.BM:We study a matrix model of RNA in which an external perturbation acts on n nucleotides of the polymer chain. The effect of the perturbation appears in the exponential generating function of the partition function as a factor $(1-\frac{n\alpha}{L})$ [where $\alpha$ is the ratio of strengths of the original to the perturbed term and L is length of the chain]. The asymptotic behaviour of the genus distribution functions for the extended matrix model are analyzed numerically when (i) $n=L$ and (ii) $n=1$. In these matrix models of RNA, as $n\alpha/L$ is increased from 0 to 1, it is found that the universality of the number of diagrams $a_{L, g}$ at a fixed length L and genus g changes from $3^{L}$ to $(3-\frac{n\alpha}{L})^{L}$ ($2^{L}$ when $n\alpha/L=1$) and the asymptotic expression of the total number of diagrams $\cal N$ at a fixed length L but independent of genus g, changes in the factor $\exp^{\sqrt{L}}$ to $\exp^{(1-\frac{n\alpha}{L})\sqrt{L}}$ ($exp^{0}=1$ when $n\alpha/L=1$)
q-bio.BM:In protein folding the term plasticity refers to the number of alternative folding pathways encountered in response to free energy perturbations such as those induced by mutation. Here we explore the relation between folding plasticity and a gross, generic feature of the native geometry, namely, the relative number of local and non-local native contacts. The results from our study, which is based on Monte Carlo simulations of simple lattice proteins, show that folding to a structure that is rich in local contacts is considerably more plastic than folding to a native geometry characterized by having a very large number of long-range contacts (i.e., contacts between amino acids that are separated by more than 12 units of backbone distance). The smaller folding plasticity of `non-local' native geometries is probably a direct consequence of their higher folding cooperativity that renders the folding reaction more robust against single- and multiple-point mutations.
q-bio.BM:Significant overweight represents a major health problem in industrialized countries. Besides its known metabolic origins, this condition may also have an infectious cause, as recently postulated. Here, it is surmised that the potentially causative adenovirus 36 contributes to such disorder by inactivating the retinoblastoma tumor suppressor protein (RB) in a manner reminiscent of a mechanism employed by both another pathogenic adenoviral agent and insulin. The present insight additionally suggests novel modes of interfering with obesity-associated pathology.
q-bio.BM:Biological forces govern essential cellular and molecular processes in all living organisms. Many cellular forces, e.g. those generated in cyclic conformational changes of biological machines, have repetitive components. However, little is known about how proteins process repetitive mechanical stresses. To obtain first insights into dynamic protein mechanics, we probed the mechanical stability of single and multimeric ubiquitins perturbed by periodic forces. Using coarse-grained molecular dynamics simulations, we were able to model repetitive forces with periods about two orders of magnitude longer than the relaxation time of folded ubiquitins. We found that even a small periodic force weakened the protein and shifted its unfolding pathways in a frequency- and amplitude-dependent manner. Our results also showed that the dynamic response of even a small protein can be complex with transient refolding of secondary structures and an increasing importance of local interactions in asymmetric protein stability. These observations were qualitatively and quantitatively explained using an energy landscape model and discussed in the light of dynamic single-molecule measurements and physiological forces. We believe that our approach and results provide first steps towards a framework to better understand dynamic protein biomechanics and biological force generation.
q-bio.BM:It is a standard exercise in mechanical engineering to infer the external forces and torques on a body from its static shape and known elastic properties. Here we apply this kind of analysis to distorted double-helical DNA in complexes with proteins. We extract the local mean forces and torques acting on each base-pair of bound DNA from high-resolution complex structures. Our method relies on known elastic potentials and a careful choice of coordinates of the well-established rigid base-pair model of DNA. The results are robust with respect to parameter and conformation uncertainty. They reveal the complex nano-mechanical patterns of interaction between proteins and DNA. Being non-trivially and non-locally related to observed DNA conformations, base-pair forces and torques provide a new view on DNA-protein binding that complements structural analysis.
q-bio.BM:Molecular dynamics studies within a coarse-grained structure based model were used on two similar proteins belonging to the transcarbamylase family to probe the effects in the native structure of a knot. The first protein, N-acetylornithine transcarbamylase, contains no knot whereas human ormithine transcarbamylase contains a trefoil knot located deep within the sequence. In addition, we also analyzed a modified transferase with the knot removed by the appropriate change of a knot-making crossing of the protein chain. The studies of thermally- and mechanically-induced unfolding processes suggest a larger intrinsic stability of the protein with the knot.
q-bio.BM:Comprehensive knowledge of protein-ligand interactions should provide a useful basis for annotating protein functions, studying protein evolution, engineering enzymatic activity, and designing drugs. To investigate the diversity and universality of ligand binding sites in protein structures, we conducted the all-against-all atomic-level structural comparison of over 180,000 ligand binding sites found in all the known structures in the Protein Data Bank by using a recently developed database search and alignment algorithm. By applying a hybrid top-down-bottom-up clustering analysis to the comparison results, we determined approximately 3000 well-defined structural motifs of ligand binding sites. Apart from a handful of exceptions, most structural motifs were found to be confined within single families or superfamilies, and to be associated with particular ligands. Furthermore, we analyzed the components of the similarity network and enumerated more than 4000 pairs of ligand binding sites that were shared across different protein folds.
q-bio.BM:Protein electrostatic states have been demonstrated to play crucial roles in catalysis, ligand binding, protein stability, and in the modulation of allosteric effects. Electrostatic states are demonstrated to appear conserved among DEAD-box motifs and evidence is presented that the structural changes that occur to DEAD box proteins upon ligand binding alter the DEAD-box motif electrostatics in a way the facilitates the catalytic role of the DEAD-box glutatmate.
q-bio.BM:ESPSim is an open source JAVA program that enables the comparisons of protein electrostatic potential maps via the computation of an electrostatic similarity measure. This program has been utilized to demonstrate a high degree of electrostatic similarity among the potential maps of lysozyme proteins, suggesting that protein electrostatic states are conserved within lysozyme proteins. ESPSim is freely available under the AGPL License from http://www.bioinformatics.org/project/?group_id=830
q-bio.BM:Protein electrostatics have been demonstrated to play a vital role in protein functionality, with many functionally important amino acid residues exhibiting an electrostatic state that is altered from that of a normal amino acid residue. Residues with altered electrostatic states can be identified by the presence of a pKa value that is perturbed by 2 or more pK units, and such residues have been demonstrated to play critical roles in catalysis, ligand binding, and protein stability. Within the HCV helicase and polymerase, as well as the HIV reverse transcriptase, highly conserved regions were demonstrated to possess a greater number and magnitude of perturbations than lesser conserved regions, suggesting that there is an interrelationship present between protein electrostatics and evolution.
q-bio.BM:Protein dynamics in cells may be different from that in dilute solutions in vitro since the environment in cells is highly concentrated with other macromolecules. This volume exclusion due to macromolecular crowding is predicted to affect both equilibrium and kinetic processes involving protein conformational changes. To quantify macromolecular crowding effects on protein folding mechanisms, here we have investigated the folding energy landscape of an alpha/beta protein, apoflavodoxin, in the presence of inert macromolecular crowding agents using in silico and in vitro approaches. By coarse-grained molecular simulations and topology-based potential interactions, we probed the effects of increased volume fraction of crowding agents (phi_c) as well as of crowding agent geometry (sphere or spherocylinder) at high phi_c. Parallel kinetic folding experiments with purified Desulfovibro desulfuricans apoflavodoxin in vitro were performed in the presence of Ficoll (sphere) and Dextran (spherocylinder) synthetic crowding agents. In conclusion, we have identified in silico crowding conditions that best enhance protein stability and discovered that upon manipulation of the crowding conditions, folding routes experiencing topological frustrations can be either enhanced or relieved. The test-tube experiments confirmed that apoflavodoxin's time-resolved folding path is modulated by crowding agent geometry. We propose that macromolecular crowding effects may be a tool for manipulation of protein folding and function in living cells.
q-bio.BM:The force generated between actin and myosin acts predominantly along the direction of the actin filament, resulting in relative sliding of the thick and thin filaments in muscle or transport of myosin cargos along actin tracks. Previous studies have also detected lateral forces or torques that are generated between actin and myosin, but the origin and biological role of these sideways forces is not known. Here we adapt an actin gliding filament assay in order to measure the rotation of an actin filament about its axis (twirling) as it is translocated by myosin. We quantify the rotation by determining the orientation of sparsely incorporated rhodamine-labeled actin monomers, using polarized total internal reflection (polTIRF) microscopy. In order to determine the handedness of the filament rotation, linear incident polarizations in between the standard s- and p-polarizations were generated, decreasing the ambiguity of our probe orientation measurement four-fold. We found that whole myosin II and myosin V both twirl actin with a relatively long (micron), left-handed pitch that is insensitive to myosin concentration, filament length and filament velocity.
q-bio.BM:Most of the theoretical models describing the translocation of a polymer chain through a nanopore use the hypothesis that the polymer is always relaxed during the complete process. In other words, models generally assume that the characteristic relaxation time of the chain is small enough compared to the translocation time that non-equilibrium molecular conformations can be ignored. In this paper, we use Molecular Dynamics simulations to directly test this hypothesis by looking at the escape time of unbiased polymer chains starting with different initial conditions. We find that the translocation process is not quite in equilibrium for the systems studied, even though the translocation time tau is about 10 times larger than the relaxation time tau_r. Our most striking result is the observation that the last half of the chain escapes in less than ~12% of the total escape time, which implies that there is a large acceleration of the chain at the end of its escape from the channel.
q-bio.BM:We present a self-contained theory for the mechanical response of DNA in single molecule experiments. Our model is based on a 1D continuum description of the DNA molecule and accounts both for its elasticity and for DNA-DNA electrostatic interactions. We consider the classical loading geometry used in experiments where one end of the molecule is attached to a substrate and the other one is pulled by a tensile force and twisted by a given number of turns. We focus on configurations relevant to the limit of a large number of turns, which are made up of two phases, one with linear DNA and the other one with superhelical DNA. The model takes into account thermal fluctuations in the linear phase and electrostatic interactions in the superhelical phase. The values of the torsional stress, of the supercoiling radius and angle, and key features of the experimental extension-rotation curves, namely the slope of the linear region and thermal buckling threshold, are predicted. They are found in good agreement with experimental data.
q-bio.BM:While slowly turning the ends of a single molecule of DNA at constant applied force, a discontinuity was recently observed at the supercoiling transition, when a small plectoneme is suddenly formed. This can be understood as an abrupt transition into a state in which stretched and plectonemic DNA coexist. We argue that there should be discontinuities in both the extension and the torque at the transition, and provide experimental evidence for both. To predict the sizes of these discontinuities and how they change with the overall length of DNA, we organize a theory for the coexisting plectonemic state in terms of four length-independent parameters. We also test plectoneme theories, including our own elastic rod simulation, finding discrepancies with experiment that can be understood in terms of the four coexisting state parameters.
q-bio.BM:We introduce a simple "patchy particle" model to study the thermodynamics and dynamics of self-assembly of homomeric protein complexes. Our calculations allow us to rationalize recent results for dihedral complexes. Namely, why evolution of such complexes naturally takes the system into a region of interaction space where (i) the evolutionarily newer interactions are weaker, (ii) subcomplexes involving the stronger interactions are observed to be thermodynamically stable on destabilization of the protein-protein interactions and (iii) the self-assembly dynamics are hierarchical with these same subcomplexes acting as kinetic intermediates.
q-bio.BM:We perform molecular dynamics simulations for a simple coarse-grained model of crambin placed inside of a softly repulsive sphere of radius R. The confinement makes folding at the optimal temperature slower and affects the folding scenarios, but both effects are not dramatic. The influence of crowding on folding are studied by placing several identical proteins within the sphere, denaturing them, and then by monitoring refolding. If the interactions between the proteins are dominated by the excluded volume effects, the net folding times are essentially like for a single protein. An introduction of inter-proteinic attractive contacts hinders folding when the strength of the attraction exceeds about a half of the value of the strength of the single protein contacts. The bigger the strength of the attraction, the more likely is the occurrence of aggregation and misfolding.
q-bio.BM:We have developed a new simulation method to estimate the distance between the native state and the first transition state, and the distance between the intermediate state and the second transition state of a protein which mechanically unfolds via intermediates. Assuming that the end-to-end extension $\Delta R$ is a good reaction coordinate to describe the free energy landscape of proteins subjected to an external force, we define the midpoint extension $\Delta R^*$ between two transition states from either constant-force or constant loading rate pulling simulations. In the former case, $\Delta R^*$ is defined as a middle point between two plateaus in the time-dependent curve of $\Delta R$, while, in the latter one, it is a middle point between two peaks in the force-extension curve. Having determined $\Delta R^*$, one can compute times needed to cross two transition state barriers starting from the native state. With the help of the Bell and microscopic kinetic theory, force dependencies of these unfolding times can be used to locate the intermediate state and to extract unfolding barriers. We have applied our method to the titin domain I27 and the fourth domain of {\em Dictyostelium discoideum} filamin (DDFLN4), and obtained reasonable agreement with experiments, using the C$_{\alpha}$-Go model.
q-bio.BM:The protective effect exerted by polyamines (Put and Spd) against cadmium (Cd) stress was investigated in Brassica juncea plants. Treatment with CdCl2 (75 micro-Mole) resulted in a rise of Cd accumulation, a decrease of fresh and dry weights in every plant organ, an increase of free polyamine content at limb and stem levels as well as a decrease at root level. On the other hand, the total conjugated polyamine levels in the stem tissues were unaffected by Cd. In the leaf tissues, this metal caused a reduction of chlorophyll a content, a rise of guaiacol peroxidase (GPOX) activity and an increase of malondialdehyde (MDA), soluble glucide, proline and amino acid contents. Exogenous application, by spraying, of putrescine (Put) and spermidine (Spd) to leaf tissues reduced CdCl2-induced stress. These polyamines proved to exert a partial, though significant, protection of the foliar fresh weight and to alleviate the oxidative stress generated by Cd through reductions of MDA amounts and GPOX (E.C.1.11.1.7) activity. The enhancement of chlorophyll a content in plants by Put and those of Chl a and Chl b by Spd both constitute evidences of their efficacy against the Cd2+-induced loss of pigments. Conversely to Put, Spd caused a decrease of Cd content in leave tissues and a rise in the stems and roots; these findings are in favour of a stimulation of Cd uptake by Spd. The proline stimulation observed with Cd was reduced further to the spraying of Put onto tissues, but the decrease induced by Spd was more limited. In the plants treated with Cd, the amino acid contents in the leaves were unaffected by Put and Spd spraying; on the other hand, Cd2+ disturbed polyamine levels (free and acido-soluble conjugated-forms); we notice the rise of total free PAs and the decrease of their conjugated-ones.
q-bio.BM:Three coarse-grained models of the double-stranded DNA are proposed and compared in the context of mechanical manipulation such as twisting and various schemes of stretching. The models differ in the number of effective beads (between two and five) representing each nucleotide. They all show similar behavior and, in particular, lead to a torque-force phase diagrams qualitatively consistent with experiments and all-atom simulations.
q-bio.BM:We incorporate hydrodynamic interactions (HI) in a coarse-grained and structure-based model of proteins by employing the Rotne-Prager hydrodynamic tensor. We study several small proteins and demonstrate that HI facilitate folding. We also study HIV-1 protease and show that HI make the flap closing dynamics faster. The HI are found to affect time correlation functions in the vicinity of the native state even though they have no impact on same time characteristics of the structure fluctuations around the native state.
q-bio.BM:Peptides and proteins exhibit a common tendency to assemble into highly ordered fibrillar aggregates, whose formation proceeds in a nucleation-dependent manner that is often preceded by the formation of disordered oligomeric assemblies. This process has received much attention because disordered oligomeric aggregates have been associated with neurodegenerative disorders such as Alzheimer's and Parkinson's diseases. Here we describe a self-templated nucleation mechanism that determines the transition between the initial condensation of polypeptide chains into disordered assemblies and their reordering into fibrillar structures. The results that we present show that at the molecular level this transition is due to the ability of polypeptide chains to reorder within oligomers into fibrillar assemblies whose surfaces act as templates that stabilise the disordered assemblies.
q-bio.BM:The presence of oligomeric aggregates, which is often observed during the process of amyloid formation, has recently attracted much attention since it has been associated with neurodegenerative conditions such as Alzheimer's and Parkinson's diseases. We provide a description of a sequence-indepedent mechanism by which polypeptide chains aggregate by forming metastable oligomeric intermediate states prior to converting into fibrillar structures. Our results illustrate how the formation of ordered arrays of hydrogen bonds drives the formation of beta-sheets within the disordered oligomeric aggregates that form early under the effect of hydrophobic forces. Initially individual beta-sheets form with random orientations, which subsequently tend to align into protofilaments as their lengths increases. Our results suggest that amyloid aggregation represents an example of the Ostwald step rule of first order phase transitions by showing that ordered cross-beta structures emerge preferentially from disordered compact dynamical intermediate assemblies.
q-bio.BM:It has been recently argued that the depletion attraction may play an important role in different aspects of the cellular organization, ranging from the organization of transcriptional activity in transcription factories to the formation of the nuclear bodies. In this paper we suggest a new application of these ideas in the context of the splicing process, a crucial step of messanger RNA maturation in Eukaryotes. We shall show that entropy effects and the resulting depletion attraction may explain the relevance of the aspecific intron length variable in the choice of the splice-site recognition modality. On top of that, some qualitative features of the genome architecture of higher Eukaryotes can find an evolutionary realistic motivation in the light of our model.
q-bio.BM:Translocation through a nanopore is a new experimental technique to probe physical properties of biomolecules. A bulk of theoretical and computational work exists on the dependence of the time to translocate a single unstructured molecule on the length of the molecule. Here, we study the same problem but for RNA molecules for which the breaking of the secondary structure is the main barrier for translocation. To this end, we calculate the mean translocation time of single-stranded RNA through a nanopore of zero thickness and at zero voltage for many randomly chosen RNA sequences. We find the translocation time to depend on the length of the RNA molecule with a power law. The exponent changes as a function of temperature and exceeds the naively expected exponent of two for purely diffusive transport at all temperatures. We interpret the power law scaling in terms of diffusion in a one-dimensional energy landscape with a logarithmic barrier.
q-bio.BM:Cellular cargo can be bound to cytoskeletal filaments by one or multiple active or passive molecular motors. Recent experiments have shown that the presence of auxiliary, nondriving motors, results in an enhanced processivity of the cargo, compared to the case of a single active motor alone. We model the observed cooperative transport process using a stochastic model that describes the dynamics of two molecular motors, an active one that moves cargo unidirectionally along a filament track and a passive one that acts as a tether. Analytical expressions obtained from our analysis are fit to experimental data to estimate the microscopic kinetic parameters of our model. Our analysis reveals two qualitatively distinct processivity-enhancing mechanisms: the passive tether can decrease the typical detachment rate of the active motor from the filament track or it can increase the corresponding reattachment rate. Our estimates unambiguously show that in the case of microtubular transport, a higher average run length arises mainly from the ability of the passive motor to keep the cargo close to the filament, enhancing the reattachment rate of an active kinesin motor that has recently detached. Instead, for myosin-driven transport along actin, the passive motor tightly tethers the cargo to the filament, suppressing the detachment rate of the active myosin.
q-bio.BM:FoF1-ATP synthase is the enzyme that provides the 'chemical energy currency' adenosine triphosphate, ATP, for living cells. The formation of ATP is accomplished by a stepwise internal rotation of subunits within the enzyme. Briefly, proton translocation through the membrane-bound Fo part of ATP synthase drives a 10-step rotary motion of the ring of c subunits with respect to the non-rotating subunits a and b. This rotation is transmitted to the gamma and epsilon subunits of the F1 sector resulting in 120 degree steps. In order to unravel this symmetry mismatch we monitor subunit rotation by a single-molecule fluorescence resonance energy transfer (FRET) approach using three fluorophores specifically attached to the enzyme: one attached to the F1 motor, another one to the Fo motor, and the third one to a non-rotating subunit. To reduce photophysical artifacts due to spectral fluctuations of the single fluorophores, a duty cycle-optimized alternating three-laser scheme (DCO-ALEX) has been developed. Simultaneous observation of the stepsizes for both motors allows the detection of reversible elastic deformations between the rotor parts of Fo and F1.
q-bio.BM:In this paper, we model the mechanics of a collagen pair in the connective tissue extracellular matrix that exists in abundance throughout animals, including the human body. This connective tissue comprises repeated units of two main structures, namely collagens as well as axial, parallel and regular anionic glycosaminoglycan between collagens. The collagen fibril can be modeled by Hooke's law whereas anionic glycosaminoglycan behaves more like a rubber-band rod and as such can be better modeled by the worm-like chain model. While both computer simulations and continuum mechanics models have been investigated the behavior of this connective tissue typically, authors either assume a simple form of the molecular potential energy or entirely ignore the microscopic structure of the connective tissue. Here, we apply basic physical methodologies and simple applied mathematical modeling techniques to describe the collagen pair quantitatively. We find that the growth of fibrils is intimately related to the maximum length of the anionic glycosaminoglycan and the relative displacement of two adjacent fibrils, which in return is closely related to the effectiveness of anionic glycosaminoglycan in transmitting forces between fibrils. These reveal the importance of the anionic glycosaminoglycan in maintaining the structural shape of the connective tissue extracellular matrix and eventually the shape modulus of human tissues. We also find that some macroscopic properties, like the maximum molecular energy and the breaking fraction of the collagen, are also related to the microscopic characteristics of the anionic glycosaminoglycan.
q-bio.BM:The wobble hypothesis does not discriminate between uracil and thymine. Methylation could favor further stabilization of uracil in the keto form. Thymine is present in keto form only and can pair up but with adenine. Uracil can easily construct the enol form; that is why it forms the U-G pair.
q-bio.BM:In this paper we analyze the vibrational spectra of a large ensemble of non-homologous protein structures by means of a novel tool, that we coin the Hierarchical Network Model (HNM). Our coarse-grained scheme accounts for the intrinsic heterogeneity of force constants displayed by protein arrangements and also incorporates side-chain degrees of freedom.   Our analysis shows that vibrational entropy per unit residue correlates with the content of secondary structure. Furthermore, we assess the individual contribution to vibrational entropy of the novel features of our scheme as compared with the predictions of state-of-the-art network models. This analysis highlights the importance of properly accounting for the intrinsic hierarchy in force strengths typical of the different atomic bonds that build up and stabilize protein scaffolds.   Finally, we discuss possible implications of our findings in the context of protein aggregation phenomena.
q-bio.BM:Because of the double-helical structure of DNA, in which two strands of complementary nucleotides intertwine around each other, a covalently closed DNA molecule with no interruptions in either strand can be viewed as two interlocked single-stranded rings. Two closed space curves have long been known by mathematicians to exhibit a property called the linking number, a topologically invariant integer, expressible as the sum of two other quantities, the twist of one of the curves about the other, and the writhing number, or writhe, a measure of the chiral distortion from planarity of one of the two closed curves. We here derive expressions for the twist of supercoiled DNA and the writhe of a closed molecule consistent with the modern view of DNA as a sequence of base-pair steps. Structural biologists commonly characterize the spatial disposition of each step in terms of six rigid-body parameters, one of which, coincidentally, is also called the twist. Of interest is the difference in the mathematical properties between this step-parameter twist and the twist of supercoiling associated with a given base-pair step. For example, it turns out that the latter twist, unlike the former, is sensitive to certain translational shearing distortions of the molecule that are chiral in nature. Thus, by comparing the values for the two twists for each step of a high-resolution structure of a protein-DNA complex, the nucleosome considered here, for example, we may be able to determine how the binding of various proteins contributes to chiral structural changes of the DNA.
q-bio.BM:The AGBNP2 implicit solvent model, an evolution of the Analytical Generalized Born plus Non-Polar (AGBNP) model we have previously reported, is presented with the aim of modeling hydration effects beyond those described by conventional continuum dielectric representations. A new empirical hydration free energy component based on a procedure to locate and score hydration sites on the solute surface is introduced to model first solvation shell effects, such as hydrogen bonding, which are poorly described by continuum dielectric models. This new component is added to the Generalized Born and non-polar AGBNP models which have been improved with respect to the description of the solute volume description. We have introduced an analytical Solvent Excluding Volume (SEV) model which reduces the effect of spurious high-dielectric interstitial spaces present in conventional van der Waals representations of the solute volume. The AGBNP2 model is parametrized and tested with respect to experimental hydration free energies of small molecules and the results of explicit solvent simulations. Modeling the granularity of water is one of the main design principles employed for the the first shell solvation function and the SEV model, by requiring that water locations have a minimum available volume based on the size of a water molecule. We show that the new volumetric model produces Born radii and surface areas in good agreement with accurate numerical evaluations. The results of Molecular Dynamics simulations of a series of mini-proteins show that the new model produces conformational ensembles in substantially better agreement with reference explicit solvent ensembles than the original AGBNP model with respect to both structural and energetics measures.
q-bio.BM:It is presented that the positions of amino acids within Genetic Code Table follow from strict their physical and chemical properties as well as from a pure formal determination by the Golden mean.
q-bio.BM:Although DNA is often bent in vivo, it is unclear how DNA-bending forces modulate DNA-protein binding affinity. Here, we report how a range of DNA-bending forces modulates the binding of the Integration Host Factor (IHF) protein to various DNAs. Using solution fluorimetry and electrophoretic mobility shift assays, we measured the affinity of IHF for DNAs with different bending forces and sequence mutations. Bending force was adjusted by varying the fraction of double-stranded DNA in a circular substrate, or by changing the overall size of the circle (1). DNA constructs contained a pair of Forster Resonance Energy Transfer dyes that served as probes for affinity assays, and read out bending forces measured by optical force sensors (2). Small bending forces significantly increased binding affinity; this effect saturated beyond ~3 pN. Surprisingly, when DNA sequences that bound IHF only weakly were mechanically bent by circularization, they bound IHF more tightly than the linear "high-affinity" binding sequence. These findings demonstrate that small bending forces can greatly augment binding at sites that deviate from a protein's consensus binding sequence. Since cellular DNA is subject to mechanical deformation and condensation, affinities of architectural proteins determined in vitro using short linear DNAs may not reflect in vivo affinities.
q-bio.BM:A statistical model of protein families, called profile conditional random fields (CRFs), is proposed. This model may be regarded as an integration of the profile hidden Markov model (HMM) and the Finkelstein-Reva (FR) theory of protein folding. While the model structure of the profile CRF is almost identical to the profile HMM, it can incorporate arbitrary correlations in the sequences to be aligned to the model. In addition, like in the FR theory, the profile CRF can incorporate long-range pairwise interactions between model states via mean-field-like approximations. We give the detailed formulation of the model, self-consistent approximations for treating long-range interactions, and algorithms for computing partition functions and marginal probabilities. We also outline the methods for the global optimization of model parameters as well as a Bayesian framework for parameter learning and selection of optimal alignments.
q-bio.BM:Mathew-Fenn et al. (Science (2008) 322, 446-9) measured end-to-end distances of short DNA and concluded that stretching fluctuations in several consecutive turns of the double helix should be strongly correlated. I argue that this conclusion is based on incorrect assumptions, notably, on a simplistic treatment of the excluded volume effect of reporter labels. Contrary to the author's claim, their conclusion is not supported by other data.
q-bio.BM:The folding dynamics of small single-domain proteins is a current focus of simulations and experiments. Many of these proteins are 'two-state folders', i.e. proteins that fold rather directly from the denatured state to the native state, without populating metastable intermediate states. A central question is how to characterize the instable, partially folded conformations of two-state proteins, in particular the rate-limiting transition-state conformations between the denatured and the native state. These partially folded conformations are short-lived and cannot be observed directly in experiments. However, experimental data from detailed mutational analyses of the folding dynamics provide indirect access to transition states. The interpretation of these data, in particular the reconstruction of transition-state conformations, requires simulation and modeling. The traditional interpretation of the mutational data aims to reconstruct the degree of structure formation of individual residues in the transition state, while a novel interpretation aims at degrees of structure formation of cooperative substructures such as alpha-helices and beta-hairpins. By splitting up mutation-induced free energy changes into secondary and tertiary structural components, the novel interpretation resolves some of the inconsistencies of the traditional interpretation.
q-bio.BM:In Sphingomonas CHY-1, a single ring-hydroxylating dioxygenase is responsible for the initial attack of a range of polycyclic aromatic hydrocarbons (PAHs) composed of up to five rings. The components of this enzyme were separately purified and characterized. The oxygenase component (ht-PhnI) was shown to contain one Rieske-type [2Fe-2S] cluster and one mononuclear Fe center per alpha subunit, based on EPR measurements and iron assay. Steady-state kinetic measurements revealed that the enzyme had a relatively low apparent Michaelis constant for naphthalene (Km= 0.92 $\pm$ 0.15 $\mu$M), and an apparent specificity constant of 2.0 $\pm$ 0.3 $\mu$M-1 s-1. Naphthalene was converted to the corresponding 1,2-dihydrodiol with stoichiometric oxidation of NADH. On the other hand, the oxidation of eight other PAHs occurred at slower rates, and with coupling efficiencies that decreased with the enzyme reaction rate. Uncoupling was associated with hydrogen peroxide formation, which is potentially deleterious to cells and might inhibit PAH degradation. In single turnover reactions, ht-PhnI alone catalyzed PAH hydroxylation at a faster rate in the presence of organic solvent, suggesting that the transfer of substrate to the active site is a limiting factor. The four-ring PAHs chrysene and benz[a]anthracene were subjected to a double ring-dihydroxylation, giving rise to the formation of a significant proportion of bis-cis-dihydrodiols. In addition, the dihydroxylation of benz[a]anthracene yielded three dihydrodiols, the enzyme showing a preference for carbons in positions 1,2 and 10,11. This is the first characterization of a dioxygenase able to dihydroxylate PAHs made up of four and five rings.
q-bio.BM:Initial reactions involved in the bacterial degradation of polycyclic aromatic hydrocarbons (PAHs) include a ring-dihydroxylation catalyzed by a dioxygenase and a subsequent oxidation of the dihydrodiol products by a dehydrogenase. In this study, the dihydrodiol dehydrogenase from the PAH-degrading Sphingomonas strain CHY-1 has been characterized. The bphB gene encoding PAH dihydrodiol dehydrogenase (PDDH) was cloned and overexpressed as a His-tagged protein. The recombinant protein was purified as a homotetramer with an apparent Mr of 110,000. PDDH oxidized the cis-dihydrodiols derived from biphenyl and eight polycyclic hydrocarbons, including chrysene, benz[a]anthracene, and benzo[a]pyene, to corresponding catechols. Remarkably, the enzyme oxidized pyrene 4,5-dihydrodiol, whereas pyrene is not metabolized by strain CHY-1. The PAH catechols produced by PDDH rapidly auto-oxidized in air but were regenerated upon reaction of the o-quinones formed with NADH. Kinetic analyses performed under anoxic conditions revealed that the enzyme efficiently utilized two- to four-ring dihydrodiols, with Km values in the range of 1.4 to 7.1 $\mu$M, and exhibited a much higher Michaelis constant for NAD+ (Km of 160 $\mu$M). At pH 7.0, the specificity constant ranged from (1.3 $\pm$ 0.1) x 106 M?1 s?1 with benz[a]anthracene 1,2-dihydrodiol to (20.0 $\pm$ 0.8) x 106 M?1 s?1 with naphthalene 1,2-dihydrodiol. The catalytic activity of the enzyme was 13-fold higher at pH 9.5. PDDH was subjected to inhibition by NADH and by 3,4-dihydroxyphenanthrene, and the inhibition patterns suggested that the mechanism of the reaction was ordered Bi Bi. The regulation of PDDH activity appears as a means to prevent the accumulation of PAH catechols in bacterial cells.
q-bio.BM:2-Ethyhexyl nitrate (2-EHN) is a major additive of fuel which is used to comply with the cetane number of diesel. Because of its wide use and possible accidental release, 2-EHN is a potential pollutant of the environment. In this study, Mycobacterium austroafricanum IFP 2173 was selected among several strains as the best 2-EHN degrader. The 2-EHN biodegradation rate was increased in biphasic cultures where the hydrocarbon was dissolved in an inert non-aqueous phase liquid (NAPL), suggesting that the transfer of the hydrophobic substrate to the cells was a growth-limiting factor. Carbon balance calculation as well as organic carbon measurement indicated a release of metabolites in the culture medium. Further analysis by gas chromatography revealed that a single metabolite accumulated during growth. This metabolite had a molecular mass of 114 Da as determined by GC/MS and was provisionally identified as 4-ethyldihydrofuran-2(3H)-one by LC-MS/MS analysis. Identification was confirmed by analysis of the chemically synthesized lactone. Based on these results, a plausible catabolic pathway is proposed whereby 2-EHN is converted to 4-ethyldihydrofuran-2(3H)-one, which cannot be metabolised further by strain IFP 2173. This putative pathway provides an explanation for the low energetic efficiency of 2-EHN degradation and its poor biodegradability.
q-bio.BM:In this study, the genes involved in the initial attack on fluorene by Sphingomonas sp. LB126 were investigated. The ? and ? subunits of a dioxygenase complex (FlnA1A2), showing 63% and 51% sequence identity respectively, with the subunits of an angular dioxygenase from Gram-positive Terrabacter sp. DBF63, were identified. When overexpressed in E. coli, FlnA1A2 was responsible for the angular oxidation of fluorene, fluorenol, fluorenone, dibenzofuran and dibenzo-p-dioxin. Moreover, FlnA1A2 was able to oxidize polycyclic aromatic hydrocarbons and heteroaromatics, some of which were not oxidized by the dioxygenase from Terrabacter sp. DBF63. Quantification of resulting oxidation products showed that fluorene and phenanthrene were preferred substrates.
q-bio.BM:We study a simplified model of the RNA molecule proposed by G. Vernizzi, H. Orland and A. Zee in the regime of strong concentration of positive ions in solution. The model considers a flexible chain of equal bases that can pairwise interact with any other one along the chain, while preserving the property of saturation of the interactions. In the regime considered, we observe the emergence of a critical temperature T_c separating two phases that can be characterized by the topology of the predominant configurations: in the large temperature regime, the dominant configurations of the molecule have very large genera (of the order of the size of the molecule), corresponding to a complex topology, whereas in the opposite regime of low temperatures, the dominant configurations are simple and have the topology of a sphere. We determine that this topological phase transition is of first order and provide an analytic expression for T_c. The regime studied for this model exhibits analogies with that for the dense polymer systems studied by de Gennes
q-bio.BM:The kinetics for the assembly of viral proteins into a population of capsids can be measured in vitro with size exclusion chromatography or dynamic light scattering, but extracting mechanistic information from these studies is challenging. For example, it is not straightforward to determine the critical nucleus size or the elongation time (the time required for a nucleated partial capsid to grow completion). We show that, for two theoretical models of capsid assembly, the critical nucleus size can be determined from the concentration dependence of the assembly reaction half-life and the elongation time is revealed by the length of the lag phase. Furthermore, we find that the system becomes kinetically trapped when nucleation becomes fast compared to elongation. Implications of this constraint for determining elongation mechanisms from experimental assembly data are discussed.
q-bio.BM:Single molecule Forster resonance energy transfer (FRET) experiments are used to infer the properties of the denatured state ensemble (DSE) of proteins. From the measured average FRET efficiency, <E>, the distance distribution P(R) is inferred by assuming that the DSE can be described as a polymer. The single parameter in the appropriate polymer model (Gaussian chain, Wormlike chain, or Self-avoiding walk) for P(R) is determined by equating the calculated and measured <E>. In order to assess the accuracy of this "standard procedure," we consider the generalized Rouse model (GRM), whose properties [<E> and P(R)] can be analytically computed, and the Molecular Transfer Model for protein L for which accurate simulations can be carried out as a function of guanadinium hydrochloride (GdmCl) concentration. Using the precisely computed <E> for the GRM and protein L, we infer P(R) using the standard procedure. We find that the mean end-to-end distance can be accurately inferred (less than 10% relative error) using <E> and polymer models for P(R). However, the value extracted for the radius of gyration (Rg) and the persistence length (lp) are less accurate. The relative error in the inferred R-g and lp, with respect to the exact values, can be as large as 25% at the highest GdmCl concentration. We propose a self-consistency test, requiring measurements of <E> by attaching dyes to different residues in the protein, to assess the validity of describing DSE using the Gaussian model. Application of the self-consistency test to the GRM shows that even for this simple model the Gaussian P(R) is inadequate. Analysis of experimental data of FRET efficiencies for the cold shock protein shows that at there are significant deviations in the DSE P(R) from the Gaussian model.
q-bio.BM:The high computational cost of carrying out molecular dynamics simulations of even small-size proteins is a major obstacle in the study, at atomic detail and in explicit solvent, of the physical mechanism which is at the basis of the folding of proteins. Making use of a biasing algorithm, based on the principle of the ratchet-and-pawl, we have been able to calculate eight folding trajectories (to an RMSD between 1.2A and 2.5A) of the B1 domain of protein G in explicit solvent without the need of high-performance computing. The simulations show that in the denatured state there is a complex network of cause-effect relationships among contacts, which results in a rather hierarchical folding mechanism. The network displays few local and nonlocal native contacts which are cause of most of the others, in agreement with the NOE signals obtained in mildly-denatured conditions. Also nonnative contacts play an active role in the folding kinetics. The set of conformations corresponding to the transition state display phi-values with a correlation coefficient of 0.69 with the experimental ones. They are structurally quite homogeneous and topologically native-like, although some of the side chains and most of the hydrogen bonds are not in place.
q-bio.BM:Rigidity analysis using the "pebble game" has been applied to protein crystal structures to obtain information on protein folding, assembly and t he structure-function relationship. However, previous work using this technique has not made clear how the set of hydrogen-bond constraints included in the rigidity analysis should be chosen, nor how sensitive the results of rigidity analysis are to small structural variations. We present a comparative study in which "pebble game" rigidity analysis is applied to multiple protein crystal structures, for each of six differen t protein families. We find that the mainchain rigidity of a protein structure at a given hydrogen-bond energy cutoff is quite sensitive to small structural variations, and conclude that the hydrogen bond constraints in rigidity analysis should be chosen so as to form and test specific hypotheses about the rigidity o f a particular protein. Our comparative approach highlights two different characteristic patterns ("sudden" or "gradual") for protein rigidity loss as constraints are re moved, in line with recent results on the rigidity transitions of glassy networks.
q-bio.BM:We carry out a theoretical study on the isotropic-nematic phase transition and phase separation in amyloid fibril solutions. Borrowing the thermodynamic model employed in the study of cylindrical micelles, we investigate the variations in the fibril length distribution and phase behavior with respect to changes in the protein concentration, fibril's rigidity, and binding energy. We then relate our theoretical findings to the nematic ordering observed in Hen Lysozyme fibril solution.
q-bio.BM:Homeodomain containing proteins are a broad class of DNA binding proteins that are believed to primarily function as transcription factors. Electrostatics interactions have been demonstrated to be critical for the binding of the homeodomain to DNA. An examination of the electrostatic state of homeodomain residues involved in DNA phosphate binding has demonstrated the conserved presence of upward shifted pKa values among the basic residue of lysine and arginine. It is believed that these pKa perturbations work to facilitate binding to DNA since they ensure that the basic residues always retain a positive charge.
q-bio.BM:2D display is a fast and economical way of visualizing polymorphism and comparing genomes, which is based on the separation of DNA fragments in two steps, according first to their size and then to their sequence composition. In this paper, we present an exhaustive study of the numerical issues associated with a model aimed at predicting the final absolute locations of DNA fragments in 2D display experiments. We show that simple expressions for the mobility of DNA fragments in both dimensions allow one to reproduce experimental final absolute locations to better than experimental uncertainties. On the other hand, our simulations also point out that the results of 2D display experiments are not sufficient to determine the best set of parameters for the modeling of fragments separation in the second dimension and that additional detailed measurements of the mobility of a few sequences are necessary to achieve this goal. We hope that this work will help in establishing simulations as a powerful tool to optimize experimental conditions without having to perform a large number of preliminary experiments and to estimate whether 2D DNA display is suited to identify a mutation or a genetic difference that is expected to exist between the genomes of closely related organisms.
q-bio.BM:Proteins are large and complex molecular machines. In order to perform their function, most of them need energy, e.g. either in the form of a photon, like in the case of the visual pigment rhodopsin, or through the breaking of a chemical bond, as in the presence of adenosine triphosphate (ATP). Such energy, in turn, has to be transmitted to specific locations, often several tens of Angstroms away from where it is initially released. Here we show, within the framework of a coarse-grained nonlinear network model, that energy in a protein can jump from site to site with high yields, covering in many instances remarkably large distances. Following single-site excitations, few specific sites are targeted, systematically within the stiffest regions. Such energy transfers mark the spontaneous formation of a localized mode of nonlinear origin at the destination site, which acts as an efficient energy-accumulating centre. Interestingly, yields are found to be optimum for excitation energies in the range of biologically relevant ones.
q-bio.BM:A protein undergoes conformational dynamics with multiple time scales, which results in fluctuating enzyme activities. Recent studies in single molecule enzymology have observe this "age-old" dynamic disorder phenomenon directly. However, the single molecule technique has its limitation. To be able to observe this molecular effect with real biochemical functions {\it in situ}, we propose to couple the fluctuations in enzymatic activity to noise propagations in small protein interaction networks such as zeroth order ultra-sensitive phosphorylation-dephosphorylation cycle. We showed that enzyme fluctuations could indeed be amplified by orders of magnitude into fluctuations in the level of substrate phosphorylation | a quantity widely interested in cellular biology. Enzyme conformational fluctuations sufficiently slower than the catalytic reaction turn over rate result in a bimodal concentration distribution of the phosphorylated substrate. In return, this network amplified single enzyme fluctuation can be used as a novel biochemical "reporter" for measuring single enzyme conformational fluctuation rates.
q-bio.BM:The microtubule assembly process has been extensively studied, but the underlying molecular mechanism remains poorly understood. The structure of an artificially generated sheet polymer that alternates two types of lateral contacts and that directly converts into microtubules, has been proposed to correspond to the intermediate sheet structure observed during microtubule assembly. We have studied the self-assembly process of GMPCPP tubulins into sheet and microtubule structures using thermodynamic analysis and stochastic simulations. With the novel assumptions that tubulins can laterally interact in two different forms, and allosterically affect neighboring lateral interactions, we can explain existing experimental observations. At low temperature, the allosteric effect results in the observed sheet structure with alternating lateral interactions as the thermodynamically most stable form. At normal microtubule assembly temperature, our work indicates that a class of sheet structures resembling those observed at low temperature is transiently trapped as an intermediate during the assembly process. This work may shed light on the tubulin molecular interactions, and the role of sheet formation during microtubule assembly.
q-bio.BM:Metamorphic proteins like Lymphotactin are a notable exception of the empirical principle that structured natural proteins possess a unique three dimensional structure. In particular, the human chemokine lymphotactin protein (Ltn) exists in two distinct conformations (one monomeric and one dimeric) under physiological conditions. In this work we use a Ca Go-model to show how this very peculiar behavior can be reproduced. From the study of the thermodynamics and of the kinetics we characterize the interconversion mechanism. In particular, this takes place through the docking of the two chains living in a third monomeric, partially unfolded, state which shows a residual structure involving a set of local contacts common to the two native conformations. The main feature of two-fold proteins appears to be the sharing of a common set of local contacts between the two distinct folds as confirmed by the study of two designed two-fold proteins. Metamorphic proteins may be more common than expected.
q-bio.BM:Computing the similarity between two protein structures is a crucial task in molecular biology, and has been extensively investigated. Many protein structure comparison methods can be modeled as maximum clique problems in specific k-partite graphs, referred here as alignment graphs. In this paper, we propose a new protein structure comparison method based on internal distances (DAST) which is posed as a maximum clique problem in an alignment graph. We also design an algorithm (ACF) for solving such maximum clique problems. ACF is first applied in the context of VAST, a software largely used in the National Center for Biotechnology Information, and then in the context of DAST. The obtained results on real protein alignment instances show that our algorithm is more than 37000 times faster than the original VAST clique solver which is based on Bron & Kerbosch algorithm. We furthermore compare ACF with one of the fastest clique finder, recently conceived by Ostergard. On a popular benchmark (the Skolnick set) we observe that ACF is about 20 times faster in average than the Ostergard's algorithm.
q-bio.BM:We use computer simulations to study a model, first proposed by Wales [1], for the reversible and monodisperse self-assembly of simple icosahedral virus capsid structures. The success and efficiency of assembly as a function of thermodynamic and geometric factors can be qualitatively related to the potential energy landscape structure of the assembling system. Even though the model is strongly coarse-grained, it exhibits a number of features also observed in experiments, such as sigmoidal assembly dynamics, hysteresis in capsid formation and numerous kinetic traps. We also investigate the effect of macromolecular crowding on the assembly dynamics. Crowding agents generally reduce capsid yields at optimal conditions for non-crowded assembly, but may increase yields for parameter regimes away from the optimum. Finally, we generalize the model to a larger triangulation number T = 3, and observe more complex assembly dynamics than that seen for the original T = 1 model.
q-bio.BM:Why reaction rate constants for enzymatic reactions are typically inversely proportional to fractional power exponents of solvent viscosity remains to be already a thirty years old puzzle. Available interpretations of the phenomenon invoke to either a modification of 1. the conventional Kramers' theory or that of 2. the Stokes law. We show that there is an alternative interpretation of the phenomenon at which neither of these modifications is in fact indispensable. We reconcile 1. and 2. with the experimentally observable dependence. We assume that an enzyme solution in solvent with or without cosolvent molecules is an ensemble of samples with different values of the viscosity for the movement of the system along the reaction coordinate. We assume that this viscosity consists of the contribution with the weight $q$ from cosolvent molecules and that with the weight $1-q$ from protein matrix and solvent molecules. We introduce heterogeneity in our system with the help of a distribution over the weight $q$. We verify the obtained solution of the integral equation for the unknown function of the distribution by direct substitution. All parameters of the model are related to experimentally observable values. General formalism is exemplified by the analysis of literature experimental data for oxygen escape from hemerythin.
q-bio.BM:A simple explanation for the symmetry and degeneracy of the genetic code has been suggested. An alternative to the wobble hypothesis has been proposed. This hypothesis offers explanations for: i) the difference between thymine and uracil, ii) encoding of tryptophan by only one codon, iii) why E. coli have no inosine in isoleucine tRNA, but isoleucine is encoded by three codons. The facts revealed in this study offer a new insight into physical mechanisms of the functioning of the genetic code.
q-bio.BM:A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model. The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids. Our previous studies have dealt with 7510 proteins of no more than 150 amino acids. The proteins are ranked according to the strength of the resistance. Most of the predicted top-strength proteins have not yet been studied experimentally. Architectures and folds which are likely to yield large forces are identified. New types of potent force clamps are discovered. They involve disulphide bridges and, in particular, cysteine slipknots. An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds. These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications. A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known. This class is characterized through molecular dynamics simulations.
q-bio.BM:We have developed a generalized semi-analytic approach for efficiently computing cyclization and looping $J$ factors of DNA under arbitrary binding constraints. Many biological systems involving DNA-protein interactions impose precise boundary conditions on DNA, which necessitates a treatment beyond the Shimada-Yamakawa model for ring cyclization. Our model allows for DNA to be treated as a heteropolymer with sequence-dependent intrinsic curvature and stiffness. In this framework, we independently compute enthlapic and entropic contributions to the $J$ factor and show that even at small length scales $(\sim \ell_{p})$ entropic effects are significant. We propose a simple analytic formula to describe our numerical results for a homogenous DNA in planar loops, which can be used to predict experimental cyclization and loop formation rates as a function of loop size and binding geometry. We also introduce an effective torsional persistence length that describes the coupling between twist and bending of DNA when looped.
q-bio.BM:We use the Dominant Reaction Pathway (DRP) approach to study the dynamics of the folding of a beta-hairpin, within a model which accounts for both native and non-native interactions. We compare the most probable folding pathways calculated with the DRP method with those obtained directly from molecular dynamics (MD) simulations. We find that the two approaches give completely consistent results. We investigate the effects of the non-native hydrophobic interactions on the folding dynamics found them to be small.
q-bio.BM:Mechanical unfolding of the fourth domain of Distyostelium discoideum filamin (DDFLN4) was studied in detail using the C$_{\alpha}$-Go model. We show that unfolding pathways of this protein depend on the pulling speed. The agreement between theoretical and experimental results on the sequencing of unfolding events is achieved at low loading rates. The unfolding free energy landscape is also constructed using dependencies of unfolding forces on pulling speeds.
q-bio.BM:Mechanical unfolding of the fourth domain of Distyostelium discoideum filamin (DDFLN4) was studied by all-atom molecular dynamics simulations, using the GROMOS96 force field 43a1 and the simple point charge explicit water solvent. Our study reveals an important role of non-native interactions in the unfolding process. Namely, the existence of a peak centered at the end-to-end extension 22 nm in the force-extension curve, is associated with breaking of non-native hydrogen bonds. Such a peak has been observed in experiments but not in Go models, where non-native interactions are neglected. We predict that an additional peak occurs at 2 nm using not only GROMOS96 force field 43a1 but also Amber 94 and OPLS force fields. This result would stimulate further experimental studies on elastic properties of DDFLN4.
q-bio.BM:With the aid of quantum mechanical calculations we investigate the electronic structure of the full length (FL) potassium channel protein, FL-KcsA, in its closed conformation, and the electronic structure of the ClC chloride channel. The results indicate that both ion channels are strongly polarized towards the extracellular region with respect to the membrane mean plane. FL-KcsA possesses an electric dipole moment of magnitude 403 Debye while ClC has a macrodipole whose magnitude is about five times larger, 1983 Debye, thereby contributing to differentiate their membrane electric barriers. The dipole vectors of both proteins are aligned along the corresponding selectivity filters. This result suggests that potassium and chloride ion channels are not passive with respect to the movement of ions across the membrane and the ionic motion might be partially driven by the electric field of the protein in conjunction with the electrochemical potential of the membrane.
q-bio.BM:We show that minuscule entropic forces, on the order of 100 fN, can prevent the formation of DNA loops--a ubiquitous means of regulating the expression of genes. We observe a tenfold decrease in the rate of LacI-mediated DNA loop formation when a tension of 200 fN is applied to the substrate DNA, biasing the thermal fluctuations that drive loop formation and breakdown events. Conversely, once looped, the DNA-protein complex is insensitive to applied force. Our measurements are in excellent agreement with a simple polymer model of loop formation in DNA, and show that an anti-parallel topology is the preferred LacI-DNA loop conformation for a generic loop-forming construct.
q-bio.BM:The Electron Microscopy Data Bank (EMDB) is a rapidly growing repository for the dissemination of structural data from single-particle reconstructions of supramolecular protein assemblies including motors, chaperones, cytoskeletal assemblies, and viral capsids. While the static structure of these assemblies provides essential insight into their biological function, their conformational dynamics and mechanics provide additional important information regarding the mechanism of their biological function. Here, we present an unsupervised computational framework to analyze and store for public access the conformational dynamics of supramolecular protein assemblies deposited in the EMDB. Conformational dynamics are analyzed using normal mode analysis in the finite element framework, which is used to compute equilibrium thermal fluctuations, cross-correlations in molecular motions, and strain energy distributions for 452 of the 681 entries stored in the EMDB at present. Results for the viral capsid of hepatitis B, ribosome-bound termination factor RF2, and GroEL are presented in detail and validated with all-atom based models. The conformational dynamics of protein assemblies in the EMDB may be useful in the interpretation of their biological function, as well as in the classification and refinement of EM-based structures.
q-bio.BM:The dynamical characterization of proteins is crucial to understand protein function. From a microscopic point of view, protein dynamics is governed by the local atomic interactions that, in turn, trigger the functional conformational changes. Unfortunately, the relationship between local atomic fluctuations and global protein rearrangements is still elusive. Here, atomistic molecular dynamics simulations in conjunction with complex network analysis show that fast peptide relaxations effectively build the backbone of the global free-energy landscape, providing a connection between local and global atomic rearrangements. A minimum-spanning-tree representation, built on the base of transition gradients networks, results in a high resolution mapping of the system dynamics and thermodynamics without requiring any a priori knowledge of the relevant degrees of freedom. These results suggest the presence of a local mechanism for the high communication efficiency generally observed in complex systems.
q-bio.BM:The functional correlation of missense mutations which cause disease remains a challenge to understanding the basis of genetic diseases. This is particularly true for proteins related to diseases for which there are no available three dimensional structures. One such disease is Shwachman Diamond syndrome SDS OMIM 260400, a multi system disease arising from loss of functional mutations. The Homo sapiens Shwachman Bodian Diamond Syndrome gene hSBDS is responsible for SDS. hSBDS is expressed in all tissues and encodes a protein of 250 amino acids SwissProt accession code Q9Y3A5. Sequence analysis of disease associated alleles has identified more than 20 different mutations in affected individuals. While a number of these mutations have been described as leading to the loss of protein function due to truncation, translation or surface epitope association, the structural basis for these mutations has yet to be determined due to the lack of a three-dimensional structure for SBDS.
q-bio.BM:Using the perturbation-response scanning (PRS) technique, we study a set of 23 proteins that display a variety of conformational motions upon ligand binding (e.g. shear, hinge, allosteric). In most cases, PRS determines residues that may be manipulated to achieve the resulting conformational change. PRS reveals that for some proteins, binding induced conformational change may be achieved through the perturbation of residues scattered throughout the protein, whereas in others, perturbation of specific residues confined to a highly specific region are necessary. Correlations between the experimental and calculated atomic displacements are always better or equivalent to those obtained from a modal analysis of elastic network models. Furthermore, best correlations obtained by the latter approach do not always appear in the most collective modes. We show that success of the modal analysis depends on the lack of redundant paths that exist in the protein. PRS thus demonstrates that several relevant modes may simultaneously be induced by perturbing a single select residue on the protein. We also illustrate the biological relevance of applying PRS on the GroEL and ADK structures in detail, where we show that the residues whose perturbation lead to the precise conformational changes usually correspond to those experimentally determined to be functionally important.
q-bio.BM:Living cells provide a fluctuating, out-of-equilibrium environment in which genes must coordinate cellular function. DNA looping, which is a common means of regulating transcription, is very much a stochastic process; the loops arise from the thermal motion of the DNA and other fluctuations of the cellular environment. We present single-molecule measurements of DNA loop formation and breakdown when an artificial fluctuating force, applied to mimic a fluctuating cellular environment, is imposed on the DNA. We show that loop formation is greatly enhanced in the presence of noise of only a fraction of $k_B T$, yet find that hypothetical regulatory schemes that employ mechanical tension in the DNA--as a sensitive switch to control transcription--can be surprisingly robust due to a fortuitous cancellation of noise effects.
q-bio.BM:Capsids of many viruses assemble around nucleic acids or other polymers. Understanding how the properties of the packaged polymer affect the assembly process could promote biomedical efforts to prevent viral assembly or nanomaterials applications that exploit assembly. To this end, we simulate on a lattice the dynamical assembly of closed, hollow shells composed of several hundred to 1000 subunits, around a flexible polymer. We find that assembly is most efficient at an optimum polymer length that scales with the surface area of the capsid; significantly longer than optimal polymers often lead to partial-capsids with unpackaged polymer `tails' or a competition between multiple partial-capsids attached to a single polymer. These predictions can be tested with bulk experiments in which capsid proteins assemble around homopolymeric RNA or synthetic polyelectrolytes. We also find that the polymer can increase the net rate of subunit accretion to a growing capsid both by stabilizing the addition of new subunits and by enhancing the incoming flux of subunits; the effects of these processes may be distinguishable with experiments that monitor the assembly of individual capsids.
q-bio.BM:We study the control parameters that govern the dynamics of in vitro DNA ejection in bacteriophage lambda. Past work has demonstrated that bacteriophage DNA is highly pressurized; this pressure has been hypothesized to help drive DNA ejection. Ions influence this process by screening charges on DNA; however, a systematic variation of salt concentrations to explore these effects has not been undertaken. To study the nature of the forces driving DNA ejection, we performed in vitro measurements of DNA ejection in bulk and at the single-phage level. We present measurements on the dynamics of ejection and on the self-repulsion force driving ejection. We examine the role of ion concentration and identity in both measurements, and show that the charge of counter-ions is an important control parameter. These measurements show that the frictional force acting on the ejecting DNA is subtly dependent on ionic concentrations for a given amount of DNA in the capsid. We also present evidence that phage DNA forms loops during ejection; we confirm that this effect occurs using optical tweezers. We speculate this facilitates circularization of the genome in the cytoplasm.
q-bio.BM:A relationship between the preexponent of the rate constant and the distribution over activation barrier energies for enzymatic/protein reactions is revealed. We consider an enzyme solution as an ensemble of individual molecules with different values of the activation barrier energy described by the distribution. From the solvent viscosity effect on the preexponent we derive the integral equation for the distribution and find its approximate solution. Our approach enables us to attain a twofold purpose. On the one hand it yields a simple interpretation of the solvent viscosity dependence for enzymatic/protein reactions that requires neither a modification of the Kramers' theory nor that of the Stokes law. On the other hand our approach enables us to deduce the form of the distribution over activation barrier energies. The obtained function has a familiar bell-shaped form and is in qualitative agreement with the results of single enzyme kinetics measurements. General formalism is exemplified by the analysis of literature experimental data.
q-bio.BM:A molecular-level model is used to study the mechanical response of empty cowpea chlorotic mottle virus (CCMV) and cowpea mosaic virus (CPMV) capsids. The model is based on the native structure of the proteins that consitute the capsids and is described in terms of the C-alpha atoms. Nanoindentation by a large tip is modeled as compression between parallel plates. Plots of the compressive force versus plate separation for CCMV are qualitatively consistent with continuum models and experiments, showing an elastic region followed by an irreversible drop in force. The mechanical response of CPMV has not been studied, but the molecular model predicts an order of magnitude higher stiffness and a much shorter elastic region than for CCMV. These large changes result from small structural changes that increase the number of bonds by only 30% and would be difficult to capture in continuum models. Direct comparison of local deformations in continuum and molecular models of CCMV shows that the molecular model undergoes a gradual symmetry breaking rotation and accommodates more strain near the walls than the continuum model. The irreversible drop in force at small separations is associated with rupturing nearly all of the bonds between capsid proteins in the molecular model while a buckling transition is observed in continuum models.
q-bio.BM:Using lattice models we explore the factors that determine the tendencies of polypeptide chains to aggregate by exhaustively sampling the sequence and conformational space. The morphologies of the fibril-like structures and the time scales ($\tau_{fib}$) for their formation depend on a subtle balance between hydrophobic and coulomb interactions. The extent of population of a fibril-prone structure in the spectrum of monomer conformations is the major determinant of $\tau_{fib}$. This observation is used to determine the aggregation-prone consensus sequences by exhaustively exploring the sequence space. Our results provide a basis for genome wide search of fragments that are aggregation prone.
q-bio.BM:Cancer is a proliferation disease affecting a genetically unstable cell population, in which molecular alterations can be somatically inherited by genetic, epigenetic or extragenetic transmission processes, leading to a cooperation of neoplastic cells within tumoral tissue. The efflux protein P-glycoprotein (P gp) is overexpressed in many cancer cells and has known capacity to confer multidrug resistance to cytotoxic therapies. Recently, cell-to-cell P-gp transfers have been shown. Herein, we combine experimental evidence and a mathematical model to examine the consequences of an intercellular P-gp trafficking in the extragenetic transfer of multidrug resistance from resistant to sensitive cell subpopulations. We report cell-to-cell transfers of functional P-gp in co-cultures of a P-gp overexpressing human breast cancer MCF-7 cell variant, selected for its resistance towards doxorubicin, with the parental sensitive cell line. We found that P-gp as well as efflux activity distribution are progressively reorganized over time in co-cultures analyzed by flow cytometry. A mathematical model based on a Boltzmann type integro-partial differential equation structured by a continuum variable corresponding to P-gp activity describes the cell populations in co-culture. The mathematical model elucidates the population elements in the experimental data, specifically, the initial proportions, the proliferative growth rates, and the transfer rates of P-gp in the sensitive and resistant subpopulations. We confirmed cell-to-cell transfer of functional P-gp. The transfer process depends on the gradient of P-gp expression in the donor-recipient cell interactions, as they evolve over time. Extragenetically acquired drug resistance is an additional aptitude of neoplastic cells which has implications in the diagnostic value of P-gp expression and in the design of chemotherapy regimens
q-bio.BM:We discuss the appropriate techniques for modelling the geometry of open ended elastic polymer molecules. The molecule is assumed to have fixed endpoints on a boundary surface. In particular we discuss the concept of the winding number, a directional measure of the linking of two curves, which can be shown to be invariant to the set of continuous deformations vanishing at the polymer's end-point and which forbid it from passing through itself. This measure is shown to be the appropriate constraint required to evaluate the geometrical properties of a constrained DNA molecule. Using the net winding measure we define a model of an open ended constrained DNA molecule which combines the necessary constraint of self-avoidance with being analytically tractable. This model builds upon the local models of Bouchiat and Mezard (2000). In particular, we present a new derivation of the polar writhe expression, which detects both the local winding of the curve and non local winding between different sections of the curve. We then show that this expression correctly tracks the net twisting of a DNA molecule subject to rotation at the endpoints, unlike other definitions used in the literature.
q-bio.BM:The electronic structure of charybdotoxin (ChTX), a scorpion venom peptide that is known to act as a potassium channel blocker, is investigated with the aid of quantum mechanical calculations. The dipole moment vector (145 D) of ChTX can be stirred by the full length KcsA potassium channel's macrodipole (403 D) thereby assuming the proper orientation before binding the ion channel on the cell surface. The localization of the frontier orbitals of ChTX has been revealed for the first time. HOMO is localized on Trp14 while the three lowest-energy MOs (LUMO, LUMO+1, and LUMO+2) are localized on the three disulfide bonds that characterize this pepetide. An effective way to engineer the HOMO-LUMO (H-L) gap of ChTX is that of replacing its Trp14 residue with Ala14 whereas deletion of the LUMO-associated disulfide bond with the insertion of a pair of L-alpha-aminobutyric acid residues does not affect the H-L energy gap.
q-bio.BM:We propose a mechanism that explains in a simple and natural form the l-homochiralization of prebiotic aminoacids in a volume of water where a geothermal gradient exists.
q-bio.BM:The coat proteins of many viruses spontaneously form icosahedral capsids around nucleic acids or other polymers. Elucidating the role of the packaged polymer in capsid formation could promote biomedical efforts to block viral replication and enable use of capsids in nanomaterials applications. To this end, we perform Brownian dynamics on a coarse-grained model that describes the dynamics of icosahedral capsid assembly around a flexible polymer. We identify several mechanisms by which the polymer plays an active role in its encapsulation, including cooperative polymer-protein motions. These mechanisms are related to experimentally controllable parameters such as polymer length, protein concentration, and solution conditions. Furthermore, the simulations demonstrate that assembly mechanisms are correlated to encapsulation efficiency, and we present a phase diagram that predicts assembly outcomes as a function of experimental parameters. We anticipate that our simulation results will provide a framework for designing in vitro assembly experiments on single-stranded RNA virus capsids.
q-bio.BM:The assumption of linear response of protein molecules to thermal noise or structural perturbations, such as ligand binding or detachment, is broadly used in the studies of protein dynamics. Conformational motions in proteins are traditionally analyzed in terms of normal modes and experimental data on thermal fluctuations in such macromolecules is also usually interpreted in terms of the excitation of normal modes. We have chosen two important protein motors - myosin V and kinesin KIF1A - and performed numerical investigations of their conformational relaxation properties within the coarse-grained elastic network approximation. We have found that the linearity assumption is deficient for ligand-induced conformational motions and can even be violated for characteristic thermal fluctuations. The deficiency is particularly pronounced in KIF1A where the normal mode description fails completely in describing functional mechanochemical motions. These results indicate that important assumptions of the theory of protein dynamics may need to be reconsidered. Neither a single normal mode, nor a superposition of such modes yield an approximation of strongly nonlinear dynamics.
q-bio.BM:We perform a quantum mechanical study of the peptides that are part of the LH2 complex from Rhodopseudomonas acidophila, a non-sulfur purple bacteria that has the ability of producing chemical energy from photosynthesis. The electronic structure calculations indicate that the transmembrane helices of these peptides are characterized by dipole moments with a magnitude of ~150 D. When the full nonamer assembly made of eighteen peptides is considered, then a macrodipole of magnitude 704 D is built up from the vector sum of each monomer dipole. The macrodipole is oriented normal to the membrane plane and with the positive tip toward the cytoplasm thereby indicating that the electronic charge of the protein scaffold is polarized toward the periplasm. The results obtained here suggest that the asymmetric charge distribution of the protein scaffold contributes an anisotropic electrostatic environment which differentiates the absorption properties of the bacteriochlorophyll pigments, B800 and B850, embedded in the LH2 complex.
q-bio.BM:The approach for calculation of the mode intensities of DNA conformational vibrations in the Raman spectra is developed. It is based on the valence-optic theory and the model for description of conformational vibrations of DNA with counterions. The calculations for Na- and Cs-DNA low-frequency Raman spectra show that the vibrations of DNA backbone chains near 15 cm-1 have the greatest intensity. In the spectrum of Na-DNA at frequency range upper than 40 cm-1 the modes of H-bond stretching in base pairs have the greatest intensities, while the modes of ion-phosphate vibrations have the lowest intensity. In Cs-DNA spectra at this frequency range the mode of ion-phosphate vibrations is prominent. Its intensity is much higher than the intensities of Na-DNA modes of this spectra range. Other modes of Cs-DNA have much lower intensities than in the case of Na-DNA. The comparison of our calculations with the experimental data shows that developed approach gives the understanding of the sensitivity of DNA low-frequency Raman bands to the neutralization of the double helix by light and heavy counterions.
q-bio.BM:The protein folding is regarded as a quantum transition between torsion states on polypeptide chain. The deduction of the folding rate formula in our previous studies is reviewed. The rate formula is generalized to the case of frequency variation in folding. Then the following problems about the application of the rate theory are discussed: 1) The unified theory on the two-state and multi-state protein folding is given based on the concept of quantum transition. 2) The relationship of folding and unfolding rates vs denaturant concentration is studied. 3) The temperature dependence of folding rate is deduced and the non-Arrhenius behaviors of temperature dependence are interpreted in a natural way. 4) The inertial moment dependence of folding rate is calculated based on the model of dynamical contact order and consistent results are obtained by comparison with one-hundred-protein experimental dataset. 5) The exergonic and endergonic foldings are distinguished through the comparison between theoretical and experimental rates for each protein. The ultrafast folding problem is viewed from the point of quantum folding theory and a new folding speed limit is deduced from quantum uncertainty relation. And finally, 6) since only the torsion-accessible states are manageable in the present formulation of quantum transition how the set of torsion-accessible states can be expanded by using statistical energy landscape approach is discussed. All above discussions support the view that the protein folding is essentially a quantum transition between conformational states.
q-bio.BM:Homologous recombination plays a key role in generating genetic diversity, while maintaining protein functionality. The mechanisms by which RecA enables a single-stranded segment of DNA to recognize a homologous tract within a whole genome are poorly understood. The scale by which homology recognition takes place is of a few tens of base pairs, after which the quest for homology is over. To study the mechanism of homology recognition, RecA-promoted homologous recombination between short DNA oligomers with different degrees of heterology was studied in vitro, using fluorescence resonant energy transfer. RecA can detect single mismatches at the initial stages of recombination, and the efficiency of recombination is strongly dependent on the location and distribution of mismatches. Mismatches near the 5' end of the incoming strand have a minute effect, whereas mismatches near the 3' end hinder strand exchange dramatically. There is a characteristic DNA length above which the sensitivity to heterology decreases sharply. Experiments with competitor sequences with varying degrees of homology yield information about the process of homology search and synapse lifetime. The exquisite sensitivity to mismatches and the directionality in the exchange process support a mechanism for homology recognition that can be modeled as a kinetic proofreading cascade.
q-bio.BM:Cell-penetrating peptides (CPPs) such as HIV's trans-activating transcriptional activator (TAT) and polyarginine rapidly pass through the plasma membranes of mammalian cells by an unknown mechanism called transduction. They may be medically useful when fused to well-chosen chains of fewer than about 35 amino acids. I offer a simple model of transduction in which phosphatidylserines and CPPs effectively form two plates of a capacitor with a voltage sufficient to cause the formation of transient pores (electroporation). The model is consistent with experimental data on the transduction of oligoarginine into mouse C2-C12 myoblasts and makes three testable predictions.
q-bio.BM:In recent years, single molecule force techniques have opened a new avenue to decipher the folding landscapes of biopolymers by allowing us to watch and manipulate the dynamics of individual proteins and nucleic acids. In single molecule force experiments, quantitative analyses of measurements employing sound theoretical models and molecular simulations play central role more than any other field. With a brief description of basic theories for force mechanics and molecular simulation technique using self-organized polymer (SOP) model, this chapter will discuss various issues in single molecule force spectroscopy (SMFS) experiments, which include pulling speed dependent unfolding pathway, measurement of energy landscape roughness, the in uence of molecular handles in optical tweezers on measurement and molecular motion, and folding dynamics of biopolymers under force quench condition.
q-bio.BM:We explore in detail the structural, mechanical and thermodynamic properties of a coarse-grained model of DNA similar to that introduced in Thomas E. Ouldridge, Ard A. Louis, Jonathan P.K. Doye, Phys. Rev. Lett. 104 178101 (2010). Effective interactions are used to represent chain connectivity, excluded volume, base stacking and hydrogen bonding, naturally reproducing a range of DNA behaviour. We quantify the relation to experiment of the thermodynamics of single-stranded stacking, duplex hybridization and hairpin formation, as well as structural properties such as the persistence length of single strands and duplexes, and the torsional and stretching stiffness of double helices. We also explore the model's representation of more complex motifs involving dangling ends, bulged bases and internal loops, and the effect of stacking and fraying on the thermodynamics of the duplex formation transition.
q-bio.BM:In this and the associated article 'BioBlender: A Software for Intuitive Representation of Surface Properties of Biomolecules', (Andrei et al) we present BioBlender as a complete instrument for the elaboration of motion (here) and the visualization (Andrei et al) of proteins and other macromolecules, using instruments of computer graphics. A vast number of protein (if not most) exert their function through some extent of motion. Despite recent advances in higly performant methods, it is very difficult to obtain direct information on conformational changes of molecules. However, several systems exist that can shed some light on the variability of conformations of a single peptide chain; among them, NMR methods provide collections of a number of static 'shots' of a moving protein. Starting from this data, and assuming that if a protein exists in more than 1 conformation it must be able to transit between the different states, we have elaborated a system that makes ample use of the computational power of 3D computer graphics technology. Considering information of all (heavy) atoms, we use animation and game engine of Blender to obtain transition states. The model we chose to elaborate our system is Calmodulin, a protein favorite among structural and dynamic studies due to its (relative) simplicity of structure and small dimension. Using Calmodulin we show a procedure that enables the building of a 'navigation map' of NMR models, that can help in the identification of movements. In the process, a number of intermediate conformations is generated, all of which respond to strict bio-physical and bio-chemical criteria. The BioBlender system is available for download from the website www.bioblender.net, together with examples, tutorial and other useful material.
q-bio.BM:Nested sampling is a Bayesian sampling technique developed to explore probability distributions lo- calised in an exponentially small area of the parameter space. The algorithm provides both posterior samples and an estimate of the evidence (marginal likelihood) of the model. The nested sampling algo- rithm also provides an efficient way to calculate free energies and the expectation value of thermodynamic observables at any temperature, through a simple post-processing of the output. Previous applications of the algorithm have yielded large efficiency gains over other sampling techniques, including parallel tempering (replica exchange). In this paper we describe a parallel implementation of the nested sampling algorithm and its application to the problem of protein folding in a Go-type force field of empirical potentials that were designed to stabilize secondary structure elements in room-temperature simulations. We demonstrate the method by conducting folding simulations on a number of small proteins which are commonly used for testing protein folding procedures: protein G, the SH3 domain of Src tyrosine kinase and chymotrypsin inhibitor 2. A topological analysis of the posterior samples is performed to produce energy landscape charts, which give a high level description of the potential energy surface for the protein folding simulations. These charts provide qualitative insights into both the folding process and the nature of the model and force field used.
q-bio.BM:The electrochemical behaviour of the biomedical and metallic alloys, especially in the orthopaedic implants fields, raises many questions. This study is dedicated for studying the Ti-6Al-4V alloy, by electrochemical impedance spectroscopy, EIS, in various physiological media,: Ringer solution, phosphate buffered solution (PBS), PBS solution and albumin, PBS solution with calf serum and PBS solution with calf serum and an antioxidant (sodium azide). Moreover, the desionised water was considered as the reference solution. The tests reproducibility was investigated. The time-frequency-Module graphs highlighted that the desionised water is the most protective for the Ti-6Al-4V alloy. This biomedical alloy is the less protected in the solution constituted by PBS and albumin. The time-frequency graph allows pointing out the graphic signatures of adsorption for organic and inorganic species (differences between the modules means in studied solution and the modules mean in the reference solution). --- Le comportement \'electrochimique des alliages m\'etalliques biom\'edicaux, notamment dans le domaine des implants orthop\'ediques, pose encore de nombreuses questions. Ce travail propose d'\'etudier l'alliage de titane Ti-6Al-4V, par spectroscopie d'imp\'edance \'electrochimique, SIE, dans diff\'erents milieux physiologiques : solution de Ringer, solution \`a base d'un tampon phosphate (PBS), solution PBS avec de l'albumine, solution PBS avec du s\'erum bovin et une solution PBS avec du s\'erum bovin et un antioxydant (azoture de sodium). De plus, une solution d'eau ultra-pure servira de r\'ef\'erence. La reproductibilit\'e des tests a \'et\'e \'etudi\'ee. Les repr\'esentations temps-fr\'equence des modules ont mis en \'evidence que l'eau d\'esionis\'ee est la solution qui pr\'esente le caract\`ere le plus protecteur pour le Ti-6Al-4V. Cet alliage de titane est le moins prot\'eg\'e dans la solution de PBS contenant de l'albumine. Cette repr\'esentation permet de mettre en \'evidence des signatures graphiques d'adsorption des esp\`eces inorganiques et organiques (diff\'erences entre les moyennes des modules dans les solutions \'etudi\'ees et la moyenne des modules dans la solution de r\'ef\'erence).
q-bio.BM:We present a rigidity analysis on a large number of X-ray crystal structures of the enzyme HIV-1 protease using the 'pebble game' algorithm of the software FIRST. We find that although the rigidity profile remains similar across a comprehensive set of high resolution structures, the profile changes significantly in the presence of an inhibitor. Our study shows that the action of the inhibitors is to restrict the flexibility of the beta-hairpin flaps which allow access to the active site. The results are discussed in the context of full molecular dynamics simulations as well as data from NMR experiments.
q-bio.BM:Conventional kinesin is a two-headed homodimeric motor protein, which is able to walk along microtubules processively by hydrolyzing ATP. Its neck linkers, which connect the two motor domains and can undergo a docking/undocking transition, are widely believed to play the key role in the coordination of the chemical cycles of the two motor domains and, consequently, in force production and directional stepping. Although many experiments, often complemented with partial kinetic modeling of specific pathways, support this idea, the ultimate test of the viability of this hypothesis requires the construction of a complete kinetic model. Considering the two neck linkers as entropic springs that are allowed to dock to their head domains and incorporating only the few most relevant kinetic and structural properties of the individual heads, here we develop the first detailed, thermodynamically consistent model of kinesin that can (i) explain the cooperation of the heads (including their gating mechanisms) during walking and (ii) reproduce much of the available experimental data (speed, dwell time distribution, randomness, processivity, hydrolysis rate, etc.) under a wide range of conditions (nucleotide concentrations, loading force, neck linker length and composition, etc.). Besides revealing the mechanism by which kinesin operates, our model also makes it possible to look into the experimentally inaccessible details of the mechanochemical cycle and predict how certain changes in the protein affect its motion.
q-bio.BM:Fifteen years ago Monique Tirion showed that the low-frequency normal modes of a protein are not significantly altered when non-bonded interactions are replaced by Hookean springs, for all atom pairs whose distance is smaller than a given cutoff value. Since then, it has been shown that coarse-grained versions of Tirion's model are able to provide fair insights on many dynamical properties of biological macromolecules. In this text, theoretical tools required for studying these so-called Elastic Network Models are described, focusing on practical issues and, in particular, on possible artifacts. Then, an overview of some typical results that have been obtained by studying such models is given.
q-bio.BM:A recent survey of 17 134 proteins has identified a new class of proteins which are expected to yield stretching induced force-peaks in the range of 1 nN. Such high force peaks should be due to forcing of a slip-loop through a cystine ring, i.e. by generating a cystine slipknot. The survey has been performed in a simple coarse grained model. Here, we perform all-atom steered molecular dynamics simulations on 15 cystine knot proteins and determine their resistance to stretching. In agreement with previous studies within a coarse grained structure based model, the level of resistance is found to be substantially higher than in proteins in which the mechanical clamp operates through shear. The large stretching forces arise through formation of the cystine slipknot mechanical clamp and the resulting steric jamming. We elucidate the workings of such a clamp in an atomic detail. We also study the behavior of five top strength proteins with the shear-based mechanostability in which no jamming is involved. We show that in the atomic model, the jamming state is relieved by moving one amino acid at a time and there is a choice in the selection of the amino acid that advances the first. In contrast, the coarse grained model also allows for a simultaneous passage of two amino acids.
q-bio.BM:A quantum theory on conformation-electron system is presented. Protein folding is regarded as the quantum transition between torsion states on polypeptide chain, and the folding rate is calculated by nonadiabatic operator method. The theory is used to study the temperature dependences of folding rate of 15 proteins and their non-Arrhenius behavior can all be deduced in a natural way. A general formula on the rate-temperature dependence has been deduced which is in good accordance with experimental data. These temperature dependences are further analyzed in terms of torsion potential parameters. Our results show it is necessary to move outside the realm of classical physics when the temperature dependence of protein folding is studied quantitatively.
q-bio.BM:Lasso peptides constitute a class of bioactive peptides sharing a knotted structure where the C-terminal tail of the peptide is threaded through and trapped within an N-terminalmacrolactamring. The structural characterization of lasso structures and differentiation from their unthreaded topoisomers is not trivial and generally requires the use of complementary biochemical and spectroscopic methods. Here we investigated two antimicrobial peptides belonging to the class II lasso peptide family and their corresponding unthreaded topoisomers: microcin J25 (MccJ25), which is known to yield two-peptide product ions specific of the lasso structure under collisioninduced dissociation (CID), and capistruin, for which CID does not permit to unambiguously assign the lasso structure. The two pairs of topoisomers were analyzed by electrospray ionization Fourier transform ion cyclotron resonance mass spectrometry (ESI-FTICR MS) upon CID, infrared multiple photon dissociation (IRMPD), and electron capture dissociation (ECD). CID and ECDspectra clearly permitted to differentiate MccJ25 from its non-lasso topoisomer MccJ25-Icm, while for capistruin, only ECD was informative and showed different extent of hydrogen migration (formation of c\bullet/z from c/z\bullet) for the threaded and unthreaded topoisomers. The ECD spectra of the triply-charged MccJ25 and MccJ25-lcm showed a series of radical b-type product ions {\eth}b0In{\TH}. We proposed that these ions are specific of cyclic-branched peptides and result from a dual c/z\bullet and y/b dissociation, in the ring and in the tail, respectively. This work shows the potentiality of ECD for structural characterization of peptide topoisomers, as well as the effect of conformation on hydrogen migration subsequent to electron capture.
q-bio.BM:Nanospray and collisionally-induced dissociation are used to evaluate the presence and absence of interstrand co-chelation of zinc ions in dimers of metallothionein. As was reported in a previous publication from this laboratory, co-chelation stabilizes the dimer to collisional activation, and facilitates asymmetrical zinc ion transfers during fragmentation. In the case of metallothionein, dimers of the holoprotein are found to share zinc ions, while dimers of metallothionein, in which one domain has been denatured, do not. Zinc ions are silent to most physicochemical probes, e.g., NMR and Mossbauer spectroscopies, and the capability of mass spectrometry to provide information on zinc complexes has widespread potential application in biochemistry.
q-bio.BM:Polymers can be modeled as open polygonal paths and their closure generates knots. Knotted proteins detection is currently achieved via high-throughput methods based on a common framework insensitive to the handedness of knots. Here we propose a topological framework for the computation of the HOMFLY polynomial, an handedness-sensitive invariant. Our approach couples a multi-component reduction scheme with the polynomial computation. After validation on tabulated knots and links the framework was applied to the entire Protein Data Bank along with a set of selected topological checks that allowed to discard artificially entangled structures. This led to an up-to-date table of knotted proteins that also includes two newly detected right-handed trefoil knots in recently deposited protein structures. The application range of our framework is not limited to proteins and it can be extended to the topological analysis of biological and synthetic polymers and more generally to arbitrary polygonal paths.
q-bio.BM:Atomic-accuracy structure prediction of macromolecules is a long-sought goal of computational biophysics. Accurate modeling should be achievable by optimizing a physically realistic energy function but is presently precluded by incomplete sampling of a biopolymer's many degrees of freedom. We present herein a working hypothesis, called the "stepwise ansatz", for recursively constructing well-packed atomic-detail models in small steps, enumerating several million conformations for each monomer and covering all build-up paths. By implementing the strategy in Rosetta and making use of high-performance computing, we provide first tests of this hypothesis on a benchmark of fifteen RNA loop modeling problems drawn from riboswitches, ribozymes, and the ribosome, including ten cases that were not solvable by prior knowledge based modeling approaches. For each loop problem, this deterministic stepwise assembly (SWA) method either reaches atomic accuracy or exposes flaws in Rosetta's all-atom energy function, indicating the resolution of the conformational sampling bottleneck. To our knowledge, SWA is the first enumerative, ab initio build-up method to systematically outperform existing Monte Carlo and knowledge-based methods for 3D structure prediction. As a rigorous experimental test, we have applied SWA to a small RNA motif of previously unknown structure, the C7.2 tetraloop/tetraloop-receptor, and stringently tested this blind prediction with nucleotide-resolution structure mapping data.
q-bio.BM:The surface accessibility of {\alpha}-bungarotoxin has been investigated by using Gd2L7, a newly designed paramagnetic NMR probe. Signal attenuations induced by Gd2L7 on {\alpha}-bungarotoxin C{\alpha}H peaks of 1H-13C HSQC spectra have been analyzed and compared with the ones previously obtained in the presence of GdDTPA-BMA. In spite of the different molecular size and shape, for the two probes a common pathway of approach to the {\alpha}-bungarotoxin surface can be observed with an equally enhanced access of both GdDTPA-BMA and Gd2L7 towards the protein surface side where the binding site is located. Molecular dynamics simulations suggest that protein backbone flexibility and surface hydration contribute to the observed preferential approach of both gadolinium complexes specifically to the part of the {\alpha}-bungarotoxin surface which is involved in the interaction with its physiological target, the nicotinic acetylcholine receptor.
q-bio.BM:The rates of protein folding with photon absorption or emission and the cross section of photon -protein inelastic scattering are calculated from the quantum folding theory by use of standard field-theoretical method. All these protein photo-folding processes are compared with common protein folding without interaction of photons (nonradiative folding). It is demonstrated that there exists a common factor (thermo-averaged overlap integral of vibration wave function, TAOI) for protein folding and protein photo-folding. Based on this finding it is predicted that: 1) the stimulated photo-folding rates show the same temperature dependence as protein folding; 2) the spectral line of electronic transition is broadened to a band which includes abundant vibration spectrum without and with conformational transition and the width of the vibration spectral line is largely reduced; 3) the resonance fluorescence cross section changes with temperature obeying the same law (Luo-Lu's law). The particular form of the folding rate - temperature relation and the abundant spectral structure imply the existence of a set of quantum oscillators in the transition process and these oscillators are mainly of torsion type of low frequency, imply the quantum tunneling between protein conformations does exist in folding and photo-folding processes and the tunneling is rooted deeply in the coherent motion of the conformational-electronic system.
q-bio.BM:In this paper we will review various aspects of the biology of prions and focus on what is currently known about the mammalian PrP prion. Also we briefly describe the prions of yeast and other fungi. Prions are infectious proteins behaving like genes, i.e. proteins that not only contain genetic information in its tertiary structure, i.e. its shape, but are also able to transmit and replicate in a manner analogous to genes but through very different mechanisms. The term prion is derived from "proteinaceous infectious particle" and arose from the Prusiner hypothesis that the infectious agent of certain neurodegenerative diseases was only in a protein, without the participation of nucleic acids. Currently there are several known types of prion, in addition to the originally described, which are pathogens of mammals, yeast and other fungi. Prion proteins are ubiquitous and not always detrimental to their hosts. This vision of the prion as a causative agent of disease is changing, finding more and more evidence that they could have important roles in cells and contribute to the phenotypic plasticity of organisms through the mechanisms of evolution.
q-bio.BM:Genomic DNA is constantly subjected to various mechanical stresses arising from its biological functions and cell packaging. If the local mechanical properties of DNA change under torsional and tensional stress, the activity of DNA-modifying proteins and transcription factors can be affected and regulated allosterically. To check this possibility, appropriate steady forces and torques were applied in the course of all-atom molecular dynamics simulations of DNA with AT- and GC-alternating sequences. It is found that the stretching rigidity grows with tension as well as twisting. The torsional rigidity is not affected by stretching, but it varies with twisting very strongly, and differently for the two sequences. Surprisingly, for AT-alternating DNA it passes through a minimum with the average twist close to the experimental value in solution. For this fragment, but not for the GC-alternating sequence, the bending rigidity noticeably changes with both twisting and stretching. The results have important biological implications and shed light upon earlier experimental observations.
q-bio.BM:The 3'-monofunctional adduct of cisplatin and d(CTCTG*G*TCTC)2 duplex DNA in solvent with explicit counter ions and water molecules were subjected to MD- simulation with AMBER force field on a nanosecond time scale. In order to simulate the closure of the bond between the Pt and 5'-guanine-N7 atoms, the forces acting between them were gradually increased during MD. After 500-800 ps the transformation of the mono-adduct (straight DNA with the cisplatin residue linked to one guanine-N7) to the bus-adduct (bent DNA where Pt atom is connected through the N7 atoms of neighboring guanines) was observed. A cavity between palatinate guanines is formed and filled with solvent molecules. The rapid inclination of the center base pairs initiates a slow transition of the whole molecule from the linear to the bent conformation. After about 1000-1300 ps a stable structure was reached, which is very similar to the one described experimentally. The attractive force between the Pt- atom and the N7 of the second guanine plays the main role in the large conformational changes induced by formation of the adduct-adduct. X-N-Pt-N-torsions accelerate the bending but a torsion force constant greater than 0.2 Kcal/mol lead to the breaking of the H-bonds within the base pairs. The present study is the first dynamical simulation that demonstrates in real time scale such a large conformational perturbation of DNA.
q-bio.BM:Despite the recognized importance of the multi-scale spatio-temporal organization of proteins, most computational tools can only access a limited spectrum of time and spatial scales, thereby ignoring the effects on protein behavior of the intricate coupling between the different scales. Starting from a physico-chemical atomistic network of interactions that encodes the structure of the protein, we introduce a methodology based on multi-scale graph partitioning that can uncover partitions and levels of organization of proteins that span the whole range of scales, revealing biological features occurring at different levels of organization and tracking their effect across scales. Additionally, we introduce a measure of robustness to quantify the relevance of the partitions through the generation of biochemically-motivated surrogate random graph models. We apply the method to four distinct conformations of myosin tail interacting protein, a protein from the molecular motor of the malaria parasite, and study properties that have been experimentally addressed such as the closing mechanism, the presence of conserved clusters, and the identification through computational mutational analysis of key residues for binding.
q-bio.BM:In this work two archaea microorganisms (Haloferax volcanii and Natrialba magadii) used as biocatalyst at a microbial fuel cell (MFC) anode were evaluated. Both archaea are able to grow at high salt concentrations. By increasing the media conductivity, the internal resistance was diminished, improving the MFCs performance. Without any added redox mediator, maximum power (Pmax) and current at Pmax were 11.87 / 4.57 / 0.12 {\mu}W cm-2 and 49.67 / 22.03 / 0.59 {\mu}A cm-2 for H. volcanii, N. magadii and E. coli, respectively. When neutral red was used as redox mediator, Pmax was 50.98 and 5.39 {\mu}W cm-2 for H. volcanii and N. magadii respectively. In this paper an archaea MFC is described and compared with other MFC systems; the high salt concentration assayed here, comparable with that used in Pt-catalyzed alkaline hydrogen fuel cells will open new options when MFC scaling-up is the objective, necessary for practical applications.
q-bio.BM:Three-dimensional RNA models fitted into crystallographic density maps exhibit pervasive conformational ambiguities, geometric errors and steric clashes. To address these problems, we present enumerative real-space refinement assisted by electron density under Rosetta (ERRASER), coupled to Python-based hierarchical environment for integrated 'xtallography' (PHENIX) diffraction-based refinement. On 24 data sets, ERRASER automatically corrects the majority of MolProbity-assessed errors, improves the average Rfree factor, resolves functionally important discrepancies in noncanonical structure and refines low-resolution models to better match higher-resolution models.
q-bio.BM:This paper has been withdrawn by the author due to a missing figure
q-bio.BM:We develop a theory of aggregation using statistical mechanical methods. An example of a complicated aggregation system with several levels of structures is peptide/protein self-assembly. The problem of protein aggregation is important for the understanding and treatment of neurodegenerative diseases and also for the development of bio-macromolecules as new materials. We write the effective Hamiltonian in terms of interaction energies between protein monomers, protein and solvent, as well as between protein filaments. The grand partition function can be expressed in terms of a Zimm-Bragg-like transfer matrix, which is calculated exactly and all thermodynamic properties can be obtained. We start with two-state and three-state descriptions of protein monomers using Potts models that can be generalized to include q-states, for which the exactly solvable feature of the model remains. We focus on n X N lattice systems, corresponding to the ordered structures observed in some real fibrils. We have obtained results on nucleation processes and phase diagrams, in which a protein property such as the sheet content of aggregates is expressed as a function of the number of proteins on the lattice and inter-protein or interfacial interaction energies. We have applied our methods to A{\beta}(1-40) and Curli fibrils and obtained results in good agreement with experiments.
q-bio.BM:By studying the literature about Tetracyclines (TCs), it becomes clearly evident that TCs are very dynamic molecules. In some cases, their structure-activity-relationship (SAR) are known, especially against bacteria, while against other targets, they are virtually unknown. In other diverse yields of research, such as neurology, oncology and virology the utility and activity of the tetracyclines are being discovered and are also emerging as new technological fronts. The first aim of this paper is classify the compounds already used in therapy and prepare the schematic structure in which include the next generation of TCs. The aim of this work is introduce a new framework for the classification of old and new TCs, using a medicinal chemistry approach to the structure of that drugs. A fully documented Structure-Activity-Relationship (SAR) is presented with the analysis data of antibacterial and nonantibacterial (antifungal, antiviral and anticancer) tetracyclines. Lipophilicity of functional groups and conformations interchangeably are determining rules in biological activities of TCs.
q-bio.BM:The consequences of recent experimental finding that hydrogen bonds of the anti-parallel $\beta $-sheet in nonspecific binding site of serine proteases become significantly shorter and stronger synchronously with the catalytic act are examined. We investigate the effect of the transformation of an ordinary hydrogen bond into a low-barrier one on the crankshaft motion a peptide group in the anti-parallel $\beta $-sheet. For this purpose we make use of a realistic model of the peptide chain with stringent microscopically derived coupling interaction potential and effective on-site potential. The coupling interaction characterizing the peptide chain rigidity is found to be surprisingly weak and repulsive in character. The effective on-site potential is found to be a hard one, i.e., goes more steep than a harmonic one. At transformation of the ordinary hydrogen bond into the low-barrier one the frequency of crankshaft motion of the corresponding peptide group in the anti-parallel $\beta $-sheet is roughly doubled.
q-bio.BM:During this era of new drug designing, medicinal plants had become a very interesting object of further research. Pharmacology screening of active compound of medicinal plants would be time consuming and costly. Molecular docking is one of the in silico method which is more efficient compare to in vitro or in vivo method for its capability of finding the active compound in medicinal plants. In this method, three-dimensional structure becomes very important in the molecular docking methods, so we need a database that provides information on three-dimensional structures of chemical compounds from medicinal plants in Indonesia. Therefore, this study will prepare a database which provides information of the three dimensional structures of chemical compounds of medicinal plants. The database will be prepared by using MySQL format and is designed to be placed in http://herbaldb.farmasi.ui.ac.id website so that eventually this database can be accessed quickly and easily by users via the Internet.
q-bio.BM:We investigate the possibility that prebiotic homochirality can be achieved exclusively through chiral-selective reaction rate parameters without any other explicit mechanism for chiral bias. Specifically, we examine an open network of polymerization reactions, where the reaction rates can have chiral-selective values. The reactions are neither autocatalytic nor do they contain explicit enantiomeric cross-inhibition terms. We are thus investigating how rare a set of chiral-selective reaction rates needs to be in order to generate a reasonable amount of chiral bias. We quantify our results adopting a statistical approach: varying both the mean value and the rms dispersion of the relevant reaction rates, we show that moderate to high levels of chiral excess can be achieved with fairly small chiral bias, below 10%. Considering the various unknowns related to prebiotic chemical networks in early Earth and the dependence of reaction rates to environmental properties such as temperature and pressure variations, we argue that homochirality could have been achieved from moderate amounts of chiral selectivity in the reaction rates.
q-bio.BM:Channels are Catalysts for Diffusion and in that sense are Enzymes. This idea is useful for the design and interpretation of experiments.
q-bio.BM:Ahmad et al. recently presented an NMR-based model for a bacterial DnaJ J domain:DnaK(Hsp70):ADP complex(1) that differs significantly from the crystal structure of a disulfide linked mammalian auxilin J domain:Hsc70 complex that we previously published(2). They claimed that their model could better account for existing mutational data, was in better agreement with previous NMR studies, and that the presence of a cross-link in our structure made it irrelevant to understanding J:Hsp70 interactions. Here we detail extensive NMR and mutational data relevant to understanding J:Hsp70 function and show that, in fact, our structure is much better able to account for the mutational data and is in much better agreement with a previous NMR study of a mammalian polyoma virus T-ag J domain:Hsc70 complex than is the Ahmad et al. complex, and that our structure is predictive and provides insight into J:Hsp70 interactions and mechanism of ATPase activation.
q-bio.BM:Protein function frequently involves conformational changes with large amplitude on timescales which are difficult and computationally expensive to access using molecular dynamics. In this paper, we report on the combination of three computationally inexpensive simulation methods-normal mode analysis using the elastic network model, rigidity analysis using the pebble game algorithm, and geometric simulation of protein motion-to explore conformational change along normal mode eigenvectors. Using a combination of ELNEMO and FIRST/FRODA software, large-amplitude motions in proteins with hundreds or thousands of residues can be rapidly explored within minutes using desktop computing resources. We apply the method to a representative set of six proteins covering a range of sizes and structural characteristics and show that the method identifies specific types of motion in each case and determines their amplitude limits.
q-bio.BM:The enzyme FoF1-ATP synthase provides the 'chemical energy currency' adenosine triphosphate (ATP) for living cells. Catalysis is driven by mechanochemical coupling of subunit rotation within the enzyme with conformational changes in the three ATP binding sites. Proton translocation through the membrane-bound Fo part of ATP synthase powers a 10-step rotary motion of the ring of c subunits. This rotation is transmitted to the gamma and epsilon subunits of the F1 part. Because gamma and epsilon subunits rotate in 120 deg steps, we aim to unravel this symmetry mismatch by real time monitoring subunit rotation using single-molecule Forster resonance energy transfer (FRET). One fluorophore is attached specifically to the F1 motor, another one to the Fo motor of the liposome-reconstituted enzyme. Photophysical artifacts due to spectral fluctuations of the single fluorophores are minimized by a previously developed duty cycle-optimized alternating laser excitation scheme (DCO-ALEX). We report the detection of reversible elastic deformations between the rotor parts of Fo and F1 and estimate the maximum angular displacement during the load-free rotation using Monte Carlo simulations
q-bio.BM:A strain of Halomonas bacteria, GFAJ-1, has been reported to be able to use arsenate as a nutrient when phosphate is limiting, and to specifically incorporate arsenic into its DNA in place of phosphorus. However, we have found that arsenate does not contribute to growth of GFAJ-1 when phosphate is limiting and that DNA purified from cells grown with limiting phosphate and abundant arsenate does not exhibit the spontaneous hydrolysis expected of arsenate ester bonds. Furthermore, mass spectrometry showed that this DNA contains only trace amounts of free arsenate and no detectable covalently bound arsenate.
q-bio.BM:The fundamental law for protein folding is the Thermodynamic Principle: the amino acid sequence of a protein determines its native structure and the native structure has the minimum Gibbs free energy. If all chemical problems can be answered by quantum mechanics, there should be a quantum mechanics derivation of Gibbs free energy formula G(X) for every possible conformation X of the protein. We apply quantum statistics to derive such a formula. For simplicity, only monomeric self folding globular proteins are covered. We point out some immediate applications of the formula. We show that the formula explains the observed phenomena very well. It gives a unified explanation to both folding and denaturation; it explains why hydrophobic effect is the driving force of protein folding and clarifies the role played by hydrogen bonding; it explains the successes and deficients of various surface area models. The formula also gives a clear kinetic force of the folding: Fi(X) = - \nablaxi G(X). This also gives a natural way to perform the ab initio prediction of protein structure, minimizing G(X) by Newton's fastest desciending method.
q-bio.BM:Learning how proteins fold will hardly have any impact in the way conventional -- active site centered -- drugs are designed. On the other hand, this knowledge is proving instrumental in defining a new paradigm for the identification of drugs against any target protein: folding inhibition. Targeting folding renders drugs less prone to elicit spontaneous genetic mutations which in many cases, notably in connection with viruses like the Human Immunodeficiency Virus (HIV), can block therapeutic action. From the progress which has taken place during the last years in the understanding of the becoming of a protein, and how to read from the corresponding sequences the associated three-dimensional, biologically active, native structure, the idea of non-conventional (folding) inhibitors and thus of leads to eventual drugs to fight disease, arguably, without creating resistance, emerges as a distinct possibility.
q-bio.BM:Ever since the disorder of proteins is the main cause for many diseases. As compared with other disorders, the major reason that causes disease is of structural inability of many proteins. The potentially imminent availability of recent datasets helps one to discover the protein disorders, however in majority of cases, the stability of proteins depend on the carbon content. Addressing this distinct feature, it is possible to hit upon the carbon distribution along the sequence and can easily recognize the stable nature of protein. There are certain reported mental disorders which fall in to this category. Regardless, such kind of disorder prone protein FMR1p (Fragile X mental retardation 1 protein) is identified as the main cause for the disease Fragile X syndrome. This paper deals with the identification of defects in the FMR1 protein sequence considering the carbon contents along the sequence. This attempt is to evaluate the stability of proteins, accordingly the protein disorders in order to improvise the certain Biological functions of proteins to prevent disease. The transition of the disorder to order protein involves careful considerations and can be achieved by detecting the unstable region that lacks hydrophobicity. This work focuses the low carbon content in the FMR1 protein so as to attain the stable status in future to reduce the morbidity rate caused by Fragile X syndrome for the society.
q-bio.BM:Influenza virus evolves to escape from immune system antibodies that bind to it. We used free energy calculations with Einstein crystals as reference states to calculate the difference of antibody binding free energy ($\Delta\Delta G$) induced by amino acid substitution at each position in epitope B of the H3N2 influenza hemagglutinin, the key target for antibody. A substitution with positive $\Delta\Delta G$ value decreases the antibody binding constant. On average an uncharged to charged amino acid substitution generates the highest $\Delta\Delta G$ values. Also on average, substitutions between small amino acids generate $\Delta\Delta G$ values near to zero. The 21 sites in epitope B have varying expected free energy differences for a random substitution. Historical amino acid substitutions in epitope B for the A/Aichi/2/1968 strain of influenza A show that most fixed and temporarily circulating substitutions generate positive $\Delta\Delta G$ values. We propose that the observed pattern of H3N2 virus evolution is affected by the free energy landscape, the mapping from the free energy landscape to virus fitness landscape, and random genetic drift of the virus. Monte Carlo simulations of virus evolution are presented to support this view.
q-bio.BM:Force field and first principles molecular dynamics simulations on complexes of pig liver esterase (pig liver isoenzymes and a mutant) and selected substrates (1-phenyl-1-ethyl acetate, 1- phenyl-2-butylacetate, proline-{\beta}-naphthylamide and methyl butyrate) are presented. By restrained force field simulations the access of the substrate to the hidden active site was probed. For a few substrates spontaneous access to the active site via a well defined entrance channel was found. The structure of the tetrahedral intermediate was simulated for several substrates and our previous assignment of GLU 452 instead of GLU 336 was confirmed. It was shown that the active site readily adapts to the embedded substrate involving a varying number of hydrophobic residues in the neighborhood. This puts into question key-lock models for enantioselectivity. Ab initio molecular dynamics showed that the structures we found for the tetrahedral intermediate in force field simulations are consistent with the presumed mechanism of ester cleavage. Product release from the active site as final step of the enzymatic reaction revealed to be very slow and took already more than 20ns for the smallest product, methanol.
q-bio.BM:Protein function often involves changes between different conformations. Central questions are how these conformational changes are coupled to the binding or catalytic processes during which they occur, and how they affect the catalytic rates of enzymes. An important model system is the enzyme dihydrofolate reductase (DHFR) from E. coli, which exhibits characteristic conformational changes of the active-site loop during the catalytic step and during unbinding of the product. In this article, we present a general kinetic framework that can be used (1) to identify the ordering of events in the coupling of conformational changes, binding and catalysis and (2) to determine the rates of the substeps of coupled processes from a combined analysis of NMR R2 relaxation dispersion experiments and traditional enzyme kinetics measurements. We apply this framework to E. coli DHFR and find that the conformational change during product unbinding follows a conformational-selection mechanism, i.e. the conformational change occurs predominantly prior to unbinding. The conformational change during the catalytic step, in contrast, is an induced change, i.e. the change occurs after the chemical reaction. We propose that the reason for these conformational changes, which are absent in human and other vertebrate DHFRs, is robustness of the catalytic rate against large pH variations and changes to substrate/product concentrations in E. coli.
q-bio.BM:Ion channels are proteins with holes down their middle that control the flow of ions and electric current across otherwise impermeable biological membranes. The flow of sodium, potassium, calcium (divalent), and chloride ions have been central issues in biology for more than a century. The flow of current is responsible for the signals of the nervous system that propagate over long distances (meters). The concentration of divalent calcium ions is a 'universal' signal that controls many different systems inside cells. The concentration of divalent calcium and other messenger ions has a role in life rather like the role of the voltage in different wires of a computer. Ion channels also help much larger solutes (e.g., organic acid and bases; perhaps polypeptides) to cross membranes but much less is known about these systems. Ion channels can select and control the movement of different types of ions because the holes in channel proteins are a few times larger than the (crystal radii of the) ions themselves. Biology uses ion channels as selective valves to control flow and thus concentration of crucial chemical signals. For example, the concentration of divalent calcium ions determines whether muscles contract or not. Ion channels have a role in biology similar to the role of transistors in computers and technology. Ion Channels Control Concentrations Important To Life The Way Computers Control Voltages Important To Computers.
q-bio.BM:Chemistry is about chemical reactions. Chemistry is about electrons changing their configurations as atoms and molecules react. Chemistry studies reactions as if they occurred in ideal infinitely dilute solutions. But most reactions occur in nonideal solutions. Then everything (charged) interacts with everything else (charged) through the electric field, which is short and long range extending to boundaries of the system. Mathematics has recently been developed to deal with interacting systems of this sort. The variational theory of complex fluids has spawned the theory of liquid crystals. In my view, ionic solutions should be viewed as complex fluids. In both biology and electrochemistry ionic solutions are mixtures highly concentrated (~10M) where they are most important, near electrodes, nucleic acids, enzymes, and ion channels. Calcium is always involved in biological solutions because its concentration in a particular location is the signal that controls many biological functions. Such interacting systems are not simple fluids, and it is no wonder that analysis of interactions, such as the Hofmeister series, rooted in that tradition, has not succeeded as one would hope. We present a variational treatment of hard spheres in a frictional dielectric. The theory automatically extends to spatially nonuniform boundary conditions and the nonequilibrium systems and flows they produce. The theory is unavoidably self-consistent since differential equations are derived (not assumed) from models of (Helmholtz free) energy and dissipation of the electrolyte. The origin of the Hofmeister series is (in my view) an inverse problem that becomes well posed when enough data from disjoint experimental traditions are interpreted with a self-consistent theory.
q-bio.BM:A quantum mechanical model on histone modification is proposed. Along with the methyl / acetate or other groups bound to the modified residues the torsion angles of the nearby histone chain are supposed to participate in the quantum transition cooperatively. The transition rate W is calculated based on the non-radiative quantum transition theory in adiabatic approximation. By using W's the reaction equations can be written for histone modification and the histone modification level can be calculable from the equations, which is decided by not only the atomic group bound to the modified residue, but also the nearby histone chain. The theory can explain the mechanism for the correlation between a pair of chromatin markers observed in histone modification. The temperature dependence and the coherence-length dependence of histone modification are deduced. Several points for checking the proposed theory and the quantum nature of histone modification are suggested as follows: 1, The relationship between lnW and 1/T is same as usual protein folding. The non-Arhenius temperature dependence of the histone modification level is predicted. 2, The variation of histone modification level through point mutation of some residues on the chain is predicted since the mutation may change the coherence-length of the system. 3, Multi-site modification obeys the quantum superposition law and the comparison between multi-site transition and single modification transition gives an additional clue to the testing of the quantum nature of histone modification.
q-bio.BM:Inverted repeat (IR) sequences in DNA can form non-canonical cruciform structures to relieve torsional stress. We use Monte Carlo simulations of a recently developed coarse-grained model of DNA to demonstrate that the nucleation of a cruciform can proceed through a cooperative mechanism. Firstly, a twist-induced denaturation bubble must diffuse so that its midpoint is near the centre of symmetry of the IR sequence. Secondly, bubble fluctuations must be large enough to allow one of the arms to form a small number of hairpin bonds. Once the first arm is partially formed, the second arm can rapidly grow to a similar size. Because bubbles can twist back on themselves, they need considerably fewer bases to resolve torsional stress than the final cruciform state does. The initially stabilised cruciform therefore continues to grow, which typically proceeds synchronously, reminiscent of the S-type mechanism of cruciform formation. By using umbrella sampling techniques we calculate, for different temperatures and superhelical densities, the free energy as a function of the number of bonds in each cruciform along the correlated but non-synchronous nucleation pathways we observed in direct simulations.
q-bio.BM:Emerging high-throughput technologies have led to a deluge of putative non-coding RNA (ncRNA) sequences identified in a wide variety of organisms. Systematic characterization of these transcripts will be a tremendous challenge. Homology detection is critical to making maximal use of functional information gathered about ncRNAs: identifying homologous sequence allows us to transfer information gathered in one organism to another quickly and with a high degree of confidence. ncRNA presents a challenge for homology detection, as the primary sequence is often poorly conserved and de novo secondary structure prediction and search remains difficult. This protocol introduces methods developed by the Rfam database for identifying "families" of homologous ncRNAs starting from single "seed" sequences using manually curated sequence alignments to build powerful statistical models of sequence and structure conservation known as covariance models (CMs), implemented in the Infernal software package. We provide a step-by-step iterative protocol for identifying ncRNA homologs, then constructing an alignment and corresponding CM. We also work through an example for the bacterial small RNA MicA, discovering a previously unreported family of divergent MicA homologs in genus Xenorhabdus in the process.
q-bio.BM:DNA is subject to large deformations in a wide range of biological processes. Two key examples illustrate how such deformations influence the readout of the genetic information: the sequestering of eukaryotic genes by nucleosomes, and DNA looping in transcriptional regulation in both prokaryotes and eukaryotes. These kinds of regulatory problems are now becoming amenable to systematic quantitative dissection with a powerful dialogue between theory and experiment. Here we use a single-molecule experiment in conjunction with a statistical mechanical model to test quantitative predictions for the behavior of DNA looping at short length scales, and to determine how DNA sequence affects looping at these lengths. We calculate and measure how such looping depends upon four key biological parameters: the strength of the transcription factor binding sites, the concentration of the transcription factor, and the length and sequence of the DNA loop. Our studies lead to the surprising insight that sequences that are thought to be especially favorable for nucleosome formation because of high flexibility lead to no systematically detectable effect of sequence on looping, and begin to provide a picture of the distinctions between the short length scale mechanics of nucleosome formation and looping.
q-bio.BM:For decades, dimethyl sulfate (DMS) mapping has informed manual modeling of RNA structure in vitro and in vivo. Here, we incorporate DMS data into automated secondary structure inference using a pseudo-energy framework developed for 2'-OH acylation (SHAPE) mapping. On six non-coding RNAs with crystallographic models, DMS- guided modeling achieves overall false negative and false discovery rates of 9.5% and 11.6%, comparable or better than SHAPE-guided modeling; and non-parametric bootstrapping provides straightforward confidence estimates. Integrating DMS/SHAPE data and including CMCT reactivities give small additional improvements. These results establish DMS mapping - an already routine technique - as a quantitative tool for unbiased RNA structure modeling.
q-bio.BM:Hydrogen bonds are a common feature in protein folding and aggregation. Due to their chemical peculiarities in terms of strength and directionality, a particular attention must be paid to the definition of the hydrogen bond potential itself. This global target has been tackled through a computational approach based on a minimalist description of the protein and the proper design of algorithms, mainly using Monte Carlo and Kinetic Monte Carlo methods. We have designed a hydrogen bond potential, see J. Chem. Phys. 132, 235102 (2010). We have been performed a complete study of sequenceless peptide systems under different conditions, such as temperature and concentration. To carry out full protein studies, we need additional potentials to describe tertiary interactions. We have discussed two different points of view. The first one is the combination of the hydrogen bond potential with a structure-based one. We have evaluated the implications of a proper definition of hydrogen bonds in the thermodynamic and dynamic aspects of protein folding. See Biophys. J. 101, 1474-1482 (2011). We have undertaken a second strategy, combining a generic hydrophobic model with the hydrogen bond one. The two main factors of folding and aggregation are merged, then, to create a simple but complete potential. Thanks to it, we have re-studied peptide aggregation including sequence. Besides, we have simulated complete proteins with different folded shapes. We have analyzed the competition between folding and aggregation, and how sequence and hydrogen bonds can influence the interplay between them. See J. Chem. Phys. 136, 215103 (2012).
q-bio.BM:Consistently predicting biopolymer structure at atomic resolution from sequence alone remains a difficult problem, even for small sub-segments of large proteins. Such loop prediction challenges, which arise frequently in comparative modeling and protein design, can become intractable as loop lengths exceed 10 residues and if surrounding side-chain conformations are erased. This article introduces a modeling strategy based on a 'stepwise ansatz', recently developed for RNA modeling, which posits that any realistic all-atom molecular conformation can be built up by residue-by-residue stepwise enumeration. When harnessed to a dynamic-programming-like recursion in the Rosetta framework, the resulting stepwise assembly (SWA) protocol enables enumerative sampling of a 12 residue loop at a significant but achievable cost of thousands of CPU-hours. In a previously established benchmark, SWA recovers crystallographic conformations with sub-Angstrom accuracy for 19 of 20 loops, compared to 14 of 20 by KIC modeling with a comparable expenditure of computational power. Furthermore, SWA gives high accuracy results on an additional set of 15 loops highlighted in the biological literature for their irregularity or unusual length. Successes include cis-Pro touch turns, loops that pass through tunnels of other side-chains, and loops of lengths up to 24 residues. Remaining problem cases are traced to inaccuracies in the Rosetta all-atom energy function. In five additional blind tests, SWA achieves sub-Angstrom accuracy models, including the first such success in a protein/RNA binding interface, the YbxF/kink-turn interaction in the fourth RNA-puzzle competition. These results establish all-atom enumeration as a systematic approach to protein structure that can leverage high performance computing and physically realistic energy functions to more consistently achieve atomic resolution.
q-bio.BM:Self-associates of nucleic acid components (stacking trimers and tetramers of the base pairs of nucleic acids) and short fragments of nucleic acids are nanoparticles (linear sizes of these particles are more than 10 A. Modern quantum-mechanical methods and softwares allow one to perform ab initio calculations of the systems consisting of 150-200 atoms with enough large basis sets (for example, 6-31G*). The aim of this work is to reveal the peculiarities of molecular and electronic structures, as well as the energy features of nanoparticles of nucleic acid components.
q-bio.BM:Influenza virus contains two highly variable envelope glycoproteins, hemagglutinin (HA) and neuraminidase (NA). The structure and properties of HA, which is responsible for binding the virus to the cell that is being infected, change significantly when the virus is transmitted from avian or swine species to humans. Here we focus on much smaller human individual evolutionary amino acid mutational changes in NA, which cleaves sialic acid groups and is required for influenza virus replication. We show that very small amino acid changes can be monitored very accurately across many Uniprot and NCBI strains using hydropathicity scales to quantify the roughness of water film packages. Quantitative sequential analysis is most effective with the differential hydropathicity scale based on protein self-organized criticality (SOC). NA exhibits punctuated evolution at the molecular scale, millions of times smaller than the more familiar species scale, and thousands of times smaller than the genomic scale. Our analysis shows that large-scale vaccination programs have been responsible for a very large convergent reduction in influenza severity in the last century, a reduction which is hidden from short-term studies of vaccine effectiveness. Hydropathic analysis is capable of interpreting and even predicting trends of functional changes in mutation prolific viruses.
q-bio.BM:Influenza virus contains two highly variable envelope glycoproteins, hemagglutinin (HA) and neuraminidase (NA). The structure and properties of HA, which is responsible for binding the virus to the cell that is being infected, change significantly when the virus is transmitted from avian or swine species to humans. Previously we identified much smaller human individual evolutionary amino acid mutational changes in NA, which cleaves sialic acid groups and is required for influenza virus replication. We showed that these smaller changes can be monitored very accurately across many Uniprot and NCBI strains using hydropathicity scales to quantify the roughness of water film packages, which increases gradually due to migration, but decreases abruptly under large-scale vaccination pressures. Here we show that, while HA evolution is much more complex, it still shows abrupt punctuation changes linked to those of NA. HA exhibits proteinquakes, which resemble earthquakes and are related to hydropathic shifting of sialic acid binding regions. HA proteinquakes based on sialic acid interactions are required for optimal balance between the receptor-binding and receptor-destroying activities of HA and NA for efficient virus replication. Our comprehensive results present an historical (1945-2011) panorama of HA evolution over thousands of strains, and are consistent with many studies of HA and NA interactions based on a few mutations of a few strains. While the common influenza virus discussed here has been rendered almost harmless by decades of vaccination programs, the sequential decoding lessons learned here are applicable to other viruses that are emerging as powerful weapons for controlling and even curing common organ cancers. Those engineered oncolytic drugs will be discussed in future papers.
q-bio.BM:While the concepts involved in Self-Organized Criticality have stimulated thousands of theoretical models, only recently have these models addressed problems of biological and clinical importance. Here we outline how SOC can be used to engineer hybrid viral proteins whose properties, extrapolated from those of known strains, may be sufficiently effective to cure cancer.
q-bio.BM:Although mechanical properties of DNA are well characterized at the kilo base-pair range, a number of recent experiments have suggested that DNA is more flexible at shorter length scales, which correspond to the regime that is crucial for cellular processes such as DNA packaging and gene regulation. Here, we perform a systematic study of the effective elastic properties of DNA at different length scales by probing the conformation and fluctuations of DNA from single base-pair level up to four helical turns, using trajectories from atomistic simulation. We find evidence that supports cooperative softening of the stretch modulus and identify the essential modes that give rise to this effect. The bend correlation exhibits modulations that reflect the helical periodicity, while it yields a reasonable value for the effective persistence length, and the twist modulus undergoes a smooth crossover---from a relatively smaller value at the single base-pair level to the bulk value---over half a DNA-turn.
q-bio.BM:Mutations and oxidative modification in the protein Cu,Zn superoxide dismutase (SOD1) have been implicated in the death of motor neurons in amyotrophic lateral sclerosis (ALS), a presently incurable, invariably fatal neurodegenerative disease. Here we employ steered, all-atom molecular dynamics simulations in implicit solvent to investigate the significance of either mutations or post-translational modifications (PTMs) to SOD1 on metal affinity, dimer stability, and mechanical malleability. The work required to induce moderate structural deformations as a function of sequence index constitutes a "mechanical fingerprint" measuring structural rigidity in the native basin, from which we are able to unambiguously distinguish wild-type (WT) SOD1 from PTM variants, and measure the severity of a given PTM on structural integrity. The cumulative distribution of work values provided a way to cleanly discriminate between SOD1 variants. Disulfide reduction destabilizes dimer stability more than the removal of either metal, but not moreso than the removal of both metals. Intriguingly, we found that disulfide reduction mechanically stabilizes apo SOD1 monomer, underscoring the differences between native basin mechanical properties and equilibrium thermodynamic stabilities, and elucidating the presence of internal stress in the apo state. All PTMs and ALS-associated mutants studied showed an increased tendency to lose either Cu or Zn, and to monomerize- processes known to be critical in the progression of ALS. The valence of Cu strongly modulates its binding free energy. As well, several mutants were more susceptible to loss of metals and monomerization than the disulfide-reduced or apo forms of SOD1. Distance constraints are required to calculate free energies for metal binding and dimer separation, which are validated using thermodynamic cycles.
q-bio.BM:Availability of high-resolution crystal structures of ribosomal subunits of different species opens a route to investigate about molecular interactions between its constituents and stabilization strategy. Structural analysis of the small ribosomal subunit shows that primary binder proteins are mainly employed in stabilizing the folded ribosomal RNA by their high negative free energy of association, where tertiary binders mainly help to stabilize protein-protein interfaces. Secondary binders perform both the functions. Conformational changes of prokaryotic and eukaryotic ribosomal proteins due to complexation with 16S ribosomal RNA are linearly correlated with their RNA-interface area and free energy of association. The proteins having long extensions buried within ribosomal RNA have more flexible structures than those found on the subunit surface. Thermus thermophilus ribosomal proteins undergo high conformational changes compared to those of Escherichia coli, assuring structural stability at high temperature environment. The general stabilization strategy of ribosomal protein-RNA interfaces is shown, where high interface polarity ensures high surface density of Hydrogen bonds even with low base/backbone ratio. Polarity is regulated in evolutionary strategy of ribosomal proteins. Thus, the habitat environmental conditions of the two species sweet up their ribosomal protein-RNA interfaces to alter its physical parameters in order of stabilization.
q-bio.BM:Motivation: Protein surface roughness is fractal in nature. Mass, hydrophobicity, polarizability distributions of protein interior are fractal too, as are the distributions of dipole moments, aromatic residues, and many other structural determinants. The open-access server ProtFract, presents a reliable way to obtain numerous fractal-dimension and correlation-dimension based results to quantify the symmetry of self-similarity in distributions of various properties of protein interior and exterior.   Results: Fractal dimension based comparative analyses of various biophysical properties of Ras superfamily proteins were conducted. Though the extent of sequence and functional overlapping across Ras superfamily structures is extremely high, results obtained from ProtFract investigation are found to be sensitive to detect differences in the distribution of each of the properties. For example, it was found that the RAN proteins are structurally most stable amongst all Ras superfamily proteins, the ARFs possess maximum extent of unused hydrophobicity in their structures, RAB protein interiors have electrostatically least conducive environment, GEM/REM/RAD proteins possess exceptionally high symmetry in the structural organization of their active chiral centres, neither hydrophobicity nor columbic interactions play significant part in stabilizing the RAS proteins but aromatic interactions do, though cation-pi interactions are found to be more dominant in RAN than in RAS proteins. Ras superfamily proteins can best be classified with respect to their class-specific pi-pi and cation-pi interaction symmetries.   Availability: ProtFract is freely available online at the URL: http://bioinfo.net.in/protfract/index.html
q-bio.BM:Upon studying the B-Factors of all the atoms of all non-redundant proteins belonging to 76 most commonly found structural domains of all four major structural classes, it was found that the residue mobility has decreased during the course of evolution. Though increased residue-flexibility was preferred in the early stages of protein structure evolution, less flexibility is preferred in the medieval and recent stages. GLU is found to be the most flexible residue while VAL recorded to have the least flexibility. General trends in decrement of B-Factors conformed to the general trend in the order of emergence of protein structural domains. Decrement of B-Factor is observed to be most decisive (monotonic and uniform) for VAL, while evolution of CYS and LYS flexibility is found to be most skewed. Barring CYS, flexibility of all the residues is found to have increased during evolution of alpha by beta folds, however flexibility of all the residues (barring CYS) is found to have decreased during evolution of all beta folds. Only in alpha by beta folds the tendency of preferring higher residue mobility could be observed, neither alpha plus beta, nor all alpha nor all beta folds were found to support higher residue-mobility. In all the structural classes, the effect of evolutionary constraint on polar residues is found to follow an exactly identical trend as that on hydrophobic residues, only the extent of these effects are found to be different. Though protein size is found to be decreasing during evolution, residue mobility of proteins belonging to ancient and old structural domains showed strong positive dependency upon protein size, however for medieval and recent domains such dependency vanished. It is found that to optimize residue fluctuations, alpha by beta class of proteins are subjected to more stringent evolutionary constraints.
q-bio.BM:Recent experimental data indicate that the elastic wormlike rod model of DNA that works well on long length scales may break down on shorter scales relevant to biology. According to Noy and Golestanian (Phys. Rev. Lett. 109, 228101, 2012) molecular dynamics (MD) simulations predict DNA rigidity close to experimental data and confirm one scenario of such breakdown, namely, that for lengths of a few helical turns, DNA dynamics exhibit long-range bending and stretching correlations. Earlier studies using similar forcefields concluded that (i) MD systematically overestimate the DNA rigidity, and (ii) no deviations from the WLR model are detectable. Here it is argued that the data analysis in the above mentioned paper was incorrect and that the earlier conclusions are valid.
q-bio.BM:Superoxide dismutase-1 (SOD1) is a ubiquitous, Cu and Zn binding, free radical defense enzyme whose misfolding and aggregation play a potential key role in amyotrophic lateral sclerosis, an invariably fatal neurodegenerative disease. Over 150 mutations in SOD1 have been identified with a familial form of the disease, but it is presently not clear what unifying features, if any, these mutants share to make them pathogenic. Here, we develop several new computational assays for probing the thermo-mechanical properties of both ALS-associated and rationally-designed SOD1 variants. Allosteric interaction free energies between residues and metals are calculated, and a series of atomic force microscopy experiments are simulated with variable tether positions, to quantify mechanical rigidity "fingerprints" for SOD1 variants. Mechanical fingerprinting studies of a series of C-terminally truncated mutants, along with an analysis of equilibrium dynamic fluctuations while varying native constraints, potential energy change upon mutation, frustratometer analysis, and analysis of the coupling between local frustration and metal binding interactions for a glycine scan of 90 residues together reveal that the apo protein is internally frustrated, that these internal stresses are partially relieved by mutation but at the expense of metal-binding affinity, and that the frustration of a residue is directly related to its role in binding metals. This evidence points to apo SOD1 as a strained intermediate with "self-allostery" for high metal-binding affinity. Thus, the prerequisites for the function of SOD1 as an antioxidant compete with apo state thermo-mechanical stability, increasing the susceptibility of the protein to misfold in the apo state.
q-bio.BM:Reply to Comment on 'Length Scale Dependence of DNA Mechanical Properties'
q-bio.BM:The conformational change of biological macromolecule is investigated from the point of quantum transition. A quantum theory on protein folding is proposed. Compared with other dynamical variables such as mobile electrons, chemical bonds and stretching-bending vibrations the molecular torsion has the lowest energy and can be looked as the slow variable of the system. Simultaneously, from the multi-minima property of torsion potential the local conformational states are well defined. Following the idea that the slow variables slave the fast ones and using the nonadiabaticity operator method we deduce the Hamiltonian describing conformational change. It is proved that the influence of fast variables on the macromolecule can fully be taken into account through a phase transformation of slow variable wave function. Starting from the conformation- transition Hamiltonian the nonradiative matrix element is calculated in two important cases: A, only electrons are fast variables and the electronic state does not change in the transition process; B, fast variables are not limited to electrons but the perturbation approximation can be used. Then, the general formulas for protein folding rate are deduced. The analytical form of the formula is utilized to study the temperature dependence of protein folding rate and the curious non-Arrhenius temperature relation is interpreted. The decoherence time of quantum torsion state is estimated and the quantum coherence degree of torsional angles in the protein folding is studied by using temperature dependence data. The proposed folding rate formula gives a unifying approach for the study of a large class problems of biological conformational change.
q-bio.BM:It is established that prion protein is the sole causative agent in a number of diseases in humans and animals. However, the nature of conformational changes that the normal cellular form PrPC undergoes in the conversion process to a self-replicating state is still not fully understood. The ordered C-terminus of PrPC proteins has three helices (H1, H2, and H3). Here, we use the Statistical Coupling Analysis (SCA) to infer co-variations at various locations using a family of evolutionarily related sequences, and the response of mouse and human PrPCs to mechanical force to decipher the initiation sites for transition from PrPC to an aggregation prone PrP* state. The sequence-based SCA predicts that the clustered residues in non-mammals are localized in the stable core (near H1) of PrPC whereas in mammalian PrPC they are localized in the frustrated helices H2 and H3 where most of the pathogenic mutations are found. Force-extension curves and free energy profiles as a function of extension of mouse and human PrPC in the absence of disulfide (SS) bond between residues Cys179 and Cys214, generated by applying mechanical force to the ends of the molecule, show a sequence of unfolding events starting first with rupture of H2 and H3. This is followed by disruption of structure in two strands. Helix H1, stabilized by three salt-bridges, resists substantial force before unfolding. Force extension profiles and the dynamics of rupture of tertiary contacts also show that even in the presence of SS bond the instabilities in most of H3 and parts of H2 still determine the propensity to form the PrP* state. In mouse PrPC with SS bond there are about ten residues that retain their order even at high forces.
q-bio.BM:Antimicrobial peptides are a class of small, usually positively charged amphiphilic peptides that are used by the innate immune system to combat bacterial infection in multicellular eukaryotes. Antimicrobial peptides are known for their broad-spectrum antimicrobial activity and thus can be used as a basis for a development of new antibiotics against multidrug-resistant bacteria. The most challengeous task on the way to a therapeutic use of antimicrobial peptides is a rational design of new peptides with enhanced activity and reduced toxicity. Here we report a molecular dynamics and circular dichroism study of a novel synthetic antimicrobial peptide D51. This peptide was earlier designed by Loose et al. using a linguistic model of natural antimicrobial peptides. Molecular dynamics simulation of the peptide folding in explicit solvent shows fast formation of two antiparallel beta strands connected by a beta-turn that is confirmed by circular dichroism measurements. Obtained from simulation amphipatic conformation of the peptide is analysed and possible mechanism of its interaction with bacterial membranes together with ways to enhance its antibacterial activity are suggested.
q-bio.BM:Side chain flexibility is an important factor in ligand binding. In order to determine the extent to which side chain flexibility is involved in ligand binding, a knowledge-based approach was taken. A database composed of examples of protein structures in the presence or absence of a given ligand is used to analyze which side chains undergo side chain conformational changes. Such an analysis has determined that up to 40% of binding site do not present side chain conformational changes. A total of three residues undergoing side chain conformational changes encompass approximately 85% of the binding sites studied. When analyzing the propensities of different amino acids to undergo side chain conformational changes we find that there are considerable differences between different amino acids. A support vector machine learning approach was used to create a classifier system utilizing information about the solvent accessible area as well as flexibility scale value of each specific side chain to be predicted together with its neighboring side chains. An accuracy level of 70% is reached using this approach. The fact that a small number of residues undergo side chain conformational changes in the majority of binding sites makes it feasible to introduce side chain flexibility in docking simulations. An algorithm has been developed for introducing side chain flexibility utilizing a hybrid genetic-algorithm/exhaustive-search procedure and a surface complementarity based scoring function. This approach is implemented in the software tool FlexAID. FlexAID utilizes a rotamer library to create alternative conformations for a list of residues that are exhaustively searched during the docking simulation. The performance of FlexAID in rigid local as well as global simulations falls in the 70-80% range for both local and global simulations.
q-bio.BM:Chemical mapping is a widespread technique for structural analysis of nucleic acids in which a molecule's reactivity to different probes is quantified at single-nucleotide resolution and used to constrain structural modeling. This experimental framework has been extensively revisited in the past decade with new strategies for high-throughput read-outs, chemical modification, and rapid data analysis. Recently, we have coupled the technique to high-throughput mutagenesis. Point mutations of a base-paired nucleotide can lead to exposure of not only that nucleotide but also its interaction partner. Carrying out the mutation and mapping for the entire system gives an experimental approximation of the molecules contact map. Here, we give our in-house protocol for this mutate-and-map strategy, based on 96-well capillary electrophoresis, and we provide practical tips on interpreting the data to infer nucleic acid structure.
q-bio.BM:A central question is how the conformational changes of proteins affect their function and the inhibition of this function by drug molecules. Many enzymes change from an open to a closed conformation upon binding of substrate or inhibitor molecules. These conformational changes have been suggested to follow an induced-fit mechanism in which the molecules first bind in the open conformation in those cases where binding in the closed conformation appears to be sterically obstructed such as for the HIV-1 protease. In this article, we present a general model for the catalysis and inhibition of enzymes with induced-fit binding mechanism. We derive general expressions that specify how the overall catalytic rate of the enzymes depends on the rates for binding, for the conformational changes, and for the chemical reaction. Based on these expressions, we analyze the effect of mutations that mainly shift the conformational equilibrium on catalysis and inhibition. If the overall catalytic rate is limited by product unbinding, we find that mutations that destabilize the closed conformation relative to the open conformation increase the catalytic rate in the presence of inhibitors by a factor exp(ddG/RT) where ddG is the mutation-induced shift of the free-energy difference between the conformations. This increase in the catalytic rate due to changes in the conformational equilibrium is independent of the inhibitor molecule and, thus, may help to understand how non-active-site mutations can contribute to the multi-drug-resistance that has been observed for the HIV-1 protease. A comparison to experimental data for the non-active-site mutation L90M of the HIV-1 protease indicates that the mutation slightly destabilizes the closed conformation of the enzyme.
q-bio.BM:Studying all non-redundant proteins in 76 most-commonly found structural domains, the present work attempts to decipher latent patterns that characterize acceptable and unacceptable symmetries in residue-residue interactions in functional proteins. We report that cutting across the structural classes, a select set of pairwise interactions are universally favored by geometrical and evolutionary constraints, termed 'acceptable' structural and evolutionary tunnels, respectively. An equally small subset of residue-residue interactions, the 'unacceptable' structural and evolutionary tunnels, is found to be universally disliked by structural and evolutionary constraints. Non-trivial overlapping is detected among acceptable structural and evolutionary tunnels, as also among unacceptable structural and evolutionary tunnels. A subset of tunnels is found to have equal relative importance, structurally and evolutionarily, in different structural classes. The MET-MET tunnel is detected to be universally most unacceptable by both structural and evolutionary constraints, whereas the ASP-LEU tunnel was found to be the closest approximation to be universally most acceptable. Residual populations in structural and evolutionary tunnels are found to be independent of stereochemical properties of individual residues. It is argued with examples that tunnels are emergent features that connect extent of symmetry in residue-residue interactions to the level of quaternary structural organization.
q-bio.BM:In many biological applications, we would like to be able to computationally predict mutational effects on affinity in protein-protein interactions. However, many commonly used methods to predict these effects perform poorly in important test cases. In particular, the effects of multiple mutations, non-alanine substitutions, and flexible loops are difficult to predict with available tools and protocols. We present here an existing method applied in a novel way to a new test case; we interrogate affinity differences resulting from mutations in a host-virus protein-protein interface. We use steered molecular dynamics (SMD) to computationally pull the machupo virus (MACV) spike glycoprotein (GP1) away from the human transferrin receptor (hTfR1). We then approximate affinity using the maximum applied force of separation and the area under the force-versus-distance curve. We find, even without the rigor and planning required for free energy calculations, that these quantities can provide novel biophysical insight into the GP1/hTfR1 interaction. First, with no prior knowledge of the system we can differentiate among wild type and mutant complexes. Moreover, we show that this simple SMD scheme correlates well with relative free energy differences computed via free energy perturbation. Second, although the static co-crystal structure shows two large hydrogen-bonding networks in the GP1/hTfR1 interface, our simulations indicate that one of them may not be important for tight binding. Third, one viral site known to be critical for infection may mark an important evolutionary suppressor site for infection-resistant hTfR1 mutants. Finally, our approach provides a framework to compare the effects of multiple mutations, individually and jointly, on protein-protein interactions.
q-bio.BM:We have studied the mobility of the folding catalyst, protein disulphide-isomerase (PDI), by molecular dynamics and by a rapid approach based on flexibility. We analysed our simulations using measures of backbone movement, relative positions and orientations of domains, and distances between functional sites. Despite their different assumptions, the two methods are surprisingly consistent. Both methods agree that motion of domains is dominated by hinge and rotation motion of the a and a' domains relative to the central b-b' domain core. We identify the a' domain as showing the greatest intra-domain mobility. The flexibility method, which requires 10^4-fold less computer power, predicts additional large-scale features of inter-domain motion that have been observed experimentally. We conclude that the methods offer complementary insight into the motion of this large protein and provide detailed structural models that characterise its functionally-significant conformational changes.
q-bio.BM:The famous series of Fibonacci numbers is defined by a recursive equation saying that each number is the sum of its two predecessors, with the initial condition that the first two numbers are equal to unity. Here, we show that the numbers of fatty acids (straight-chain aliphatic monocarboxylic acids) with n carbon atoms is exactly given by the Fibonacci numbers. Thus, by investing one more carbon atom into extending a fatty acid, an organism can increase the variability of the fatty acids approximately by the factor of the Golden section, 1.618. As the Fibonacci series grows asymptotically exponentially, our results are in line with combinatorial complexity found generally in biology. We also outline potential extensions of the calculations to modified (e.g., hydroxylated) fatty acids. The presented enumeration method may be of interest for lipidomics, combinatorial chemistry, synthetic biology and the theory of evolution (including prebiotic evolution).
q-bio.BM:Adenosine triphosphate (ATP)-binding cassette (ABC) transporters form a family of molecular motor proteins that couple ATP hydrolysis to substrate translocation across cell membranes. Each nucleotide binding domain of ABC-transporters contains a highly conserved H-loop Histidine residue, whose precise mechanistic role to motor functions has remained elusive. By using combined quantum mechanical and molecular mechanical calculations, we showed that the conserved H-loop residue H662 in E. coli HlyB, a bacterial ABC transporter, can act first as a general acid and then as a general base to facilitate proton transfers in ATP hydrolysis. Without the assistance of H662, direct proton transfer from the lytic water to ATP results in a greatly elevated barrier height. Our findings suggest that the essential function of the H-loop residue H662 is to provide a "chemical linchpin" that shuttles protons between reactants through a relay mechanism, thereby catalyzing ATP hydrolysis in HlyB.
q-bio.BM:Proofreading/editing in protein synthesis is essential for accurate translation of information from the genetic code. In this article we present a theoretical investigation of efficiency of a kinetic proofreading mechanism that employs hydrolysis of the wrong substrate as the discriminatory step in enzyme catalytic reactions. We consider aminoacylation of tRNA^{Ile} which is a crucial step in protein synthesis and for which experimental results are now available. We present an augmented kinetic scheme and then employ methods of stochastic simulation algorithm to obtain time dependent concentrations of different substances involved in the reaction and their rates of formation. We obtain the rates of product formation and ATP hydrolysis for both correct and wrong substrates (isoleucine and valine in our case), in single molecular enzyme as well as ensemble enzyme kinetics. The present theoretical scheme correctly reproduces (i) the amplitude of the discrimination factor in the overall rates between isoleucine and valine which is obtained as (1.8 \times 10^2).(4.33 \times 10^2) = 7.8 \times 10^4, (ii) the rates of ATP hydrolysis for both Ile and Val at different substrate concentrations in the aminoacylation of tRNA^{Ile}. The present study shows a non-michaelis type dependence of rate of reaction on tRNA^{Ile} concentration in case of valine. The editing in steady state is found to be independent of amino acid concentration. Interestingly, the computed ATP hydrolysis rate for valine at high substrate concentration is same as the rate of formation of Ile-tRNA^{Ile} whereas at intermediate substrate concentration the ATP hydrolysis rate is relatively low.
q-bio.BM:So far, more than 82,000 protein structures have been reported in the Protein Data Bank, but the driving force and structures that allow for protein functions have not been elucidated at the atomic level for even one protein. We have been able to clarify that the inter-subunit hydrophobic interaction driving the electrostatic opening of the pore in aquaporin 0 (AQP0). Aquaporins are membrane channels for water and small non-ionic solutes found in animals, plants, and microbes. The structures of aquaporins have high homology and consist of homotetramers, each monomer of which has one pore for a water channel. Each pore has two narrow portions: one is the narrowest constriction region consisting of aromatic residues and an arginine (ar/R), and another is two asparagine-proline-alanine (NPA) homolog portions. Here we show that an inter-subunit hydrophobic interaction in AQP0 drives a stick portion consisting of four amino acids toward the pore and the tip of the stick portion, consisting of a nitrogen atom, opens the pore: that movement is the swing mechanism (this http URL). The energetics and conformational change of amino acids participating in the swing mechanism confirm this view. The swing mechanism in which inter-subunit hydrophobic interactions in the tetramer drive the on-off switching of the pore explains why aquaporins consist of tetramers. Here, we report that experimental and molecular dynamics findings using various mutants support this view of the swing mechanism. The finding that mutants of amino acids in AQP2 corresponding to the stick of the swing mechanism cause severe recessive nephrogenic diabetes insipidus (NDI) demonstrates the critical role of the swing mechanism for the aquaporin function. We report first that the inter-subunit hydrophobic interaction in aquaporin 0 drives the electrostatic opening of the aquaporin pore at the atomic level.
q-bio.BM:The study reported here concerns a phenomenon, discovered and extensively investigated by Sorin Comorosan, wherein enzyme initial reaction rates are enhanced as a consequence of incorporation of solutions derived from previously irradiated crystalline material into the reaction medium. Effective irradiation times conform to a sharply oscillatory pattern. In most reports, the irradiated crystalline material has been the substrate for the enzyme reaction to be studied, but there have been exceptions. The experiments presented here serve to confirm and extend this latter aspect of the phenomenon. It is found that the initial reaction rates for the lactic acid dehydrogenase (LDH) conversion of pyruvate to lactate can be stimulated by irradiation of crystalline deposits of sodium chloride, sodium bromide, potassium chloride and diatomaceous earth. Similarly, stimulation of the LDH conversion of lactate to pyruvate is demonstrated for irradiated sodium chloride. There appears to be no required chemical feature of the irradiated material other than crystalline state.
q-bio.BM:ATP-hydrolysis is the basic energy source of many physiological processes, but there is a lack of knowledge regarding its biological role other than energy transfer and thermogenesis. Not all the energy released by ATP-hydrolysis could be used in powering target biological processes and functions. Partial energy dissipates into water. By validating the impact of this dissipated energy, we found that energy released by ATP hydrolysis could induce notable regulation of biomolecule's properties 100 nanometers away. Namely ATP hydrolyzation is recycled in remote biochemical property modulation.
q-bio.BM:The conformational vibrations of Z-DNA with counterions are studied in framework of phenomenological model developed. The structure of left-handed double helix with counterions neutralizing the negatively charged phosphate groups of DNA is considered as the ion-phosphate lattice. The frequencies and Raman intensities for the modes of Z-DNA with Na+ and Mg2+ ions are calculated, and the low-frequency Raman spectra are built. At the spectra range about the frequency 150 cm-1 new mode of ion-phosphate vibrations is found, which characterizes the vibrations of Mg2+ counterions. The results of our calculations show that the intensities of Z-DNA modes are sensitive to the concentration of magnesium counterions. The obtained results describe well the experimental Raman spectra of Z-DNA.
q-bio.BM:Ions in water are important in biology, from molecules to organs. Classically, ions in water are treated as ideal noninteracting particles in a perfect gas. Excess free energy of ion was zero. Mathematics was not available to deal consistently with flows, or interactions with ions or boundaries. Non-classical approaches are needed because ions in biological conditions flow and interact. The concentration gradient of one ion can drive the flow of another, even in a bulk solution. A variational multiscale approach is needed to deal with interactions and flow. The recently developed energetic variational approach to dissipative systems allows mathematically consistent treatment of bio-ions Na, K, Ca and Cl as they interact and flow. Interactions produce large excess free energy that dominate the properties of the high concentration of ions in and near protein active sites, channels, and nucleic acids: the number density of ions is often more than 10 M. Ions in such crowded quarters interact strongly with each other as well as with the surrounding protein. Non-ideal behavior has classically been ascribed to allosteric interactions mediated by protein conformation changes. Ion-ion interactions present in crowded solutions--independent of conformation changes of proteins--are likely to change interpretations of allosteric phenomena. Computation of all atoms is a popular alternative to the multiscale approach. Such computations involve formidable challenges. Biological systems exist on very different scales from atomic motion. Biological systems exist in ionic mixtures (extracellular/intracellular solutions), and usually involve flow and trace concentrations of messenger ions (e.g., 10-7 M Ca2+). Energetic variational methods can deal with these characteristic properties of biological systems while we await the maturation and calibration of all atom simulations of ionic mixtures and divalents.
q-bio.BM:A mechanism of the replication of proto-RNAs in oligomer world is proposed. The replication is carried out by a minimum cycle which is sustained by a ligase and a helicase. We expect that such a cycle actually worked in the primordial soup and can be constructed in vitro. By computer simulation the products of the replication acquires diversity and complexity. Such diversity and complexity are the bases of the evolution.
q-bio.BM:To determine the 3D conformation of proteins is a necessity to understand their functions or interactions with other molecules. It is commonly admitted that, when proteins fold from their primary linear structures to their final 3D conformations, they tend to choose the ones that minimize their free energy. To find the 3D conformation of a protein knowing its amino acid sequence, bioinformaticians use various models of different resolutions and artificial intelligence tools, as the protein folding prediction problem is a NP complete one. More precisely, to determine the backbone structure of the protein using the low resolution models (2D HP square and 3D HP cubic), by finding the conformation that minimize free energy, is intractable exactly. Both the proof of NP-completeness and the 2D prediction consider that acceptable conformations have to satisfy a self-avoiding walk (SAW) requirement, as two different amino acids cannot occupy a same position in the lattice. It is shown in this document that the SAW requirement considered when proving NP-completeness is different from the SAW requirement used in various prediction programs, and that they are different from the real biological requirement. Indeed, the proof of NP completeness and the predictions in silico consider conformations that are not possible in practice. Consequences of this fact are investigated in this research work.
q-bio.BM:Self-avoiding walks (SAW) are the source of very difficult problems in probabilities and enumerative combinatorics. They are also of great interest as they are, for instance, the basis of protein structure prediction in bioinformatics. Authors of this article have previously shown that, depending on the prediction algorithm, the sets of obtained conformations differ: all the self-avoiding walks can be reached using stretching-based algorithms whereas only the folded SAWs can be attained with methods that iteratively fold the straight line. A first study of (un)folded self-avoiding walks is presented in this article. The contribution is majorly a survey of what is currently known about these sets. In particular we provide clear definitions of various subsets of self-avoiding walks related to pivot moves (folded or unfoldable SAWs, etc.) and the first results we have obtained, theoretically or computationally, on these sets. A list of open questions is provided too, and the consequences on the protein structure prediction problem is finally investigated.
q-bio.BM:The notion of energy landscapes provides conceptual tools for understanding the complexities of protein folding and function. Energy Landscape Theory indicates that it is much easier to find sequences that satisfy the "Principle of Minimal Frustration" when the folded structure is symmetric (Wolynes, P. G. Symmetry and the Energy Landscapes of Biomolecules. Proc. Natl. Acad. Sci. U.S.A. 1996, 93, 14249-14255). Similarly, repeats and structural mosaics may be fundamentally related to landscapes with multiple embedded funnels. Here we present analytical tools to detect and compare structural repetitions in protein molecules. By an exhaustive analysis of the distribution of structural repeats using a robust metric we define those portions of a protein molecule that best describe the overall structure as a tessellation of basic units. The patterns produced by such tessellations provide intuitive representations of the repeating regions and their association towards higher order arrangements. We find that some protein architectures can be described as nearly periodic, while in others clear separations between repetitions exist. Since the method is independent of amino acid sequence information we can identify structural units that can be encoded by a variety of distinct amino acid sequences.
q-bio.BM:Proteins experience a wide variety of conformational dynamics that can be crucial for facilitating their diverse functions. How is the intrinsic flexibility required for these motions encoded in their three-dimensional structures? Here, the overall flexibility of a protein is demonstrated to be tightly coupled to the total amount of surface area buried within its fold. A simple proxy for this, the relative solvent accessible surface area (Arel), therefore shows excellent agreement with independent measures of global protein flexibility derived from various experimental and computational methods. Application of Arel on a large scale demonstrates its utility by revealing unique sequence and structural properties associated with intrinsic flexibility. In particular, flexibility as measured by Arel shows little correspondence with intrinsic disorder, but instead tends to be associated with multiple domains and increased {\alpha}- helical structure. Furthermore, the apparent flexibility of monomeric proteins is found to be useful for identifying quaternary structure errors in published crystal structures. There is also a strong tendency for the crystal structures of more flexible proteins to be solved to lower resolutions. Finally, local solvent accessibility is shown to be a primary determinant of local residue flexibility. Overall this work provides both fundamental mechanistic insight into the origin of protein flexibility and a simple, practical method for predicting flexibility from protein structures.
q-bio.BM:Protein structure prediction is one of the most important problems in computational biology. The most successful computational approach, also called template-based modeling, identifies templates with solved crystal structures for the query proteins and constructs three dimensional models based on sequence/structure alignments. Although substantial effort has been made to improve protein sequence alignment, the accuracy of alignments between distantly related proteins is still unsatisfactory. In this thesis, I will introduce a number of statistical machine learning methods to build accurate alignments between a protein sequence and its template structures, especially for proteins having only distantly related templates. For a protein with only one good template, we develop a regression-tree based Conditional Random Fields (CRF) model for pairwise protein sequence/structure alignment. By learning a nonlinear threading scoring function, we are able to leverage the correlation among different sequence and structural features. We also introduce an information-theoretic measure to guide the learning algorithm to better exploit the structural features for low-homology proteins with little evolutionary information in their sequence profile. For a protein with multiple good templates, we design a probabilistic consistency approach to thread the protein to all templates simultaneously. By minimizing the discordance between the pairwise alignments of the protein and templates, we are able to construct a multiple sequence/structure alignment, which leads to better structure predictions than any single-template based prediction.
q-bio.BM:The entire T7 bacteriophage genome contains 39937 base pairs (Database NCBI RefSeq N1001604). Here, electrostatic potential distribution around double helical T7 DNA was calculated by Coulomb method using the computer program of Sorokin A.A. Electrostatic profiles of 17 promoters recognized by T7 phage specific RNA polymerase were analyzed. It was shown that electrostatic profiles of all T7 RNA polymerase specific promoters can be characterized by distinctive motifs which are specific for each promoter class. Comparative analysis of electrostatic profiles of native T7 promoters of different classes demonstrates that T7 RNA polymerase can differentiate them due to their electrostatic features.
q-bio.BM:The primary structure of proteins, that is their sequence, represents one of the most abundant set of experimental data concerning biomolecules. The study of correlations in families of co--evolving proteins by means of an inverse Ising--model approach allows to obtain information on their native conformation. Following up on a recent development along this line, we optimize the algorithm to calculate effective energies between the residues, validating the approach both back-calculating interaction energies in a model system, and predicting the free energies associated to mutations in real systems. Making use of these effective energies, we study the networks of interactions which stabilizes the native conformation of some well--studied proteins, showing that it display different properties than the associated contact network.
q-bio.BM:Plasmodium falciparum 70 kDa heat shock proteins (PfHsp70s) are expressed at all stages of the pathogenic erythrocytic phase of the malaria parasite lifecycle. There are six PfHsp70s, all of which have orthologues in other plasmodial species, except for PfHsp70-x which is unique to P. falciparum. This paper highlights a number of original results obtained by a detailed bioinformatics analysis of the protein. Large scale sequence analysis indicated the presence of an extended transit peptide sequence of PfHsp70-x which potentially directs it to the endoplasmic reticulum (ER). Further analysis showed that PfHsp70-x does not have an ER-retention sequence, suggesting that the protein transits through the ER and is secreted into the parasitophorous vacuole (PV) or beyond into the erythrocyte cytosol. These results are consistent with experimental findings. Next, possible interactions between PfHsp70-x and exported P. falciparum Hsp40s or host erythrocyte DnaJs were interrogated by modeling and docking. Docking results indicated that interaction between PfHsp70-x and each of the Hsp40s, regardless of biological feasibility, seems equally likely. This suggests that J domain might not provide the specificity in the formation of unique Hsp70-Hsp40 complexes, but that the specificity might be provided by other domains of Hsp40s. By studying different structural conformations of PfHsp70-x, it was shown that Hsp40s can only bind when PfHsp70-x is in a certain conformation. Additionally, this work highlighted the possible dependence of the substrate binding domain residues on the orientation of the {\alpha}-helical lid for formation of the substrate binding pocket.
q-bio.BM:Most available antimicrobial peptides (AMP) prediction methods use common approach for different classes of AMP. Contrary to available approaches, we suggest, that a strategy of prediction should be based on the fact, that there are several kinds of AMP which are vary in mechanisms of action, structure, mode of interaction with membrane etc. According to our suggestion for each kind of AMP a particular approach has to be developed in order to get high efficacy. Consequently in this paper a particular but the biggest class of AMP - linear cationic antimicrobial peptides (LCAP) - has been considered and a newly developed simple method of LCAP prediction described. The aim of this study is the development of a simple method of discrimination of AMP from non-AMP, the efficiency of which will be determined by efficiencies of selected descriptors only and comparison the results of the discrimination procedure with the results obtained by more complicated discriminative methods. As descriptors the physicochemical characteristics responsible for capability of the peptide to interact with an anionic membrane were considered. The following characteristics such as hydrophobicity, amphiphaticity, location of the peptide in relation to membrane, charge, propensity to disordered structure were studied. On the basis of these characteristics a new simple algorithm of prediction is developed and evaluation of efficacies of the characteristics as descriptors performed. The results show that three descriptors: hydrophobic moment, charge and location of the peptide along the membranes can be used as discriminators of LCAPs. For the training set our method gives the same level of accuracy as more complicated machine learning approaches offered as CAMP database service tools. For the test set sensitivity obtained by our method gives even higher value than the one obtained by CAMP prediction tools.
q-bio.BM:Development of the new antimicrobial agents against antibiotic resistance pathogens is the nowadays challenge. Antimicrobial peptides (AMP) occur as important defence agents in many organisms and offer a viable alternative to conventional antibiotics. Therefore they have become increasingly recognized in current research as templates for prospective antibiotic agents. The efficient designing of the new antimicrobials on the basis of antimicrobial peptides requires comprehensive knowledge on those general physical-chemical characteristics which allow to differ antimicrobial peptides from non-active against microbs ones. According to supposed mechanisms of action, AMP interact with and physically disrupt the bacterial membranes. Consequently, hydrophobicity, amphiphilicity and intrinsic aggregation propensities are considered as such major characteristics of the peptide, which determine the results of peptide-membrane interactions. For some kind of peptides such characteristics as hydrophobicity, amphiphilicity and aggregation bias determines their ability to compose transmembrane domain of the membrane protein, whilst for others the same properties are respond for their antimicrpobial activity, i.e. give them ability of membrane permeability and its damage. In this review we analyze the data about hydrophobicity, amphiphilicity and intrinsic aggregation propensities available in literature in order to compare antimicrobial and transmembrane peptides and show what is the common and what is the difference in this respect between them.
q-bio.BM:Quantum mechanical calculations are performed on the proteins that constitute the ubiquitin-Dsk2 complex whose atomic structure has been experimentally determined by NMR spectroscopy (PDB id 1WR1). The results indicate that the dipole moment vectors of the two proteins are aligned in a head-to-tail orientation while forming and angle of ~130{\deg}. Hence, attractive dipole-dipole interactions not only stabilize the protein-protein complex but they are likely to favor the correct orientation of the proteins during the formation of the complex.
q-bio.BM:Quantum mechanical calculations are performed on 116 conformers of the protein ubiquitin (Lange et al., Science 2008, 320, 1471-1475). The results indicate that the heat of formation (HOF), dipole moment, energy of the frontier orbitals HOMO and LUMO, and HOMO-LUMO gap fluctuate within their corresponding ranges. This study thus provides a link between the conformational dynamics of a protein and its electronic structure.
q-bio.BM:Van der Waals density functional theory is integrated with analysis of a non-redundant set of protein-DNA crystal structures from the Nucleic Acid Database to study the stacking energetics of CG:CG base-pair steps, specifically the role of cytosine 5-methylation. Principal component analysis of the steps reveals the dominant collective motions to correspond to a tensile 'opening' mode and two shear 'sliding' and 'tearing' modes in the orthogonal plane. The stacking interactions of the methyl groups globally inhibit CG:CG step overtwisting while simultaneously softening the modes locally via potential energy modulations that create metastable states. Additionally, the indirect effects of the methyl groups on possible base-pair steps neighboring CG:CG are observed to be of comparable importance to their direct effects on CG:CG. The results have implications for the epigenetic control of DNA mechanics.
q-bio.BM:Protein function depends on both protein structure and amino acid (aa) sequence. Here we show that modular features of both structure and function can be quantified from the aa sequence alone for the amyloid 770 aa precursor protein A4. Both the new second order hydropathicity scale, based on evolutionary optimization (self-organized criticality), and the standard first order scale, based on complete protein (water-air) unfolding, yield thermodynamically significant features of amyloid function related to spinodal decomposition into modules. Spinodal crossings are marked by breaks in slope of algebraic functions of aa sequences. The new scale is associated with a lower effective temperature than the standard scale.
q-bio.BM:Standard models of ion channel voltage gating require substantial movement of one transmembrane segment, S4, of the voltage sensing domain. Evidence comes from the accessibility to external methanethiosulfonate (MTS) reagents of the positively charged arginine residues (R) on this segment. These are first mutated to cysteines (C), which in turn react with MTS reagents; it is assumed that the C is passively carried in the S4 movement, becoming accessible on one side or the other of the membrane. However, the Rs were salt bridged to negatively charged residues on other transmembrane segments, or hydrogen bonded, while C reacts as a negative ion. The space available for MTS is fairly close to the difference in volume between the large R residue and much smaller C, so the MTS is not severely sterically hindered. A reagent molecule can reach a C side chain; the C can react if not repelled by a negative charge from the amino acid to which the R had been salt bridged. Nearby protons may also make reaction possible unless the C itself is protonated. Therefore interpretation of the C substitution results requires reconsideration. To test the idea we have done quantum calculations on part of a mutated S4 and the nearby section of the channel. The mutation is R300C of the 2A79/3Lut structure, a mutation that would be done to test MTS reagent access; there is a large cavity where the R is replaced by C. Two quantum calculations show a substantial difference in the structure of this cavity with 2 water molecules compared to 4. This suggests that the structure, and presumably reaction probability, could depend on water molecules, very likely also protons, in or near the cavity that the R300C mutation produces.
q-bio.BM:Hen egg white lysozyme (HEWL) co-crystallisation conditions of carboplatin without sodium chloride (NaCl) have been utilised to eliminate partial conversion of carboplatin to cisplatin observed previously. Tetragonal HEWL crystals were successfully obtained in 65% MPD with 0.1M citric acid buffer at pH 4.0 including DMSO. The X-ray diffraction data resolution to be used for the model refinement was reviewed using several topical criteria together. The CC1/2 criterion implemented in XDS led to data being significant to 2.0{\AA}, compared to the data only being able to be processed to 3.0{\AA} using the Bruker software package (SAINT). Then using paired protein model refinements and DPI values based on the FreeR value, the resolution limit was fine tuned to be 2.3{\AA}. Interestingly this was compared with results from the EVAL software package which gave a resolution limit of 2.2{\AA} solely using <I/sigI> crossing 2, but 2.8{\AA} based on the Rmerge values (60%). The structural results showed that carboplatin bound to only the N{\delta} binding site of His-15 one week after crystal growth, whereas five weeks after crystal growth, two molecules of carboplatin are bound to the His-15 residue. In summary several new results have emerged: - firstly non-NaCl conditions showed a carboplatin molecule bound to His-15 of HEWL; secondly binding of one molecule of carboplatin was seen after one week of crystal growth and two molecules were bound after five weeks of crystal growth; and thirdly the use of several criteria to determine the diffraction resolution limit led to the successful use of data to higher resolution.
q-bio.BM:The original ideas of Cooper and Dryden, that allosteric signalling can be induced between distant binding sites on proteins without any change in mean structural conformation, has proved to be a remarkably prescient insight into the rich structure of protein dynamics. It represents an alternative to the celebrated Monod-Wyman-Changeux mechanism and proposes that modulation of the amplitude of thermal fluctuations around a mean structure, rather than shifts in the structure itself, give rise to allostery in ligand binding. In a complementary approach to experiments on real proteins, here we take a theoretical route to identify the necessary structural components of this mechanism. By reviewing and extending an approach that moves from very coarse-grained to more detailed models, we show that, a fundamental requirement for a body supporting fluctuation-induced allostery is a strongly inhomogeneous elastic modulus. This requirement is reflected in many real proteins, where a good approximation of the elastic structure maps strongly coherent domains onto rigid blocks connected by more flexible interface regions.
q-bio.BM:The heat shock organizing protein (Hop) is important in modulating the activity and co-interaction of two chaperones: heat shock protein 70 and 90 (Hsp70 and Hsp90). Recent research suggested that Plasmodium falciparum Hop (PfHop), PfHsp70 and PfHsp90 form a complex in the trophozoite infective stage. However, there has been little computational research on the malarial Hop protein in complex with other malarial Hsps. Using in silico characterization of the protein, this work showed that individual domains of Hop are evolving at different rates within the protein. Differences between human Hop (HsHop) and PfHop were identified by motif analysis. Homology modeling of PfHop and HsHop in complex with their own cytosolic Hsp90 and Hsp70 C-terminal peptide partners indicated excellent conservation of the Hop concave TPR sites bound to the C-terminal motifs of partner proteins. Further, we analyzed additional binding sites between Hop and Hsp90, and showed, for the first time, that they are distinctly less conserved between human and malaria parasite. These sites are located on the convex surface of Hop TPR2, and involved in interactions with the Hsp90 middle domain. Since the convex sites are less conserved than the concave sites, it makes their potential for malarial inhibitor design extremely attractive (as opposed to the concave sites which have been the focus of previous efforts).
q-bio.BM:An intrinsically disordered protein (IDP) lacks a stable three-dimensional structure, while it folds into a specific structure when it binds to a target molecule. In some IDP-target complexes, not all target binding surfaces are exposed on the outside, and intermediate states are observed in their binding processes. We consider that stepwise target recognition via intermediate states is a characteristic of IDP binding to targets with "hidden" binding sites. To investigate IDP binding to hidden target binding sites, we constructed an IDP lattice model based on the HP model. In our model, the IDP is modeled as a chain and the target is modeled as a highly coarse-grained object. We introduced motion and internal interactions to the target to hide its binding sites. In the case of unhidden binding sites, a two-state transition between the free states and a bound state is observed, and we consider that this represents coupled folding and binding. Introducing hidden binding sites, we found an intermediate bound state in which the IDP forms various structures to temporarily stabilize the complex. The intermediate state provides a scaffold for the IDP to access the hidden binding site. We call this process multiform binding. We conclude that structural flexibility of IDPs enables them to access hidden binding sites, and this is a functional advantage of IDPs.
q-bio.BM:Mapping between sequence and structure is currently an open problem in structural biology. Despite many experimental and computational efforts it is not clear yet how the structure is encoded in the sequence. Answering this question may pave the way for predicting a protein fold given its sequence.   My doctoral studies have focused on a particular phenomenon relevant to the protein sequence-structure relationship. It has been observed that many proteins having apparently dissimilar sequences share the same native fold. The phenomenon of mapping many divergent sequences into a single fold raises the question of which positions along the sequence are important for the conservation of fold and function in dissimilar sequences. Detecting those positions, and classifying them according to role can help understand which elements in a sequence are important for maintenance of structure and/or function. In the course of my doctoral research I have attempted to discover and characterize those positions.
q-bio.BM:Protein function depends on both protein structure and amino acid (aa) sequence. Here we show that modular features of both structure and function can be quantified from the aa sequences alone for the small (40,42 aa) plaque-forming amyloid beta fragments. Some edge and center features of the fragments are predicted. Contrasting results from the second order hydropathicity scale based on evolutionary optimization (self-organized criticality) and the first order scale based on complete protein (water-air) unfolding show that fragmentation has mixed first- and second-order character.
q-bio.BM:Large-scale DNA deformation is ubiquitous in transcriptional regulation in prokaryotes and eukaryotes alike. Though much is known about how transcription factors and constellations of binding sites dictate where and how gene regulation will occur, less is known about the role played by the intervening DNA. In this work we explore the effect of sequence flexibility on transcription factor-mediated DNA looping, by drawing on sequences identified in nucleosome formation and ligase-mediated cyclization assays as being especially favorable for or resistant to large deformations. We examine a poly(dA:dT)-rich, nucleosome-repelling sequence that is often thought to belong to a class of highly inflexible DNAs; two strong nucleosome positioning sequences that share a set of particular sequence features common to nucleosome-preferring DNAs; and a CG-rich sequence representative of high G+C-content genomic regions that correlate with high nucleosome occupancy in vivo. To measure the flexibility of these sequences in the context of DNA looping, we combine the in vitro single-molecule tethered particle motion assay, a canonical looping protein, and a statistical mechan- ical model that allows us to quantitatively relate the looping probability to the looping free energy. We show that, in contrast to the case of nucleosome occupancy, G+C content does not positively correlate with looping probability, and that despite sharing sequence features that are thought to determine nucleosome affinity, the two strong nucleosome positioning sequences behave markedly dissimilarly in the context of looping. Most surprisingly, the poly(dA:dT)-rich DNA that is often characterized as highly inflexible in fact exhibits one of the highest propensities for looping that we have measured.
q-bio.BM:DNA Polymerase is an enzyme that participates in the process of DNA replication. In general, forward DNA polymerase moves along the template strand in a 3'-5' direction, and the daughter strand is formed in a 5'-3' direction. This paper theoretically analyzes the mechanism of forward DNA polymerase, and gives the design proposal of a new reverse DNA polymerase (3'-5' direction).
q-bio.BM:Different models such as diffusion-collision and nucleation-condensation have been used to unravel how secondary and tertiary structures form during protein folding. However, a simple mechanism based on physical principles that provide an accurate description of kinetics and thermodynamics for such phenomena has not yet been identified. This study introduces the hypothesis that the synchronization of the peptide plane oscillatory movements throughout the backbone must also play a key role in the folding mechanism. Based on that, we draw a parallel between the folding process and the dynamics for a network of coupled oscillators described by the Kuramoto model. The amino acid coupling may explain the mean-field character of the force that propels an amino acid sequence into a structure through self-organization. Thus, the pattern of synchronized cluster formation and growing helps to solve the Levinthal's paradox.Synchronization may also help us to understand the success of homology structural modeling, allosteric effect, and the mechanism responsible for the recognition of odorants by olfactory receptors.
q-bio.BM:RNA protein interactions control the fate of cellular RNAs and play an important role in gene regulation. An interdependency between such interactions allows for the implementation of logic functions in gene regulation. We investigate the interplay between RNA binding partners in the context of the statistical physics of RNA secondary structure, and define a linear correlation function between the two partners as a measurement of the interdependency of their binding events. We demonstrate the emergence of a long-range power-law behavior of this linear correlation function. This suggests RNA secondary structure driven interdependency between binding sites as a general mechanism for combinatorial post-transcriptional gene regulation.
q-bio.BM:The inositol trisphosphate receptor (IPR) is a crucial ion channel that regulates the Ca$^{2+}$ influx from the endoplasmic reticulum (ER) to the cytoplasm. A thorough study of the IPR channel contributes to a better understanding of calcium oscillations and waves. It has long been observed that the IPR channel is a typical biological system which performs adaptation. However, recent advances on the physical essence of adaptation show that adaptation systems with a negative feedback mechanism, such as the IPR channel, must break detailed balance and always operate out of equilibrium with energy dissipation. Almost all previous IPR models are equilibrium models assuming detailed balance and thus violate the physical essence of adaptation. In this article, we constructed a nonequilibrium allosteric model of single IPR channels based on the patch-clamp experimental data obtained from the IPR in the outer membranes of isolated nuclei of the \emph{Xenopus} oocyte. It turns out that our model reproduces the patch-clamp experimental data reasonably well and produces both the correct steady-state and dynamic properties of the channel. Particularly, our model successfully describes the complicated bimodal [Ca$^{2+}$] dependence of the mean open duration at high [IP$_3$], a steady-state behavior which fails to be correctly described in previous IPR models. Finally, we used the patch-clamp experimental data to validate that the IPR channel indeed breaks detailed balance and thus is a nonequilibrium system which consumes energy.
q-bio.BM:English version of abstract   A growing number of scientific researches have been demonstrating that olive oil operates a crucial role on the prevention of cardiovascular and tumoral diseases, being related with low mortality and morbidity in populations that tend to follow a Mediterranean diet. Amongst its minor components, polyphenols have been subject to several clinical trials that have established health benefits associated to their antioxidant, antitumoral and anti-atherosclerotic activity. However the biological activity of polyphenols is dependent not only of their absorption but also of their metabolization. Bioavailability studies have demonstrated that after olive oil intake, not only hydroxytyrosol but also its metabolites can be found in plasma, such as 3,4-dihydroxyphenylacetaldehyde, 3,4-dihydroxyphenylacetic acid, homovanillyl alcohol and its glucoronides.   -----   Portuguese version of abstract   Um numero crescente de pesquisas cientificas demonstram que o azeite opera um papel crucial na preven\c{c}ao de doen\c{c}as cardiovasculares e doen\c{c}as tumorais, estando relacionado com a baixa mortalidade e morbilidade em popula\c{c}oes que tradicionalmente seguem uma dieta Mediterranica. De entre os seus componentes minorit\'arios, os polifenois t\^em vindo a ser alvo de estudos clinicos que demonstraram o seu benef\'icio para a saude pela atividade antioxidante, anti-tumoral e anti-aterosclerotica que possuem. No entanto a atividade biol\'ogica destes polifenois \'e dependente nao s\'o da sua absor\c{c}ao como poder\'a tambem depender da sua metaboliza\c{c}ao. Estudos de biodisponibilidade demonstraram que ap\'os o consumo de azeite encontram-se no plasma nao s\'o hidroxitirosol, mas igualmente os seus metabolitos, tais como o \'acido 3,4-dihidroxifenilacetico, o 3,4-dihidroxifenilacetaldeido, o \'alcool homovanilico e os respectivos glucoronideos.
q-bio.BM:The three-dimensional conformations of non-coding RNAs underpin their biochemical functions but have largely eluded experimental characterization. Here, we report that integrating a classic mutation/rescue strategy with high-throughput chemical mapping enables rapid RNA structure inference with unusually strong validation. We revisit a paradigmatic 16S rRNA domain for which SHAPE (selective 2`-hydroxyl acylation with primer extension) suggested a conformational change between apo- and holo-ribosome conformations. Computational support estimates, data from alternative chemical probes, and mutate-and-map (M2) experiments expose limitations of prior methodology and instead give a near-crystallographic secondary structure. Systematic interrogation of single base pairs via a high-throughput mutation/rescue approach then permits incisive validation and refinement of the M2-based secondary structure and further uncovers the functional conformation as an excited state (25+/-5% population) accessible via a single-nucleotide register shift. These results correct an erroneous SHAPE inference of a ribosomal conformational change and suggest a general mutate-map-rescue approach for dissecting RNA dynamic structure landscapes.
q-bio.BM:The mechanosensitive channel of large conductance, which serves as a model system for mechanosensitive channels, has previously been crystallized in the closed form, but not in the open form. Ensemble measurements and electrophysiological sieving experiments show that the open-diameter of the channel pore is >25{\AA}, but the exact size and whether the conformational change follows a helix-tilt or barrel-stave model are unclear. Here we report measurements of the distance changes on liposome-reconstituted MscL transmembrane {\alpha}-helices, using a "virtual sorting" single-molecule fluorescence energy transfer. We observed directly that the channel opens via the helix-tilt model and the open pore reaches 2.8 nm in diameter. In addition, based on the measurements, we developed a molecular dynamics model of the channel structure in the open state which confirms our direct observations.
q-bio.BM:Zinc-finger nucleases (ZFNs) and transcription activator-like effector nucleases (TALENs) comprise a powerful class of tools that are redefining the boundaries of biological research. Although these technologies have begun to enable targeted genome modifications, there remains a need for new technologies that are affordable, scalable, and easy to engineer. In this paper, we propose a new tool for genetic engineering, the pseudocomplementary peptide nucleic acid nucleases (pcPNANs), which are composed of a pseudocomplementary PNA (pcPNA) specific for a DNA target sequence, a FokI nuclease cleavage domain and a nuclear localization signal. pcPNANs may induce targeted DNA double-strand breaks that activate DNA damage response pathways and enable custom alterations. Their cleavage-site is determined by simple Watson-Crick rule, and thus pcPNANs for aimed cleavage of genomes can be straightforwardly designed and synthesized without any selection procedure. Accordingly, the cleavage-site and site-specificity are freely chosen by changing the sequences and the lengths of pcPNA strands. We believe that the potentiality of pcPNAN as a new tool for genetic engineering will be confirmed in the future.
q-bio.BM:The current capacity of computers makes it possible to perform simulations of small systems with portable, explicit-solvent potentials achieving high degree of accuracy. However, simplified models must be employed to exploit the behaviour of large systems or to perform systematic scans of smaller systems. While powerful algorithms are available to facilitate the sampling of the conformational space, successful applications of such models are hindered by the availability of simple enough potentials able to satisfactorily reproduce known properties of the system. We develop an interatomic potential to account for a number of properties of proteins in a computationally economic way. The potential is defined within an all-atom, implicit solvent model by contact functions between the different atom types. The associated numerical values can be optimised by an iterative Monte Carlo scheme on any available experimental data, provided that they are expressible as thermal averages of some conformational properties. We test this model on three different proteins, for which we also perform a scan of all possible point mutations with explicit conformational sampling. The resulting models, optimised solely on a subset of native distances, not only reproduce the native conformations within a few Angstroms from the experimental ones, but show the cooperative transition between native and denatured state and correctly predict the measured free--energy changes associated with point mutations. Moreover, differently from other structure-based models, our method leaves a residual degree of frustration, which is known to be present in protein molecules.
q-bio.BM:F1-ATPase is the soluble portion of the membrane-embedded enzyme FoF1-ATP synthase that catalyzes the production of adenosine triphosphate in eukaryotic and eubacterial cells. In reverse, the F1 part can also hydrolyze ATP quickly at three catalytic binding sites. Therefore, catalysis of 'non-productive' ATP hydrolysis by F1 (or FoF1) must be minimized in the cell. In bacteria, the epsilon subunit is thought to control and block ATP hydrolysis by mechanically inserting its C-terminus into the rotary motor region of F1. We investigate this proposed mechanism by labeling F1 specifically with two fluorophores to monitor the C-terminus of the epsilon subunit by F\"orster resonance energy transfer. Single F1 molecules are trapped in solution by an Anti-Brownian electrokinetic trap which keeps the FRET-labeled F1 in place for extended observation times of several hundreds of milliseconds, limited by photobleaching. FRET changes in single F1 and FRET histograms for different biochemical conditions are compared to evaluate the proposed regulatory mechanism.
q-bio.BM:Subunit epsilon is an intrinsic regulator of the bacterial FoF1-ATP synthase, the ubiquitous membrane-embedded enzyme that utilizes a proton motive force in most organisms to synthesize adenosine triphosphate (ATP). The C-terminal domain of epsilon can extend into the central cavity formed by the alpha and beta subunits, as revealed by the recent X-ray structure of the F1 portion of the Escherichia coli enzyme. This insertion blocks the rotation of the central gamma subunit and, thereby, prevents wasteful ATP hydrolysis. Here we aim to develop an experimental system that can reveal conditions under which epsilon inhibits the holoenzyme FoF1-ATP synthase in vitro. Labeling the C-terminal domain of epsilon and the gamma subunit specifically with two different fluorophores for single-molecule Foerster resonance energy transfer (smFRET) allowed monitoring of the conformation of epsilon in the reconstituted enzyme in real time. New mutants were made for future three-color smFRET experiments to unravel the details of regulatory conformational changes in epsilon.
q-bio.BM:This paper presents and tests a previously unrecognised mechanism for driving a replicating molecular system on the prebiotic earth. It is proposed that cell-free RNA replication in the primordial soup may have been driven by self-sustained oscillatory thermochemical reactions. To test this hypothesis a well-characterised hydrogen peroxide oscillator was chosen as the driver and complementary RNA strands with known association and melting kinetics were used as the substrate. An open flow system model for the self-consistent, coupled evolution of the temperature and concentrations in a simple autocatalytic scheme is solved numerically, and it is shown that thermochemical cycling drives replication of the RNA strands. For the (justifiably realistic) values of parameters chosen for the simulated example system, the mean amount of replicant produced at steady state is 6.56 times the input amount, given a constant supply of substrate species. The spontaneous onset of sustained thermochemical oscillations via slowly drifting parameters is demonstrated, and a scheme is given for prebiotic production of complementary RNA strands on rock surfaces.
q-bio.BM:We have studied the apo (Fe3+ free) form of periplasmic ferric binding protein (FbpA) under different conditions and we have monitored the changes in the binding and release dynamics of H2PO4- that acts as a synergistic anion in the presence of Fe3+. Our simulations predict a dissociation constant of 2.2$\pm$0.2 mM which is in remarkable agreement with the experimentally measured value of 2.3$\pm$0.3 mM under the same ionization strength and pH conditions. We apply perturbations relevant for changes in environmental conditions as (i) different values of ionic strength (IS), and (ii) protonation of a group of residues to mimic a different pH environment. Local perturbations are also studied by protonation or mutation of a site distal to the binding region that is known to mechanically manipulate the hinge-like motions of FbpA. We find that while the average conformation of the protein is intact in all simulations, the H2PO4- dynamics may be substantially altered by the changing conditions. In particular, the bound fraction which is 20$\%$ for the wild type system is increased to 50$\%$ with a D52A mutation/protonation and further to over 90$\%$ at the protonation conditions mimicking those at pH 5.5. The change in the dynamics is traced to the altered electrostatic distribution on the surface of the protein which in turn affects hydrogen bonding patterns at the active site. The observations are quantified by rigorous free energy calculations. Our results lend clues as to how the environment versus single residue perturbations may be utilized for regulation of binding modes in hFbpA systems in the absence of conformational changes.
q-bio.BM:Motivation: Identification of flexible regions of protein structures is important for understanding of their biological functions. Recently, we have developed a fast approach for predicting protein structure fluctuations from a single protein model: the CABS-flex. CABS-flex was shown to be an efficient alternative to conventional all-atom molecular dynamics (MD). In this work, we evaluate CABS-flex and MD predictions by comparison with protein structural variations within NMR ensembles.   Results: Based on a benchmark set of 140 proteins, we show that the relative fluctuations of protein residues obtained from CABS-flex are well correlated to those of NMR ensembles. On average, this correlation is stronger than that between MD and NMR ensembles. In conclusion, CABS-flex is useful and complementary to MD in predicting of protein regions that undergo conformational changes and the extent of such changes.
q-bio.BM:RNA secondary structure prediction and classification are two important problems in the field of RNA biology. Here, we propose a new permutation based approach to create logical non-disjoint clusters of different secondary structures of a single class or type. Many different types of techniques exist to classify RNA secondary structure data but none of them have ever used permutation based approach which is very simple and yet powerful. We have written a small JAVA program to generate permutation, apply our algorithm on those permutations and analyze the data and create different logical clusters. We believe that these clusters can be utilized to untangle the mystery of RNA secondary structure and analyze the development patterns of unknown RNA.
q-bio.BM:To obtain an electron-density map from a macromolecular crystal the phase-problem needs to be solved, which often involves the use of heavy-atom derivative crystals and concomitantly the determination of the heavy atom substructure. This is customarily done by direct methods or Patterson-based approaches, which however may fail when only poorly diffracting derivative crystals are available, as often the case for e.g. membrane proteins. Here we present an approach for heavy atom site identification based on a Molecular Replacement Parameter Matrix (MRPM) search. It involves an n-dimensional search to test a wide spectrum of molecular replacement parameters, such as clusters of different conformations. The result is scored by the ability to identify heavy-atom positions, from anomalous difference Fourier maps, that allow meaningful phases to be determined. The strategy was successfully applied in the determination of a membrane protein structure, the CopA Cu+-ATPase, when other methods had failed to resolve the heavy atom substructure. MRPM is particularly suited for proteins undergoing large conformational changes where multiple search models should be generated, and it enables the identification of weak but correct molecular replacement solutions with maximum contrast to prime experimental phasing efforts.
q-bio.BM:Understanding protein folding has been one of the great challenges in biochemistry and molecular biophysics. Over the past 50 years, many thermodynamic and kinetic studies have been performed addressing the stability of globular proteins. In comparison, advances in the membrane protein folding field lag far behind. Although membrane proteins constitute about a third of the proteins encoded in known genomes, stability studies on membrane proteins have been impaired due to experimental limitations. Furthermore, no systematic experimental strategies are available for folding these biomolecules in vitro. Common denaturing agents such as chaotropes usually do not work on helical membrane proteins, and ionic detergents have been successful denaturants only in few cases. Refolding a membrane protein seems to be a craftsman work, which is relatively straightforward for transmembrane {\beta}-barrel proteins but challenging for {\alpha}-helical membrane proteins. Additional complexities emerge in multidomain membrane proteins, data interpretation being one of the most critical. In this review, we will describe some recent efforts in understanding the folding mechanism of membrane proteins that have been reversibly refolded allowing both thermodynamic and kinetic analysis. This information will be discussed in the context of current paradigms in the protein folding field.
q-bio.BM:Molecular docking is a central method in the computer-based screening of compound libraries as a part of the rational approach to drug design. Although the method has proved its competence in predicting binding modes correctly, its inherent complexity puts high demands on computational resources. Moreover the chemical space to be screened is prohibitively large. Therefore the application of filtering prior to docking is a promising concept. We implemented a pre-docking filter based on the tangent distance algorithm originally conceived for optical character recognition. The challenging transfer of the method from two-dimensional to three-dimensional data was achieved by representing the molecular structure by a set of density maps extracted from different views of the compound. Additionally, our program applies a binary classification using principal component analysis. Ligand and binding pocket are aligned according to their centroidal axes, enabling a size-based filtering for the purpose of enriching the dataset regarding ligands before docking. The evaluation of our program via redocking produced RMSD values between 8{\AA} and 25{\AA}, indicating that the tangent distance approach is not suited for optimizing the orientation of a ligand and binding pocket. Investigating probable explanations lead to the conclusion that a likely cause for these results is the method's known inability to approximate large transformations. A validation of the principal component analysis alone performed better: Tests on a dataset of 170 ligands and 6,435 decoys yielded a sensitivity of 0.81, while keeping the runtime within a reasonable timeframe (1 to 4 seconds). The dataset's enrichment increased from 2.64% to 2.82%.
q-bio.BM:Energy landscapes provide a valuable means for studying the folding dynamics of short RNA molecules in detail by modeling all possible structures and their transitions. Higher abstraction levels based on a macro-state decomposition of the landscape enable the study of larger systems, however they are still restricted by huge memory requirements of exact approaches.   We present a highly parallelizable local enumeration scheme that enables the computation of exact macro-state transition models with highly reduced memory requirements. The approach is evaluated on RNA secondary structure landscapes using a gradient basin definition for macro-states. Furthermore, we demonstrate the need for exact transition models by comparing two barrier-based appoaches and perform a detailed investigation of gradient basins in RNA energy landscapes.   Source code is part of the C++ Energy Landscape Library available at http://www.bioinf.uni-freiburg.de/Software/.
q-bio.BM:Escherichia coli DNA polymerase V (pol V), a heterotrimeric complex composed of UmuD'2C, is marginally active. ATP and RecA play essential roles in the activation of pol V for DNA synthesis including translesion synthesis (TLS). We have established three features of the roles of ATP and RecA. 1) RecA-activated DNA polymerase V (pol V Mut), is a DNA-dependent ATPase; 2) bound ATP is required for DNA synthesis; 3) pol V Mut function is regulated by ATP, with ATP required to bind primer/template (p/t) DNA and ATP hydrolysis triggering dissociation from the DNA. Pol V Mut formed with an ATPase-deficient RecA E38K/K72R mutant hydrolyzes ATP rapidly, establishing the DNA-dependent ATPase as an intrinsic property of pol V Mut distinct from the ATP hydrolytic activity of RecA when bound to single-stranded (ss)DNA as a nucleoprotein filament (RecA*). No similar ATPase activity or autoregulatory mechanism has previously been found for a DNA polymerase.
q-bio.BM:Proteins belonging to immunoglobulin superfamily(IgSF) show remarkably conserved nature both in their folded structure and in their folding process, but they neither originate from very similar sequences nor demonstrate functional conservation. Treating proteins as fractal objects, without studying spatial conservation in positioning of particular residues in IgSF, this work probed the roots structural invariance of immunoglobulins(Ig). Symmetry in distribution of mass, hydrophobicity, polarizability recorded very similar extents in Ig and in structurally-closest non-Ig structures. They registered similar symmetries in dipole-dipole, {\pi}-{\pi}, cation-{\pi} cloud interactions and also in distribution of active chiral centers, charged residues and hydrophobic residues. But in contrast to non-Ig proteins, extents of residual interaction symmetries in Ig.s of largely varying sizes are found to converge to exactly same magnitude of correlation dimension - these are named 'structural attractors', who's weightages depend on ensuring exact convergence of pairwise-interaction symmetries to attractor magnitude. Small basin of attraction for Ig attractors explained the strict and consistent quality control in ensuring stability and functionality of IgSF proteins. Low dependency of attractor weightage on attractor magnitude demonstrated that residual-interaction symmetry with less pervasive nature can also be crucial in ensuring Ig stability.
q-bio.BM:HiRE-RNA is a simplified, coarse-grained RNA model for the prediction of equilibrium configurations, dynamics and thermodynamics. Using a reduced set of particles and detailed interactions accounting for base-pairing and stacking we show that non-canonical and multiple base interactions are necessary to capture the full physical behavior of complex RNAs. In this paper we give a full account of the model and we present results on the folding, stability and free energy surfaces of 16 systems with 12 to 76 nucleotides of increasingly complex architectures, ranging from monomers to dimers, using a total of 850$\mu$s simulation time.
q-bio.BM:The active sites of enzymes consist of residues necessary for catalysis, and structurally important non-catalytic residues that together maintain the architecture and function of the active site. Examples of evolutionary interactions between active site residue have been difficult to define and experimentally validate due to a general intolerance to substitution. Here, using computational methods to predict co-evolving residues, we identify a network of positions consisting of two catalytic metal-binding residues and two adjacent non- catalytic residues in LAGLIDADG family homing endonucleases (LHEs). Distinct combinations of residues in the network map to clades of LHE sub-families, with a striking distribution of the metal-binding Asp (D) and Glu (E) residues. Mutation of these four positions in two LHEs, I-LtrI and I-OnuI, indicate that combinations of residues tolerated are specific to each enzyme. Kinetic analyses under single-turnover conditions indicated a substantial defect in either the catalytic rate (kcat*) or the Michaelis constant (KM*) for I- LtrI variants with networks predicted to be suboptimal. Enzymatic activity could be restored to near wild-type levels with a single compensatory mutation that recreated a network of optimally predicted residues. Our results demonstrate that LHE activity is constrained by an evolutionary barrier of residues with strong context-dependent effects. Creation of optimal co-evolving active site networks is therefore an important consideration in engineering of LHEs and other enzymes.
q-bio.BM:Structural bioinformatics and van der Waals density functional theory are combined to investigate the mechanochemical impact of a major class of histone-DNA interactions, namely the formation of salt bridges between arginine residues in histones and phosphate groups on the DNA backbone. Principal component analysis reveals that the configurational fluctuations of the sugar-phosphate backbone display sequence-specific variability, and clustering of nucleosomal crystal structures identifies two major salt bridge configurations: a monodentate form in which the arginine end-group guanidinium only forms one hydrogen bond with the phosphate, and a bidentate form in which it forms two. Density functional theory calculations highlight that the combination of sequence, denticity and salt bridge positioning enable the histones to tunably activate specific backbone deformations via mechanochemical stress. The results suggest that selection for specific placements of van der Waals contacts, with high-precision control of the spatial distribution of intermolecular forces, may serve as an underlying evolutionary design principle for the structure and function of nucleosomes, a conjecture that is corroborated by previous experimental studies.
q-bio.BM:Elucidation of possible pathways between folded (native) and unfolded states of a protein is a challenging task, as the intermediates are often hard to detect. Here we alter the solvent environment in a controlled manner by choosing two different co-solvents of water, urea and dimethyl sulphoxide (DMSO), and study unfolding of four different proteins to understand the respective sequence of melting by computer simulation methods. We indeed find interesting differences in the sequence of melting of alpha-helices and beta-sheets in these two solvents. For example, at 8M urea solution, beta-sheet parts of a protein is found to unfold preferentially, followed by the unfolding of alpha helices. In contrast, 8M DMSO solution unfolds alpha helices first, followed by the separation of beta-sheets for majority of proteins. Sequence of unfolding events in four different alpha/beta proteins and also in chicken villin head piece (HP-36) both in urea and DMSO solution demonstrate that the unfolding pathways are determined jointly by relative exposure of polar and non-polar residues of a protein and the mode of molecular action of a solvent on that protein.
q-bio.BM:As part of a program to develop compounds with potential to treat cocaine abuse, eleven (+/-)-threo-methylphenidate (TMP; Ritalin) derivatives were synthesized and tested in rat striatal tissue preparations for inhibitory potency against [3H]WIN 35,428 binding (WIN) to the dopamine (DA) transporter, [3H]citalopram binding (CIT) to the serotonin transporter, and [3H]DA uptake. The ester function was replaced by other functional groups in all of the compounds; some also contained substituents on the phenyl ring and/or the piperidine nitrogen. Potencies against WIN, measured as IC50, ranged from 27 nM to 7,000 nM, compared to an IC50 of 83 nM for TMP itself. Potency against [3H]DA uptake was approximately two-fold less than that against WIN, but generally exhibited the same rank order. With one exception, the compounds were significantly less potent against CIT than WIN. The one exception, which has a rigid planar conformation at the altered ester position, is unique in that it also is much less potent against [3H]DA uptake relative to WIN, compared to the other derivatives. The three compounds with dichloro groups on the phenyl ring did not exhibit positive cooperativity, as has been observed with several previously synthesized halogenated TMP derivatives. Taken together, these compounds should help to further our understanding of the stimulant binding sites on both the dopamine and serotonin transporters.
q-bio.BM:Protein-protein interactions are fundamental to many biological processes. Experimental screens have identified tens of thousands of interactions and structural biology has provided detailed functional insight for select 3D protein complexes. An alternative rich source of information about protein interactions is the evolutionary sequence record. Building on earlier work, we show that analysis of correlated evolutionary sequence changes across proteins identifies residues that are close in space with sufficient accuracy to determine the three-dimensional structure of the protein complexes. We evaluate prediction performance in blinded tests on 76 complexes of known 3D structure, predict protein-protein contacts in 32 complexes of unknown structure, and demonstrate how evolutionary couplings can be used to distinguish between interacting and non-interacting protein pairs in a large complex. With the current growth of sequence databases, we expect that the method can be generalized to genome-wide elucidation of protein-protein interaction networks and used for interaction predictions at residue resolution.
q-bio.BM:Statistical coupling analysis (SCA) is a method for analyzing multiple sequence alignments that was used to identify groups of coevolving residues termed "sectors". The method applies spectral analysis to a matrix obtained by combining correlation information with sequence conservation. It has been asserted that the protein sectors identified by SCA are functionally significant, with different sectors controlling different biochemical properties of the protein. Here we reconsider the available experimental data and note that it involves almost exclusively proteins with a single sector. We show that in this case sequence conservation is the dominating factor in SCA, and can alone be used to make statistically equivalent functional predictions. Therefore, we suggest shifting the experimental focus to proteins for which SCA identifies several sectors. Correlations in protein alignments, which have been shown to be informative in a number of independent studies, would then be less dominated by sequence conservation.
q-bio.BM:The growing usage of nanoparticles of zinc sulfide as quantum dots and biosensors calls for a theoretical assessment of interactions of ZnS with biomolecules. We employ the molecular-dynamics-based umbrella sampling method to determine potentials of mean force for 20 single amino acids near the ZnS (110) surface in aqueous solutions. We find that five amino acids do not bind at all and the binding energy of the remaining amino acids does not exceed 4.3 kJ/mol. Such energies are comparable to those found for ZnO (and to hydrogen bonds in proteins) but the nature of the specificity is different. Cysteine can bind with ZnS in a covalent way, e.g. by forming the disulfide bond with S in the solid. If this effect is included within a model incorporating the Morse potential, then the potential well becomes much deeper - the binding energy is close to 98 kJ/mol. We then consider tryptophan cage, a protein of 20 residues, and characterize its events of adsorption to ZnS. We demonstrate the relevance of interactions between the amino acids in the selection of optimal adsorbed conformations and recognize the key role of cysteine in generation of lasting adsorption. We show that ZnS is more hydrophobic than ZnO and that the density profile of water is quite different than that forming near ZnO - it has only a minor articulation into layers. Furthermore, the first layer of water is disordered and mobile.
q-bio.BM:We determine potentials of the mean force for interactions of amino acids with four common surfaces of ZnO in aqueous solutions. The method involves all-atom molecular dynamics simulations combined with the umbrella sampling technique. The profiled nature of the density of water with the strongly adsorbed first layer affects the approach of amino acids to the surface and generates either repulsion or weak binding. The largest binding energy is found for tyrosine interacting with the surface in which the Zn ions are at the top. It is equal to 7 kJ/mol which is comparable to that of the hydrogen bonds in a protein. This makes the adsorption of amino acids onto the ZnO surface much weaker than onto the well studied surface of gold. Under vacuum, binding energies are more than 40 times stronger (for one of the surfaces). The precise manner in which water molecules interact with a given surface influences the binding energies in a way that depends on the surface. Among the four considered surfaces the one with Zn at the top is recognized as binding almost all amino acids with an average binding energy of 2.60 kJ/mol. Another (O at the top) is non-binding for most amino acids. For binding situations the average energy is 0.66 kJ/mol. The remaining two surfaces bind nearly as many amino acids as they do not and the average binding energies are 1.46 and 1.22 kJ/mol. For all of the surfaces the binding energies vary between amino acids significantly: the dispersion in the range of 68-154% of the mean. A small protein is shown to adsorb onto ZnO only intermittently and with only a small deformation. Various adsorption events lead to different patterns in mobilities of amino acids within the protein.
q-bio.BM:We calculate potentials of the mean force for twenty amino acids in the vicinity of the (111) surface of gold, for several dipeptides, and for some analogs of the side chains, using molecular dynamics simulations and the umbrella sampling method. We compare results obtained within three different force fields: one hydrophobic (for a contaminated surface) and two hydrophilic. All of these fields lead to good binding with very different specificities and different patterns in the density and polarization of water. The covalent bond with the sulfur atom on cysteine is modeled by the Morse potential. We demonstrate that binding energies of dipeptides are different than the combined binding energies of their amino-acidic components. For the hydrophobic gold, adsorption events of a small protein are driven by attraction to the strongest binding amino acids. This is not so in the hydrophilic cases - a result of smaller specificities combined with the difficulty for proteins, but not for single amino acids, to penetrate the first layer of water. The properties of water near the surface sensitively depend on the force field.
q-bio.BM:Bacteria produce a range of proteolytic enzymes, for which a number human equivalent or structurally similar examples exist and the primary focus of this study was to analyse the published literature to find proteolytic enzymes, specifically endoproteses and to examine the similarity in the substrates that they act on so as to predict a suitable structural motif which can be used as the basis for preparation of useful prodrug carriers against diseases caused by specific bacteria like Salmonella. Also, the similarities between the bacterial proteases and the action of human matrix metalloproteinases (MMPs), together with the MMP-like activity of bacterial endoproteases to activate human MMPs, were also analysed. This information was used to try to identify substrates on which the MMPs and bacterial proteases act, to aid the design of oligopeptide prodrug carriers to treat cancer and its metastatic spread. MMPs are greatly involved in cancer growth and progression, a few MMPs and certain proteases share a similar type of activity in degrading the extra cellular matrix (ECM) and substrates including gelatin. Our primary targets of study were to identify the proteases and MMPs that facilitate the migration of bacteria and growth of tumour cells respectively. The study was thus a two-way approach to study the substrate specificity of both bacterial proteases and MMPs, thereby to help in characterisation of their substrates. Various bioinformatics tools were used in the characterisation of the proteases and substrates as well as in the identification of possible binding sites and conserved regions in a range of candidate proteins. Central to this project was the salmonella derived PgtE surface protease that has been shown recently to act upon the pro-forms of human MMP-9.
q-bio.BM:Wolumes is a fast and stand-alone computer program written in standard C that allows the measure of atom volumes in proteins. Its algorithm is a simple discretization of the space by means of a grid of points at 0.75 Angstroms from each other and it uses a set of van der Waals radii optimized for protein atoms. By comparing the computed values with distributions derived from a non-redundant subset of the Protein Data Bank, the new methods allows to identify atoms and residues abnormally large/small. The source code is freely available, together with some examples.
q-bio.BM:We show that a mesoscale model, with a minimal number of parameters, can well describe the thermomechanical and mechanochemical behavior of homogeneous DNA at thermal equilibrium under tension and torque. We predict critical temperatures for denaturation under torque and stretch, phase diagrams for stable DNA, probe/response profiles under mechanical loads, and the density of dsDNA as a function of stretch and twist. We compare our predictions with available single molecule manipulation experiments and find strong agreement. In particular we elucidate the difference between angularly constrained and unconstrained overstretching. We propose that the smoothness of the angularly constrained overstreching transition is a consequence of the molecule being in the vicinity of criticality for a broad range of values of applied tension.
q-bio.BM:We perform extensive coarse-grained (CG) Langevin dynamics simulations of intrinsically disordered proteins (IDPs), which possess fluctuating conformational statistics between that for excluded volume random walks and collapsed globules. Our CG model includes repulsive steric, attractive hydrophobic, and electrostatic interactions between residues and is calibrated to a large collection of single-molecule fluorescence resonance energy transfer data on the inter-residue separations for 36 pairs of residues in five IDPs: $\alpha$-, $\beta$-, and $\gamma$-synuclein, the microtubule-associated protein $\tau$, and prothymosin $\alpha$. We find that our CG model is able to recapitulate the average inter-residue separations regardless of the choice of the hydrophobicity scale, which shows that our calibrated model can robustly capture the conformational dynamics of IDPs. We then employ our model to study the scaling of the radius of gyration with chemical distance in 11 known IDPs. We identify a strong correlation between the distance to the dividing line between folded proteins and IDPs in the mean charge and hydrophobicity space and the scaling exponent of the radius of gyration with chemical distance along the protein.
q-bio.BM:The biomolecules in and around a living cell -- proteins, nucleic acids, lipids, carbohydrates -- continuously sample myriad conformational states that are thermally accessible at physiological temperatures. Simultaneously, a given biomolecule also samples (and is sampled by) a rapidly fluctuating local environment comprised of other biopolymers, small molecules, water, ions, etc. that diffuse to within a few nanometers, leading to inter-molecular contacts that stitch together large supramolecular assemblies. Indeed, all biological systems can be viewed as dynamic networks of molecular interactions. As a complement to experimentation, molecular simulation offers a uniquely powerful approach to analyze biomolecular structure, mechanism, and dynamics; this is possible because the molecular contacts that define a complicated biomolecular system are governed by the same physical principles (forces, energetics) that characterize individual small molecules, and these simpler systems are relatively well-understood. With modern algorithms and computing capabilities, simulations are now an indispensable tool for examining biomolecular assemblies in atomic detail, from the conformational motion in an individual protein to the diffusional dynamics and inter-molecular collisions in the early stages of formation of cellular-scale assemblies such as the ribosome. This text introduces the physicochemical foundations of molecular simulations and docking, largely from the perspective of biomolecular interactions.
q-bio.BM:The determination of a 'folding core' can help to provide insight into the structure, flexibility, mobility and dynamics, and hence, ultimately, function of a protein - a central concern of structural biology. Changes in the folding core upon ligand binding are of particular interest because they may be relevant to drug-induced functional changes. Cyclophilin A is a multi-functional ligand-binding protein and a significant drug target. It acts principally as an enzyme during protein folding, but also as the primary binding partner for the immunosuppressant drug cyclosporin A (CsA). Here, we have used hydrogen-deuterium exchange (HDX) NMR spectroscopy to determine the folding core of the CypA-CsA complex. We also use the rapid computational tool of rigidity analysis, implemented in FIRST, to determine a theoretical folding core of the complex. In addition we generate a theoretical folding core for the unbound protein and compare this with previously published HDX data. The FIRST method gives a good prediction of the HDX folding core, but we find that it is not yet sufficiently sensitive to predict the effects of ligand binding on CypA.
q-bio.BM:Determining the folding core of a protein yields information about its folding process and dynamics. The experimental procedures for identifying the amino acids which make up the folding core include hydrogen-deuterium exchange and $\Phi$-value analysis and can be expensive and time consuming. As such there is a desire to improve upon existing methods for determining protein folding cores theoretically. Here, we use a combined method of rigidity analysis alongside coarse-grained simulations of protein motion in order to improve folding core predictions for unbound CypA and for the CypA-CsA complex. We find that the most specific prediction of folding cores in CypA and CypA-CsA comes from the intersection of the results of static rigidity analysis, implemented in the FIRST software suite, and simulations of the propensity for flexible motion, using the FRODA tool.
q-bio.BM:The analysis of correlations of amino acid occurrences in globular proteins has led to the development of statistical tools that can identify native contacts -- portions of the chains that come to close distance in folded structural ensembles. Here we introduce a statistical coupling analysis for repeat proteins -- natural systems for which the identification of domains remains challenging. We show that the inherent translational symmetry of repeat protein sequences introduces a strong bias in the pair correlations at precisely the length scale of the repeat-unit. Equalizing for this bias reveals true co-evolutionary signals from which local native-contacts can be identified. Importantly, parameter values obtained for all other interactions are not significantly affected by the equalization. We quantify the robustness of the procedure and assign confidence levels to the interactions, identifying the minimum number of sequences needed to extract evolutionary information in several repeat protein families. The overall procedure can be used to reconstruct the interactions at long distances, identifying the characteristics of the strongest couplings in each family, and can be applied to any system that appears translationally symmetric.
q-bio.BM:Given the importance of non-coding RNAs to cellular regulatory functions and rapid growth of RNA transcripts, computational prediction of RNA tertiary structure remains highly demanded yet significantly challenging. Even for a short RNA sequence, the space of tertiary conformations is immense; existing methods to identify native-like conformations mostly resort to random sampling of conformations to gain computational feasibility. However native conformations may not be examined and prediction accuracy may be compromised due to sampling. In particular, the state-of-the-art methods have yet to deliver the desired prediction performance for RNAs of length beyond 50.   This paper presents the work to tackle a key step in the RNA tertiary structure prediction problem, the prediction of the nucleotide interactions that constitute the desired tertiary structure. The research is established upon a novel graph model, called backbone k-tree, to markably constrain nucleotide interaction relationships in RNA tertiary structure. It is shown that the new model makes it possible to efficiently predict the optimal set of nucleotide interactions from the query sequence, including the interactions in all recently revealed families. Evident by the preliminary results, the new method can predict with a high accuracy the nucleotide interactions that constitute the tertiary structure of the query sequence, thus providing a viable solution towards ab initio prediction of RNA tertiary structure.
q-bio.BM:After a brief review of the protein folding quantum theory and a short discussion on its experimental evidences the mechanism of glucose transport across membrane is studied from the point of quantum conformational transition. The structural variations among four kinds of conformations of the human glucose transporter GLUT1 (ligand free occluded, outward open, ligand bound occluded and inward open) are looked as the quantum transition. The comparative studies between mechanisms of uniporter (GLUT1) and symporter (XylE and GlcP) are given. The transitional rates are calculated from the fundamental theory. The monosaccharide transport kinetics is proposed. The steady state of the transporter is found and its stability is studied. The glucose (xylose) translocation rates in two directions and in different steps are compared. The mean transport time in a cycle is calculated and based on it the comparison of the transport times between GLUT1,GlcP and XylE can be drawn. The non-Arrhenius temperature dependence of the transition rate and the mean transport time is predicted. It is suggested that the direct measurement of temperature dependence is a useful tool for deeply understanding the transmembrane transport mechanism.
q-bio.BM:Nanotechnology and synthetic biology currently constitute one of the most innovative, interdisciplinary fields of research, poised to radically transform society in the 21st century. This paper concerns the synthetic design of ribonucleic acid molecules, using our recent algorithm, RNAiFold, which can determine all RNA sequences whose minimum free energy secondary structure is a user-specified target structure. Using RNAiFold, we design ten cis-cleaving hammerhead ribozymes, all of which are shown to be functional by a cleavage assay. We additionally use RNAiFold to design a functional cis-cleaving hammerhead as a modular unit of a synthetic larger RNA. Analysis of kinetics on this small set of hammerheads suggests that cleavage rate of computationally designed ribozymes may be correlated with positional entropy, ensemble defect, structural flexibility/rigidity and related measures. Artificial ribozymes have been designed in the past either manually or by SELEX (Systematic Evolution of Ligands by Exponential Enrichment); however, this appears to be the first purely computational design and experimental validation of novel functional ribozymes. RNAiFold is available at http://bioinformatics.bc.edu/clotelab/RNAiFold/.
q-bio.BM:Protein conformational transitions, which are essential for function, may be driven either by entropy or enthalpy when molecular systems comprising solute and solvent molecules are the focus. Revealing thermodynamic origin of a given molecular process is an important but difficult task, and general principles governing protein conformational distributions remain elusive. Here we demonstrate that when protein molecules are taken as thermodynamic systems and solvents being treated as the environment, conformational entropy is an excellent proxy for free energy and is sufficient to explain protein conformational distributions. Specifically, by defining each unique combination of side chain torsional state as a conformer, the population distribution (or free energy) on an arbitrarily given order parameter is approximately a linear function of conformational entropy. Additionally, span of various microscopic potential energy terms is observed to be highly correlated with both conformational entropy and free energy. Presently widely utilized free energy proxies, including minimum potential energy, average potential energy terms by themselves or in combination with vibrational entropy\cite, are found to correlate with free energy rather poorly. Therefore, our findings provide a fundamentally new theoretical base for development of significantly more reliable and efficient next generation computational tools, where the number of available conformers,rather than poential energy of microscopic configurations, is the central focus. We anticipate that many related research fields, including structure based drug design and discovery, protein design, docking and prediction of general intermolecular interactions involving proteins, are expected to benefit greatly.
q-bio.BM:Protein binding and function often involves conformational changes. Advanced NMR experiments indicate that these conformational changes can occur in the absence of ligand molecules (or with bound ligands), and that the ligands may 'select' protein conformations for binding (or unbinding). In this review, we argue that this conformational selection requires transition times for ligand binding and unbinding that are small compared to the dwell times of proteins in different conformations, which is plausible for small ligand molecules. Such a separation of timescales leads to a decoupling and temporal ordering of binding/unbinding events and conformational changes. We propose that conformational-selection and induced-change processes (such as induced fit) are two sides of the same coin, because the temporal ordering is reversed in binding and unbinding direction. Conformational-selection processes can be characterized by a conformational excitation that occurs prior to a binding or unbinding event, while induced-change processes exhibit a characteristic conformational relaxation that occurs after a binding or unbinding event. We discuss how the ordering of events can be determined from relaxation rates and effective on- and off-rates determined in mixing experiments, and from the conformational exchange rates measured in advanced NMR or single-molecule FRET experiments. For larger ligand molecules such as peptides, conformational changes and binding events can be intricately coupled and exhibit aspects of conformational-selection and induced-change processes in both binding and unbinding direction.
q-bio.BM:In this paper we show the existence of three dimensional rigid, and thus unfoldable, lattice conformations. The structure we found has 450+ bonds, and we provide a computer assisted proof of the existence of such structures. The existence of such rigid structures illustrates why protein folding problems are hard also in dimension three. The existence of two rigid two dimensional structures was shown earlier. This work answers question 8 in \cite{2D} in the affirmative: rigid self avoiding walks exist in three dimensional lattice configurations.
q-bio.BM:Leading Ebola subtypes exhibit a wide mortality range, here explained at the molecular level by using fractal hydropathic scaling of amino acid sequences based on protein self-organized criticality. Specific hydrophobic features in the hydrophilic mucin-like domain suffice to account for the wide mortality range. Significance statement: Ebola virus is spreading rapidly in Africa. The connection between protein amino acid sequence and mortality is identified here.
q-bio.BM:More virulent strains of influenza virus subtypes H1N1 appeared in 2007 and H3N2 in 2011. The amino acid differences from prior less virulent strains appear to be small when tabulated through sequence alignments and counting site identities and similarities. Here we show how analyzing fractal hydropathic forces responsible for globular compaction and modularity quantifies the mutational origins of increased virulence, and also analyzes receptor sites and N-linked glycan accretion.
q-bio.BM:Thermodynamic tools are well suited to connecting evolution of protein functionalities to mutations of amino acid sequences, especially for neuronal network structures. These tools enable one to quantify changes in modular structure and correlate them with corresponding changes in observable properties. Here we quantify modular rodent-primate changes in amyloid precursor protein A4 and \b{eta} amyloid fragments. These are related to changes in cortical connectivity and to the presence (absence) of plaque formation in primates (rodents). Two thermodynamic scales are used, descriptive of water/air protein unfolding (old), or fractal conformational restructuring (new). These describe complementary aspects of protein activity, at respectively higher and lower effective temperatures.
q-bio.BM:In post-transcriptional regulation, an mRNA molecule is bound by many proteins and/or miRNAs to modulate its function. To enable combinatorial gene regulation, these binding partners of an RNA must communicate with each other, exhibiting cooperativity. Even in the absence of direct physical interactions between the binding partners, such cooperativity can be mediated through RNA secondary structures, since they affect the accessibility of the binding sites. Here we propose a quantitative measure of this structure-mediated cooperativity that can be numerically calculated for an arbitrary RNA sequence. Focusing on an RNA with two binding sites, we derive a characteristic difference of free energy differences, i.e. \Delta\Delta G, as a measure of the effect of the occupancy of one binding site on the binding strength of another. We apply this measure to a large number of human and C. elegans mRNAs, and find that structure-mediated cooperativity is a generic feature. Interestingly, this cooperativity not only affects binding sites in close proximity along the sequence but also configurations in which one binding site is located in the 5'UTR and the other is located in the 3'UTR of the mRNA. Furthermore, we find that this end-to-end cooperativity is determined by the UTR sequences while the sequences of the coding regions are irrelevant.
q-bio.BM:Deoxycitidine in solution exists as two tautomers one of which is an uncanonical imino one. The latter can dominate with such derivatives as 5-methyl, 5-hydroxymethyl- and 5-formylcytosine. The imino tautomer potentially is able to form a hoosteen GC base pair. To detect such pair, it is suggested to use 1H15N NMR. Formation of GC-Hoogsteen base pair with imino tautomer of cytosine can be a reason for epigenetic effects of 5-methyl- and 5-hydroxymethylcytosine.
q-bio.BM:Something about structure of the water shell of DNA and how it can help to design of novel biologically-active molecules and potential drugs with sequence-specific binding to nucleic acids.
q-bio.BM:Consider the network of all secondary structures of a given RNA sequence, where nodes are connected when the corresponding structures have base pair distance one. The expected degree of the network is the average number of neighbors, where average may be computed with respect to the either the uniform or Boltzmann probability. Here we describe the first algorithm, RNAexpNumNbors, that can compute the expected number of neighbors, or expected network degree, of an input sequence. For RNA sequences from the Rfam database, the expected degree is significantly less than the CMFE structure, defined to have minimum free energy over all structures consistent with the Rfam consensus structure. The expected degree of structural RNAs, such as purine riboswitches, paradoxically appears to be smaller than that of random RNA, yet the difference between the degree of the MFE structure and the expected degree is larger than that of random RNA. Expected degree does not seem to correlate with standard structural diversity measures of RNA, such as positional entropy, ensemble defect, etc. The program {\tt RNAexpNumNbors} is written in C, runs in cubic time and quadratic space, and is publicly available at http://bioinformatics.bc.edu/clotelab/RNAexpNumNbors.
q-bio.BM:Toxic fibrillar aggregates of Islet Amyloid PolyPeptide (IAPP) appear as the physical outcome of a peptidic phase-transition signaling the onset of type-2 diabetes mellitus in different mammalian species. In particular, experimentally verified mutations on the amyloidogenic segment 20-29 in humans, cats and rats are highly correlated with the molecular aggregation propensities. Through a microcanonical analysis of the aggregation of IAPP_{20-29} isoforms, we show that a minimalist one-bead hydrophobic-polar continuum model for protein interactions properly quantifies those propensities from free-energy barriers. Our results highlight the central role of sequence-dependent hydrophobic mutations on hot spots for stabilization, and so for the engineering, of such biological peptides.
q-bio.BM:Allosteric regulation at distant sites is central to many cellular processes. In particular, allosteric sites in proteins are a major target to increase the range and selectivity of new drugs, and there is a need for methods capable of identifying intra-molecular signalling pathways leading to allosteric effects. Here, we use an atomistic graph-theoretical approach that exploits Markov transients to extract such pathways and exemplify our results in an important allosteric protein, caspase-1. Firstly, we use Markov Stability community detection to perform a multiscale analysis of the structure of caspase-1 which reveals that the active conformation has a weaker, less compartmentalised large-scale structure as compared to the inactive conformation, resulting in greater intra-protein coherence and signal propagation. We also carry out a full computational point mutagenesis and identify that only a few residues are critical to such structural coherence. Secondly, we characterise explicitly the transients of random walks originating at the active site and predict the location of a known allosteric site in this protein quantifying the contribution of individual bonds to the communication pathway between the active and allosteric sites. Several of the bonds we find have been shown experimentally to be functionally critical, but we also predict a number of as yet unidentified bonds which may contribute to the pathway. Our approach offers a computationally inexpensive method for the identification of allosteric sites and communication pathways in proteins using a fully atomistic description.
q-bio.BM:{\alpha}-synuclein ({\alpha}-syn) is an intrinsically disordered protein which is considered to be one of the causes of Parkinson's disease. This protein forms amyloid fibrils when in a highly concentrated solution. The fibril formation of {\alpha}-syn is induced not only by increases in {\alpha}-syn concentration but also by macromolecular crowding. In order to investigate the coupled effect of the intrinsic disorder of {\alpha}-syn and macromolecular crowding, we construct a lattice gas model of {\alpha}-syn in contact with a crowding agent reservoir based on statistical mechanics. The main assumption is that {\alpha}-syn can be expressed as coarse-grained particles with internal states coupled with effective volume; and disordered states are modeled by larger particles with larger internal entropy than other states. Thanks to the simplicity of the model, we can exactly calculate the number of conformations of crowding agents, and this enables us to prove that the original grand canonical ensemble with a crowding agent reservoir is mathematically equivalent to a canonical ensemble without crowding agents. In this expression, the effect of macromolecular crowding is absorbed in the internal entropy of disordered states; it is clearly shown that the crowding effect reduces the internal entropy. Based on Monte Carlo simulation, we provide scenarios of crowding-induced fibril formation. We also discuss the recent controversy over the existence of helically folded tetramers of {\alpha}-syn, and suggest that macromolecular crowding is the key to resolving the controversy.
q-bio.BM:Human serum transferrin (hTf) transports ferric ions in the blood stream and inflamed mucosal surfaces with high affinity and delivers them to cells via receptor mediated endocytosis. A typical hTf is folded into two homologous lobes; each lobe is further divided into two similar sized domains. Three different crystal structures of hTf delineate large conformational changes involved in iron binding/dissociation. However, whether the release process follows the same trend at serum (~7.4) and endosomal (~5.6) pH remains unanswered. The specialized role of the two lobes and if communication between them leads to efficient and controlled release is also debated. Here, we study the dynamics of the full structure as well as the separate lobes in different closed, partially open, and open conformations under the nearly neutral pH conditions in the blood serum and the more acidic one in the endosome. The results corroborate experimental observations and underscore the distinguishing effect of pH on the dynamics of hTf. Furthermore, in a total of 2 {\mu}s molecular dynamics simulation of different forms of hTf, residue fluctuations elucidate the cross talk between the two lobes correlated by the peptide linker bridging the two lobes at serum pH, while their correlated motions is lost under endosomal conditions. At serum pH, the presence of even a single iron on either lobe leads C-lobe fluctuations to subside, making it the target for recognition by human cells or hostile bacteria seeking iron sequestration. The N-lobe, on the other hand, has a propensity to open, making iron readily available when needed at regions of serum pH. At endosomal pH, both lobes readily open, making irons available for delivery. The interplay between the relative mobility of the lobes renders efficient mechanism for the recognition and release of hTf at the cell surface, and therefore its recycling in the organism.
q-bio.BM:For more than 30 years, the only enzymatic system known to catalyze the elimination of superoxide was superoxide dismutase, SOD. SOD has been found in almost all organisms living in the presence of oxygen, including some anaerobic bacteria, supporting the notion that superoxide is a key and general component of oxidative stress. Recently, a new concept in the field of the mechanisms of cellular defense against superoxide has emerged. It was discovered that elimination of superoxide in some anaerobic and microaerophilic bacteria could occur by reduction, a reaction catalyzed by a small metalloenzyme thus named superoxide reductase, SOR. Having played a major role in this discovery, we describe here how the concept of superoxide reduction emerged and how it was experimentally substantiated independently in our laboratory.
q-bio.BM:Persistent homology provides a new approach for the topological simplification of big data via measuring the life time of intrinsic topological features in a filtration process and has found its success in scientific and engineering applications. However, such a success is essentially limited to qualitative data characterization, identification and analysis (CIA). In this work, we outline a general protocol to construct objective-oriented persistent homology methods. The minimization of the objective functional leads to a Laplace-Beltrami operator which generates a multiscale representation of the initial data and offers an objective oriented filtration process. The resulting differential geometry based objective-oriented persistent homology is able to preserve desirable geometric features in the evolutionary filtration and enhances the corresponding topological persistence. The consistence between Laplace-Beltrami flow based filtration and Euclidean distance based filtration is confirmed on the Vietoris-Rips complex for a large amount of numerical tests. The convergence and reliability of the present Laplace-Beltrami flow based cubical complex filtration approach are analyzed over various spatial and temporal mesh sizes. The efficiency and robustness of the present method are verified by more than 500 fullerene molecules. It is shown that the proposed persistent homology based quantitative model offers good predictions of total curvature energies for ten types of fullerene isomers. The present work offers the first example to design objective-oriented persistent homology to enhance or preserve desirable features in the original data during the filtration process and then automatically detect or extract the corresponding topological traits from the data.
q-bio.BM:Persistent homology is a relatively new tool often used for \emph{qualitative} analysis of intrinsic topological features in images and data originated from scientific and engineering applications. In this paper, we report novel \emph{quantitative} predictions of the energy and stability of fullerene molecules, the very first attempt in employing persistent homology in this context. The ground-state structures of a series of small fullerene molecules are first investigated with the standard Vietoris-Rips complex. We decipher all the barcodes, including both short-lived local bars and long-lived global bars arising from topological invariants, and associate them with fullerene structural details. By using accumulated bar lengths, we build quantitative models to correlate local and global Betti-2 bars respectively with the heat of formation and total curvature energies of fullerenes. It is found that the heat of formation energy is related to the local hexagonal cavities of small fullerenes, while the total curvature energies of fullerene isomers are associated with their sphericities, which are measured by the lengths of their long-lived Betti-2 bars. Excellent correlation coefficients ($>0.94$) between persistent homology predictions and those of quantum or curvature analysis have been observed. A correlation matrix based filtration is introduced to further verify our findings.
q-bio.BM:In this work, we introduce persistent homology for the analysis of cryo-electron microscopy (cryo-EM) density maps. We identify the topological fingerprint or topological signature of noise, which is widespread in cryo-EM data. For low signal to noise ratio (SNR) volumetric data, intrinsic topological features of biomolecular structures are indistinguishable from noise. To remove noise, we employ geometric flows which are found to preserve the intrinsic topological fingerprints of cryo-EM structures and diminish the topological signature of noise. In particular, persistent homology enables us to visualize the gradual separation of the topological fingerprints of cryo-EM structures from those of noise during the denoising process, which gives rise to a practical procedure for prescribing a noise threshold to extract cryo-EM structure information from noise contaminated data after certain iterations of the geometric flow equation. To further demonstrate the utility of persistent homology for cryo-EM data analysis, we consider a microtubule intermediate structure (EMD-1129). Three helix models, an alpha-tubulin monomer model, an alpha- and beta-tubulin model, and an alpha- and beta-tubulin dimer model, are constructed to fit the cryo-EM data. The least square fitting leads to similarly high correlation coefficients, which indicates that structure determination via optimization is an ill-posed inverse problem. By a comparison of the topological fingerprints of the original data and those of three models, we found that the third model is topologically favored. The present work offers persistent homology based new strategies for topological denoising and for resolving ill-posed inverse problems.
q-bio.BM:Superoxide radical (O2.-) is the univalent reduction product of molecular oxygen and belongs to the group of the so-called toxic oxygen derivatives. For years the only enzymatic system known to catalyze the elimination of superoxide was the superoxide dismutase (SOD), which catalyzes dismutation of superoxide radical anions to hydrogen peroxide and molecular oxygen
q-bio.BM:Proteins are the most important biomolecules for living organisms. The understanding of protein structure, function, dynamics and transport is one of most challenging tasks in biological science. In the present work, persistent homology is, for the first time, introduced for extracting molecular topological fingerprints (MTFs) based on the persistence of molecular topological invariants. MTFs are utilized for protein characterization, identification and classification. The method of slicing is proposed to track the geometric origin of protein topological invariants. Both all-atom and coarse-grained representations of MTFs are constructed. A new cutoff-like filtration is proposed to shed light on the optimal cutoff distance in elastic network models. Based on the correlation between protein compactness, rigidity and connectivity, we propose an accumulated bar length generated from persistent topological invariants for the quantitative modeling of protein flexibility. To this end, a correlation matrix based filtration is developed. This approach gives rise to an accurate prediction of the optimal characteristic distance used in protein B-factor analysis. Finally, MTFs are employed to characterize protein topological evolution during protein folding and quantitatively predict the protein folding stability. An excellent consistence between our persistent homology prediction and molecular dynamics simulation is found. This work reveals the topology-function relationship of proteins.
q-bio.BM:The flexibility-rigidity index (FRI) is a newly proposed method for the construction of atomic rigidity functions. The FRI method analyzes protein rigidity and flexibility and is capable of predicting protein B-factors without resorting to matrix diagonalization. A fundamental assumption used in the FRI is that protein structures are uniquely determined by various internal and external interactions, while the protein functions, such as stability and flexibility, are solely determined by the structure. As such, one can predict protein flexibility without resorting to the protein interaction Hamiltonian. Consequently, bypassing the matrix diagonalization, the original FRI has a computational complexity of O(N^2). This work introduces a fast FRI (fFRI) algorithm for the flexibility analysis of large macromolecules. The proposed fFRI further reduces the computational complexity to O(N). Additionally, we propose anisotropic FRI (aFRI) algorithms for the analysis of protein collective dynamics. The aFRI algorithms admit adaptive Hessian matrices, from a completely global 3N*3N matrix to completely local 3*3 matrices. However, these local 3*3 matrices have built in much non-local correlation. Furthermore, we compare the accuracy and efficiency of FRI with some {established} approaches to flexibility analysis, namely, normal mode analysis (NMA) and Gaussian network model (GNM). The accuracy of the FRI method is tested. The FRI, particularly the fFRI, is orders of magnitude more efficient and about 10% more accurate overall than some of the most popular methods in the field. The proposed fFRI is able to predict B-factors for alpha-carbons of the HIV virus capsid (313,236 residues) in less than 30 seconds on a single processor using only one core. Finally, we demonstrate the application of FRI and aFRI to protein domain analysis.
q-bio.BM:Desulfoferrodoxin is a small protein found in sulfate-reducing bacteria that contains two independent mononuclear iron centers, one ferric and one ferrous. Expression of desulfoferrodoxin from Desulfoarculus baarsii has been reported to functionally complement a superoxide dismutase deficient Escherichia coli strain. To elucidate by which mechanism desulfoferrodoxin could substitute for superoxide dismutase in E. coli, we have purified the recombinant protein and studied its reactivity toward O-(2). Desulfoferrodoxin exhibited only a weak superoxide dismutase activity (20 units mg(-1)) that could hardly account for its antioxidant properties. UV-visible and electron paramagnetic resonance spectroscopy studies revealed that the ferrous center of desulfoferrodoxin could specifically and efficiently reduce O-(2), with a rate constant of 6-7 x 10(8) M(-1) s(-1). In addition, we showed that membrane and cytoplasmic E. coli protein extracts, using NADH and NADPH as electron donors, could reduce the O-(2) oxidized form of desulfoferrodoxin. Taken together, these results strongly suggest that desulfoferrodoxin behaves as a superoxide reductase enzyme and thus provide new insights into the biological mechanisms designed for protection from oxidative stresses.
q-bio.BM:Flavin reductase catalyzes the reduction of free flavins by NAD(P)H. As isolated, Escherichia coli flavin reductase does not contain any flavin prosthetic group but accommodates both the reduced pyridine nucleotide and the flavin substrate in a ternary complex prior to oxidoreduction. The reduction of riboflavin by NADPH catalyzed by flavin reductase has been studied by static and rapid kinetics absorption spectroscopies. Static absorption spectroscopy experiments revealed that, in the presence of riboflavin and reduced pyridine nucleotide, flavin reductase stabilizes, although to a small extent, a charge-transfer complex of NADP+ and reduced riboflavin. In addition, reduction of riboflavin was found to be essentially irreversible. Rapid kinetics absorption spectroscopy studies demonstrated the occurrence of two intermediates with long-wavelength absorption during the catalytic cycle. Such intermediate species exhibit spectroscopic properties similar to those of charge-transfer complexes of oxidized flavin and NAD(P)H, and reduced flavin and NAD(P)+, respectively, which have been identified as intermediates during the reaction of flavoenzymes of the ferredoxin-NADP+ reductase family. Thus, a minimal kinetic scheme for the reaction of flavin reductase with NADPH and riboflavin can be proposed. After formation of the Michaelis complex of flavin reductase with NADPH and riboflavin, a first intermediate, identified as a charge-transfer complex of NADPH and riboflavin, is formed. It is followed by a second charge-transfer intermediate of enzyme-bound NADP+ and reduced riboflavin. The latter decays, yielding the Michaelis complex of flavin reductase with NADP+ and reduced riboflavin, which then dissociates to complete the reaction. These results support the initial hypothesis of a structural similarity between flavin reductase and the enzymes of the ferredoxin-NADP+ reductase family and extend it at a functional level.
q-bio.BM:Iron-peroxide intermediates are central in the reaction cycle of many iron-containing biomolecules. We trapped iron(III)-(hydro)peroxo species in crystals of superoxide reductase (SOR), a nonheme mononuclear iron enzyme that scavenges superoxide radicals. X-ray diffraction data at 1.95 angstrom resolution and Raman spectra recorded in crystallo revealed iron-(hydro)peroxo intermediates with the (hydro)peroxo group bound end-on. The dynamic SOR active site promotes the formation of transient hydrogen bond networks, which presumably assist the cleavage of the iron-oxygen bond in order to release the reaction product, hydrogen peroxide.
q-bio.BM:Persistent homology has emerged as a popular technique for the topological simplification of big data, including biomolecular data. Multidimensional persistence bears considerable promise to bridge the gap between geometry and topology. However, its practical and robust construction has been a challenge. We introduce two families of multidimensional persistence, namely pseudo-multidimensional persistence and multiscale multidimensional persistence. The former is generated via the repeated applications of persistent homology filtration to high dimensional data, such as results from molecular dynamics or partial differential equations. The latter is constructed via isotropic and anisotropic scales that create new simiplicial complexes and associated topological spaces. The utility, robustness and efficiency of the proposed topological methods are demonstrated via protein folding, protein flexibility analysis, the topological denoising of cryo-electron microscopy data, and the scale dependence of nano particles. Topological transition between partial folded and unfolded proteins has been observed in multidimensional persistence. The separation between noise topological signatures and molecular topological fingerprints is achieved by the Laplace-Beltrami flow. The multiscale multidimensional persistent homology reveals relative local features in Betti-0 invariants and the relatively global characteristics of Betti-1 and Betti-2 invariants.
q-bio.BM:Allosteric communication in proteins is a central and yet unsolved problem of structural biochemistry. Previous findings, from computational biology (Ota and Agard, 2005), have proposed that heat diffuses in a protein through cognate protein allosteric pathways. This work studied heat diffusion in the well-known PDZ-2 protein, and confirmed that this protein has two cognate allosteric pathways and that heat flows preferentially through these. Also, a new property was also observed for protein structures - heat diffuses asymmetrically through the structures. The underling structure of this asymmetrical heat flow was a normal length hydrogen bond (~2.85 {\AA}) that acted as a thermal rectifier. In contrast, thermal rectification was compromised in short hydrogen bonds (~2.60 {\AA}), giving rise to symmetrical thermal diffusion. Asymmetrical heat diffusion was due, on a higher scale, to the local, structural organization of residues that, in turn, was also mediated by hydrogen bonds. This asymmetrical/symmetrical energy flow may be relevant for allosteric signal communication directionality in proteins and for the control of heat flow in materials science.
q-bio.BM:The paper presents a geometrical model for protein secondary structure analysis which uses only the positions of the $C_{\alpha}$-atoms. We construct a space curve connecting these positions by piecewise polynomial interpolation and describe the folding of the protein backbone by a succession of screw motions linking the Frenet frames at consecutive $C_{\alpha}$-positions. Using the ASTRAL subset of the SCOPe data base of protein structures, we derive thresholds for the screw parameters of secondary structure elements and demonstrate that the latter can be reliably assigned on the basis of a $C_{\alpha}$-model. For this purpose we perform a comparative study with the widely used DSSP (Define Secondary Structure of Proteins) algorithm.
q-bio.BM:Superoxide reductases (SORs) are superoxide (O2-)-detoxifying enzymes that catalyse the reduction of O2- into hydrogen peroxide. Three different classes of SOR have been reported on the basis of the presence or not of an additional N-terminal domain. They all share a similar active site, with an unusual non-heme Fe atom coordinated by four equatorial histidines and one axial cysteine residues. Crucial catalytic reaction intermediates of SOR are purported to be Fe(3+)-(hydro)peroxo species. Using resonance Raman spectroscopy, we compared the vibrational properties of the Fe3+ active site of two different classes of SOR, from Desulfoarculus baarsii and Treponema pallidum, along with their ferrocyanide and their peroxo complexes. In both species, rapid treatment with H2O2 results in the stabilization of a side-on high spin Fe(3+)-(eta(2)-OO) peroxo species. Comparison of these two peroxo species reveals significant differences in vibrational frequencies and bond strengths of the Fe-O2 (weaker) and O-O (stronger) bonds for the T. pallidum enzyme. Thus, the two peroxo adducts in these two SORs have different stabilities which are also seen to be correlated with differences in the Fe-S coordination strengths as gauged by the Fe-S vibrational frequencies. This was interpreted from structural variations in the two active sites, resulting in differences in the electron donating properties of the trans cysteine ligand. Our results suggest that the structural differences observed in the active site of different classes of SORs should be a determining factor for the rate of release of the iron-peroxo intermediate during enzymatic turnover.
q-bio.BM:Superoxide reductase SOR is a non-heme iron metalloenzyme that detoxifies superoxide radical in microorganisms. Its active site consists of an unusual non-heme Fe2+ center in a [His4 Cys1] square pyramidal pentacoordination, with the axial cysteine ligand proposed to be an essential feature in catalysis. Two NH peptide groups from isoleucine 118 and histidine 119 establish H-bondings with the sulfur ligand (Desulfoarculus baarsii SOR numbering). In order to investigate the catalytic role of these H-bonds, the isoleucine 118 residue of the SOR from Desulfoarculus baarsii was mutated into alanine, aspartate or serine residues. Resonance Raman spectroscopy showed that the mutations specifically induced an increase of the strength of the Fe3+-S(Cys) and S-C$\beta$(Cys) bonds as well as a change in conformation of the cysteinyl side chain, which was associated with the alteration of the NH hydrogen bonding to the sulfur ligand. The effects of the isoleucine mutations on the reactivity of SOR with $O2\bullet$- were investigated by pulse radiolysis. These studies showed that the mutations induced a specific increase of the pKa of the first reaction intermediate, recently proposed to be an $Fe2+-O2\bullet-$ species. These data were supported by DFT calculations carried out on three models of the $Fe2+-O2\bullet-$ intermediate, with one, two or no H-bonds on the sulfur ligand. Our results demonstrated that the hydrogen bonds between the NH (peptide) and the cysteine ligand tightly control the rate of protonation of the $Fe2+-O2\bullet-$ reaction intermediate to form an Fe3+-OOH species.
q-bio.BM:The NAD(P)H:flavin oxidoreductase from Escherichia coli, named Fre, is a monomer of 26.2 kDa that catalyzes the reduction of free flavins using NADPH or NADH as electron donor. The enzyme does not contain any prosthetic group but accommodates both the reduced pyridine nucleotide and the flavin in a ternary complex prior to oxidoreduction. The specificity of the flavin reductase for the pyridine nucleotide was studied by steady-state kinetics using a variety of NADP analogs. Both the nicotinamide ring and the adenosine part of the substrate molecule have been found to be important for binding to the polypeptide chain. However, in the case of NADPH, the 2'-phosphate group destabilized almost completely the interaction with the adenosine moiety. Moreover, NADPH and NMNH are very good substrates for the flavin reductase, and we have shown that both these molecules bind to the enzyme almost exclusively by the nicotinamide ring. This provides evidence that the flavin reductase exhibits a unique mode for recognition of the reduced pyridine nucleotide. In addition, we have shown that the flavin reductase selectively transfers the pro-R hydrogen from the C-4 position of the nicotinamide ring and is therefore classified as an A-side-specific enzyme.
q-bio.BM:The two-component flavin-dependent monooxygenases belong to an emerging class of enzymes involved in oxidation reactions in a number of metabolic and biosynthetic pathways in microorganisms. One component is a NAD(P)H:flavin oxidoreductase, which provides a reduced flavin to the second component, the proper monooxygenase. There, the reduced flavin activates molecular oxygen for substrate oxidation. Here, we study the flavin reductase ActVB and ActVA-ORF5 gene product, both reported to be involved in the last step of biosynthesis of the natural antibiotic actinorhodin in Streptomyces coelicolor. For the first time we show that ActVA-ORF5 is a FMN-dependent monooxygenase that together with the help of the flavin reductase ActVB catalyzes the oxidation reaction. The mechanism of the transfer of reduced FMN between ActVB and ActVA-ORF5 has been investigated. Dissociation constant values for oxidized and reduced flavin (FMNox and FMNred) with regard to ActVB and ActVA-ORF5 have been determined. The data clearly demonstrate a thermodynamic transfer of FMNred from ActVB to ActVA-ORF5 without involving a particular interaction between the two protein components. In full agreement with these data, we propose a reaction mechanism in which FMNox binds to ActVB, where it is reduced, and the resulting FMNred moves to ActVA-ORF5, where it reacts with O2 to generate a flavinperoxide intermediate. A direct spectroscopic evidence for the formation of such species within ActVA-ORF5 is reported.
q-bio.BM:Some sulfate-reducing and microaerophilic bacteria rely on the enzyme superoxide reductase (SOR) to eliminate the toxic superoxide anion radical (O2*-). SOR catalyses the one-electron reduction of O2*- to hydrogen peroxide at a nonheme ferrous iron center. The structures of Desulfoarculus baarsii SOR (mutant E47A) alone and in complex with ferrocyanide were solved to 1.15 and 1.7 A resolution, respectively. The latter structure, the first ever reported of a complex between ferrocyanide and a protein, reveals that this organo-metallic compound entirely plugs the SOR active site, coordinating the active iron through a bent cyano bridge. The subtle structural differences between the mixed-valence and the fully reduced SOR-ferrocyanide adducts were investigated by taking advantage of the photoelectrons induced by X-rays. The results reveal that photo-reduction from Fe(III) to Fe(II) of the iron center, a very rapid process under a powerful synchrotron beam, induces an expansion of the SOR active site.
q-bio.BM:Binding of a ligand on a protein changes the flexibility of certain parts of the protein, which directly affects its function. These changes are not the same at each point, some parts become more flexible and some others become stiffer. Here, an equation is derived that gives the stiffness map for proteins. The model is based on correlations of fluctuations of pairs of points that need to be evaluated by molecular dynamics simulations. The model is also cast in terms of the Gaussian Network Model and changes of stiffness upon dimerization of AKT1 are evaluated as an example.
q-bio.BM:Although, several factors have been attributed to thermostability, the stabilization strategies used by proteins are still enigmatic. Studies on recombinant xylanase which has the ubiquitous (\b{eta}/{\alpha})8 TIM (Triosephosphate isomerase) barrel fold showed that, just a single extreme N-terminus mutation (V1L) markedly enhanced the thermostability by 5 {\deg}C without loss of catalytic activity whereas another mutation, V1A at the same position decreased the stability by 2 {\deg}C. Based on computational analysis of their crystal structures including residue interaction network, we established a link between N- to C-terminal contacts and protein stability. We demonstrate that augmenting of N- to C-terminal non-covalent interactions is associated with the enhancement of protein stability. We propose that the strategy of mutations at the termini could be exploited with a view to modulate stability without compromising on enzymatic activity, or in general, protein function, in diverse folds where N- and C-termini are in close proximity. Finally, we discuss the implications of our results for the development of therapeutics involving proteins and for designing effective protein engineering strategies.
q-bio.BM:Background. The calculation of diffusion-controlled ligand binding rates is important for understanding enzyme mechanisms as well as designing enzyme inhibitors. We demonstrate the accuracy and effectiveness of a Lagrangian particle-based method, smoothed particle hydrodynamics (SPH), to study diffusion in biomolecular systems by numerically solving the time-dependent Smoluchowski equation for continuum diffusion.   Results. The numerical method is first verified in simple systems and then applied to the calculation of ligand binding to an acetylcholinesterase monomer. Unlike previous studies, a reactive Robin boundary condition (BC), rather than the absolute absorbing (Dirichlet) boundary condition, is considered on the reactive boundaries. This new boundary condition treatment allows for the analysis of enzymes with "imperfect" reaction rates. Rates for inhibitor binding to mAChE are calculated at various ionic strengths and compared with experiment and other numerical methods. We find that imposition of the Robin BC improves agreement between calculated and experimental reaction rates.   Conclusions. Although this initial application focuses on a single monomer system, our new method provides a framework to explore broader applications of SPH in larger-scale biomolecular complexes by taking advantage of its Lagrangian particle-based nature.
q-bio.BM:In this paper, we introduce the software suite, Hermes, which provides fast, novel algorithms for RNA secondary structure kinetics. Using the fast Fourier transform to efficiently compute the Boltzmann probability that a secondary structure S of a given RNA sequence has base pair distance x [resp. y] from reference structure A [resp. B], Hermes computes the exact kinetics of folding from A to B in this coarse-grained model. In particular, Hermes computes the mean first passage time from the transition probability matrix by using matrix inversion, and also computes the equilibrium time from the rate matrix by using spectral decomposition. Due to the model granularity and the speed of Hermes, it is capable of determining secondary structure refolding kinetics for large RNA sequences, beyond the range of other methods. Comparative benchmarking of Hermes with other methods indicates that Hermes provides refolding kinetics of accuracy suitable for use in computational design of RNA, an important area of synthetic biology. Source code and documentation for Hermes are available at http://bioinformatics. bc.edu/clotelab/Hermes/.
q-bio.BM:FoF1-ATP synthases are membrane-embedded protein machines that catalyze the synthesis of adenosine triphosphate. Using photoactivation-based localization microscopy (PALM) in TIR-illumination as well as structured illumination microscopy (SIM), we explore the spatial distribution and track single FoF1-ATP synthases in living E. coli cells under physiological conditions at different temperatures. For quantitative diffusion analysis by mean-squared-displacement measurements, the limited size of the observation area in the membrane with its significant membrane curvature has to be considered. Therefore, we applied a 'sliding observation window' approach (M. Renz et al., Proc. SPIE 8225, 2012) and obtained the one-dimensional diffusion coefficient of FoF1-ATP synthase diffusing on the long axis in living E. coli cells.
q-bio.BM:We study four citrate synthase homodimeric proteins within a structure-based coarse-grained model. Two of these proteins come from thermophilic bacteria, one from a cryophilic bacterium and one from a mesophilic organism; three are in the closed and two in the open conformations. Even though the proteins belong to the same fold, the model distinguishes the properties of these proteins in a way which is consistent with experiments. For instance, the thermophilic proteins are more stable thermodynamically than their mesophilic and cryophilic homologues, which we observe both in the magnitude of thermal fluctuations near the native state and in the kinetics of thermal unfolding. The level of stability correlates with the average coordination number for amino acids contacts and with the degree of structural compactness. The pattern of positional fluctuations along the sequence in the closed conformation is different than in the open conformation, including within the active site. The modes of correlated and anticorrelated movements of pairs of amino acids forming the active site are very different in the open and closed conformations. Taken together, our results show that the precise location of amino acid contacts in the native structure appears to be a critical element in explaining the similarities and differences in the thermodynamic properties, local flexibility and collective motions of the different forms of the enzyme.
q-bio.BM:Using coarse-grained molecular dynamics simulations, we analyze mechanically induced dissociation and unfolding of the protein complex CD48-2B4. This heterodimer is an indispensable component of the immunological system: 2B4 is a receptor on natural killer cells whereas CD48 is expressed on surfaces of various immune cells. So far, its mechanostability has not been assessed either experimentally or theoretically. We find that the dissociation processes strongly depend on the direction of pulling and may take place in several pathways. Interestingly, the CD48-2B4 interface can be divided into three distinct patches that act as units when resisting the pulling forces. At experimentally accessible pulling speeds, the characteristic mechanostability forces are in the range between 100 and 200 pN, depending on the pulling direction. These characteristic forces need not be associated with tensile forces involved in the act of separation of the complex because prior shear-involving unraveling within individual proteins may give rise to a higher force peak.
q-bio.BM:Protein-peptide interactions play a key role in cell functions. Their structural characterization, though challenging, is important for the discovery of new drugs. The CABS-dock web server provides an interface for modeling protein-peptide interactions using a highly efficient protocol for the flexible docking of peptides to proteins. While other docking algorithms require pre-defined localization of the binding site, CABS-dock doesn't require such knowledge. Given a protein receptor structure and a peptide sequence (and starting from random conformations and positions of the peptide), CABS-dock performs simulation search for the binding site allowing for full flexibility of the peptide and small fluctuations of the receptor backbone. This protocol was extensively tested over the largest dataset of non-redundant protein-peptide interactions available to date (including bound and unbound docking cases). For over 80% of bound and unbound data set cases, we obtained models with high or medium accuracy (sufficient for practical applications). Additionally, as optional features, CABS-dock can exclude user-selected binding modes from docking search or to increase the level of flexibility for chosen receptor fragments. CABS-dock is freely available as a web server at http://biocomp.chem.uw.edu.pl/CABSdock
q-bio.BM:Structuring of DNA counterions around the double helix has been studied by the molecular dynamics method. A DNA dodecamer d(CGCGAATTCGCG) in water solution with the alkali metal counterions Na$^{+}$, K$^{+}$, and Cs$^{+}$ has been simulated. The systems have been considered in the regimes without excess salt and with different salts (0.5 M of NaCl, KCl or CsCl) added. The results have showed that the Na$^{+}$ counterions interact with the phosphate groups directly from outside of the double helix and via water molecules at the top edge of DNA minor groove. The potassium ions are mostly localized in the grooves of the double helix, and the cesium ions penetrate deeply inside the minor groove being bonded directly to the atoms of nucleic bases. Due to the electrostatic repulsion the chlorine ions tend to be localized at large distances from the DNA polyanion, but some Cl$^{-}$ anions have been detected near atomic groups of the double helix forming electrically neutral pairs with counterions already condensed on DNA. The DNA sites, where counterions are incorporated, are characterized by local changes of double helix structure. The lifetime of Na$^{+}$ and K$^{+}$ in complex with DNA atomic groups is less than 0.5 ns, while in the case of the cesium ions it may reach several nanoseconds. In this time scale, the Cs$^{+}$ counterions form a structured system of charges in the DNA minor groove that can be considered as ionic lattice.
q-bio.BM:We analyze a model statistical description of the polypeptide chain helix-coil transition, where we take into account the specificity of its primary sequence, as quantified by the phase space volume ratio of the number of all accessible states to the number corresponding to a helical conformation. The resulting transition phase diagram is then juxtaposed with the unusual behavior of the secondary structures in Intrinsically Disordered Proteins (IDPs) and a number of similarities are observed, even if the protein folding is a more complex transition than the helix-coil transition. In fact, the deficit in bulky and hydrophobic amino acids observed in IDPs, translated into larger values of phase space volume, allows us to locate the region in parameter space of the helix-coil transition that would correspond to the secondary structure transformations that are intrinsic to conformational transitions in IDPs and that is characterized by a modified phase diagram when compared to globular proteins. Here, we argue how the nature of this modified phase diagram, obtained from a model of the helix-coil transition in a solvent, would illuminate the turned-out response of IDPs to the changes in the environment conditions that follow straightforwardly from the re-entrant (cold denaturation) branch in their folding phase diagram.
q-bio.BM:Persistent homology has been devised as a promising tool for the topological simplification of complex data. However, it is computationally intractable for large data sets. In this work, we introduce multiresolution persistent homology for tackling large data sets. Our basic idea is to match the resolution with the scale of interest so as to create a topological microscopy for the underlying data. We utilize flexibility-rigidity index (FRI) to access the topological connectivity of the data set and define a rigidity density for the filtration analysis. By appropriately tuning the resolution, we are able to focus the topological lens on a desirable scale. The proposed multiresolution topological analysis is validated by a hexagonal fractal image which has three distinct scales. We further demonstrate the proposed method for extracting topological fingerprints from DNA and RNA molecules. In particular, the topological persistence of a virus capsid with 240 protein monomers is successfully analyzed which would otherwise be inaccessible to the normal point cloud method and unreliable by using coarse-grained multiscale persistent homology. The proposed method has also been successfully applied to the protein domain classification, which is the first time that persistent homology is used for practical protein domain analysis, to our knowledge. The proposed multiresolution topological method has potential applications in arbitrary data sets, such as social networks, biological networks and graphs.
q-bio.BM:Bacterial vaginosis (BV) increases transmission of HIV, enhances the risk of preterm labour, and its associated malodour impacts the quality of life for many women. Clinical diagnosis primarily relies on microscopy to presumptively detect a loss of lactobacilli and acquisition of anaerobes. This diagnostic does not reflect the microbiota composition accurately as lactobacilli can assume different morphotypes, and assigning BV associated morphotypes to specific organisms is challenging. Using an untargeted metabolomics approach we identify novel biomarkers for BV in a cohort of 131 Rwandan women, and demonstrate that metabolic products in the vagina are strongly associated with bacterial diversity. Metabolites associated with high diversity and clinical BV include 2-hydroxyisovalerate and gamma-hydroxybutyrate (GHB), but not the anaerobic end-product succinate. Low diversity, and high relative abundance of lactobacilli, is characterized by lactate and amino acids. Biomarkers associated with diversity and BV are independent of pregnancy status, and were validated in a blinded replication cohort from Tanzania (n=45), in which we predicted clinical BV with 91% accuracy. Correlations between the metabolome and microbiota identified Gardnerella vaginalis as a putative producer of GHB, and we demonstrate production by this species in vitro. This work provides a deeper understanding of the relationship between the vaginal microbiota and biomarkers of vaginal health and dysbiosis.
q-bio.BM:We discuss a model of protein conformations where the conformations are combinations of short fragments from some small set. For these fragments we consider a distribution of frequencies of occurrence of pairs (sequence of amino acids, conformation), averaged over some balls in the spaces of sequences and conformations. These frequencies can be estimated due to smallness of epsilon-entropy of the set of conformations of protein fragments.   We consider statistical potentials for protein fragments which describe the mentioned frequencies of occurrence and discuss model of free energy of a protein where the free energy is equal to a sum of statistical potentials of the fragments.   We also consider contribution of contacts of fragments to the energy of protein conformation, and contribution from statistical potentials of some hierarchical set of larger protein fragments. This set of fragments is constructed using the distribution of frequencies of occurrence of short fragments.   We discuss applications of this model to problem of prediction of the native conformation of a protein from its primary structure and to description of dynamics of a protein. Modification of structural alignment taking into account statistical potentials for protein fragments is considered and application to threading procedure for proteins is discussed.
q-bio.BM:Protein side-chain packing is a critical component in obtaining the 3D coordinates of a structure and drug discovery. Single-domain protein side-chain packing has been thoroughly studied. A major challenge in generalizing these methods to protein complexes is that they, unlike monomers, often have very large treewidth, and thus algorithms such as TreePack cannot be directly applied. To address this issue, SCWRL4 treats the complex effectively as a monomer, heuristically excluding weak interactions to decrease treewidth; as a result, SCWRL4 generates poor packings on protein interfaces. To date, few side-chain packing methods exist that are specifically designed for protein complexes. In this paper, we introduce a method, iTreePack, which solves the side-chain packing problem for complexes by using a novel combination of dual decomposition and tree decomposition. In particular, iTreePack overcomes the problem of large treewidth by decomposing a protein complex into smaller subgraphs and novelly reformulating the complex side-chain packing problem as a dual relaxation problem; this allows us to solve the side-chain packing of each small subgraph separately using tree-decomposition. A projected subgradient algorithm is applied to enforcing the consistency among the side-chain packings of all the small subgraphs. Computational results demonstrate that our iTreePack program outperforms SCWRL4 on protein complexes. In particular, iTreePack places side-chain atoms much more accurately on very large complexes, which constitute a significant portion of protein-protein interactions. Moreover, the advantage of iTreePack over SCWRL4 increases with respect to the treewidth of a complex. Even for monomeric proteins, iTreePack is much more efficient than SCWRL and slightly more accurate.
q-bio.BM:Protein data bank entries obtain distinct, reproducible flexibility characteristics determined by normal mode analyses of their three dimensional coordinate files. We study the effectiveness and sensitivity of this technique by analyzing the results on one class of glycosidases: family 10 xylanases. A conserved tryptophan that appears to affect access to the active site can be in one of two conformations according to X-ray crystallographic electron density data. The two alternate orientations of this active site tryptophan lead to distinct flexibility spectra, with one orientation thwarting the oscillations seen in the other. The particular orientation of this sidechain furthermore affects the appearance of the motility of a distant, C terminal region we term the mallet. The mallet region is known to separate members of this family of enzymes into two classes.
q-bio.BM:Protein-peptide interactions play essential functional roles in living organisms and their structural characterization is a hot subject of current experimental and theoretical research. Computational modeling of the structure of protein-peptide interactions is usually divided into two stages: prediction of the binding site at a protein receptor surface, and then docking (and modeling) the peptide structure into the known binding site. This paper presents a comprehensive CABS-dock method for the simultaneous search of binding sites and flexible protein-peptide docking, available as a users friendly web server. We present example CABS-dock results obtained in the default CABS-dock mode and using its advanced options that enable the user to increase the range of flexibility for chosen receptor fragments or to exclude user-selected binding modes from docking search. Furthermore, we demonstrate a strategy to improve CABS-dock performance by assessing the quality of models with classical molecular dynamics. Finally, we discuss the promising extensions and applications of the CABS-dock method and provide a tutorial appendix for the convenient analysis and visualization of CABS-dock results. The CABS-dock web server is freely available at http://biocomp.chem.uw.edu.pl/CABSdock/
q-bio.BM:We have developed an analytical, ligand-specific and scalable algorithm that detects a "signature" of the 3D binding site of a given ligand in a protein 3D structure. The said signature is a 3D motif in the form of an irregular tetrahedron whose vertices represent the backbone or side-chain centroids of the amino acid residues at the binding site that physically interact with the bound ligand atoms. The motif is determined from a set of solved training structures, all of which bind the ligand. Just as alignment of linear amino acid sequences enables one to determine consensus sequences in proteins, the present method allows the determination of three-dimensional consensus structures or "motifs" in folded proteins. Although such is accomplished by the present method not by alignment of 3D protein structures or parts thereof (e.g., alignment of ligand atoms from different structures) but by nearest-neighbor analysis of ligand atoms in protein-bound forms, the same effect, and thus the same goal, is achieved. We have applied our method to the prediction of GTP- and ATP-binding protein families, namely, the small Ras-type G-protein and ser/thr protein kinase families. Validation tests reveal that the specificity of our method is nearly 100% for both protein families, and a sensitivity of greater than 60% for the ser/thr protein kinase family and approx. 93% for the small, Ras-type G-protein family. Further tests reveal that our algorithm can distinguish effectively between GTP and GTP-like ligands, and between ATP- and ATP-like ligands. The method was applied to a set of predicted (by 123D threading) protein structures from the slime mold (D. dictyostelium) proteome, with promising results.
q-bio.BM:We describe two complementary methods to quantify the degree of burial of ligand and/or ligand binding site (LBS) in a protein-ligand complex, namely, the "cutting plane" (CP) and the "tangent sphere" (TS) methods. To construct the CP and TS, two centroids are required: the protein molecular centroid (global centroid, GC), and the LBS centroid (local centroid, LC). The CP is defined as the plane passing through the LBS centroid (LC) and normal to the line passing through the LC and the protein molecular centroid (GC). The "anterior side" of the CP is the side not containing the GC (which the "posterior" side does). The TS is defined as the sphere with center at GC and tangent to the CP at LC. The percentage of protein atoms (a.) inside the TS, and (b.) on the anterior side of the CP, are two complementary measures of ligand or LBS burial depth since the latter is directly proportional to (b.) and inversely proportional to (a.). We tested the CP and TS methods using a test set of 67 well characterized protein-ligand structures (Laskowski et al., 1996), as well as the theoretical case of an artificial protein in the form of a cubic lattice grid of points in the overall shape of a sphere and in which LBS of any depth can be specified. Results from both the CP and TS methods agree very well with data reported by Laskowski et al., and results from the theoretical case further confirm that that both methods are suitable measures of ligand or LBS burial. Prior to this study, there were no such numerical measures of LBS burial available, and hence no way to directly and objectively compare LBS depths in different proteins. LBS burial depth is an important parameter as it is usually directly related to the amount of conformational change a protein undergoes upon ligand binding, and ability to quantify it could allow meaningful comparison of protein dynamics and flexibility.
q-bio.BM:Due to increased activity in high-throughput structural genomics efforts around the globe, there has been an accumulation of experimental protein 3D structures lacking functional annotation, thus creating a need for structure-based protein function assignment methods. Computational prediction of ligand binding sites (LBS) is a well-established protein function assignment method. Here we apply the specific LBS detection algorithm we recently described (Reyes, V.M. & Sheth, V.N., 2011; Reyes, V.M., 2015a) to some 801 functionally unannotated experimental structures in the Protein Data Bank by screening for the binding sites (BS) of 6 biologically important ligands: GTP in small Ras-type G-proteins, ATP in ser/thr protein kinases, sialic acid (SIA), retinoic acid (REA), and heme-bound and unbound (free) nitric oxide (hNO, fNO). Validation of the algorithm for the GTP- and ATP-binding sites has been previously described in detail (ibid.); here, validation for the BSs of the 4 other ligands shows both good specificity and sensitivity. Of the 801 structures screened, 8 tested positive for GTP binding, 61 for ATP binding, 35 for SIA binding, 132 for REA binding, 33 for hNO binding, and 10 for fNO binding. Using the cutting plane and tangent sphere methods we described previously, (Reyes, V.M., 2015b), we also determined the depth of burial of the LBSs detected above and compared the values with those from the respective training structures, and the degree of similarity between the two values taken as a further validation of the predicted LBSs. Applying this criterion, we were able to narrow down the predicted GTP-binding proteins to 2, the ATP-binding proteins to 13, the SIA-binding proteins to 2, the REA-binding proteins to 14, the hNO-binding proteins to 4, and the fNO-binding proteins to 1. We believe this further criterion increases the confidence level of our LBS predictions.
q-bio.BM:We report a 3D structure-based method of predicting protein-protein interaction partners. It involves screening for pairs of tetrahedra representing interacting amino acids at the interface of the protein-protein complex, with one tetrahedron on each protomer. H-bonds and VDW interactions at their interface are first determined and then interacting tetrahedral motifs (one from each protomer) representing backbone or side chain centroids of the interacting amino acids, are then built. The method requires that the protein protomers be transformed first into double-centroid reduced representation (Reyes, V.M. & Sheth, V.N., 2011; Reyes, V.M., 2015a). The method is applied to a set of 801 protein structures in the PDB with unknown functions, which were screened for pairs of tetrahedral motifs characteristic of nine binary complexes, namely: (1.) RAP-Gmppnp-cRAF1 Ras-binding domain; (2.) RHOA-protein kinase PKN/PRK1 effector domain; (3.) RAC-HOGD1; (4.) RAC-P67PHOX; (5.) kinase-associated phosphatase (KAP)-phosphoCDK2; (6.) Ig Fc-protein A fragment B; (7.) Ig light chain dimers; (8.) beta catenin-HTCF-4; and (9.) IL-2 homodimers. Our search method found 33, 297, 62, 63, 120, 0, 108, 16 and 504 putative complexes, respectively. After considering the degree of interface overlap between the protomers, these numbers were significantly trimmed down to 4, 2, 1, 8, 3, 0, 1, 1 and 1, respectively. Negative and positive control experiments indicate that the screening process has acceptable specificity and sensitivity. The results were further validated by applying the CP and TS methods (Reyes, V.M., 2015b) for the quantitative determination of interface burial and inter-protomer overlap in the complex. Our method is simple, fast and scalable, and once the partner interface 3D SMs are identified, they can be used to computationally dock the two protomers together to form the complex.
q-bio.BM:The identification of low-energy conformers for a given molecule is a fundamental problem in computational chemistry and cheminformatics. We assess here a conformer search that employs a genetic algorithm for sampling the low-energy segment of the conformation space of molecules. The algorithm is designed to work with first-principles methods, facilitated by the incorporation of local optimization and blacklisting conformers to prevent repeated evaluations of very similar solutions. The aim of the search is not only to find the global minimum, but to predict all conformers within an energy window above the global minimum. The performance of the search strategy is: (i) evaluated for a reference data set extracted from a database with amino acid dipeptide conformers obtained by an extensive combined force field and first-principles search and (ii) compared to the performance of a systematic search and a random conformer generator for the example of a drug-like ligand with 43 atoms, 8 rotatable bonds and 1 cis/trans bond.
q-bio.BM:Several algorithms for RNA inverse folding have been used to design synthetic riboswitches, ribozymes and thermoswitches, whose activity has been experimentally validated. The RNAiFold software is unique among approaches for inverse folding in that (exhaustive) constraint programming is used instead of heuristic methods. For that reason, RNAiFold can generate all sequences that fold into the target structure, or determine that there is no solution. RNAiFold 2.0 is a complete overhaul of RNAiFold 1.0, rewritten from the now defunct COMET language to C++. The new code properly extends the capabilities of its predecessor by providing a user-friendly pipeline to design synthetic constructs having the functionality of given Rfam families. In addition, the new software supports amino acid constraints, even for proteins translated in different reading frames from overlapping coding sequences; moreover, structure compatibility/incompatibility constraints have been expanded. With these features, RNAiFold 2.0 allows the user to design single RNA molecules as well as hybridization complexes of two RNA molecules.   The web server, source code and linux binaries are publicly accessible at http://bioinformatics.bc.edu/clotelab/RNAiFold2.0
q-bio.BM:Existing elastic network models are typically parametrized at a given cutoff distance and often fail to properly predict the thermal fluctuation of many macromolecules that involve multiple characteristic length scales. We introduce a multiscale flexibility-rigidity index (mFRI) method to resolve this problem. The proposed mFRI utilizes two or three correlation kernels parametrized at different length scales to capture protein interactions at corresponding scales. It is about 20% more accurate than the Gaussian network model (GNM) in the B-factor prediction of a set of 364 proteins. Additionally, the present method is able to delivery accurate predictions for multiscale macromolecules that fail GNM. Finally, or a protein of $N$ residues, mFRI is of linear scaling (O(N)) in computational complexity, in contrast to the order of O(N^3) for GNM.
q-bio.BM:Peptide Nucleic Acids (PNA) are non-natural oligonucleotides mimics. {\gamma}-PNA backbone are formed by standard nucleic acids nucleobases connected by a neutral N-(2-aminoethyl)glycine backbone linked by a peptide bond. In this study, we use Nuclear Magnetic Resonance (NMR) and Circular Dichroism (CD) to explore the properties of the supramolecular duplexes formed by these species. We show that standard Watson-Crick base pair as well as non-standard ones are formed in solution. The duplexes thus formed present marked melting transition temperatures substantially higher than their nucleic acid homologs. Moreover, the presence of a chiral group on the {\gamma}-peptidic backbone increases further more this transition temperature, leading to very stable duplexes.
q-bio.BM:In this work, we report the heat rectifying capability of a-helices. Using molecular dynamics simulations we show an increased thermal diffusivity in the C-Terminal to N-Terminal direction of propagation. The origin of this effect seems to be a function of the particular orientation of the hydrogen bonds stabilizing these a-helices. Our results may be relevant for the design of thermal rectification devices for materials science and lend support to the role of normal length hydrogen bonds in the asymmetrical energy flow in proteins.
q-bio.BM:Black cumin (Nigella sativa) is a spice having medicinal properties with pungent and bitter odour. It is used since thousands of years to treat various ailments, including cancer mainly in South Asia and Middle Eastern regions. Substantial evidence in multiple research studies emphasizes about the therapeutic importance of bioactive principles of N. sativa in cancer bioassays; however, the exact mechanism of their anti-tumour action is still to be fully comprehended. The current study makes an attempt in this direction by exploiting the advancements in the Insilico reverse screening technology. In this study, three different Insilico Reverse Screening approaches have been employed for identifying the putative molecular targets of the bioactive principles in Black cumin (thymoquinone, alpha-hederin, dithymoquinone and thymohydroquinone) relevant to its anti-tumour functionality. The identified set of putative targets is further compared with the existing set of experimentally validated targets, so as to estimate the performance of insilico platforms. Subsequently, molecular docking simulations studies were performed to elucidate the molecular interactions between the bioactive compounds & their respective identified targets. The molecular interactions of one such target identified i.e. VEGF2 along with thymoquinone depicted one H-bond formed at the catalytic site. The molecular targets identified in this study need further confirmatory tests on cancer bioassays, in order to justify the research findings from Insilico platforms. This study has brought to light the effectiveness of usage of Insilico Reverse Screening protocols to characterise the un-identified target-ome of poly pharmacological bioactive agents in spices.
q-bio.BM:A quantum model on the chemically and physically induced pluripotency in stem cells is proposed. Based on the conformational Hamiltonian and the idea of slow variables (molecular torsions) slaving fast ones the conversion from the differentiate state to pluripotent state is defined as the quantum transition between conformational states. The transitional rate is calculated and an analytical form for the rate formulas is deduced. Then the dependence of the rate on the number of torsion angles of the gene and the magnitude of the rate can be estimated by comparison with protein folding. The reaction equations of the conformational change of the pluripotency genes in chemical reprogramming are given. The characteristic time of the chemical reprogramming is calculated and the result is consistent with experiments. The dependence of the transition rate on physical factors such as temperature, PH value and the volume and shape of the coherent domain is analyzed from the rate equation. It is suggested that by decreasing the coherence degree of some pluripotency genes a more effective approach to the physically induced pluripotency can be made.
q-bio.BM:The study of correlated mutations in alignments of homologous proteins proved to be succesful not only in the prediction of their native conformation, but also in the developement of a two-body effective potential between pairs of amino acids. In the present work we extend the effective potential, introducing a many--body term based on the same theoretical framework, making use of a principle of maximum entropy. The extended potential performs better than the two--body one in predicting the energetic effect of 308 mutations in 14 proteins (including membrane proteins). The average value of the parameters of the many-body term correlates with the degree of hydrophobicity of the corresponding residues, suggesting that this term partly reflects the effect of the solvent.
q-bio.BM:Numerical methods are proposed for an advanced Poisson-Nernst-Planck-Fermi (PNPF) model for studying ion transport through biological ion channels. PNPF contains many more correlations than most models and simulations of channels, because it includes water and calculates dielectric properties consistently as outputs. This model accounts for the steric effect of ions and water molecules with different sizes and interstitial voids, the correlation effect of crowded ions with different valences, and the screening effect of polarized water molecules in an inhomogeneous aqueous electrolyte. The steric energy is shown to be comparable to the electrical energy under physiological conditions, demonstrating the crucial role of the excluded volume of particles and the voids in the natural function of channel proteins. Water is shown to play a critical role in both correlation and steric effects in the model. We extend the classical Scharfetter-Gummel (SG) method for semiconductor devices to include the steric potential for ion channels, which is a fundamental physical property not present in semiconductors. Together with a simplified matched interface and boundary (SMIB) method for treating molecular surfaces and singular charges of channel proteins, the extended SG method is shown to exhibit important features in flow simulations such as optimal convergence, efficient nonlinear iterations, and physical conservation. Two different methods --- called the SMIB and multiscale methods --- are proposed for two different types of channels, namely, the gramicidin A channel and an L-type calcium channel, depending on whether water is allowed to pass through the channel. The PNPF currents are in accord with the experimental I-V (V for applied voltages) data of the gramicidin A channel and I-C (C for bath concentrations) data of the calcium channel.
q-bio.BM:In lymphoma, mutations in genes of histone modifying proteins are frequently observed. Notably, somatic mutations in the activatory histone modification writing protein MLL2 and the repressive modification writer EZH2 are the most frequent. Gain of function mutations are typically detected in EZH2 whilst MLL2 mutations are usually observed as conferring a homozygous loss of function. The gain-of-function mutations in EZH2 provide an obvious target for the development of inhibitors with therapeutic potential. To counter the loss of functional MLL2 protein, we computationally predicted compounds that are able to modulate the reader of the corresponding modifications, BPTF, to recognize other forms of the histone H3 lysine 4, instead of the tri-methylated form normally produced by MLL2. By forming a synthetic triple-complex of a compound, the histone H3 tail and BPTF we potentially circumvent the requirement for functional MLL2 methyl-transferase through the modulation of BPTF activity. Here we show a proof-of-principle that special compounds, named variators, can reprogram selectivity of protein binding and thus create artificial regulatory pathways which can have a potential therapeutic role. A therapeutic role of BPTF variators may extend to other diseases that involve loss of MLL2 function, such as Kabuki syndrome or the aberrant functioning of H3K4 modification as observed in Huntington disease and in memory formation.
q-bio.BM:Previously reported [http://arxiv.org/abs/1506.06433] reprogramming of substrate specificity of H3K4Me3 epigenetic marks reading PHD domain of BPTF protein illustrates therapeutic potential of a new class of non-inhibitor small organic compounds - variators. Here we address the question about reproducibility of rational design of variators by reprogramming of the second epigenetic marks reading domain of BPTF protein - bromodomain. Bromodomain of BPTF binds to epigenetic marks in form of acetylated lysine of histone H4 (H4K12Ac, H4K16Ac and H4K20Ac), which physicochemical properties and binding mode differs considerably from those of methylated H3K4 marks. Thus, detailed description of computational approach for reprogramming of bromodomain substrate specificity illustrates both general and target specific attributes of computer aided variators design.
q-bio.BM:DNA bending is biologically important for genome regulation and is relevant to a range of nanotechnological systems. Recent results suggest that sharp bending is much easier than implied by the widely-used worm-like chain model; many of these studies, however, remain controversial. We use a coarse-grained model, previously fitted to DNA's basic thermodynamic and mechanical properties, to explore strongly bent systems. We find that as the end-to-end distance is decreased sufficiently short duplexes undergo a transition to a state in which the bending strain is localized at a flexible kink that involves disruption of base-pairing and stacking. This kinked state, which is not well-described by the worm-like chain model, allows the duplex to more easily be sharply bent. It is not completely flexible, however, due to constraints arising from the connectivity of both DNA backbones. We also perform a detailed comparison to recent experiments on a "molecular vice" that probes highly bent DNA. Close agreement between simulations and experiments strengthens the hypothesis that localised bending via kinking occurs in the molecular vice and causes enhanced flexibility of duplex DNA. Our calculations therefore suggests that the cost of kinking implied by this experiment is consistent with the known thermodynamic and mechanical properties of DNA.
q-bio.BM:DNA cyclization is a powerful technique to gain insight into the nature of DNA bending. The worm-like chain model provides a good description of small to moderate bending fluctuations, but some experiments on strongly-bent shorter molecules suggest enhanced flexibility over and above that expected from the worm-like chain. Here, we use a coarse-grained model of DNA to investigate the thermodynamics of DNA cyclization for molecules with less than 210 base pairs. As the molecules get shorter we find increasing deviations between our computed equilibrium j-factor and the worm-like chain predictions of Shimada and Yamakawa. These deviations are due to sharp kinking, first at nicks, and only subsequently in the body of the duplex. At the shortest lengths, substantial fraying at the ends of duplex domains is the dominant method of relaxation. We also estimate the dynamic j-factor measured in recent FRET experiments. We find that the dynamic j-factor is systematically larger than its equilibrium counterpart, with the deviation larger for shorter molecules, because not all the stress present in the fully cyclized state is present in the transition state. These observations are important for the interpretation of recent experiments, as only kinking within the body of the duplex is genuinely indicative of non-worm-like chain behaviour.
q-bio.BM:Folding of Ubiquitin (Ub) is investigated at low and neutral pH at different temperatures using simulations of the coarse-grained Self-Organized-Polymer model with side chains. The calculated radius of gyration, showing dramatic variations with pH, is in excellent agreement with scattering experiments. At $T_m$ Ub folds in a two-state manner at low and neutral pH. Clustering analysis of the conformations sampled in equilibrium folding trajectories at $T_m$, with multiple transitions between the folded and unfolded states, show a network of metastable states connecting the native and unfolded states. At low and neutral pH, Ub folds with high probability through a preferred set of conformations resulting in a pH-dependent dominant folding pathway. Folding kinetics reveal that Ub assembly at low pH occurs by multiple pathways involving a combination of nucleation-collapse and diffusion collision mechanism. The mechanism by which Ub folds is dictated by the stability of the key secondary structural elements responsible for establishing long range contacts and collapse of Ub. Nucleation collapse mechanism holds if the stability of these elements are marginal, as would be the case at elevated temperatures. If the lifetimes associated with these structured microdomains are on the order of hundreds of $\mu sec$ then Ub folding follows the diffusion-collision mechanism with intermediates many of which coincide with those found in equilibrium. Folding at neutral pH is a sequential process with a populated intermediate resembling that sampled at equilibrium. The transition state structures, obtained using a $P_{fold}$ analysis, are homogeneous and globular with most of the secondary and tertiary structures being native-like. Many of our findings are not only in agreement with experiments but also provide missing details not resolvable in standard experiments.
q-bio.BM:There are several applications in computational biophysics which require the optimization of discrete interacting states; e.g., amino acid titration states, ligand oxidation states, or discrete rotamer angles. Such optimization can be very time-consuming as it scales exponentially in the number of sites to be optimized. In this paper, we describe a new polynomial-time algorithm for optimization of discrete states in macromolecular systems. This algorithm was adapted from image processing and uses techniques from discrete mathematics and graph theory to restate the optimization problem in terms of "maximum flow-minimum cut" graph analysis. The interaction energy graph, a graph in which vertices (amino acids) and edges (interactions) are weighted with their respective energies, is transformed into a flow network in which the value of the minimum cut in the network equals the minimum free energy of the protein, and the cut itself encodes the state that achieves the minimum free energy. Because of its deterministic nature and polynomial-time performance, this algorithm has the potential to allow for the ionization state of larger proteins to be discovered.
q-bio.BM:Developing an accurate scoring function is essential for successfully predicting protein structures. In this study, we developed a statistical potential function, called OPUS-Beta, for energetically evaluating beta-sheet contact pattern (the entire residue-residue beta-contacts of a protein) independent of the atomic coordinate information. The OPUS-Beta potential contains five terms, i.e., a self-packing term, a pairwise inter-strand packing term, a pairwise intra-strand packing term, a lattice term and a hydrogen-bonding term. The results show that, in recognizing the native beta-contact pattern from decoys, OPUS-Beta potential outperforms the existing methods in literature, especially in combination with a method using 2D-recursive neural networks (about 5% and 23% improvements in top-1 and top-5 selections). We expect OPUS-Beta potential to be useful in beta-sheet modeling for proteins.
q-bio.BM:Methods for patterning biomolecules on a substrate at the single molecule level have been studied as a route to sensors with single-molecular sensitivity or as a way to probe biological phenomena at the single-molecule level. However, the arrangement and orientation of single biomolecules on substrates has been less investigated. Here, we examined the arrangement and orientation of two rod-like coiled-coil proteins, cortexillin and tropomyosin, around patterned gold nanostructures. The high aspect ratio of the coiled coils made it possible to study their orientations and to pursue a strategy of protein orientation via two-point attachment. The proteins were anchored to the surfaces using thiol groups, and the number of cysteine residues in tropomyosin was varied to test how this variation affected the structure and arrangement of the surface-attached proteins. Molecular dynamics studies were used to interpret the observed positional distributions. Based on initial studies of protein attachment to gold post structures, two 31-nm-long tropomyosin molecules were aligned between the two sidewalls of a trench with a width of 68 nm. Because the approach presented in this study uses one of twenty natural amino acids, this method provides a convenient way to pattern biomolecules on substrates using standard chemistry.
q-bio.BM:The eukaryotic Cys2His2 zinc finger proteins bind to DNA ubiquitously at highly conserved domains, responsible for gene regulation and the spatial organization of DNA. To study and understand the zinc finger DNA-protein interaction, we use the extended ladder in the DNA model proposed by Zhu, Rasmussen, Balatsky \& Bishop (2007) \cite{Zhu-2007}. Considering one single spinless electron in each nucleotide $\pi$-orbital along a double DNA chain (dDNA), we find a typical pattern for the {\color{black} bottom of the occupied molecular orbital (BOMO)}, highest occupied molecular orbital (HOMO) and lowest unoccupied orbital (LUMO) along the binding sites. We specifically looked at two members of zinc finger protein family: specificity protein 1 (SP1) and early grown response 1 transcription factors (EGR1). When the valence band is filled, we find electrons in the purines along the nucleotide sequence, compatible with the electric charges of the binding amino acids in SP1 and EGR1 zinc finger.
q-bio.BM:Given an RNA sequence a, consider the network G = (V;E), where the set V of nodes consists of all secondary structures of a, and whose edge set E consists of all edges connecting two secondary structures whose base pair distance is 1. Define the network connectivity, or expected network degree, as the average number of edges incident to vertices of G. Using algebraic combinatorial methods, we prove that the asymptotic connectivity of length n homopolymer sequences is 0:473418 ? n. This raises the question of what other network properties are characteristic of the network of RNA secondary structures. Programs in Python, C and Mathematica are available at the web site http://bioinformatics.bc.edu/clotelab/ RNAexpNumNbors.
q-bio.BM:Conformational entropy for atomic-level, three dimensional biomolecules is known experimentally to play an important role in protein-ligand discrimination, yet reliable computation of entropy remains a difficult problem. Here we describe the first two accurate and efficient algorithms to compute the conformational entropy for RNA secondary structures, with respect to the Turner energy model, where free energy parameters are determined from UV aborption experiments. An algorithm to compute the derivational entropy for RNA secondary structures had previously been introduced, using stochastic context free grammars (SCFGs). However, the numerical value of derivational entropy depends heavily on the chosen context free grammar and on the training set used to estimate rule probabilities. Using data from the Rfam database, we determine that both of our thermodynamic methods, which agree in numerical value, are substantially faster than the SCFG method. Thermodynamic structural entropy is much smaller than derivational entropy, and the correlation between length-normalized thermodynamic entropy and derivational entropy is moderately weak to poor. In applications, we plot the structural entropy as a function of temperature for known thermoswitches, determine that the correlation between hammerhead ribozyme cleavage activity and total free energy is improved by including an additional free energy term arising from conformational entropy, and plot the structural entropy of windows of the HIV-1 genome. Our software RNAentropy can compute structural entropy for any user-specified temperature, and supports both the Turner'99 and Turner'04 energy parameters. It follows that RNAentropy is state-of-the-art software to compute RNA secondary structure conformational entropy. The software is available at http://bioinformatics.bc.edu/clotelab/RNAentropy.
q-bio.BM:Caffeine (CAF) is one of the most widely and regularly consumed biologically active substances. We use computer simulation approach to the study of CAF activity by searching for its possible complexes with biopolymer fragments. The principal CAF target at physiologically important concentrations refers to adenosine receptors. It is a common opinion that CAF is a competitive antagonist of adenosine. At the first step to molecular level elucidation of CAF action, we have found a set of the minima of the interaction energy between CAF and the fragments of human A1 adenosine receptor. Molecular mechanics is the main method for the calculations of the energy of interactions between CAF and the biopolymer fragments. We use the Monte Carlo simulation to follow various mutual arrangements of CAF molecule near the receptor. It appears that the deepest energy minima refer to hydrogen-bond formation of CAF with amino acid residues involved in interactions with adenosine, its agonists and antagonists. The results suggest that the formation of such CAF-receptor complexes enforced by a close packing of CAF and the receptor fragments is the reason of CAF actions on nervous system. CAF can block the atomic groups of the adenosine repressors responsible for the interactions with adenosine, not necessary by the formation of H bonds with them, but simply hide these groups from the interactions with adenosine.
q-bio.BM:We used statistical thermodynamics of conformational fluctuations and the elements of algebraic graph theory together with data from 2000 protein crystal structures, and showed that folded native proteins with harmonic interactions exhibit distribution functions each of which appear to be universal across all proteins. The three universal distributions are: (i) the eigenvalue spectrum of the protein graph Laplacian, (ii) the B-factor distribution of residues, and (iii) the vibrational entropy difference per residue between the unfolded and the folded states. The three distributions, which look independent of each other at first sight, are strongly associated with the Rouse chain model of a polymer as the unfolded protein. We treat the folded protein as the strongly perturbed state of the Rouse chain. We explain the underlying factors controlling the three distributions and discuss the differences from those of randomly folded structures.
q-bio.BM:Questions in computational molecular biology generate various discrete optimization problems, such as DNA sequence alignment and RNA secondary structure prediction. However, the optimal solutions are fundamentally dependent on the parameters used in the objective functions. The goal of a parametric analysis is to elucidate such dependencies, especially as they pertain to the accuracy and robustness of the optimal solutions. Techniques from geometric combinatorics, including polytopes and their normal fans, have been used previously to give parametric analyses of simple models for DNA sequence alignment and RNA branching configurations. Here, we present a new computational framework, and proof-of-principle results, which give the first complete parametric analysis of the branching portion of the nearest neighbor thermodynamic model for secondary structure prediction for real RNA sequences.
q-bio.BM:Nonlinear effects in protein dynamics are expected to play role in function, particularly of allosteric nature, by facilitating energy transfer between vibrational modes. A recently proposed method focusing on the non-Gaussian shape of the population near equilibrium projects this information onto real space in order to identify the aminoacids relevant to function. We here apply this method to three ancestral proteins in glucocorticoid receptor (GR) family and show that the mutations that restrict functional activity during GR evolution correlate significantly with locations that are highlighted by the nonlinear contribution to the near-native configurational distribution. Our findings demonstrate that nonlinear effects are not only indispensible for understanding functionality in proteins, but they can also be harnessed into a predictive tool for functional site determination.
q-bio.BM:A modified ssDNA SELEX protocol was developed in order to evolve a randomized library of imidazole modified ssDNA sequences towards sequences that mediate the formation of gold nanoparticles from Au3+ precursor ions in aqueous solution. Active sequences bound to nanoparticles were partitioned from inactive sequences based on density via ultracentrifugation through a discontinuous sucrose gradient. Colloidal gold solutions produced by the evolved pool had a distinct absorbance spectra and produced nanoparticles with a narrower distribution of sizes compared to colloidal gold solutions produced by the starting, randomized pool of imidazole modified ssDNA. Sequencing data from the evolved pool shows that conserved 5 and 6 nt motifs were shared amongst many of the isolates, which indicates that these motifs could serve as chelation sites for gold atoms or help stabilize colloidal gold solutions in a base specific manner.
q-bio.BM:Protein function and dynamics are closely related to its sequence and structure. However prediction of protein function and dynamics from its sequence and structure is still a fundamental challenge in molecular biology. Protein classification, which is typically done through measuring the similarity be- tween proteins based on protein sequence or physical information, serves as a crucial step toward the understanding of protein function and dynamics. Persistent homology is a new branch of algebraic topology that has found its success in the topological data analysis in a variety of disciplines, including molecular biology. The present work explores the potential of using persistent homology as an indepen- dent tool for protein classification. To this end, we propose a molecular topological fingerprint based support vector machine (MTF-SVM) classifier. Specifically, we construct machine learning feature vectors solely from protein topological fingerprints, which are topological invariants generated during the filtration process. To validate the present MTF-SVM approach, we consider four types of problems. First, we study protein-drug binding by using the M2 channel protein of influenza A virus. We achieve 96% accuracy in discriminating drug bound and unbound M2 channels. Additionally, we examine the use of MTF-SVM for the classification of hemoglobin molecules in their relaxed and taut forms and obtain about 80% accuracy. The identification of all alpha, all beta, and alpha-beta protein domains is carried out in our next study using 900 proteins. We have found a 85% success in this identifica- tion. Finally, we apply the present technique to 55 classification tasks of protein superfamilies over 1357 samples. An average accuracy of 82% is attained. The present study establishes computational topology as an independent and effective alternative for protein classification.
q-bio.BM:Non-coding RNAs are ubiquitous, but the discovery of new RNA gene sequences far outpaces research on their structure and functional interactions. We mine the evolutionary sequence record to derive precise information about function and structure of RNAs and RNA-protein complexes. As in protein structure prediction, we use maximum entropy global probability models of sequence co-variation to infer evolutionarily constrained nucleotide-nucleotide interactions within RNA molecules, and nucleotide-amino acid interactions in RNA-protein complexes. The predicted contacts allow all-atom blinded 3D structure prediction at good accuracy for several known RNA structures and RNA-protein complexes. For unknown structures, we predict contacts in 160 non-coding RNA families. Beyond 3D structure prediction, evolutionary couplings help identify important functional interactions, e.g., at switch points in riboswitches and at a complex nucleation site in HIV. Aided by accelerating sequence accumulation, evolutionary coupling analysis can accelerate the discovery of functional interactions and 3D structures involving RNA.
q-bio.BM:Some natural proteins display recurrent structural patterns. Despite being highly similar at the tertiary structure level, repetitions within a single repeat protein can be extremely variable at the sequence level. We propose a mathematical definition of a repeat and investigate the occurrences of these in different protein families. We found that long stretches of perfect repetitions are infrequent in individual natural proteins, even for those which are known to fold into structures of recurrent structural motifs. We found that natural repeat proteins are indeed repetitive in their families, exhibiting abundant stretches of 6 amino acids or longer that are perfect repetitions in the reference family. We provide a systematic quantification for this repetitiveness, and show that this form of repetitiveness is not exclusive of repeat proteins, but also occurs in globular domains. A by-product of this work is a fast classifier of proteins into families, which yields likelihood value about a given protein belonging to a given family.
q-bio.BM:Molecular recognition between two double stranded (ds) DNA with homologous sequences may not seem compatible with the B-DNA structure because the sequence information is hidden when it is used for joining the two strands. Nevertheless, it has to be invoked to account for various biological data. Using quantum chemistry, molecular mechanics, and hints from recent genetics experiments I show here that direct recognition between homologous dsDNA is possible through formation of short quadruplexes due to direct complementary hydrogen bonding of major groove surfaces in parallel alignment. The constraints imposed by the predicted structures of the recognition units determine the mechanism of complexation between long dsDNA. This mechanism and concomitant predictions agree with available experimental data and shed light upon the sequence effects and the possible involvement of topoisomerase II in the recognition.
q-bio.BM:Shortly after the determination of the first protein x-ray crystal structures, researchers analyzed their cores and reported packing fractions $\phi \approx 0.75$, a value that is similar to close packing equal-sized spheres. A limitation of these analyses was the use of `extended atom' models, rather than the more physically accurate `explicit hydrogen' model. The validity of using the explicit hydrogen model is proved by its ability to predict the side chain dihedral angle distributions observed in proteins. We employ the explicit hydrogen model to calculate the packing fraction of the cores of over $200$ high resolution protein structures. We find that these protein cores have $\phi \approx 0.55$, which is comparable to random close-packing of non-spherical particles. This result provides a deeper understanding of the physical basis of protein structure that will enable predictions of the effects of amino acid mutations and design of new functional proteins.
q-bio.BM:In this paper, we study the selectivity of the potassium channel KcsA by a recently developed image-charge solvation method(ICSM) combined with molecular dynamics simulations. The hybrid solvation model in the ICSM is able to demonstrate atomistically the function of the selectivity filter of the KcsA channel when potassium and sodium ions are considered and their distributions inside the filter are simulated. Our study also shows that the reaction field effect, explicitly accounted for through image charge approximation in the ICSM model, is necessary in reproducing the correct selectivity property of the potassium channels.
q-bio.BM:Modern biomedicine is challenged to predict the effects of genetic variation. Systematic functional assays of point mutants of proteins have provided valuable empirical information, but vast regions of sequence space remain unexplored. Fortunately, the mutation-selection process of natural evolution has recorded rich information in the diversity of natural protein sequences. Here, building on probabilistic models for correlated amino-acid substitutions that have been successfully applied to determine the three-dimensional structures of proteins, we present a statistical approach for quantifying the contribution of residues and their interactions to protein function, using a statistical energy, the evolutionary Hamiltonian. We find that these probability models predict the experimental effects of mutations with reasonable accuracy for a number of proteins, especially where the selective pressure is similar to the evolutionary pressure on the protein, such as antibiotics.
q-bio.BM:Although significant insight has been gained into biochemical, genetic and structural features of oxidative phosphorylation (OXPHOS) at the single-enzyme level, relatively little was known of how the component complexes function together in time and space until recently. Several pioneering single-molecule studies have emerged over the last decade in particular, which have illuminated our knowledge of OXPHOS, most especially on model bacterial systems. Here, we discuss these recent findings of bacterial OXPHOS, many of which generate time-resolved information of the OXPHOS machinery with the native physiological context intact. These new investigations are transforming our knowledge of not only the molecular arrangement of OXPHOS components in live bacteria, but also of the way components dynamically interact with each other in a functional state. These new discoveries have important implications towards putative supercomplex formation in bacterial OXPHOS in particular.
q-bio.BM:The direct-coupling analysis is a powerful method for protein contact prediction, and enables us to extract "direct" correlations between distant sites that are latent in "indirect" correlations observed in a protein multiple-sequence alignment. I show that the direct correlation can be obtained by using a formulation analogous to the Ornstein-Zernike integral equation in liquid theory. This formulation intuitively illustrates how the indirect or apparent correlation arises from an infinite series of direct correlations, and provides interesting insights into protein structure prediction.
q-bio.BM:Gaussian network model(GNM) and anisotropic network model(ANM) are some of the most popular methods for the study of protein flexibility and related functions. In this work, we propose generalized GNM(gGNM) and ANM methods and show that the GNM Kirchhoff matrix can be built from the ideal low-pass filter, which is a special case of a wide class of correlation functions underpinning the linear scaling flexibility-rigidity index(FRI) method. Based on the mathematical structure of correlation functions, we propose a unified framework to construct generalized Kirchhoff matrices whose matrix inverse leads to gGNMs, whereas, the direct inverse of its diagonal elements gives rise to FRI method.With this connection,we further introduce two multiscale elastic network models, namely, multiscale GNM(mGNM) and multiscale ANM(mANM), which are able to incorporate different scales into the generalized Kirchkoff matrices or generalized Hessian matrices.We validate our new multiscale methods with extensive numerical experiments. We illustrate that gGNMs outperform the original GNM method in the B-factor prediction of a set of 364 proteins.We demonstrate that for a given correlation function, FRI and gGNM methods provide essentially identical B-factor predictions when the scale value in the correlation function is sufficiently large.More importantly,we reveal intrinsic multiscale behavior in protein structures. The proposed mGNM and mANM are able to capture this multiscale behavior and thus give rise to a significant improvement of more than 11% in B-factor predictions over the original GNM and ANM methods. We further demonstrate benefit of our mGNM in the B-factor predictions on many proteins that fail the original GNM method. We show that the present mGNM can also be used to analyze protein domain separations. Finally, we showcase the ability of our mANM for the simulation of protein collective motions.
q-bio.BM:Protein-nucleic acid complexes are important for many cellular processes including the most essential function such as transcription and translation. For many protein-nucleic acid complexes, flexibility of both macromolecules has been shown to be critical for specificity and/or function. Flexibility-rigidity index (FRI) has been proposed as an accurate and efficient approach for protein flexibility analysis. In this work, we introduce FRI for the flexibility analysis of protein-nucleic acid complexes. We demonstrate that a multiscale strategy, which incorporates multiple kernels to capture various length scales in biomolecular collective motions, is able to significantly improve the state of art in the flexibility analysis of protein-nucleic acid complexes. We take the advantage of the high accuracy and ${\cal O}(N)$ computational complexity of our multiscale FRI method to investigate the flexibility of large ribosomal subunits, which is difficult to analyze by alternative approaches. An anisotropic FRI approach, which involves localized Hessian matrices, is utilized to study the translocation dynamics in an RNA polymerase.
q-bio.BM:As protein folding is a NP-complete problem, artificial intelligence tools like neural networks and genetic algorithms are used to attempt to predict the 3D shape of an amino acids sequence. Underlying these attempts, it is supposed that this folding process is predictable. However, to the best of our knowledge, this important assumption has been neither proven, nor studied. In this paper the topological dynamic of protein folding is evaluated. It is mathematically established that protein folding in 2D hydrophobic-hydrophilic (HP) square lattice model is chaotic as defined by Devaney. Consequences for both structure prediction and biology are then outlined.
q-bio.BM:A disease that caused by dengue virus (DENV) has become the major health problem of the world. Nowadays, no effective treatment is available to overcome the disease due to the level of dengue virus pathogeneses. A novel treatment method such as antiviral drug is highly necessary for coping with the dengue disease. Envelope protein is one of the non-structural proteins of DENV, which engaged in the viral fusion process. The fusion process is mediated by the conformational change in the protein structure from dimer to trimer state. The previous research showed the existing cavity on the dimer structure of the envelope protein. The existing ligand could get into cavity of the envelope protein, stabilize the dimer structure or hamper the transition of dimer protein into trimer. In this fashion, the fusion process can be prevented. The aim of this research is designing the cyclic peptide with prolin-prolin bond as fusion inhibitor of DENV envelope protein through molecular docking and molecular dynamics simulation. The screening of 3,883 cyclic peptides, each of them connected by prolin-prolin bond, through molecular docking resulted in five best ligands. The pharmacological and toxicity character of these five ligands were analised in silico. The result showed that PYRRP was the best ligand. PAWRP was also chosen as the best ligand because it showed good affinity with protein cavity. Stability of ligand-protein complex was analyzed by molecular dynamics simulation. The result showed that PYRRP ligand was able to support the stability of DENV envelope protein dimer structure at 310 K and 312 K. While PAWRP ligand actively formed complex with the DENV envelope protein at 310 K compared to 312 K. Thus the PYRRP ligand has a potential to be developed as DENV fusion inhibitor.
q-bio.BM:Data curation presents a challenge to all scientific disciplines to ensure public availability and reproducibility of experimental data. Standards for data preservation and exchange are central to addressing this challenge: the Investigation-Study-Assay Tabular (ISA-Tab) project has developed a widely used template for such standards in biological research. This paper describes the application of ISA-Tab to protein titration data. Despite the importance of titration experiments for understanding protein structure, stability, and function and for testing computational approaches to protein electrostatics, no such mechanism currently exists for sharing and preserving biomolecular titration data. We have adapted the ISA-Tab template to provide a structured means of supporting experimental structural chemistry data with a particular emphasis on the calculation and measurement of pKa values. This activity has been performed as part of the broader pKa Cooperative effort, leveraging data that has been collected and curated by the Cooperative members. In this article, we present the details of this specification and its application to a broad range of pKa and electrostatics data obtained for multiple protein systems. The resulting curated data is publicly available at http://pkacoop.org.
q-bio.BM:Validation of research methodology is critical in research design. Correlation between experimental observables must be established before undertaking extensive experiments or propose mechanisms. This article shows that, observables in the popular calcium flux strength assay used in the characterization of sweetener-sweet taste receptor (STR) interaction are uncorrelated. In pursuit to find potential sweeteners and enhancers, calcium flux generated via G-protein coupling for wildtype and mutant receptors expressed on cell surface is measured to identify and localize sweetener binding sites. Results are channeled for sweetener development with direct impact on public health.   We show that flux strength is independent of EC50 and sweet potency. Sweet potency-EC50 relation is non-linear and anti-correlated. Single point mutants affecting receptor efficiency, without significant shift in EC50 have been published, indicating flux strength is independent of ligand binding. G-protein coupling step is likely observed in the assay. Thus, years have been spent generating uncorrelated data. Data from uncorrelated observables does not give meaningful results. Still, majority of research in the field, uses change in calcium flux strength to study the receptor. Methodology away from flux strength monitor is required for sweetener development, reestablish binding localization of sweeteners established by flux strength method. This article serves to remind researchers to validate methodology before plunging into long term projects. Ignoring validation test on methodology, have been a costly mistake in the field. Concepts discussed here is applicable, whenever observable in biological systems are many steps moved from the event of interest.
q-bio.BM:Despite the significant increase in computational power, molecular modeling of protein structure using classical all-atom approaches remains inefficient, at least for most of the protein targets in the focus of biomedical research. Perhaps the most successful strategy to overcome the inefficiency problem is multiscale modeling to merge all-atom and coarse-grained models. This chapter describes a well-established CABS coarse-grained protein model. The CABS (C-Alpha, C-Beta and Side chains) model assumes a 2-4 united-atom representation of amino acids, knowledge-based force field (derived from the statistical regularities seen in known protein sequences and structures) and efficient Monte Carlo sampling schemes (MC dynamics, MC replica-exchange, and combinations). A particular emphasis is given to the unique design of the CABS force-field, which is largely defined using one-dimensional structural properties of proteins, including protein secondary structure. This chapter also presents CABS-based modeling methods, including multiscale tools for de novo structure prediction, modeling of protein dynamics and prediction of protein-peptide complexes. CABS-based tools are freely available at http://biocomp.chem.uw.edu.pl/tools
q-bio.BM:Calculating solvent accessible surface areas (SASA) is a run-of-the-mill calculation in structural biology. Although there are many programs available for this calculation, there are no free-standing, open-source tools designed for easy tool-chain integration. FreeSASA is an open source C library for SASA calculations that provides both command-line and Python interfaces in addition to its C API. The library implements both Lee and Richards' and Shrake and Rupley's approximations, and is highly configurable to allow the user to control molecular parameters, accuracy and output granularity. It only depends on standard C libraries and should therefore be easy to compile and install on any platform. The source code is freely available from http://freesasa.github.io/. The library is well-documented, stable and efficient. The command-line interface can easily replace closed source legacy programs, with comparable or better accuracy and speed, and with some added functionality.
q-bio.BM:Adenosine triphosphate (ATP) is the universal chemical energy currency for cellular activities provided mainly by the membrane enzyme FoF1-ATP synthase in bacteria, chloroplasts and mitochondria. Synthesis of ATP is accompanied by subunit rotation within the enzyme. Over the past 15 years we have developed a variety of single-molecule FRET (smFRET) experiments to monitor catalytic action of individual bacterial enzymes in vitro. By specifically labeling rotating and static subunits within a single enzyme we were able to observe three-stepped rotation in the F1 motor, ten-stepped rotation in the Fo motor and transient elastic deformation of the connected rotor subunits. However, the spatial and temporal resolution of motor activities measured by smFRET were limited by the photophysics of the FRET fluorophores. Here we evaluate the novel FRET donor mNeonGreen as a fusion to FoF1-ATP synthase and compare it to the previously used fluorophore EGFP. Topics of this manuscript are the biochemical purification procedures and the activity measurements of the fully functional mutant enzyme.
q-bio.BM:Motivation: To assess the quality of a protein model, i.e. to estimate how close it is to its native structure, using no other information than the structure of the model has been shown to be useful for structure prediction. The state of the art method, ProQ2, is based on a machine learning approach that uses a number of features calculated from a protein model. Here, we examine if these features can be exchanged with energy terms calculated from Rosetta and if a combination of these terms can improve the quality assessment. Results: When using the full atom energy function from Rosetta in ProQRosFA the QA is on par with our previous state-of-the-art method, ProQ2. The method based on the low-resolution centroid scoring function, ProQRosCen, performs almost as well and the combination of all the three methods, ProQ2, ProQRosFA and ProQCenFA into ProQ3 show superior performance over ProQ2. Availability: ProQ3 is freely available on BitBucket at https://bitbucket.org/ElofssonLab/proq3
q-bio.BM:We study the conformational freedom of a protein made by two rigid domains connected by a flexible linker. The conformational freedom is represented as an unknown probability distribution on the space of allowed states. A new algorithm for the calculation of the Maximum Allowable Probability is proposed, which can be extended to any type of measurements. In this paper we use Pseudo Contact Shifts and Residual Dipolar Coupling. We reconstruct a single central tendency in the distribution and discuss in depth the results.
q-bio.BM:3 transmembrane and 4 transmembrane helices models are suggested for the human vitamin K epoxide reductase (VKOR). In this study, we investigate the stability of the human 3 transmembrane/4 transmembrane VKOR models employing a coarse-grained normal mode analysis and molecular dynamics simulation. Based on the analysis of the mobility of each transmembrane domain, we suggest that the 3 transmembrane human VKOR model is more stable than the 4 transmembrane human VKOR model.
q-bio.BM:A re-refinement of 4xan, hen egg white lysozyme (HEWL) with carboplatin crystallised in NaBr solution, has been made (Tanley et al 2016). This follows our Response article (Tanley et al 2015) to the Critique article of Shabalin et al 2015, suggesting the need for corrections to some solute molecule interpretations of electron density in 4xan and removal of an organic moiety as a ligand to the platinum ion coordinated to His15. We note Shabalin et al (2015) model of a chlorine in that density and a close by bromine at partial occupancy to explain the shape. However, as the bromide concentration is in huge excess over chloride (by 20 fold), we think that the 4yem Shabalin et al 2015 interpretation highly unlikely, but nevertheless we still cannot offer an explanation for that shape, confirming our earlier analysis described in Tanley et al (2014). Following Shabalin et al (2015) reprocessing of the raw diffraction data for 4g4a, we also redid the diffraction data processing for 4xan to a higher resolution using EVAL (Schreurs et al 2010) concluding in favour of 1.3 Angstrom as the resolution limit and which is the basis for our revised PDB file for 4xan (5HMJ). It is very interesting that there is extra X-ray diffraction data from 1.47 to 1.30 Angstrom resolution e.g. with <I/sigma(I)> =0.39 and CC1/2 = 0.181 in the final shell (1.30 to 1.322 Angstrom). In this arXiv article we document in detail our different solvent and split occupancy side chain electron density interpretations as evidence for our statement of approach in our Response article (Tanley et al 2015). Our critical re-examination includes comparisons based on the 4xan diffraction data images reprocessing with three different software packages so as to evaluate the possibility of variations in electron density interpretations due to that. Overall our finalised model (PDB code 5HMJ) is now improved over 4xan.
q-bio.BM:Proteins change their charge state through protonation and redox reactions as well as through binding charged ligands. The free energy of these reactions are dominated by solvation and electrostatic energies and modulated by protein conformational relaxation in response to the ionization state changes. Although computational methods for calculating these interactions can provide very powerful tools for predicting protein charge states, they include several critical approximations of which users should be aware. This chapter discusses the strengths, weaknesses, and approximations of popular computational methods for predicting charge states and understanding their underlying electrostatic interactions. The goal of this chapter is to inform users about applications and potential caveats of these methods as well as outline directions for future theoretical and computational research.
q-bio.BM:Structural relationships among proteins are important in the study of their evolution as well as in drug design and development. The protein 3D structure has been shown to be effective with respect to classifying proteins. Prior work has shown that the Double Centroid Reduced Representation (DCRR) model is a useful geometric representation for protein structure with respect to visual models, reducing the quantity of modeled information for each amino acid, yet retaining the most important geometrical and chemical features of each: the centroids of the backbone and of the side-chain. DCRR has not yet been applied in the calculation of geometric structural similarity. Meanwhile, multi-dimensional indexing (MDI) of protein structure combines protein structural analysis with distance metrics to facilitate structural similarity queries and is also used for clustering protein structures into related groups. In this respect, the combination of geometric models with MDI has been shown to be effective. Prior work, notably Distance and Density based Protein Indexing (DDPIn), applies MDI to protein models based on the geometry of the CA backbone. DDPIn distance metrics are based on radial and density functions that incorporate spherical-based metrics, and the indices are built from metric tree (M-tree) structures. This work combines DCRR with DDPIn for the development of new DCRR centroid-based metrics: spherical binning distance and inter-centroid spherical distance. The use of DCRR models will provide additional significant structural information via the inclusion of side-chain centroids. Additionally, the newly developed distance metric functions combined with DCRR and M-tree indexing attempt to improve upon the performance of prior work (DDPIn), given the same data set, with respect to both individual k-nearest neighbor (kNN) search queries as well as clustering all proteins in the index.
q-bio.BM:Based on overall 3D structure, proteins may be grouped into two broad categories, namely, globular proteins (spheroproteins), and elongated or rod-shaped proteins (RSP). The former comprises a significant majority of proteins. This work concerns the second category. Unlike a spheroprotein, an RSP possesses a conspicuous axis along its longest dimension. To take advantage of this symmetry element, we decided to represent RSPs using cylindrical coordinates, (rho, theta, z), with the z-axis as the main axis, and one tip of the protein at the origin. A "tip" is one of two extreme points in the protein lying along the protein axis along its longest dimension. We first identify the two tips, T1 and T2, of the RSP using a protein graphics software, then determine their (Cartesian) coordinates, (h, k, l) and (m, n, o), respectively. Arbitrarily selecting T1 as the tip at the origin, we translate the protein by subtracting (h, k, l) from all structural coordinates. We then find the angle alpha between vector T1-T2 and the positive z-axis by computing the scalar product of vectors T1- T2 and OP where P is an arbitrary point along the positive z-axis. We typically use (0, 0, p) where p is a suitable positive number. Then we compute the cross product of the two vectors to determine the axis about which we should rotate vector T1-T2 to make it coincide with the positive z-axis. We use a matrix form of Rodrigues' formula to do the rotation. We then apply the Cartesian to cylindrical coordinate transformation equations to the system. We have applied the above transformation to 15 RSPs: 1QCE, 2JJ7, 2KPE, 3K2A, 3LHP, 2LOE, 2L3H, 2L1P, 1KSG, 1KSJ, 1KSH, 2KOL, 2KZG, 2KPF and 3MQC. We have also created a web server that can take the PDB coordinate file of an RSP and output its cylindrical coordinates. The URL of our web server will be announced publicly in due course.
q-bio.BM:We have developed a web server that implements two complementary methods to quantify the depth of ligand binding site (LBS) in protein-ligand complexes: the "secant plane" (SP) and "tangent sphere" (TS) methods. The protein molecular centroid (global centroid, GC), and the LBS centroid (local centroid, LC) are first determined. The SP is the plane passing through the LC and normal to the line passing through the LC and the GC. The "exterior side" of the SP is the side opposite GC. The TS is the sphere with center at GC and tangent to the SP at LC. The percentage of protein atoms inside the TS (TS index) and on the exterior side of the SP (SP index), are complementary measures of LBS depth. The SPi is directly proportional to LBS depth while the TSi is inversely proportional. We tested the two methods using a test set of 67 well-characterized protein-ligand structures (Laskowski, et al. 1996), as well as that of an artificial protein in the form of a grid of points in the overall shape of a sphere and in which LBS of any depth can be specified. Results from both the SP and TS methods agree well with reported data (ibid.), and results from the artificial case confirm that both methods are suitable measures of LBS depth. The web server may be used in two modes. In the "ligand mode", user inputs the protein PDB coordinates as well as those of the ligand. The "LBS mode" is the same as the former, except that the ligand coordinates are assumed to be unavailable; hence the user inputs what s/he believes to be the coordinates of the LBS amino acid residues. In both cases, the web server outputs the SP and TS indices. LBS depth is usually directly related to the amount of conformational change a protein undergoes upon ligand binding - ability to quantify it could allows meaningful comparison of protein flexibility and dynamics. The URL of our web server will be announced publicly in due course.
q-bio.BM:Protein structure representation is an important tool in structural biology. There exists different methods of representing the protein 3D structures and different biologists favor different methods based on the information they require. Currently there is no available method of protein 3D structure representation which captures enough chemical information from the protein sequence and clearly shows the intra-molecular and the inter-molecular H-bonds and VDW interactions at the same time. This project aims to reduce the 3D structure of a protein and display the reduced representation along with intermolecular and the intra- molecular H-bonds and van der Waals interactions. A reduced protein representation has a significantly lower atomicity (i.e., number of atom coordinates) than one which is in all- atom representation. In this work, we transform the protein structure from all-atom representation" (AAR) to double-centroid reduced representation (DCRR), which contains amino acid backbone (N, CA, C', O) and side chain (CB and beyond) centroid coordinates instead of atomic coordinates. Another aim of this project is to develop a visualization interface for the reduced representation. This interface is implemented in MATLAB and displays the protein in DCRR along with its inter-molecular, as well as intra-molecular, interaction. Visually, DCRR is easier to comprehend than AAR. We also developed a Web Server called the Protein DCRR Web Server wherein users can enter the PDB id or upload a modeled protein and get the DCRR of that protein. The back end to the Web Server is a database which has the reduced representation for all the x-ray crystallographic structure in the PDB.
q-bio.BM:A pharmacophore consists of the parts of the structure of the ligand that are sufficient to express the biological and pharmacological effects of the ligand. It is usually a substructure of the entire structure of the ligand. Small organic molecules called ligands or metabolites in the cell form complexes with biomolecules (usually proteins) to serve different purposes. The sites at which the ligands bind are known as ligand binding sites, which are essentially "pockets" which have complementary shapes and patterns of charge distribution with the ligands. Sometimes a pocket is induced by the ligand itself. If we study different bound conformations of ligands it is found that they share a specific three- dimensional pattern that is more or less common and is responsible for its binding and which is complementary in three-dimensional geometry and charge distribution pattern with its cognate binding site in the protein. This work studies the three dimensional structure of the consensus ligand binding site for the ligand FMN. A training set for the ligand binding sites was made and a 3D consensus binding site motif was determined for FMN. The FMN system was studied and its binding sites in its respective regulator proteins. The ability to identify ligand binding site by scanning the 3D binding site consensus motif in protein 3D structures is an important step in drug target discovery. Once a pharmacophore template is found it can also be used to design other potential molecules that can bind to it and thus serve as novel drugs.
q-bio.BM:A fundamental question in protein folding is whether the coil to globule collapse transition occurs during the initial stages of folding (burst-phase) or simultaneously with the protein folding transition. Single molecule fluorescence resonance energy transfer (FRET) and small angle X-ray scattering (SAXS) experiments disagree on whether Protein L collapse transition occurs during the burst-phase of folding. We study Protein L folding using a coarse-grained model and molecular dynamics simulations. The collapse transition in Protein L is found to be concomitant with the folding transition. In the burst-phase of folding, we find that FRET experiments overestimate radius of gyration, $R_g$, of the protein due to the application of Gaussian polymer chain end-to-end distribution to extract $R_g$ from the FRET efficiency. FRET experiments estimate $\approx$ 6\AA \ decrease in $R_g$ when the actual decrease is $\approx$ 3\AA \ on Guanidinium Chloride denaturant dilution from 7.5M to 1M, and thereby suggesting pronounced compaction in the protein dimensions in the burst-phase. The $\approx$ 3\AA \ decrease is close to the statistical uncertainties of the $R_g$ data measured from SAXS experiments, which suggest no compaction, leading to a disagreement with the FRET experiments. The transition state ensemble (TSE) structures in Protein L folding are globular and extensive in agreement with the $\Psi$-analysis experiments. The results support the hypothesis that the TSE of single domain proteins depend on protein topology, and are not stabilised by local interactions alone.
q-bio.BM:Protein allostery requires dynamical structural correlations. Physical origin of which, however, remain elusive despite intensive studies during last two decades. Based on analysis of molecular dynamics (MD) simulation trajectories for ten proteins with different sizes and folds, we found that nonlinear backbone torsional pair (BTP) correlations, which are spatially more long-ranged and are mainly executed by loop residues, exist extensively in most analyzed proteins. Examination of torsional motion for correlated BTPs suggested that aharmonic torsional state transitions are essential for such non-linear correlations, which correspondingly occur on widely different and relatively longer time scales. In contrast, BTP correlations between backbone torsions in stable $\alpha$ helices and $\beta$ strands are mainly linear and spatially more short-ranged, and are more likely to associate with intra-well torsional dynamics. Further analysis revealed that the direct cause of non-linear contributions are heterogeneous, and in extreme cases canceling, linear correlations associated with different torsional states of participating torsions. Therefore, torsional state transitions of participating torsions for a correlated BTP are only necessary but not sufficient condition for significant non-linear contributions. These findings implicate a general search strategy for novel allosteric modulation of protein activities. Meanwhile, it was suggested that ensemble averaged correlation calculation and static contact network analysis, while insightful, are not sufficient to elucidate mechanisms underlying allosteric signal transmission in general, dynamical and time scale resolved analysis are essential.
q-bio.BM:Free energy calculation is critical in predictive tasks such as protein folding, docking and design. However, rigorous calculation of free energy change is prohibitively expensive in these practical applications. The minimum potential energy is therefore widely utilized to approximate free energy. In this study, based on analysis of extensive molecular dynamics (MD) simulation trajectories of a few native globular proteins, we found that change of minimum and corresponding maximum potential energy terms exhibit similar level of correlation with change of free energy. More importantly, we demonstrated that change of span (maximum - minimum) of potential energy terms, which engender negligible additional computational cost, exhibit considerably stronger correlations with change of free energy than the corresponding change of minimum and maximum potential energy terms. Therefore, potential energy span may serve as an alternative efficient approximate free energy proxy.
q-bio.BM:As an example of topic where biology and physics meet, we present the issue of protein folding and stability, and the development of thermodynamics-based bioinformatics tools that predict the stability and thermal resistance of proteins and the change of these quantities upon amino acid substitutions. These methods are based on knowledge-driven statistical potentials, derived from experimental protein structures using the inverse Boltzmann law. We also describe an application of these predictors, which contributed to the understanding of the mechanisms of aggregation of a particular protein known to cause a neuronal disease.
q-bio.BM:Summary: The development of automated servers to predict the three-dimensional structure of proteins has seen much progress over the years. These servers make modeling simpler, but largely exclude users from the process. We present an Interactive Modeling Pipeline (IMPi) for homology modeling. The pipeline simplifies the modeling process and reduces the workload required by the user, while still allowing engagement from the user during every step. Default parameters are given for each step, which can either be modified or supplemented with additional external input. As such, it has been designed for users of varying levels of experience with homology modeling.   Availability and implementation: The IMPi site is free for non-commercial use and can be accessed at https://impi.rubi.ru.ac.za.   Supplementary information: Documentation available at https://impi.rubi.ru.ac.za/#documentation.
q-bio.BM:Self-assembly of proteins is a biological phenomenon which gives rise to spontaneous formation of amyloid fibrils or polymers. The starting point of this phase, called nucleation exhibits an important variability among replicated experiments.To analyse the stochastic nature of this phenomenon, one of the simplest models considers two populations of chemical components: monomers and polymerised monomers. Initially there are only monomers. There are two reactions for the polymerization of a monomer: either two monomers collide to combine into two polymerised monomers or a monomer is polymerised after the encounter of a polymerised monomer. It turns out that this simple model does not explain completely the variability observed in the experiments. This paper investigates extensions of this model to take into account other mechanisms of the polymerization process that may have impact an impact on fluctuations.The first variant consists in introducing a preliminary conformation step to take into account the biological fact that, before being polymerised, a monomer has two states, regular or misfolded. Only misfolded monomers can be polymerised so that the fluctuations of the number of misfolded monomers can be also a source of variability of the number of polymerised monomers. The second variant represents the reaction rate $\alpha$ of spontaneous formation of a polymer as of the order of $N^{-\nu}$, with $\nu$ some positive constant. First and second order results for the starting instant of nucleation are derived from these limit theorems. The proofs of the results rely on a study of a stochastic averaging principle for a model related to an Ehrenfest urn model, and also on a scaling analysis of a population model.
q-bio.BM:We present a semi-quantitative model of condensation of short nucleic acid (NA) duplexes induced by tri-valent cobalt(III) hexammine (CoHex) ions. The model is based on partitioning of bound counterion distribution around singleNA duplex into "external" and "internal" ion binding shells distinguished by the proximity to duplex helical axis. In the aggregated phase the shells overlap, which leads to significantly increased attraction of CoHex ions in these overlaps with the neighboring duplexes. The duplex aggregation free energy is decomposed into attractive and repulsive components in such a way that they can be represented by simple analytical expressions with parameters derived from molecular dynamic (MD) simulations and numerical solutions of Poisson equation. The short-range interactions described by the attractive term depend on the fractions of bound ions in the overlapping shells and affinity of CoHex to the "external" shell of nearly neutralized duplex. The repulsive components of the free energy are duplex configurational entropy loss upon the aggregation and the electrostatic repulsion of the duplexes that remains after neutralization by bound CoHex ions. The estimates of the aggregation free energy are consistent with the experimental range of NA duplex condensation propensities, including the unusually poor condensation of RNA structures and subtle sequence effects upon DNA condensation. The model predicts that, in contrast to DNA, RNA duplexes may condense into tighter packed aggregates with a higher degree of duplex neutralization. The model also predicts that longer NA fragments will condense more readily than shorter ones. The ability of this model to explain experimentally observed trends in NA condensation, lends support to proposed NA condensation picture based on the multivalent "ion binding shells".
q-bio.BM:Pentameric ligand-gated ion channels (pLGICs) of the Cys-loop superfamily are important neuroreceptors that mediate fast synaptic transmission. They are activated by the binding of a neurotransmitter, but the details of this process are still not fully understood. As a prototypical pLGIC, here we choose the insect resistance to dieldrin (RDL) receptor, involved in the resistance to insecticides, and investigate the binding of the neurotransmitter GABA to its extracellular domain at the atomistic level. We achieve this by means of $\mu$-sec funnel-metadynamics simulations, which efficiently enhance the sampling of bound and unbound states by using a funnel-shaped restraining potential to limit the exploration in the solvent. We reveal the sequence of events in the binding process, from the capture of GABA from the solvent to its pinning between the charged residues Arg111 and Glu204 in the binding pocket. We characterize the associated free energy landscapes in the wild-type RDL receptor and in two mutant forms, where the key residues Arg111 and Glu204 are mutated to Ala. Experimentally these mutations produce non-functional channels, which is reflected in the reduced ligand binding affinities, due to the loss of essential interactions. We also analyze the dynamical behaviour of the crucial loop C, whose opening allows the access of GABA to the binding site, while its closure locks the ligand into the protein. The RDL receptor shares structural and functional features with other pLGICs, hence our work outlines a valuable protocol to study the binding of ligands to pLGICs beyond conventional docking and molecular dynamics techniques.
q-bio.BM:In this work, we develop first near-complete 3D models for NTD-hXPB - the N-terminal protein domain of the human transcription factor XPB. The results are very significant as NTD-hXPB plays a critical role in the synthesis of proteins (specifically transcription) and DNA damage repair (specifically nucleotide excision repair). NTD-hXPB is directly implicated in rare diseases XP-B, XP-CS, and TTD2, whose symptoms include neurodegenerative disorders, premature aging, and decreased fertility. NTDhXPB is also linked to anti-cancer therapies. As a bi-product we derived 3D models of NTD-mXPB - homologue of NTD-hXPB in mycobacterium tuberculosis aka MTB (causative agent of most cases of tuberculosis, which surpassed HIV as #1 infectious disease killer in 2014). These could be potential target for TB therapeutics. Our ab-initio modelling protocol takes advantage of recent powerful advances (prediction of contact residues) in protein structure modeling. Our protocol also includes human in the loop, inspired by the prevailing theory that computational abilities of human minds can be powerfully harnessed in engineering/problem-solving. Using the developed models in this paper, we are able to propose significant insights into (a) the role of NTD-hXPB in the process of transcription and DNA damage repair (specifically NER), (b) the critical interactions of NTD-hXPB with another co-labourer protein p52, (c) diseases associated with NTD-hXPB, and (d) alterations in functionalities between NTD-hXPB (in human) and NTD-mXPB (in TB pathogen).
q-bio.BM:We present a new methodology for efficient and high-quality patterning of biological reagents for surface-based biological assays. The method relies on hydrodynamically confined nanoliter volumes of reagents to interact with the substrate at the micrometer-length scale. We study the interplay between diffusion, advection, and surface chemistry and present the design of a noncontact scanning microfluidic device to efficiently present reagents on surfaces. By leveraging convective flows, recirculation, and mixing of a processing liquid, this device overcomes limitations of existing biopatterning approaches, such as passive diffusion of analytes, uncontrolled wetting, and drying artifacts. We demonstrate the deposition of analytes, showing a 2- to 5-fold increase in deposition rate together with a 10-fold reduction in analyte consumption while ensuring less than 6% variation in pattern homogeneity on a standard biological substrate. In addition, we demonstrate the recirculation of a processing liquid using a microfluidic probe (MFP) in the context of a surface assay for (i) probing 12 independent areas with a single microliter of processing liquid and (ii) processing a 2 mm2 surface to create 170 antibody spots of 50 x 100 {\mu}m2 area using 1.6 {\mu}L of liquid. We observe high pattern quality, conservative usage of reagents, micrometer precision of localization and convection-enhanced fast deposition. Such a device and method may facilitate quantitative biological assays and spur the development of the next generation of protein microarrays.
q-bio.BM:A mystery about the origins of life is which molecular structures $-$ and what spontaneous processes $-$ drove the autocatalytic transition from simple chemistry to biology? Using the HP lattice model of polymer sequence spaces leads to the prediction that random sequences of hydrophobic ($H$) and polar ($P$) monomers can collapse into relatively compact structures, exposing hydrophobic surfaces, acting as primitive versions of today's protein catalysts, elongating other such HP polymers, as ribosomes would now do. Such foldamer-catalysts form an autocatalytic set, growing short chains into longer chains that have particular sequences. The system has capacity for the multimodality: ability to settle at multiple distinct quasi-stable states characterized by different groups of dominating polymers. This is a testable mechanism that we believe is relevant to the early origins of life.
q-bio.BM:The macromolecules that encode and translate information in living systems, DNA and RNA, exhibit distinctive structural asymmetries, including homochirality or mirror image asymmetry and $3' - 5'$ directionality, that are invariant across all life forms. The evolutionary advantages of these broken symmetries remain unknown. Here we utilize a very simple model of hypothetical self-replicating polymers to show that asymmetric autocatalytic polymers are more successful in self-replication compared to their symmetric counterparts in the Darwinian competition for space and common substrates. This broken-symmetry property, called asymmetric cooperativity, arises with the maximization of a replication potential, where the catalytic influence of inter-strand bonds on their left and right neighbors is unequal. Asymmetric cooperativity also leads to tentative, qualitative and simple evolution-based explanations for a number of other properties of DNA that include four nucleotide alphabet, three nucleotide codons, circular genomes, helicity, anti-parallel double-strand orientation, heteromolecular base-pairing, asymmetric base compositions, and palindromic instability, apart from the structural asymmetries mentioned above. Our model results and tentative explanations are consistent with multiple lines of experimental evidence, which include evidence for the presence of asymmetric cooperativity in DNA.
q-bio.BM:To better understand protein-solvent interaction we have analyzed a variety of physical and geometrical properties of the solvent-excluded surfaces (SESs) over a large set of soluble proteins with crystal structures. We discover that all have net negative surface charges and permanent electric dipoles. Moreover both SES area and surface charge as well as several physical and geometrical properties defined by them change with protein size via well-fitted power laws. The relevance to protein-solvent interaction of these physical and geometrical properties is supported by strong correlations between them and known hydrophobicity scales and by their large changes upon protein unfolding. The universal existence of negative surface charge and dipole, the characteristic surface geometry and power laws reveal fundamental but distinct roles of surface charge and SES in protein-solvent interaction and make it possible to describe solvation and hydrophobic effect using theories on anion solute in protic solvent. In particular the great significance of surface charge for protein-solvent interaction suggests that a change of perception may be needed since from solvation perspective folding into a native state is to optimize surface negative charge rather than to minimize the hydrophobic surface area.
q-bio.BM:Motivation: RNA thermometers (RNATs) are cis-regulatory ele- ments that change secondary structure upon temperature shift. Often involved in the regulation of heat shock, cold shock and virulence genes, RNATs constitute an interesting potential resource in synthetic biology, where engineered RNATs could prove to be useful tools in biosensors and conditional gene regulation. Results: Solving the 2-temperature inverse folding problem is critical for RNAT engineering. Here we introduce RNAiFold2T, the first Constraint Programming (CP) and Large Neighborhood Search (LNS) algorithms to solve this problem. Benchmarking tests of RNAiFold2T against existent programs (adaptive walk and genetic algorithm) inverse folding show that our software generates two orders of magnitude more solutions, thus allow- ing ample exploration of the space of solutions. Subsequently, solutions can be prioritized by computing various measures, including probability of target structure in the ensemble, melting temperature, etc. Using this strategy, we rationally designed two thermosensor internal ribosome entry site (thermo-IRES) elements, whose normalized cap-independent transla- tion efficiency is approximately 50% greater at 42?C than 30?C, when tested in reticulocyte lysates. Translation efficiency is lower than that of the wild-type IRES element, which on the other hand is fully resistant to temperature shift-up. This appears to be the first purely computational design of functional RNA thermoswitches, and certainly the first purely computational design of functional thermo-IRES elements. Availability: RNAiFold2T is publicly available as as part of the new re- lease RNAiFold3.0 at https://github.com/clotelab/RNAiFold and http: //bioinformatics.bc.edu/clotelab/RNAiFold, which latter has a web server as well. The software is written in C++ and uses OR-Tools CP search engine.
q-bio.BM:We propose a new topological characterization of RNA secondary structures with pseudoknots based on two topological invariants. Starting from the classic arc-representation of RNA secondary structures, we consider a model that couples both I) the topological genus of the graph and II) the number of crossing arcs of the corresponding primitive graph. We add a term proportional to these topological invariants to the standard free energy of the RNA molecule, thus obtaining a novel free energy parametrization which takes into account the abundance of topologies of RNA pseudoknots observed in RNA databases.
q-bio.BM:In structure-based models of proteins, one often assumes that folding is accomplished when all contacts are established. This assumption may frequently lead to a conceptual problem that folding takes place in a temperature region of very low thermodynamic stability, especially when the contact map used is too sparse. We consider six different structure-based models and show that allowing for a small, but model-dependent, percentage of the native contacts not being established boosts the folding temperature substantially while affecting the time scales of folding only in a minor way. We also compare other properties of the six models. We show that the choice of the description of the backbone stiffness has a substantial effect on the values of characteristic temperatures that relate both to equilibrium and kinetic properties. Models without any backbone stiffness (like the self-organized polymer) are found to perform similar to those with the stiffness, including in the studies of stretching.
q-bio.BM:Antibiotics such as the quinolones and fluoroquinolones kill bacterial pathogens ultimately through DNA damage. They target the essential type IIA topoisomerases in bacteria by stabilising the normally transient double strand break state which is created to modify the supercoiling state of the DNA. Here we discuss the development of these antibiotics and their method of action. Existing methods for DNA damage visualisation, such as the comet assay and immunofluorescence imaging can often only be analysed qualitatively and this analysis is subjective. We describe a putative single-molecule fluorescence technique for quantifying DNA damage via the total fluorescence intensity of a DNA origami tile fully saturated with an intercalating dye, along with the optical requirements for how to implement these into a light microscopy imaging system capable of single-molecule millisecond timescale imaging. This system promises significant improvements in reproducibility of the quantification of DNA damage over traditional techniques.
q-bio.BM:The protein-protein interactions (PPIs) are crucial for understanding the majority of cellular processes. PPIs play important role in gene transcription regulation, cellular signaling, molecular basis of immune response and more. Moreover, a disruption of hese mechanisms is frequently postulated as a possible cause of diseases such as Alzheimer's or cancer. For many of biologically relevant cases the structure of protein-protein complexes remain unknown. Therefore computational techniques, including molecular docking, have become a valuable part of drug discovery pipelines. Unfortunately, none of the widely used protein-protein docking tools is free from serious limitations. Typically, in docking simulations the protein flexibility is either completely neglected or very limited. Additionally, some knowledge of the approximate location and/or the shape of the active site is also required. Such limitations arise mostly from the enormous number of degrees of freedom of protein-protein systems. In this paper, an efficient computational method for protein-protein docking is proposed and initially tested on a single docking case. The proposed method is based on a two-step procedure. In the first step, CABS-dock web server for protein-peptide docking is used to dock a peptide, which is the appropriate protein fragment responsible for the protein-protein interaction, to the other protein partner. During peptide docking, no knowledge about the binding site, nor the peptide structure, is used and the peptide is allowed to be fully flexible. In the second step, the docked peptide is used in the structural adjustment of protein complex partners. The proposed method allowed us to obtain a high accuracy model, therefore it provides a promising framework for further advances.
q-bio.BM:Despite considerable efforts, structural prediction of protein-peptide complexes is still a very challenging task, mainly due to two reasons: high flexibility of the peptides and transient character of their interactions with proteins. Recently we have developed an automated web server CABS-dock (http://biocomp.chem.uw.edu.pl/CABSdock), which conducts flexible protein-peptide docking without any knowledge about the binding site. Our method allows for full flexibility of the peptide, whereas the flexibility of the receptor is restricted to near native conformations considering the main chain, and full flexibility of the side chains. Performance of the CABS-dock server was thoroughly tested on a benchmark of 171 test cases, both bound and unbound. Evaluation of the obtained results showed overall good performance of the method, especially that no information of the binding site was used. From unsuccessful experiments we learned that the accuracy of docking might be significantly improved, if only little information of the binding site was considered. In fact, in real-life applications user typically has access to some data indicating the location and/or structure of the binding site. In the current work, we test and demonstrate the performance of the CABS-dock server with two new features. The first one allows to utilize the knowledge about receptor residue(s) constituting the binding site, and the second one allows to enforce the desired secondary structure on the peptide structure. Based on the given example, we observe significant improvement of the docking accuracy in comparison to the default CABS-dock mode.
q-bio.BM:Protein-peptide molecular docking is a difficult modeling problem. It is even more challenging when significant conformational changes that may occur during the binding process need to be predicted. In this chapter, we demonstrate the capabilities and features of the CABS-dock server for flexible protein-peptide docking. CABS-dock allows highly efficient modeling of full peptide flexibility and significant flexibility of a protein receptor. During CABS-dock docking, the peptide folding and binding process is explicitly simulated and no information about the peptide binding site or its structure, is used. This chapter presents a successful CABS-dock use for docking a potentially therapeutic peptide to a protein target. Moreover, simulation contact maps, a new CABS-dock feature, are described and applied to the docking test case. Finally, a tutorial for running CABS-dock from the command line or command line scripts is provided. The CABS-dock web server is available from http://biocomp.chem.uw.edu.pl/CABSdock/
q-bio.BM:Hemoglobin (Hgb) forms tetramers (dimerized dimers), which enhance its globular stability and may also facilitate small gas molecule transport, as shown by recent all-atom Newtonian solvated simulations. Hydropathic bioinformatic scaling reveals many wave-like features of strained Hgb structures at the coarse-grained amino acid level, while distinguishing between these features thermodynamically. Strain fields localized near hemes interfere with extended strain fields associated with dimer interfacial misfit, resulting in wave-length dependent dimer correlation function antiresonances.
q-bio.BM:Following the interest of L Messori and A Merlino 2016 Coordination Chemistry Reviews in the platinum ions coordination geometries in our PDB entries 4dd4 and 4dd6 we have extended our original analyses.
q-bio.BM:Sm proteins were discovered nearly 20 years ago as a group of small antigenic proteins ($\approx$ 90-120 residues). Since then, an extensive amount of biochemical and genetic data have illuminated the crucial roles of these proteins in forming ribonucleoprotein (RNP) complexes that are used in RNA processing, e.g., spliceosomal removal of introns from pre-mRNAs. Spliceosomes are large macromolecular machines that are comparable to ribosomes in size and complexity, and are composed of uridine-rich small nuclear RNPs (U snRNPs). Various sets of seven different Sm proteins form the cores of most snRNPs. Despite their importance, very little is known about the atomic-resolution structure of snRNPs or their Sm cores. As a first step towards a high-resolution image of snRNPs and their hierarchic assembly, we have determined the crystal structures of archaeal homologs of Sm proteins, which we term Sm-like archaeal proteins (SmAPs).
q-bio.BM:This document attempts to clarify potential confusion regarding electrostatics calculations, specifically in the context of biomolecular structure and specifically as regards the units typically used to contour/visualize isopotential surfaces, potentials mapped onto molecular solvent-accessible surfaces, etc.
q-bio.BM:The model for cAMP-dependent synaptic plasticity relates the characterization of a noradrenaline-stimulated adenylyl cyclase with DNA unzipping. Specific proteins condition cAMP insertion in specific sites of the DNA structure to direct cellular and synaptic differentiation in brain tissues, also providing coding. Metabolic-dependent ATP binding of Mg2+, could control feedback by inactivating AC dependent formation of cAMP. The level of cAMP and cGMP, which could be assayed in red cells and cerebrospinal fluid, allows a clinical lab diagnostic improvement. Also, provides a relationship of best fitting to cAMP control by binding to DNA. The cAMP level allows the prediction of an inverse relationship between neurodegeneration and cancer. The latter, could be characterized by uncontrolled proliferation, whereas metabolic dominance by stress over a long period of time, may deplete cerebral cAMP.
q-bio.BM:In many virus families, tens to thousands of proteins assemble spontaneously into a capsid (protein shell) while packaging the genomic nucleic acid. This review summarizes recent advances in computational modeling of these dynamical processes. We present an overview of recent technological and algorithmic developments, which are enabling simulations to describe the large ranges of length-and time-scales relevant to assembly, under conditions more closely matched to experiments than in earlier work. We then describe two examples in which computational modeling has recently provided an important complement to experiments.
q-bio.BM:Considerable mechanistic insight has been gained into amyloid aggregation; however, a large class of non-amyloid protein aggregates are considered 'amorphous,' and in most cases little is known about their mechanisms. Amorphous aggregation of {\gamma}-crystallins in the eye lens causes a widespread disease of aging, cataract. We combined simulations and experiments to study the mechanism of aggregation of two {\gamma}D-crystallin mutants, W42R and W42Q - the former a congenital cataract mutation, and the latter a mimic of age-related oxidative damage. We found that formation of an internal disulfide was necessary and sufficient for aggregation under physiological conditions. Two-chain all-atom simulations predicted that one non-native disulfide in particular, between Cys32 and Cys41, was likely to stabilize an unfolding intermediate prone to intermolecular interactions. Mass spectrometry and mutagenesis experiments confirmed the presence of this bond in the aggregates and its necessity for oxidative aggregation under physiological conditions in vitro. Mining the simulation data linked formation of this disulfide to extrusion of the N-terminal \b{eta}-hairpin and rearrangement of the native \b{eta}-sheet topology. Specific binding between the extruded hairpin and a distal \b{eta}-sheet, in an intermolecular chain reaction similar to domain swapping, is the most probable mechanism of aggregate propagation.
q-bio.BM:Alchemical theory is emerging as a promising tool in the context of molecular dynamics simulations for drug discovery projects. In this theoretical contribution, I revisit the statistical mechanics foundation of non covalent interactions in drug-receptor systems, providing a unifying treatment that encompasses the most important variants in the alchemical approaches, from the seminal Double Annihilation Method by Jorgensen and Ravimohan [W.L. Jorgensen and C. Ravimohan, J. Chem. Phys. 83,3050, 1985], to the Gilson's Double Decoupling Method [M. K. Gilson and J. A. Given and B. L. Bush and J. A. McCammon, Biophys. J. 72, 1047 1997] and the Deng and Roux alchemical theory [Y. Deng and B. Roux, J. Chem. Theory Comput., 2, 1255 2006]. Connections and differences between the various alchemical approaches are highlighted and discussed, and finally placed into the broader context of nonequilibrium thermodynamics.
q-bio.BM:We report that protein confinement within nanoscopic vesicular compartments corresponds to a liquid-liquid phase transition with the protein/water within vesicle lumen interacting very differently than in bulk. We show this effect leads to considerable structural changes on the proteins with evidence suggesting non-alpha helical conformations. Most importantly both aspects lead to a significant improvement on protein stability against thermal denaturation up to 95degC at neutral pH, with little or no evidence of unfolding or reduced enzymatic activity. The latter parameter does indeed exhibit an increase after thermal cycling. Our results suggest that nanoscopic confinement is a promising new avenue for the enhanced long-term storage of proteins. Moreover, our investigations have potentially important implications for the origin of life, since such compartmentalization may well have been critical for ensuring the preservation of primordial functional proteins under relatively harsh conditions, thus playing a key role in the subsequent emergence of primitive life forms.
q-bio.BM:Water molecules inside G-protein coupled receptor have recently been spotlighted in a series of crystal structures. To decipher the dynamics and functional roles of internal waters in GPCR activity, we studied A$_{\text{2A}}$ adenosine receptor using $\mu$sec-molecular dynamics simulations. Our study finds that the amount of water flux across the transmembrane (TM) domain varies depending on the receptor state, and that the water molecules of the TM channel in the active state flow three times slower than those in the inactive state. Depending on the location in solvent-protein interface as well as the receptor state, the average residence time of water in each residue varies from $\sim\mathcal{O}(10^2)$ psec to $\sim\mathcal{O}(10^2)$ nsec. Especially, water molecules, exhibiting ultraslow relaxation ($\sim\mathcal{O}(10^2)$ nsec) in the active state, are found around the microswitch residues that are considered activity hotspots for GPCR function. A continuous allosteric network spanning the TM domain, arising from water-mediated contacts, is unique in the active state, underscoring the importance of slow waters in the GPCR activation.
q-bio.BM:The $\alpha$ and $\beta$ subunits comprising the hexameric assembly of F1-ATPase share a high degree of structural identity, though low primary identity. Each subunit binds nucleotide in similar pockets, yet only $\beta$ subunits are catalytically active. Why? We re-examine their internal symmetry axes and observe interesting differences. Dividing each chain into an N-terminal head region, a C-terminal foot region, and a central torso, we observe (1) that while the foot and head regions in all chains obtain high and similar mobility, the torsos obtain different mobility profiles, with the $\beta$ subunits exhibiting a higher motility compared to the $\alpha$ subunits, a trend supported by the crystallographic B-factors. The $\beta$ subunits have greater torso mobility by having fewer distributed, nonlocal packing interactions providing a spacious and soft connectivity, and offsetting the resultant softness with local stiffness elements, including an additional $\beta$ sheet. (2) A loop near the nucleotide binding-domain of the $\beta$ subunits, absent in the $\alpha$ subunits, swings to create a large variation in the occlusion of the nucleotide binding region. (3) A combination of the softest three eigenmodes significantly reduces the RMSD between the open and closed conformations of the $\beta$ subnits. (4) Comparisons of computed and observed crystallographic B-factors suggest a suppression of a particular symmetry axis in an $\alpha$ subunit. (5) Unexpectedly, the soft intra-monomer oscillations pertain to distortions that do not create inter-monomer steric clashes in the assembly, suggesting that structural optimization of the assembly evolved at all levels of complexity.
q-bio.BM:We present the open source distributed software package Poisson-Boltzmann Analytical Method (PB-AM), a fully analytical solution to the linearized Poisson Boltzmann equation, for molecules represented as non-overlapping spherical cavities. The PB-AM software package includes the generation of outputs files appropriate for visualization using VMD, a Brownian dynamics scheme that uses periodic boundary conditions to simulate dynamics, the ability to specify docking criteria, and offers two different kinetics schemes to evaluate biomolecular association rate constants. Given that PB-AM defines mutual polarization completely and accurately, it can be refactored as a many-body expansion to explore 2- and 3-body polarization. Additionally, the software has been integrated into the Adaptive Poisson-Boltzmann Solver (APBS) software package to make it more accessible to a larger group of scientists, educators and students that are more familiar with the APBS framework.
q-bio.BM:Various genome evolutionary models have been proposed these last decades to predict the evolution of a DNA sequence over time, essentially described using a mutation matrix. By essence, all of these models relate the evolution of DNA sequences to the computation of the successive powers of the mutation matrix. To make this computation possible, hypotheses are assumed for the matrix, such as symmetry and time-reversibility, which are not compatible with mutation rates that have been recently obtained experimentally on genes ura3 and can1 of the Yeast Saccharomyces cerevisiae. In this work, authors investigate systematically the possibility to relax either the symmetry or the time-reversibility hypothesis of the mutation matrix, by investigating all the possible matrices of size 2*2 and 3*3. As an application example, the experimental study on the Yeast Saccharomyces cerevisiae has been used in order to deduce a simple mutation matrix, and to compute the future evolution of the rate purine/pyrimidine for $ura3$ on the one hand, and of the particular behavior of cytosines and thymines compared to purines on the other hand.
q-bio.BM:DNA replication is a process which is common to all domains of life yet different replication mechanisms are seen among different organisms. The mechanism of replication on such a structure is not yet understood. With this bigger picture in mind, we have initiated the characterization involved in DNA replication.
q-bio.BM:Among all the proteins of Periplasmic C type cytochrome A (PPCA) family obtained from cytochrome C7 found in Geobacter sulfurreducens, PPCA protein can interact with Deoxycholate (DXCA), while its other homologs do not, as observed from the crystal structures. Utilizing the concept of 'structure-function relationship', an effort has been initiated towards understanding the driving force for recognition of DXCA exclusively by PPCA among its homologs. Further, a combinatorial analysis of the binding sequences (contiguous sequence of amino acid residues of binding locations) is performed to build graph-theoretic models, which show that PPCA differs from its homologues. Analysis of the results suggests that the underlying impetus of recognition of DXCA by PPCA is embedded in its primary sequence and 3D conformation.
q-bio.BM:Amyloid fibrillation is a protein self-assembly phenomenon that is intimately related to well-known human neurodegenerative diseases. During the past few decades, striking advances have been achieved in our understanding of the physical origin of this phenomenon and they constitute the contents of this review. Starting from a minimal model of amyloid fibrils, we explore systematically the equilibrium and kinetic aspects of amyloid fibrillation in both dilute and semi-dilute limits. We then incorporate further molecular mechanisms into the analyses. We also discuss the mathematical foundation of kinetic modeling based on chemical mass-action equations, the quantitative linkage with experimental measurements, as well as the procedure to perform global fitting.
q-bio.BM:Flexibility-rigidity index (FRI) has been developed as a robust, accurate and efficient method for macromolecular thermal fluctuation analysis and B-factor prediction. The performance of FRI depends on its formulations of rigidity index and flexibility index. In this work, we introduce alternative rigidity and flexibility formulations. The structure of the classic Gaussian surface is utilized to construct a new type of rigidity index, which leads to a new class of rigidity densities with the classic Gaussian surface as a special case. Additionally, we introduce a new type of flexibility index based on the domain indicator property of normalized rigidity density. These generalized FRI (gFRI) methods have been extensively validated by the B-factor predictions of 364 proteins. Significantly outperforming the classic Gaussian network model (GNM), gFRI is a new generation of methodologies for accurate, robust and efficient analysis of protein flexibility and fluctuation. Finally, gFRI based molecular surface generation and flexibility visualization are demonstrated.
q-bio.BM:Predicting the 3D structure of a macromolecule, such as a protein or an RNA molecule, is ranked top among the most difficult and attractive problems in bioinformatics and computational biology. Its importance comes from the relationship between the 3D structure and the function of a given protein or RNA. 3D structures also help to find the ligands of the protein, which are usually small molecules, a key step in drug design. Unfortunately, there is no shortcut to accurately obtain the 3D structure of a macromolecule. Many physical measurements of macromolecular 3D structures cannot scale up, due to their large labor costs and the requirements for lab conditions.   In recent years, computational methods have made huge progress due to advance in computation speed and machine learning methods. These methods only need the sequence information to predict 3D structures by employing various mathematical models and machine learning methods. The success of computational methods is highly dependent on a large database of the proteins and RNA with known structures.   However, the performance of computational methods are always expected to be improved. There are several reasons for this. First, we are facing, and will continue to face sparseness of data.Secondly, the 3D structure space is too large for our computational capability.   The two obstacles can be removed by knowledge-based methods, which combine knowledge learned from the known structures and biologists' knowledge of the folding process of protein or RNA. In the dissertation, I will present my results in building a knowledge-based method by using machine learning methods to tackle this problem. My methods include the knowledge constraints on intermediate states, which can highly reduce the solution space of a protein or RNA, in turn increasing the efficiency of the structure folding method and improving its accuracy.
q-bio.BM:Energy evaluation using fast Fourier transforms enables sampling billions of putative complex structures and hence revolutionized rigid protein-protein docking. However, in current methods efficient acceleration is achieved only in either the translational or the rotational subspace. Developing an efficient and accurate docking method that expands FFT based sampling to 5 rotational coordinates is an extensively studied but still unsolved problem. The algorithm presented here retains the accuracy of earlier methods but yields at least tenfold speedup. The improvement is due to two innovations. First, the search space is treated as the product manifold $\mathbf{SO(3)x(SO(3)\setminus S^1)}$, where $\mathbf{SO(3)}$ is the rotation group representing the space of the rotating ligand, and $\mathbf{(SO(3)\setminus S^1)}$ is the space spanned by the two Euler angles that define the orientation of the vector from the center of the fixed receptor toward the center of the ligand. This representation enables the use of efficient FFT methods developed for $\mathbf{SO(3)}$. Second, we select the centers of highly populated clusters of docked structures, rather than the lowest energy conformations, as predictions of the complex, and hence there is no need for very high accuracy in energy evaluation. Therefore it is sufficient to use a limited number of spherical basis functions in the Fourier space, which increases the efficiency of sampling while retaining the accuracy of docking results. A major advantage of the method is that, in contrast to classical approaches, increasing the number of correlation function terms is computationally inexpensive, which enables using complex energy functions for scoring.
q-bio.BM:Cellulosomes are complex multi-enzyme machineries which efficiently degrade plant cell-wall polysaccharides. The multiple domains of the cellulosome proteins are often tethered together by intrinsically disordered regions. The properties and functions of these disordered linkers are not well understood. In this work, we study endoglucanase Cel8A, which is a relevant enzymatic component of the cellulosomes of Clostridium thermocellum. We use both all-atom and coarse-grained simulations to investigate how the equilibrium conformations of the catalytic domain of Cel8A are affected by the disordered linker at its C terminus. We find that when the endoglucanase is bound to its substrate, the effective stiffness of the linker can influence the distances between groups of amino-acid residues throughout the entire enzymatic domain. In particular, variations in the linker stiffness can lead to small changes in the geometry of the active-site cleft. We suggest that such geometrical changes may, in turn, have an effect on the catalytic activity of the enzyme.
q-bio.BM:Native extracellular matrices (ECMs), such as those of the human brain and other neural tissues, exhibit networks of molecular interactions between specific matrix proteins and other tissue components. Guided by these naturally self-assembling supramolecular systems, we have designed a matrix-derived protein chimera that contains a laminin globular-like (LG) domain fused to an elastin-like polypeptide (ELP). All-atom, classical molecular dynamics simulations of our designed laminin-elastin fusion protein reveal temperature-dependent conformational changes, in terms of secondary structure composition, solvent accessible surface area, hydrogen bonding, and surface hydration. These properties illuminate the phase behavior of this fusion protein, via the emergence of $\beta$-sheet character in physiologically-relevant temperature ranges.
q-bio.BM:A simple theory of ion permeation through a channel is presented, in which diffusion occurs according to Fick's law and drift according to Ohm's law, in the electric field determined by all the charges present. This theory accounts for permeation in the channels studied to date in a wide range of solutions. Interestingly, the theory works because the shape of the electric field is a sensitive function of experimental conditions, e.g., ion concentration. Rate constants for flux are sensitive functions of ionic concentration because the fixed charge of the channel protein is shielded by the ions in an near it. Such shielding effects are not included in traditional theories of ionic channels, or other proteins, for that matter.
q-bio.BM:Knots in proteins have been proposed to resist proteasomal degradation. Ample evidence associates proteasomal degradation with neurodegeneration. One interesting possibility is that indeed knotted conformers stall this machinery leading to toxicity. However, although the proteasome is known to unfold mechanically its substrates, at present there are no experimental methods to emulate this particular traction geometry. Here, we consider several dynamical models of the proteasome in which the complex is represented by an effective potential with an added pulling force. This force is meant to induce translocation of a protein or a polypeptide into the catalytic chamber. The force is either constant or applied periodically. The translocated proteins are modelled in a coarse-grained fashion. We do comparative analysis of several knotted globular proteins and the transiently knotted polyglutamine tracts of length 60 alone and fused in exon 1 of the huntingtin protein. Huntingtin is associated with Huntington disease, a well-known genetically-determined neurodegenerative disease. We show that the presence of a knot hinders and sometimes even jams translocation. We demonstrate that the probability to do so depends on the protein, the model of the proteasome, the magnitude of the pulling force, and the choice of the pulled terminus. In any case, the net effect would be a hindrance in the proteasomal degradation process in the cell. This would then yield toxicity \textit{via} two different mechanisms: one through toxic monomers compromising degradation and another by the formation of toxic oligomers. Our work paves the way to the mechanistic investigation of the mechanical unfolding of knotted structures by the proteasome and its relation to toxicity and disease.
q-bio.BM:The influenza pandemic of 1918-1919 killed at least 50 million people. The reasons why this pandemic was so deadly remain largely unknown. However, It has been shown that the 1918 viral hemagglutinin allows to reproduce the hallmarks of the illness observed during the original pandemic. Thanks to the wealth of hemagglutinin sequences accumulated over the last decades, amino-acid substitutions that are found in the 1918-1919 sequences but rare otherwise can be identified with high confidence. Such an analysis reveals that Gly188, which is located within a key motif of the receptor binding site, is so rarely found in hemagglutinin sequences that, taken alone, it is likely to be deleterious. Monitoring this singular mutation in viral sequences may help prevent another dramatic pandemic.
q-bio.BM:Summary: Protein quality assessment is a long-standing problem in bioinformatics. For more than a decade we have developed state-of-art predictors by carefully selecting and optimising inputs to a machine learning method. The correlation has increased from 0.60 in ProQ to 0.81 in ProQ2 and 0.85 in ProQ3 mainly by adding a large set of carefully tuned descriptions of a protein. Here, we show that a substantial improvement can be obtained using exactly the same inputs as in ProQ2 or ProQ3 but replacing the support vector machine by a deep neural network. This improves the Pearson correlation to 0.90 (0.85 using ProQ2 input features).   Availability: ProQ3D is freely available both as a webserver and a stand-alone program at http://proq3.bioinfo.se/
q-bio.BM:We have performed all atom molecular dynamics simulations on the projection domain of the intrinsically disordered htau40 protein. After generating a suitable ensemble of starting conformations at high temperatures, at room temperature in an adaptive box algorithm we have generated histograms for the radius of gyration, secondary structure time series, generated model small angle x-ray scattering intensities, and model chemical shift plots for comparison to nuclear magnetic resonance data for solvated and filamentous tau. Significantly, we find that the chemical shift spectrum is more consistent with filamentous tau than full length solution based tau. We have also carried out principle component analysis and find three basics groups: compact globules, tadpoles, and extended hinging structures. To validate the adaptive box and our force field choice, we have run limited simulations in a large conventional box with varying force fields and find that our essential results are unchanged. We also performed two simulations with the TIP4P-D water model, the effects of which depended on whether the initial configuration was compact or extended.
q-bio.BM:Previous kinetic models had assumed that the reaction medium was reacting at random and without a turnover associated to thermodynamics exchanges, with a rigid active site on the enzyme. The experimental studies show that coupling factor 1 (CF1) from spinach chloroplasts has latent ATPase activity, which become expressed after heat-treatment and incubation with calcium. The sigmoidal kinetics observed on the competitive effect of glycerol on water saturating a protein, suggests that the role of the hydration shell in the catalytic mechanism of the CF1-ATPase, modify the number of water molecules associated with the conformational turnover required for active site activity. It is assume that the water associated to the hydrophilic state of the enzyme produces a fit-in of the substrate to form (ES), follow by the catalytic action with product formation (EP). This one induces the dissociation of water and increases the hydrophobic attractions between R-groups. The latter, becomes the form of the enzyme interacting with water to form the dissociated free enzyme (E) and free product (P). Glycerol dependent suppression of the water dynamics on two interacting sites configuration shows a change in the H-bond-configuration. The thermodynamics modeling requires an energy expenditure of about 4kcal/mol per each H-bond in turnover. Glycerol determined a turnover of 14 molecules of water released from the active sites to reach the inactive form of the enzyme. The entropy generated by turnover of the fit-in and -out substrate and product from the protein could be dissipated-out of the enzyme-water system itself. Coupling with the surrounding water clusters allows to recreate H-bonds. This should involve a decrease in the number of H-bonds present in the clusters. These changes in the mass action capability of the water clusters could eventually become dissipated through a cooling effect.
q-bio.BM:Summary: We introduce RBPBind, a web-based tool for the quantitative prediction of RNA-protein interactions. Given a user-specified RNA and a protein selected from a set of several common RNA-binding proteins, RBPBind computes the binding curve and effective binding constant of the reaction in question. The server also computes the probability that, at a given concentration of protein, a protein molecule will bind to any particular nucleotide along the RNA. The software for RBPBind is adapted from the Vienna RNA package, whose source code has been modified to accommodate the effects of single stranded RNA binding proteins. RBPBind thus fully incorporates the effect of RNA secondary structure on protein-RNA interactions. Availability: Our web server is available at http://bioserv.mps.ohio-state.edu/RBPBind
q-bio.BM:Bacterial mobility is powered by rotation of helical flagellar filaments driven by rotary motors. Flagellin isolated from {\it Salmonella Typhimurium} SJW1660 strain, which differs by a point mutation from the wild-type strain, assembles into straight filaments in which flagellin monomers are arranged into left-handed helix. Using small-angle X-ray scattering (SAXS) and osmotic stress methods, we investigated the high-resolution structure of SJW1660 flagellar filaments as well as intermolecular forces that govern their assembly into dense hexagonal bundle. The scattering data were fitted to high-resolution models, which took into account the atomic structure of the flagellin subunits. The analysis revealed the exact helical arrangement and the super-helical twist of the flagellin subunits within the filaments. Under osmotic stress the filaments formed $2D$ hexagonal bundles. Monte-Carlo simulations and continuum theories were used to analyze the scattering data from hexagonal arrays, revealing how bulk modulus, as well as how the deflection length depends on the applied osmotic stress. Scattering data from aligned flagellar bundles confirmed the predicated structure-factor scattering peak line-shape. Quantitative analysis of the measured equation of state of the bundles revealed the contributions of the electrostatic, hydration, and elastic interactions to the intermolecular interactions associated with bundling of straight semi-flexible flagellar filaments.}%1 {Insert Received for publication Date and in final form Date.
q-bio.BM:Intrinsically disordered proteins (IDPs) and proteins with intrinsically disordered regions (IDRs) govern a daunting number of physiological processes. For such proteins, molecular mechanisms governing their interactions with proteins involved in signal transduction pathways remain unclear. Using the folded, calcium-loaded calmodulin (CaM) interaction with the calcineurin regulatory IDP as a prototype for IDP-mediated signal transduction events, we uncover the interplay of IDP structure and electrostatic interactions in determining the kinetics of protein-protein association. Using an array of biophysical approaches including stopped-flow and computational simulation, we quantify the relative contributions of electrostatic interactions and conformational ensemble characteristics in determining association kinetics of calmodulin (CaM) and the calcineurin regulatory domain (CaN RD). Our chief findings are that CaM/CN RD association rates are strongly dependent on ionic strength and that observed rates are largely determined by the electrostatically-driven interaction strength between CaM and the narrow CaN RD calmodulin recognition domain. These studies highlight a molecular mechanism of controlling signal transduction kinetics that may be utilized in wide-ranging signaling cascades that involve IDPs.
q-bio.BM:The molecular mechanism of ion channel gating and substrate modulation is elusive for many voltage gated ion channels, such as eukaryotic sodium ones. The understanding of channel functions is a pressing issue in molecular biophysics and biology. Mathematical modeling, computation and analysis of membrane channel charge transport have become an emergent field and give rise to significant contributions to our understanding of ion channel gating and function. This review summarizes recent progresses and outlines remaining challenges in mathematical modeling, simulation and analysis of ion channel charge transport. One of our focuses is the Poisson-Nernst-Planck (PNP) model and its generalizations. Specifically, the basic framework of the PNP system and some of its extensions, including size effects, ion-water interactions, coupling with density functional theory and relation to fluid flow models. A reduced theory, the Poisson- Boltzmann-Nernst-Planck (PBNP) model, and a differential geometry based ion transport model are also discussed. For proton channel, a multiscale and multiphysics Poisson-Boltzmann-Kohn-Sham (PBKS) model is presented. We show that all of these ion channel models can be cast into a unified variational multiscale framework with a macroscopic continuum domain of the solvent and a microscopic discrete domain of the solute. The main strategy is to construct a total energy functional of a charge transport system to encompass the polar and nonpolar free energies of solvation and chemical potential related energies. Current computational algorithms and tools for numerical simulations and results from mathematical analysis of ion channel systems are also surveyed.
q-bio.BM:A pattern Recognition of a probability distribution of amino acids is obtained for selected families of proteins. The mathematical model is derived from a theory of protein families formation which is derived from application of a Pauli's master equation method.
q-bio.BM:In this paper we propose a straightforward operational definition of variants of disordered proteins, taking the human proteome as a case study. The focus is on a distinction between mostly unstructured proteins and proteins which contain long unstructured regions accommodated in an overall folded structure. In particular we distinguish: i) Not disordered proteins (NDPs), that either have all their residues ordered or do not have disordered segments longer than 30 residues nor more than 30% of disordered residues; ii) Proteins with intrinsically disordered regions (IDRPs), that have at least one disordered domain longer than 30 residues, but disordered in less than 30% of their residues; iii) Proteins that are intrinsically disordered (IDPs), that have both at least one disordered segment longer than 30 residues and that are disordered on more than 30% of their residues; iv) Proteins with fragmented disorder (FRAG_IDPs), that do not have a disordered fragment longer than 30 residues but that, nevertheless, have at least 30% or more of their residues predicted as disordered. The potential use of these variants is checked over several groups of disease-related proteins. Our main conclusions point out that IDRPs are more similar to NDPs than to IDPs. IDRPs and NDPs have a similar functional repertoire and probably share a lock-and-key mechanism of interaction with substrates. IDRPs and IDPs are differently present among human disease-related proteins. IDRPs probably do not play a specific role in the development of complex diseases, since their frequency is similar in disease-related proteins and in the entire human proteome. IDPs can play a role in the emergence of cancer, neurodegenerative, thyroid and liver disease.
q-bio.BM:Characterization of B-cell protein epitope and developing critical parameters for its identification is one of the long standing interests. Using Layers algorithm, we introduced the concept of anchor residues to identify epitope. We have shown that majority of the epitope is composed of anchor residues and have significant bias in epitope for these residues. We optimized the search space reduction for epitope identification. We used Layers to non-randomly sample the antigen surface reducing the molecular surface to an average of 75 residues while preserving 50% of the epitope in the sample surface. To facilitate the comparison of favorite methods of researchers we compared the popular techniques used to identify epitope with their sampling performance and evaluation. We proposed an optimum Sr of 16 {\AA} to sample the antigen molecules to reduce the search space, in which epitope is identified using buried surface area method. We used the combinations of molecular surface sampling, anchor residue intensity in surface, secondary structure and sequence information to predict epitope at an accuracy of 89%. A web application is made available at http://www.csb.iitkgp.ernet.in/applications/b_cell_epitope_pred/main.
q-bio.BM:N-methyl-D-aspartate receptors (NMDARs) are glycoproteins in the brain central to learning and memory. The effects of glycosylation on the structure and dynamics of NMDARs are largely unknown. In this work, we use extensive molecular dynamics simulations of GluN1 and GluN2B ligand binding domains (LBDs) of NMDARs to investigate these effects. Our simulations predict that intra-domain interactions involving the glycan attached to residue GluN1-N440 stabilize closed-clamshell conformations of the GluN1 LBD. The glycan on GluN2B-N688 shows a similar, though weaker, effect. Based on these results, and assuming the transferability of the results of LBD simulations to the full receptor, we predict that glycans at GluN1-N440 might play a potentiator role in NMDARs. To validate this prediction, we perform electrophysiological analysis of full-length NMDARs with a glycosylation-preventing GluN1-N440Q mutation, and demonstrate an increase in the glycine EC50 value. Overall, our results suggest an intramolecular potentiating role of glycans on NMDA receptors.
q-bio.BM:In this paper, we introduce multiscale persistent functions for biomolecular structure characterization. The essential idea is to combine our multiscale rigidity functions with persistent homology analysis, so as to construct a series of multiscale persistent functions, particularly multiscale persistent entropies, for structure characterization. To clarify the fundamental idea of our method, the multiscale persistent entropy model is discussed in great detail. Mathematically, unlike the previous persistent entropy or topological entropy, a special resolution parameter is incorporated into our model. Various scales can be achieved by tuning its value. Physically, our multiscale persistent entropy can be used in conformation entropy evaluation. More specifically, it is found that our method incorporates in it a natural classification scheme. This is achieved through a density filtration of a multiscale rigidity function built from bond and/or dihedral angle distributions. To further validate our model, a systematical comparison with the traditional entropy evaluation model is done. It is found that our model is able to preserve the intrinsic topological features of biomolecular data much better than traditional approaches, particularly for resolutions in the mediate range. Moreover, our method can be successfully used in protein classification. For a test database with around nine hundred proteins, a clear separation between all-alpha and all-beta proteins can be achieved, using only the dihedral and pseudo-bond angle information. Finally, a special protein structure index (PSI) is proposed, for the first time, to describe the "regularity" of protein structures. Essentially, PSI can be used to describe the "regularity" information in any systems.
q-bio.BM:Using a structure-based coarse-grained model of proteins, we study the mechanism of unfolding of knotted proteins through heating. We find that the dominant mechanisms of unfolding depend on the temperature applied and are generally distinct from those identified for folding at its optimal temperature. In particular, for shallowly knotted proteins, folding usually involves formation of two loops whereas unfolding through high-temperature heating is dominated by untying of single loops. Untying the knots is found to generally precede unfolding unless the protein is deeply knotted and the heating temperature exceeds a threshold value. We then use a phenomenological model of the air-water interface to show that such an interface can untie shallow knots, but it can also make knots in proteins that are natively unknotted.
q-bio.BM:Identifying protein functional sites (PFSs) and protein-ligand interactions (PLIs) are critically important in understanding the protein function and the involved biochemical reactions. As large amount of unknown proteins are quickly accumulated in this post-genome era, an urgent task arises to predict PFSs and PLIs at residual level. Nowadays many knowledge-based methods have been well developed for prediction of PFSs, however, accurate methods for PLI prediction are still lacking. In this study, we have presented a new method for prediction of PLIs and PFSs based on sequence of the inquiry protein. The key of the method hinges on a function- and interaction-annotated protein domain profile database, called fiDPD, which was built from the Structural Classification of Proteins (SCOP) database, using a hidden Markov model program. The method was applied to 13 target proteins from the recent Critical Assessment of Structure Prediction (CASP10/11). Our calculations gave a Matthews correlation coefficient (MCC) value of 0.66 for prediction of PFSs, and an 80% recall in prediction of PLIs. Our method reveals that PLIs are conserved during the evolution of proteins, and they can be reliably predicted from fiDPD. fiDPD can be used as a complement to existent bioinformatics tools for protein function annotation.
q-bio.BM:The key finding in the DNA double helix model is the specific pairing or binding between nucleotides A-T and C-G, and the pairing rules are the molecule basis of genetic code. Unfortunately, no such rules have been discovered for proteins. Here we show that similar rules and intrinsic sequence patterns between intra-protein binding peptide fragments do exist, and they can be extracted using a deep learning algorithm. Multi-millions of binding and non-binding peptide fragments from currently available protein X-ray structures are classified with an accuracy of up to 93%. This discovery has the potential in helping solve protein folding and protein-protein interaction problems, two open and fundamental problems in molecular biology.
q-bio.BM:Proteins have evolved to perform diverse cellular functions, from serving as reaction catalysts to coordinating cellular propagation and development. Frequently, proteins do not exert their full potential as monomers but rather undergo concerted interactions as either homo-oligomers or with other proteins as hetero-oligomers. The experimental study of such protein complexes and interactions has been arduous. Theoretical structure prediction methods are an attractive alternative. Here, we investigate homo-oligomeric interfaces by tracing residue coevolution via the global statistical Direct Coupling Analysis (DCA). DCA can accurately infer spatial adjacencies between residues. These adjacencies can be included as constraints in structure-prediction techniques to predict high-resolution models. By taking advantage of the on-going exponential growth of sequence databases, we go significantly beyond anecdotal cases of a few protein families and apply DCA to a systematic large-scale study of nearly 2000 PFAM protein families with sufficient sequence information and structurally resolved homo-oligomeric interfaces. We find that large interfaces are commonly identified by DCA. We further demonstrate that DCA can differentiate between subfamilies of different binding modes within one large PFAM family. Sequence derived contact information for the subfamilies proves sufficient to assemble accurate structural models of the diverse protein-oligomers. Thus, we provide an approach to investigate oligomerization for arbitrary protein families leading to structural models complementary to often difficult experimental methods. Combined with ever more abundant sequential data, we anticipate that this study will be instrumental to allow the structural description of many hetero-protein complexes in the future.
q-bio.BM:Liquid-liquid phase separation of intrinsically disordered proteins (IDPs) is a major undergirding factor in the regulated formation of membraneless organelles in the cell. The phase behavior of an IDP is sensitive to its amino acid sequence. Here we apply a recent random-phase-approximation polymer theory to investigate how the tendency for multiple chains of a protein to phase separate, as characterized by the critical temperature $T^*_{\rm cr}$, is related to the protein's single-chain average radius of gyration $\langle R_{\rm g} \rangle$. For a set of sequences containing different permutations of an equal number of positively and negatively charged residues, we found a striking correlation $T^*_{\rm cr}\sim \langle R_{\rm g} \rangle^{-\gamma}$ with $\gamma$ as large as $\sim 6.0$, indicating that electrostatic effects have similarly significant impact on promoting single-chain conformational compactness and phase separation. Moreover, $T^*_{\rm cr}\propto -{\rm SCD}$, where SCD is a recently proposed "sequence charge decoration" parameter determined solely by sequence information. Ramifications of our findings for deciphering the sequence dependence of IDP phase separation are discussed.
q-bio.BM:Natural protein sequences contain a record of their history. A common constraint in a given protein family is the ability to fold to specific structures, and it has been shown possible to infer the main native ensemble by analyzing covariations in extant sequences. Still, many natural proteins that fold into the same structural topology show different stabilization energies, and these are often related to their physiological behavior. We propose a description for the energetic variation given by sequence modifications in repeat proteins, systems for which the overall problem is simplified by their inherent symmetry. We explicitly account for single amino acid and pair-wise interactions and treat higher order correlations with a single term. We show that the resulting force field can be interpreted with structural detail. We trace the variations in the energetic scores of natural proteins and relate them to their experimental characterization. The resulting energetic force field allows the prediction of the folding free energy change for several mutants, and can be used to generate synthetic sequences that are statistically indistinguishable from the natural counterparts.
q-bio.BM:Herein (the first part of my work), I debunk the long-standing hypotheses that explain mitochondrial oxidative phosphorylation. Simple calculations point out that mitochondria are highly proton-deficient microcosms and therefore, elaborate proton pump machinery are not tenable. Further, other elements like the elaborate electron transport chain, chemiosmosis, rotary ATP synthesis, etc. are also critically evaluated to point out that such complicated systems are non-viable. The communication necessitates a new explanatory paradigm for cellular respiration. In the second part of my work, I have put forward a viable alternative explanatory paradigm for mitochondrial oxidative phosphorylation.
q-bio.BM:Deciphering the links between amino acid sequence and amyloid fibril formation is key for understanding protein misfolding diseases. Here we use Monte Carlo simulations to study aggregation of short peptides in a coarse-grained model with hydrophobic-polar (HP) amino acid sequences and correlated side chain orientations for hydrophobic contacts. A significant heterogeneity is observed in the aggregate structures and in the thermodynamics of aggregation for systems of different HP sequences and different number of peptides. Fibril-like ordered aggregates are found for several sequences that contain the common HPH pattern while other sequences may form helix bundles or disordered aggregates. A wide variation of the aggregation transition temperatures among sequences, even among those of the same hydrophobic fraction, indicates that not all sequences undergo aggregation at a presumable physiological temperature. The transition is found to be the most cooperative for sequences forming fibril-like structures. For a fibril-prone sequence, it is shown that fibril formation follows the nucleation and growth mechanism. Interestingly, a binary mixture of peptides of an aggregation-prone and a non-aggregation-prone sequence shows association and conversion of the latter to the fibrillar structure. Our study highlights the role of sequence in selecting fibril-like aggregates and also the impact of structural template on fibril formation by peptides of unrelated sequences.
q-bio.BM:Collapsin response mediator protein CRMP2 (gene: DPYSL2) is crucial for neuronal development. The homotetrameric CRMP2 complex is regulated via two mechanisms, first by phosphorylation at, and second by reduction and oxidation of the Cys504 residues of two adjacent subunits. Here, we analyzed the effects of this redox switch on the protein in vitro combined with force field molecular dynamics (MD). Earlier X-ray data contain the structure of the rigid body of the molecule but lack the flexible C-terminus with the important sites for phosphorylation and redox regulation. An in silico model for this part was established by replica exchange simulations and homology modelling, which is consistent with results gained from CD spectroscopy with recombinant protein. Thermofluor data indicated that the protein aggregates at bivalent ion concentrations below 200 mM. In simulations the protein surface was covered at these conditions by large amounts of ions, which most likely prevent aggregation. A tryptophan residue (Trp295) in close proximity to the forming disulfide allowed the measurement of the structural relaxation of the rigid body upon reduction by fluorescent quenching. We were also able to determine the second order rate constant of CRMP2 oxidation by H2O2. The simulated solvent accessible surface of the hydroxyl group of Ser518 significantly increased upon reduction of the disulfide bond. Our results give first detailed insight in the profound structural changes of the tetrameric CRMP2 due to oxidation and indicate a tightly connected regulation by phosphorylation and redox modification.
q-bio.BM:Because of the limitations of classical silicon based computational technology, several alternatives to traditional method in form of unconventional computing have been proposed. In this paper we will focus on DNA computing which is showing the possibility of excellence for its massive parallelism, potential for information storage, speed and energy efficiency. In this paper we will describe how syllogistic reasoning by DNA tweezers can be presented by the semantics of process calculus and DNA strand graph. Syllogism is an essential ingredient for commonsense reasoning of an individual. This paper enlightens the procedure to deduce a precise conclusion from a set of propositions by using formal language theory in form of process calculus and the expressive power of DNA strand graph.
q-bio.BM:Recent experiments and simulations have demonstrated that proteins can fold on the ribosome. However, the extent and generality of fitness effects resulting from co-translational folding remain open questions. Here we report a genome-wide analysis that uncovers evidence of evolutionary selection for co-translational folding. We describe a robust statistical approach to identify loci within genes that are both significantly enriched in slowly translated codons and evolutionarily conserved. Surprisingly, we find that domain boundaries can explain only a small fraction of these conserved loci. Instead, we propose that regions enriched in slowly translated codons are associated with co-translational folding intermediates, which may be smaller than a single domain. We show that the intermediates predicted by a native-centric model of co-translational folding account for the majority of these loci across more than 500 E. coli proteins. By making a direct connection to protein folding, this analysis provides strong evidence that many synonymous substitutions have been selected to optimize translation rates at specific locations within genes. More generally, our results indicate that kinetics, and not just thermodynamics, can significantly alter the efficiency of self-assembly in a biological context.
q-bio.BM:A free energy landscape estimation-method based on Bayesian inference is presented and used for comparing the efficiency of thermally enhanced sampling methods with respect to regular molecular dynamics, where the simulations are carried out on two binding states of calmodulin. The proposed free energy estimation method (the GM method) is compared to other estimators using a toy model showing that the GM method provides a robust estimate not subject to overfitting. The continuous nature of the GM method, as well as predictive inference on the number of basis functions, provide better estimates on sparse data. We find that the free energy diffusion proper- ties determine sampling method effectiveness, such that the diffusion dominated apo-calmodulin is most efficiently sampled by regular molecular dynamics, while the holo with its rugged free energy landscape is better sampled by enhanced methods.
q-bio.BM:Transcription is regulated through binding factors to gene promoters to activate or repress expression, however, the mechanisms by which factors find targets remain unclear. Using single-molecule fluorescence microscopy, we determined in vivo stoichiometry and spatiotemporal dynamics of a GFP tagged repressor, Mig1, from a paradigm signaling pathway of Saccharomyces cerevisiae. We find the repressor operates in clusters, which upon extracellular signal detection, translocate from the cytoplasm, bind to nuclear targets and turnover. Simulations of Mig1 configuration within a 3D yeast genome model combined with a promoter-specific, fluorescent translation reporter confirmed clusters are the functional unit of gene regulation. In vitro and structural analysis on reconstituted Mig1 suggests that clusters are stabilized by depletion forces between intrinsically disordered sequences. We observed similar clusters of a co-regulatory activator from a different pathway, supporting a generalized cluster model for transcription factors that reduces promoter search times through intersegment transfer while stabilizing gene expression.
q-bio.BM:The ionic environment of biomolecules strongly influences their structure, conformational stability, and inter-molecular interactions.This paper introduces GIBS, a grand-canonical Monte Carlo (GCMC) simulation program for computing the thermodynamic properties of ion solutions and their distributions around biomolecules. This software implements algorithms that automate the excess chemical potential calculations for a given target salt concentration. GIBS uses a cavity-bias algorithm to achieve high sampling acceptance rates for inserting ions and solvent hard spheres in simulating dense ionic systems. In the current version, ion-ion interactions are described using Coulomb, hard-sphere, or Lennard-Jones (L-J) potentials; solvent-ion interactions are described using hard-sphere, L-J and attractive square-well potentials; and, solvent-solvent interactions are described using hard-sphere repulsions. This paper and the software package includes examples of using GIBS to compute the ion excess chemical potentials and mean activity coefficients of sodium chloride as well as to compute the cylindrical radial distribution functions of monovalent (Na$^+$, Rb$^+$), divalent (Sr$^{2+}$), and trivalent (CoHex$^{3+}$) around fixed all-atom models of 25 base-pair nucleic acid duplexes. GIBS is written in C++ and is freely available community use; it can be downloaded at https://github.com/Electrostatics/GIBS.
q-bio.BM:We used a microfluidic platform to address the problems of obtaining diffraction quality crystals and crystal handling during transfer to the X-ray diffractometer. We optimize crystallization conditions of a pharmaceutical protein and collect X-ray data both in situ and ex situ.
q-bio.BM:Since there is now a growing wish by referees to judge the underpinning data for a submitted article it is timely to provide a summary of the data evaluation checks required to be done by a referee. As these checks will vary from field to field this article focuses on the needs of biological X-ray crystallography articles, which is the predominantly used method leading to depositions in the PDB. The expected referee checks of data underpinning an article are described with examples. These checks necessarily include that a referee checks the PDB validation report for each crystal structure accompanying the article submission; this check whilst necessary is not sufficient for a complete evaluation. A referee would be expected to undertake one cycle of model refinement of the authors biological macromolecule coordinates against the authors processed diffraction data and look at the various validation checks of the model and Fo-Fc electron density maps in e.g. Phenix_refine and in COOT. If the referee deems necessary the diffraction data images should be reprocessed (e.g. to a different diffraction resolution than the authors submission). This can be requested to be done by the authors or if the referee prefers can be undertaken directly by the referee themselves. A referee wishing to do these data checks may wish to receive a certificate that they have command of these data science skills. The organisation of such voluntary certification training can e.g. be via those crystallography associations duly recognised by the IUCr to issue such certificates.
q-bio.BM:A mathematico-physically valid formulation is required to infer properties of disordered protein conformations from single-molecule F\"orster resonance energy transfer (smFRET). Conformational dimensions inferred by conventional approaches that presume a homogeneous conformational ensemble can be unphysical. When all possible---heterogeneous as well as homogeneous---conformational distributions are taken into account without prejudgement, a single value of average transfer efficiency $\langle E\rangle$ between dyes at two chain ends is generally consistent with highly diverse, multiple values of the average radius of gyration $\langle R_{\rm g}\rangle$. Here we utilize unbiased conformational statistics from a coarse-grained explicit-chain model to establish a general logical framework to quantify this fundamental ambiguity in smFRET inference. As an application, we address the long-standing controversy regarding the denaturant dependence of $\langle R_{\rm g}\rangle$ of unfolded proteins, focusing on Protein L as an example. Conventional smFRET inference concluded that $\langle R_{\rm g}\rangle$ of unfolded Protein L is highly sensitive to [GuHCl], but data from small-angle X-ray scattering (SAXS) suggested a near-constant $\langle R_{\rm g}\rangle$ irrespective of [GuHCl]. Strikingly, the present analysis indicates that although the reported $\langle E\rangle$ values for Protein L at [GuHCl] = 1 M and 7 M are very different at 0.75 and 0.45, respectively, the Bayesian $R^2_{\rm g}$ distributions consistent with these two $\langle E\rangle$ values overlap by as much as $75\%$. Our findings suggest, in general, that the smFRET-SAXS discrepancy regarding unfolded protein dimensions likely arise from highly heterogeneous conformational ensembles at low or zero denaturant, and that additional experimental probes are needed to ascertain the nature of this heterogeneity.
q-bio.BM:At 49 C erythrocytes undergo morphological changes due to an internal force, but the origin of the force that drives changes is not clear. Here we point out that our recent experiments on thermally induced force-release in hemoglobin can provide an explanation for the morphological changes of erythrocytes.
q-bio.BM:Proteins perform their functions usually by interacting with other proteins. Predicting which proteins interact is a fundamental problem. Experimental methods are slow, expensive, and have a high rate of error. Many computational methods have been proposed among which sequence-based ones are very promising. However, so far no such method is able to predict effectively the entire human interactome: they require too much time or memory. We present SPRINT (Scoring PRotein INTeractions), a new sequence-based algorithm and tool for predicting protein-protein interactions. We comprehensively compare SPRINT with state-of-the-art programs on seven most reliable human PPI datasets and show that it is more accurate while running orders of magnitude faster and using very little memory. SPRINT is the only program that can predict the entire human interactome. Our goal is to transform the very challenging problem of predicting the entire human interactome into a routine task. The source code of SPRINT is freely available from github.com/lucian-ilie/SPRINT/ and the datasets and predicted PPIs from www.csd.uwo.ca/faculty/ilie/SPRINT/.
q-bio.BM:Sex hormone-binding globulin (SHBG) is a binding protein that regulates availability of steroids hormones in the plasma. Although best known as steroid carrier, studies have associated SHBG in modulating behavioral aspects related to sexual receptivity. Among steroids, estradiol (17\b{eta}-estradiol, oestradiol or E2) is well recognized as the most active endogenous female hormone, exerting important roles in reproductive and nonreproductive functions. Thus, in this study we aimed to employ molecular dynamics (MD) and docking techniques for quantifying the interaction energy between a complex aqueous solution, composed by different salts, SHBG and E2. Due to glucose concentration resembles those observed in diabetic levels, special emphasis was devoted to uncover the main consequences of this carbohydrate on the SHBG and E2 molecules. We also examined possible energetic changes due to solution on the binding energy of SHBG-E2 complex. In this framework, our calculations uncovered a remarkable interaction energy between glucose and SHBG surface. Surprisingly, we also observed solute components movement toward SHBG yielding clusters surrounding the protein. This finding, corroborated by the higher energy and shorter distance found between glucose and SHBG, suggests a scenario in favor of a detainment state. In addition, in spite of protein superficial area increment it does not exerted modification on binding site area nor over binding energy SHBG-E2 complex. Finally, our calculations also highlighted an interaction between E2 and glucose when the hormone was immersed in the solution. In summary, our findings contribute for a better comprehension of both SHBG and E2 interplay with aqueous solution components.
q-bio.BM:Atomic radii and charges are two major parameters used in implicit solvent electrostatics and energy calculations. The optimization problem for charges and radii is under-determined, leading to uncertainty in the values of these parameters and in the results of solvation energy calculations using these parameters. This paper presents a new method for quantifying this uncertainty in implicit solvation calculations of small molecules using surrogate models based on generalized polynomial chaos (gPC) expansions. There are relatively few atom types used to specify radii parameters in implicit solvation calculations; therefore, surrogate models for these low-dimensional spaces could be constructed using least-squares fitting. However, there are many more types of atomic charges; therefore, construction of surrogate models for the charge parameter space requires compressed sensing combined with an iterative rotation method to enhance problem sparsity. We demonstrate the application of the method by presenting results for the uncertainties in small molecule solvation energies based on these approaches. The method presented in this paper is a promising approach for efficiently quantifying uncertainty in a wide range of force field parameterization problems, including those beyond continuum solvation calculations.The intent of this study is to provide a way for developers of implicit solvent model parameter sets to understand the sensitivity of their target properties (solvation energy) on underlying choices for solute radius and charge parameters.
q-bio.BM:The Adaptive Poisson-Boltzmann Solver (APBS) software was developed to solve the equations of continuum electrostatics for large biomolecular assemblages that has provided impact in the study of a broad range of chemical, biological, and biomedical applications. APBS addresses three key technology challenges for understanding solvation and electrostatics in biomedical applications: accurate and efficient models for biomolecular solvation and electrostatics, robust and scalable software for applying those theories to biomolecular systems, and mechanisms for sharing and analyzing biomolecular electrostatics data in the scientific community. To address new research applications and advancing computational capabilities, we have continually updated APBS and its suite of accompanying software since its release in 2001. In this manuscript, we discuss the models and capabilities that have recently been implemented within the APBS software package including: a Poisson-Boltzmann analytical and a semi-analytical solver, an optimized boundary element solver, a geometry-based geometric flow solvation model, a graph theory based algorithm for determining p$K_a$ values, and an improved web-based visualization tool for viewing electrostatics.
q-bio.BM:We present a parameterized analytical model of alchemical molecular binding. The model describes accurately the free energy profiles of linear single-decoupling alchemical binding free energy calculations. The parameters of the model, which are physically motivated, are obtained by fitting model predictions to numerical simulations. The validity of the model has been assessed on a set of host-guest complexes. The model faithfully reproduces both the binding free energy profiles and the probability densities of the perturbation energy as a function of the alchemical progress parameter $\lambda$. The model offers a rationalization for the characteristic shape of the free energy profiles. The parameters obtained from the model are potentially useful descriptors of the association equilibrium of molecular complexes.
q-bio.BM:Many biologically important ligands of proteins are large, flexible, and often charged molecules that bind to extended regions on the protein surface. It is infeasible or expensive to locate such ligands on proteins with standard methods such as docking or molecular dynamics (MD) simulation. The alternative approach proposed here is the scanning of a spatial and angular grid around the protein with smaller fragments of the large ligand. Energy values for complete grids can be computed efficiently with a well-known Fast Fourier Transform accelerated algorithm and a physically meaningful interaction model. We show that the approach can readily incorporate flexibility of protein and ligand. The energy grids (EGs) resulting from the ligand fragment scans can be transformed into probability distributions, and then directly compared to probability distributions estimated from MD simulations and experimental structural data. We test the approach on a diverse set of complexes between proteins and large, flexible ligands, including a complex of Sonic Hedgehog protein and heparin, three heparin sulfate substrates or non-substrates of an epimerase, a multi-branched supramolecular ligand that stabilizes a protein-peptide complex, and a flexible zwitterionic ligand that binds to a surface basin of a Kringle domain. In all cases the EG approach gives results that are in good agreement with experimental data or MD simulations.
q-bio.BM:We use a coarse-grained model to study the conformational changes in two barley proteins, LTP1 and its ligand adduct isoform LTP1b, that result from their adsorption to the air-water interface. The model introduces the interface through hydropathy indices. We justify the model by all-atom simulations. The choice of the proteins is motivated by making attempts to understand formation and stability of foam in beer. We demonstrate that both proteins flatten out at the interface and can make a continuous stabilizing and denser film. We show that the degree of the flattening depends on the protein -- the layers of LTP1b should be denser than those of LTP1 -- and on the presence of glycation. It also depends on the number ($\le 4$) of the disulfide bonds in the proteins. The geometry of the proteins is sensitive to the specificity of the absent bonds. We provide estimates of the volume of cavities of the proteins when away from the interface.
q-bio.BM:We consider multi-chain protein native structures and propose a criterion that determines whether two chains in the system are entangled or not. The criterion is based on the behavior observed by pulling at both temini of each chain simultaneously in the two chains. We have identified about 900 entangled systems in the Protein Data Bank and provided a more detailed analysis for several of them. We argue that entanglement enhances the thermodynamic stability of the system but it may have other functions: burying the hydrophobic residues at the interface, and increasing the DNA or RNA binding area. We also study the folding and stretching properties of the knotted dimeric proteins MJ0366, YibK and bacteriophytochrome. These proteins have been studied theoretically in their monomeric versions so far. The dimers are seen to separate on stretching through the tensile mechanism and the characteristic unraveling force depends on the pulling direction.
q-bio.BM:DDX3X is a human DEAD-box RNA helicase implicated in many cancers and in viral progression. In addition to the RecA-like catalytic core, DDX3X contains N- and C-terminal domains. Here, we investigate the substrate and protein requirements to support the ATPase activity of a DDX3X construct lacking 80 residues from its C-terminal domain. Our data confirmed previous results that for an RNA molecule to support the ATPase activity of DDX3X it must contain a single-stranded-double-stranded region. We investigated protein and RNA structural reasons for this requirement. First, the RNA substrates consisting only of a double-helix were unable to support DDX3X binding. A single-stranded RNA substrate supported DDX3X binding, while an RNA substrate consisting of a single-stranded-double-stranded region not only supported the binding of DDX3X to RNA, but also promoted DDX3X trimer formation. Thus, the single-stranded-double-stranded RNA region is needed for DDX3X trimer formation, and trimer formation is required for ATPase activity. Interestingly, the dependence of ATP hydrolysis on the protein concentration suggests that the DDX3X trimer hydrolyzes only two molecules of ATP. Lastly, a DNA substrate that contains single-stranded-double-stranded regions does not support the ATPase activity of DDX3X.
q-bio.BM:RNA secondary structure folding kinetics is known to be important for the biological function of certain processes, such as the hok/sok system in E. coli. Although linear algebra provides an exact computational solution of secondary structure folding kinetics with respect to the Turner energy model for tiny (~ 20 nt) RNA sequences, the folding kinetics for larger sequences can only be approximated by binning structures into macrostates in a coarse-grained model, or by repeatedly simulating secondary structure folding with either the Monte Carlo algorithm or the Gillespie algorithm. Here we investigate the relation between the Monte Carlo algorithm and the Gillespie algorithm. We prove that asymptotically, the expected time for a K-step trajectory of the Monte Carlo algorithm is equal to <N> times that of the Gillespie algorithm, where <N> denotes the Boltzmann expected network degree. If the network is regular (i.e. every node has the same degree), then the mean first passage time (MFPT) computed by the Monte Carlo algorithm is equal to MFPT computed by the Gillespie algorithm multiplied by <N>; however, this is not true for non-regular networks. In particular, RNA secondary structure folding kinetics, as computed by the Monte Carlo algorithm, is not equal to the folding kinetics, as computed by the Gillespie algorithm, although the mean first passage times are roughly correlated. Simulation software for RNA secondary structure folding according to the Monte Carlo and Gille- spie algorithms is publicly available, as is our software to compute the expected degree of the net- work of secondary structures of a given RNA sequence { see http://bioinformatics.bc.edu/clote/ RNAexpNumNbors.
q-bio.BM:In response to cold stress, Drosophila Melanogaster increase their expression of Frost, a candidate gene involved in cold response. The direct role of Frost in cold tolerance is yet to be determined, and its importance for survival in cold environments has been questioned. In this study, I attempt to better understand the molecular machinery of Frost by selecting for its protein in fly lysate knowing only its RNA has been found. Detection of Frost expression and subsequently studying its protein will hopefully lead to enhanced comprehension of cold responses in insects, which is the long-term goal of this research. I predict that Frost will be expressed in flies that undergo cold stress at 0oC for 2 hours before recovering at 220C for 3 hours before lysing. Two Western blots were executed using fly lysate that underwent the above treatment, whose antibodies were specific to Frost. No bands were seen in any of the lanes containing treated samples in either of the Immunoblots, indicating that Frost was either not expressed or not detected in fly lysate.
q-bio.BM:Biologically functional liquid-liquid phase separation of intrinsically disordered proteins (IDPs) is driven by interactions encoded by their amino acid sequences. Little is currently known about the molecular recognition mechanisms for distributing different IDP sequences into various cellular membraneless compartments. Pertinent physics was addressed recently by applying random-phase-approximation (RPA) polymer theory to electrostatics, which is a major energetic component governing IDP phase properties. RPA accounts for charge patterns and thus has advantages over Flory-Huggins and Overbeek-Voorn mean-field theories. To make progress toward deciphering the phase behaviors of multiple IDP sequences, the RPA formulation for one IDP species plus solvent is hereby extended to treat polyampholyte solutions containing two IDP species. The new formulation generally allows for binary coexistence of two phases, each containing a different set of volume fractions $(\phi_1,\phi_2)$ for the two different IDP sequences. The asymmetry between the two predicted coexisting phases with regard to their $\phi_1/\phi_2$ ratios for the two sequences increases with increasing mismatch between their charge patterns. This finding points to a multivalent, stochastic, "fuzzy" mode of molecular recognition that helps populate various IDP sequences differentially into separate phase compartments. An intuitive illustration of this trend is provided by Flory-Huggins models, whereby a hypothetical case of ternary coexistence is also explored. Augmentations of the present RPA theory with a relative permittivity $\epsilon_{\rm r}(\phi)$ that depends on IDP volume fraction $\phi=\phi_1+\phi_2$ lead to higher propensities to phase separate, in line with the case with one IDP species we studied previously. ...
q-bio.BM:Protein-protein interactions (PPIs) perform important roles on biological functions. Researches of mutants on protein interactions can further understand PPIs. In the past, many researchers have developed databases that stored mutants on protein interactions, which are old and not updated till now. To address the issue, we developed a kinetic and thermodynamic database of mutant protein interactions (dbMPIKT) that can be freely accessible at our website. This database contains 5291 mutants that integrated data from previous databases and data from literatures for nearly three years. Furthermore, the data were analyzed, involving mutation number, mutation type, protein pair source and network map construction. On the whole, the database provides new data to further improve the study on PPIs. Website: http://210.45.212.128/lqy/index.php
q-bio.BM:Due to the asymmetric nature of the nucleotides, the extant informational biomolecule, DNA, is constrained to replicate unidirectionally on a template. As a product of molecular evolution that sought to maximize replicative potential, DNA's unidirectional replication poses a mystery since symmetric bidirectional self-replicators obviously would replicate faster than unidirectional self-replicators and hence would have been evolutionarily more successful. Here we carefully examine the physico-chemical requirements for evolutionarily successful primordial self-replicators and theoretically show that at low monomer concentrations that possibly prevailed in the primordial oceans, asymmetric unidirectional self-replicators would have an evolutionary advantage over bidirectional self-replicators. The competing requirements of low and high kinetic barriers for formation and long lifetime of inter-strand bonds respectively are simultaneously satisfied through asymmetric kinetic influence of inter-strand bonds, resulting in evolutionarily successful unidirectional self-replicators.
q-bio.BM:Allostery is an intrinsic spatiotemporal property of all proteins, resulting from long range correlations in the order of several nanometers and time scales of nanoseconds. Information is carried asymmetrically from one part to another by entropy transfer. Here, we present a master equation model of allosteric communication in proteins based on the transfer entropy concept of Schreiber (PRL, 85, 465, 2000). We show how the model relates the path and velocity of asymmetric entropy transfer to conformational transitions over the rugged energy surface of proteins and how this relates to function.
q-bio.BM:The prevalent method for RNA secondary structure prediction for a single sequence is free energy minimization based on the nearest neighbor thermodynamic model (NNTM). One of the least well-developed parts of the model is the energy function assigned to the multibranch loops. Parametric analysis can be performed to elucidate the dependance of the prediction on the branching parameters used in the NNTM. Since the objective function is linear, this boils down to analyzing the normal fans of the branching polytopes. Here we show that because of the way the multibranch loops are scored under the NNTM, certain branching patterns are possible for all sequences. We do this by characterizing the dominant parts of the parameter space obtained by looking at the relevant section of the normal fan; therefore, we conclude that the structures that are normally found in nature are obtained for a relatively small set of parameters.
q-bio.BM:In this paper, a multiscale virtual particle based elastic network model (MVP-ENM) is proposed for biomolecular normal mode analysis. The multiscale virtual particle model is proposed for the discretization of biomolecular density data in different scales. Essentially, the model works as the coarse-graining of the biomolecular structure, so that a delicate balance between biomolecular geometric representation and computational cost can be achieved. To form "connections" between these multiscale virtual particles, a new harmonic potential function, which considers the influence from both mass distributions and distance relations, is adopted between any two virtual particles. Unlike the previous ENMs that use a constant spring constant, a particle-dependent spring parameter is used in MVP-ENM. Two independent models, i.e., multiscale virtual particle based Gaussian network model (MVP-GNM) and multiscale virtual particle based anisotropic network model (MVP-ANM), are proposed. Even with a rather coarse grid and a low resolution, the MVP-GNM is able to predict the Debye-Waller factors (B-factors) with considerable good accuracy. Similar properties have also been observed in MVP-ANM. More importantly, in B-factor predictions, the mismatch between the predicted results and experimental ones is predominantly from higher fluctuation regions. Further, it is found that MVP-ANM can deliver a very consistent low-frequency eigenmodes in various scales. This demonstrates the great potential of MVP-ANM in the deformation analysis of low resolution data. With the multiscale rigidity function, the MVP-ENM can be applied to biomolecular data represented in density distribution and atomic coordinates. Further, the great advantage of my MVP-ENM model in computational cost has been demonstrated by using two poliovirus virus structures. Finally, the paper ends with a conclusion.
q-bio.BM:We have extended our analytically derived PDB-NMA formulation, ATMAN [1], to include protein dimers using mixed internal and Cartesian coordinates. A test case on a 1.3\AA\ resolution model of a small homodimer, ActVA-ORF6, consisting of two 112-residue subunits identically folded in a compact 50\AA\ sphere, reproduces the distinct experimental Debye-Waller motility asymmetry for the two chains, demonstrating that structure sensitively selects vibrational signatures. The vibrational analysis of this PDB entry, together with biochemical and crystallographic data, demonstrates the cooperative nature of the dimeric interaction of the two subunits and suggests a mechanical model for subunit interconversion during the catalytic cycle.
q-bio.BM:Leucine zippers are alpha helical monomers dimerized to a coiled coil structure. Various scenarios for dimerization span interaction of unstructured monomers that form alpha helices in the process of dimerization to preformed alpha helical monomers dimerizing.Another suggested possibility, a trigger sequence hypothesis (M. O. Steinmetz et al., Proc. Natl. Acad. Sci. U. S. A. 2007, 104, 7062-7067), is that a C terminal (CT) part of each monomer is a trigger sequence that is dimerized and, subsequently, the remainder N terminal (NT) part of each monomer (that is partially ordered) zips together to form the coiled coil. In this work, methods are developed to computationally explore the trigger sequence hypothesis based on an extension of a previously introduced (R. I. Cukier, J. Chem. Phys. 2011, 134, 045104) Hamiltonian Temperature Replica Exchange Method Mean Square (HTREM_MS) procedure, which scales the Hamiltonian in both potential and kinetic energies, to enhance sampling generically, along with additional restraints to enhance sampling in desired regions of conformational space. The method is applied to a 31-residue truncation of the 33-residue leucine zipper (GCN4-p1) of the yeast transcriptional activator GCN4. Using a variety of HTREM_MS simulations, we find that the NT of one monomer of the dimer becomes disordered when the NT parts of the monomers are separated. In contrast, when the CT parts of the monomers are separated, both monomers remain alpha helical. These simulations suggest that the CT does act as a trigger sequence and are consistent with a disordered NT. We also investigate whether the disordered NT can be induced to re-form the dimerized leucine zipper. It does, but with some lack of recovery of alpha helical hydrogen bonding structure in the NT.
q-bio.BM:Predicting three dimensional residue-residue contacts from evolutionary information in protein sequences was attempted already in the early 1990s. However, contact prediction accuracies of methods evaluated in CASP experiments before CASP11 remained quite low, typically with $<20$% true positives. Recently, contact prediction has been significantly improved to the level that an accurate three dimensional model of a large protein can be generated on the basis of predicted contacts. This improvement was attained by disentangling direct from indirect correlations in amino acid covariations or cosubstitutions between sites in protein evolution. Here, we review statistical methods for extracting causative correlations and various approaches to describe protein structure, complex, and flexibility based on predicted contacts.
q-bio.BM:We investigate the effects of the relative dielectric coefficient on ionic flows in open ion channels, using mathematical analysis of reasonably general Poisson-Nernst-Planck type models that can include the finite sizes of ions. The value of the relative dielectric coefficient is of course a crucial parameter for ionic behavior in general. Using the powerful theory of singularly perturbed problems in applied mathematics, we show that some properties of open channels are quite insensitive to variation in the relative dielectric coefficient, thereby explaining such effects seen unexpectedly in simulations. The ratio between the total number of one ion species and that of another ion species, and the ratio between the flux of one ion species and that of another ion species do not depend significantly on the relative dielectric coefficient.
q-bio.BM:Natural protein molecules are exceptional polymers. Encoded in apparently random strings of amino-acids, these objects perform clear physical tasks that are rare to find by simple chance. Accurate folding, specific binding, powerful catalysis, are examples of basic chemical activities that the great majority of polypeptides do not display, and are thought to be the outcome of the natural history of proteins. Function, a concept genuine to Biology, is at the core of evolution and often conflicts with the physical constraints. Locating the frustration between discrepant goals in a recurrent system leads to fundamental insights about the chances and necessities that shape the encoding of biological information.
q-bio.BM:The ability to precisely visualize the atomic geometry of the interactions between a drug and its protein target in structural models is critical in predicting the correct modifications in previously identified inhibitors to create more effective next generation drugs. It is currently common practice among medicinal chemists while attempting the above to access the information contained in three-dimensional structures by using two-dimensional projections, which can preclude disclosure of useful features. A more precise visualization of the three-dimensional configuration of the atomic geometry in the models can be achieved through the implementation of immersive virtual reality (VR). In this work, we present a freely available software pipeline for visualising protein structures through VR. New customer hardware, such as the HTC Vive and the Oculus Rift utilized in this study, are available at reasonable prices. Moreover, we have combined VR visualization with fast algorithms for simulating intramolecular motions of protein flexibility, in an effort to further improve structure-lead drug design by exposing molecular interactions that might be hidden in the less informative static models.
q-bio.BM:Molecular dynamics simulation is used to model the self-assembly of polyhedral shells containing 180 trapezoidal particles that correspond to the T=3 virus capsid. Three kinds of particle, differing only slightly in shape, are used to account for the effect of quasi-equivalence. Bond formation between particles is reversible and an explicit atomistic solvent is included. Under suitable conditions the simulations are able to produce complete shells, with the majority of unused particles remaining as monomers, and practically no other clusters. There are also no incorrectly assembled clusters. The simulations reveal details of intermediate structures along the growth pathway, information that is relevant for interpreting experiment.
q-bio.BM:While a monomer of the ubiquitous hormone insulin is the biologically active form in the human body, its hexameric assembly acts as an efficient storage unit. However, the role of water molecules in the structure, stability and dynamics of the insulin hexamer is poorly understood. Here we combine experimental data with molecular dynamics simulations to investigate the shape, structure and stability of an insulin hexamer focusing on the role of water molecules. Both X-Ray analysis and computer simulations show that the core of the hexamer cavity is barrel-shaped, holding, on an average, sixteen water molecules. These encapsulated and constrained molecules impart structural stability to the hexamer. Apart from the electrostatic interactions with Zn2+ ions, an intricate hydrogen bond network amongst cavity water and neighboring protein residues stabilizes the hexameric association. These water molecules solvate six glutamate residues inside the cavity decreasing electrostatic repulsions amongst the negatively charged carboxylate groups. They also prevent association between glutamate residues and Zn2+ ions and maintain the integrity of the cavity. Simulations reveal that removal of these waters results in a collapse of the cavity. Subsequent analyses also show that the hydrogen bond network among these water molecules and protein residues that face the inner side of the cavity is more rigid with a slower relaxation as compared to that of the bulk solvent. Dynamics of cavity water reveal certain slow water molecules which form the back bone of the stable hydrogen bond network. The analysis presented here suggests a dominant role of structurally conserved water molecules in maintaining the integrity of the hexameric assembly and potentially modulating the dissociation of this assembly into the functional monomeric form.
q-bio.BM:The Na$^+$/K$^+$ ATPase is an essential component of cardiac electrophysiology, maintaining physiological Na$^+$ and K$^+$ concentrations over successive heart beats. Terkildsen et al. (2007) developed a model of the ventricular myocyte Na$^+$/K$^+$ ATPase to study extracellular potassium accumulation during ischaemia, demonstrating the ability to recapitulate a wide range of experimental data, but unfortunately there was no archived code associated with the original manuscript. Here we detail an updated version of the model and provide CellML and MATLAB code to ensure reproducibility and reusability. We note some errors within the original formulation which have been corrected to ensure that the model is thermodynamically consistent, and although this required some reparameterisation, the resulting model still provides a good fit to experimental measurements that demonstrate the dependence of Na$^+$/K$^+$ ATPase pumping rate upon membrane voltage and metabolite concentrations. To demonstrate thermodynamic consistency we also developed a bond graph version of the model. We hope that these models will be useful for community efforts to assemble a whole-cell cardiomyocyte model which facilitates the investigation of cellular energetics.
q-bio.BM:Topology affects physical and biological properties of DNA and impacts fundamental cellular processes, such as gene expression, genome replication, chromosome structure and segregation. In all organisms DNA topology is carefully modulated and the supercoiling degree of defined genome regions may change according to physiological and environmental conditions. Elucidation of structural properties of DNA molecules with different topology may thus help to better understand genome functions. Whereas a number of structural studies have been published on highly negatively supercoiled DNA molecules, only preliminary observations of highly positively supercoiled are available, and a description of DNA structural properties over the full range of supercoiling degree is lacking. Atomic Force Microscopy (AFM) is a powerful tool to study DNA structure at single molecule level. We here report a comprehensive analysis by AFM of DNA plasmid molecules with defined supercoiling degree, covering the full spectrum of biologically relevant topologies, under different observation conditions. Our data, supported by statistical and biochemical analyses, revealed striking differences in the behavior of positive and negative plasmid molecules.
q-bio.BM:Because human serum transferrin (hTF) exists freely in serum, it is a potential target for cancer treatment drugs and in curing iron-overloaded conditions in patients via long-term transfusion therapy. The understanding of the interactions between hTF and metal ions is very important for biological, pharmalogical, toxicological, and other protein engineering purposes. In this paper, a simple linear free energy correlation is proposed to predict the binding strength between hTF protein and metal cations. The stability constants for a family of metal-hTF complexes can be correlated to the non-solvation energies and the radii of cations. The binding strength is determined by both the physical properties (charge and size or ionic radius) and chemical properties (non-solvation energy) of a given cation. The binding strengths of either divalent and trivalent metals can then be predicted systematically.
q-bio.BM:We have introduced a Pareto sorting algorithm into Synopsis, a de novo design program that generates synthesizable molecules with desirable properties. We give a detailed description of the algorithm and illustrate its working in 2 different de novo design settings: the design of putative dual and selective FGFR and VEGFR inhibitors, and the successful design of organic structure determining agents (OSDAs) for the synthesis of zeolites. We show that the introduction of Pareto sorting not only enables the simultaneous optimization of multiple properties but also greatly improves the performance of the algorithm to generate molecules with hard-to-meet constraints. This in turn allows us to suggest approaches to address the problem of false positive hits in de novo structure based drug design by introducing structural and physicochemical constraints in the designed molecules, and by forcing essential interactions between these molecules and their target receptor.
q-bio.BM:Modern algorithms for de novo prediction of protein structures typically output multiple full-length models (decoys) rather than a single solution. Subsequent clustering of such decoys is used both to gauge the success of the modelling and to decide on the most native-like conformation. At the same time, partial protein models are sufficient for some applications such as crystallographic phasing by molecular replacement (MR) in particular, provided these models represent a certain part of the target structure with reasonable accuracy. Here we propose a novel clustering algorithm that natively operates in the space of partial models through an approach known as granular clustering (GC). The algorithm is based on growing local similarities found in a pool of initial decoys. We demonstrate that the resulting clusters of partial models provide a substantially more accurate structural detail on the target protein than those obtained upon a global alignment of decoys. As the result, the partial models output by our GC algorithm are also much more effective towards the MR procedure, compared to the models produced by existing software. The source code is freely available at https://github.com/biocryst/gc
q-bio.BM:Pure and homogeneous biological macromolecules (i.e. proteins, nucleic acids, protein-protein or protein-nucleic acid complexes, and functional assemblies such as ribosomes and viruses) are the key for consistent and reliable biochemical and biophysical measurements, as well as for reproducible crystallizations, best crystal diffraction properties, and exploitable electron microscopy images. Highlights: Pure and homogeneous macromolecules are the key for the best experimental results; They warrant the consistency and the reliability of biochemical and biophysical data; They give more reproducible crystallography and electron microscopy results as well.
q-bio.BM:This chapter gives a graceful introduction to problem of protein three- dimensional structure prediction, and focuses on how to make structural sense out of a single input sequence with unknown structure, the 'query' or 'target' sequence. We give an overview of the different classes of modelling techniques, notably template-based and template free. We also discuss the way in which structural predictions are validated within the global com- munity, and elaborate on the extent to which predicted structures may be trusted and used in practice. Finally we discuss whether the concept of a sin- gle fold pertaining to a protein structure is sustainable given recent insights. In short, we conclude that the general protein three-dimensional structure prediction problem remains unsolved, especially if we desire quantitative predictions. However, if a homologous structural template is available in the PDB model or reasonable to high accuracy may be generated.
q-bio.BM:This chapter deals with approaches for protein three-dimensional structure prediction, starting out from a single input sequence with unknown struc- ture, the 'query' or 'target' sequence. Both template based and template free modelling techniques are treated, and how resulting structural models may be selected and refined. We give a concrete flowchart for how to de- cide which modelling strategy is best suited in particular circumstances, and which steps need to be taken in each strategy. Notably, the ability to locate a suitable structural template by homology or fold recognition is crucial; without this models will be of low quality at best. With a template avail- able, the quality of the query-template alignment crucially determines the model quality. We also discuss how other, courser, experimental data may be incorporated in the modelling process to alleviate the problem of missing template structures. Finally, we discuss measures to predict the quality of models generated.
q-bio.BM:De novo prediction of protein folding is an open scientific challenge. Many folding models and force fields have been developed, yet all face difficulties converging to native conformations. Hydrophobicity scales (HSs) play a crucial role in such simulations as they define the energetic interactions between protein residues, thus determining the energetically favorable conformation. While many HSs have been developed over the years using various methods, it is surprising that the scales show very weak consensus in their assignment of hydrophobicity indexes to the various residues. In this work, several HSs are systematically assessed via atomistic Monte Carlo simulation of folding of small proteins, by converting the HSs of interest into residue-residue contact energy matrices. HSs that poorly preserve native structures of proteins were tuned by applying a linear transformation. Subsequently, folding simulations were used to examine the ability of the HSs to correctly fold the proteins from a random initial conformation. Root mean square deviation (RMSD) and energy of the proteins during folding were sampled and used to define an ER-score, as the correlation between the 2-dimensional energy-RMSD (ER) histogram with 50% lowest energy conformations and the ER histogram with 50% lowest RMSD conformations. Thus, we were able to compare the ability of the different HSs to predict de novo protein folding quantitatively.
q-bio.BM:Amyloid fibrils are stable aggregates of misfolded proteins and polypeptides that are insoluble and resistant to protease activity. Abnormal formation of amyloid fibrils in vivo may lead to neurodegenerative disorders and other systemic amyloidosis such as Alzheimer's, Parkinson's, and atherosclerosis. Because of their clinical importance amyloids are found under intense scientific research. Amyloidogenic sequences of short polypeptide segments within proteins are responsible for the transformation of correctly folded proteins into parts of larger amyloid fibrils. The {\alpha}-helical secondary structure is believed to host many amyloidogenic sequences and be a key player in different stages of the amyloidogenesis process. Most of the studies on amyloids focus on the role of amyloidogenic sequences. The focus of this study is the relation between amyloidogenicity and the structure of the amyloidogenic {\alpha}-helical sequence. We have previously shown that the {\alpha}-helical conformation may be expressed by two parameters ({\theta} and \{rho}) that form orthogonal coordinates based on the Ramachandran dihedrals ({\phi} and {\psi}) and provide an illuminating interpretation of the {\alpha}-helical conformation. By performing statistical analysis on {\alpha}-helical conformations found in the protein data bank, an apparent relation between {\alpha}-helical conformation, as expressed by {\theta} and \{rho}, and amyloidogenicity is revealed. Remarkably, random amino acid sequences, whose helical structure was obtained from the most probably dihedral angles as obtained from PDB data, revealed the same dependency of amyloidogenicity, suggesting the importance of {\alpha}-helical structure as opposed to sequence.
q-bio.BM:Quantum calculations on the voltage sensing domain (VSD) of the Kv1.2 potassium channel (pdb: 3Lut)have been carried out on a 904 atoms subset of the VSD, plus 24 water molecules. Side chains pointing away from the center of the VSD were truncated; S1,S2,S3 end atoms were were fixed (all calculations); S4 end atoms could be fixed or free. Open conformations (membrane potentials >= 0) closely match the known X-ray structure of the open state with salt bridges in the in the VSD not ionized (H+ on the acid) whether S4 end atoms were fixed or free (slightly closer fixed than free).The S4 segment backbone, free or not, moves less than 2.5 A for positive to negative membrane potential switches, not entirely in the expected direction, leaving H+ motion as the principal component of the gating current. Groups of 3 - 5 side chains are important for proton transport, based on the calculations. A proton transfers from tyrosine (Y266), through arginine (R300), to glutamate (E183), accounting for approximately 20 - 25% of the gating charge. Clusters of amino acids that can transfer protons (acids, bases, tyrosine, histidine) are the main paths for proton transport. A group of five amino acids, bounded by the conserved aromatic F233, appears to exchange a proton. Dipole rotations may also contribute. A proton path (calculations still in progress) is proposed for the remainder of the VSD, suggesting a hypothesis for a complete gating mechanism.
q-bio.BM:The largely intrinsically disordered phenylalanine-glycine-rich nucleoporins (FG Nups) underline a selectivity mechanism, which enables the rapid translocation of transport factors (TFs) through the nuclear pore complexes (NPCs). Conflicting models of NPC transport have assumed that FG Nups undergo different conformational transitions upon interacting with TFs. To selectively characterize conformational changes in FG Nups induced by TFs we performed small-angle neutron scattering (SANS) with contrast matching. Conformational ensembles derived SANS data indicate an increase in the overall size of FG Nups is associated with TF interaction. Moreover, the organization of the FG motif in the interacting state is consistent with prior experimental analyses defining that FG motifs undergo conformational restriction upon interacting with TFs. These results provide structural insights into a highly dynamic interaction and illustrate how functional disorder imparts rapid and selective FG Nup / TF interactions.
q-bio.BM:Allosteric transcription factors undergo binding events both at their inducer binding sites as well as at distinct DNA binding domains, and it is often difficult to disentangle the structural and functional consequences of these two classes of interactions. In this work, we compare the ability of two statistical mechanical models - the Monod-Wyman-Changeux (MWC) and the Koshland-N\'emethy-Filmer (KNF) models of protein conformational change - to characterize the multi-step activation mechanism of the broadly acting cyclic-AMP receptor protein (CRP). We first consider the allosteric transition resulting from cyclic-AMP binding to CRP, then analyze how CRP binds to its operator, and finally investigate the ability of CRP to activate gene expression. In light of these models, we examine data from a beautiful recent experiment that created a single-chain version of the CRP homodimer, thereby enabling each subunit to be mutated separately. Using this construct, six mutants were created using all possible combinations of the wild type subunit, a D53H mutant subunit, and an S62F mutant subunit. We demonstrate that both the MWC and KNF models can explain the behavior of all six mutants using a small, self-consistent set of parameters. In comparing the results, we find that the MWC model slightly outperforms the KNF model in the quality of its fits, but more importantly the parameters inferred by the MWC model are more in line with structural knowledge of CRP. In addition, we discuss how the conceptual framework developed here for CRP enables us to not merely analyze data retrospectively, but has the predictive power to determine how combinations of mutations will interact, how double mutants will behave, and how each construct would regulate gene expression.
q-bio.BM:Background. Protein dihedral angles provide a detailed description of protein local conformation. Predicted dihedral angles can be used to narrow down the conformational space of the whole polypeptide chain significantly, thus aiding protein tertiary structure prediction. However, direct angle prediction from sequence alone is challenging.   Method. In this study, we present a novel method to predict real-valued angles by combining clustering and deep learning. That is, we first generate certain clusters of angles (each assigned a label) and then apply a deep residual neural network to predict the label posterior probability. Finally, we output real-valued prediction by a mixture of the clusters with their predicted probabilities. At the same time, we also estimate the bound of the prediction errors at each residue from the predicted label probabilities.   Result. In this article, we present a novel method (named RaptorX-Angle) to predict real-valued angles by combining clustering and deep learning. Tested on a subset of PDB25 and the targets in the latest two Critical Assessment of protein Structure Prediction (CASP), our method outperforms the existing state-of-art method SPIDER2 in terms of Pearson Correlation Coefficient (PCC) and Mean Absolute Error (MAE). Our result also shows approximately linear relationship between the real prediction errors and our estimated bounds. That is, the real prediction error can be well approximated by our estimated bounds.   Conclusions. Our study provides an alternative and more accurate prediction of dihedral angles, which may facilitate protein structure prediction and functional study.
q-bio.BM:Hyperpolarized 13C-MRI allows real time observation of metabolism in vivo. Imaging sequences have been developed to follow the metabolism of [1-13C] pyruvate and extract reaction kinetics, which can show tumour treatment response. We applied the fitting model and algorithm for the imaging data of mice tumour models and determined error estimates for the parameters of interest. Data was least-squares fitted onto a two-site exchange model in MATLAB, followed by statistic computation to assess model performance. Inference through the application of MCMC was also performed. The modelling and inference process extracted quantitative information satisfactorily and reproducibly, demonstrating metabolic activity and intratumour heterogeneity. Finally, novel fitting methods were evaluated and further recommendations were made.
q-bio.BM:The CryoEM single particle imaging method has recently received broad attention in the field of structural biology for determining the structures of biological molecules. The structures can be resolved to near-atomic resolutions after rending a large number of CryoEM images measuring molecules in different orientations. However, the factors for model resolution need to be further explored. Here, we provide a theoretical framework in conjunction with numerical simulations to gauge the influence of several key factors that are determinant in model resolution. We found that the number of measured projection images and the quality of each measurement (quantified using average signal-noise-ratio) can be combined to a single factor, which is dominant to the constructed model resolution. Furthermore, the intrinsic thermal motion of the molecules and the defocus levels of the electron microscope both have significant effects on the model resolution. These effects can be quantitatively summarized using an analytical formula that provides a theoretical guideline on structure resolutions for given experimental measurements.
q-bio.BM:Alzheimer's Disease (AD) is a neurodegenerative disorder that lacks effective treatment options. Anti-amyloid beta (ABeta) antibodies are the leading drug candidates to treat AD, but the results of clinical trials have been disappointing. Introducing rational mutations into anti-ABeta antibodies to increase their effectiveness is a way forward, but the path to take is unclear. In this study, we demonstrate the use of computational fragment-based docking and MMPBSA binding free energy calculations in the analysis of anti-ABeta antibodies for rational drug design efforts. Our fragment-based docking method successfully predicted the emergence of the common EFRH epitope, MD simulations coupled with MMPBSA binding free energy calculations were used to analyze scenarios described in prior studies, and we introduced rational mutations into PFA1 to improve its calculated binding affinity towards the pE3-ABeta3-8 form of ABeta. Two out of four proposed mutations stabilized binding. Our study demonstrates that a computational approach may lead to an improved drug candidate for AD in the future.
q-bio.BM:ATP synthases utilize a proton motive force to synthesize ATP. In reverse, these membrane-embedded enzymes can also hydrolyze ATP to pump protons over the membrane. To prevent wasteful ATP hydrolysis, distinct control mechanisms exist for ATP synthases in bacteria, archaea, chloroplasts and mitochondria. Single-molecule F\"orster resonance energy transfer (smFRET) demonstrated that the C-terminus of the rotary subunit epsilon in the Escherichia coli enzyme changes its conformation to block ATP hydrolysis. Previously we investigated the related conformational changes of subunit F of the A1AO-ATP synthase from the archaeon Methanosarcina mazei G\"o1. Here, we analyze the lifetimes of fluorescence donor and acceptor dyes to distinguish between smFRET signals for conformational changes and potential artefacts.
q-bio.BM:The computational prediction of a protein structure from its sequence generally relies on a method to assess the quality of protein models. Most assessment methods rank candidate models using heavily engineered structural features, defined as complex functions of the atomic coordinates. However, very few methods have attempted to learn these features directly from the data. We show that deep convolutional networks can be used to predict the ranking of model structures solely on the basis of their raw three-dimensional atomic densities, without any feature tuning. We develop a deep neural network that performs on par with state-of-the-art algorithms from the literature. The network is trained on decoys from the CASP7 to CASP10 datasets and its performance is tested on the CASP11 dataset. On the CASP11 stage 2 dataset, it achieves a loss of 0.064, whereas the best performing method achieves a loss of 0.063. Additional testing on decoys from the CASP12, CAMEO, and 3DRobot datasets confirms that the network performs consistently well across a variety of protein structures. While the network learns to assess structural decoys globally and does not rely on any predefined features, it can be analyzed to show that it implicitly identifies regions that deviate from the native structure.
q-bio.BM:DNA and protein microarrays are a high-throughput technology that allow the simultaneous quantification of tens of thousands of different biomolecular species. The mediocre sensitivity and dynamic range of traditional fluorescence microarrays compared to other techniques have been the technology's Achilles' Heel, and prevented their adoption for many biomedical and clinical diagnostic applications. Previous work to enhance the sensitivity of microarray readout to the single-molecule ('digital') regime have either required signal amplifying chemistry or sacrificed throughput, nixing the platform's primary advantages. Here, we report the development of a digital microarray which extends both the sensitivity and dynamic range of microarrays by about three orders of magnitude. This technique uses functionalized gold nanorods as single-molecule labels and an interferometric scanner which can rapidly enumerate individual nanorods by imaging them with a 10x objective lens. This approach does not require any chemical enhancement such as silver deposition, and scans arrays with a throughput similar to commercial fluorescence devices. By combining single-nanoparticle enumeration and ensemble measurements of spots when the particles are very dense, this system achieves a dynamic range of about one million directly from a single scan.
q-bio.BM:Neurotensin receptor 1 (NTSR1) is a G protein-coupled receptor that is important for signaling in the brain and the gut. Its agonist ligand neurotensin (NTS), a 13-amino-acid peptide, binds with nanomolar affinity from the extracellular side to NTSR1 and induces conformational changes that trigger intracellular signaling processes. Our goal is to monitor the conformational dynamics of single fluorescently labeled NTSR1. For this, we fused the fluorescent protein mNeonGreen to the C terminus of NTSR1, purified the receptor fusion protein from E. coli membranes, and reconstituted NTSR1 into liposomes with E. coli polar lipids. Using single-molecule anisotropy measurements, NTSR1 was found to be monomeric in liposomes, with a small fraction being dimeric and oligomeric, showing homoFRET. Similar results were obtained for NTSR1 in detergent solution. Furthermore, we demonstrated agonist binding to NTSR1 by time-resolved single-molecule F\"orster resonance energy transfer (smFRET), using neurotensin labeled with the fluorophore ATTO594.
q-bio.BM:While many good textbooks are available on Protein Structure, Molecular Simulations, Thermodynamics and Bioinformatics methods in general, there is no good introductory level book for the field of Structural Bioinformatics. This book aims to give an introduction into Structural Bioinformatics, which is where the previous topics meet to explore three dimensional protein structures through computational analysis. We provide an overview of existing computational techniques, to validate, simulate, predict and analyse protein structures. More importantly, it will aim to provide practical knowledge about how and when to use such techniques. We will consider proteins from three major vantage points: Protein structure quantification, Protein structure prediction, and Protein simulation & dynamics.
q-bio.BM:Organization and maintenance of the chromosomal DNA in living cells strongly depends on the DNA interactions with a plethora of DNA-binding proteins. Single-molecule studies show that formation of nucleoprotein complexes on DNA by such proteins is frequently subject to force and torque constraints applied to the DNA. Although the existing experimental techniques allow to exert these type of mechanical constraints on individual DNA biopolymers, their exact effects in regulation of DNA-protein interactions are still not completely understood due to the lack of systematic theoretical methods able to efficiently interpret complex experimental observations. To fill this gap, we have developed a general theoretical framework based on the transfer-matrix calculations that can be used to accurately describe behaviour of DNA-protein interactions under force and torque constraints. Potential applications of the constructed theoretical approach are demonstrated by predicting how these constraints affect the DNA-binding properties of different types of architectural proteins. Obtained results provide important insights into potential physiological functions of mechanical forces in the chromosomal DNA organization by architectural proteins as well as into single-DNA manipulation studies of DNA-protein interactions.
q-bio.BM:Motivated by the problem of domain formation in chromosomes, we studied a co--polymer model where only a subset of the monomers feel attractive interactions. These monomers are displaced randomly from a regularly-spaced pattern, thus introducing some quenched disorder in the system. Previous work has shown that in the case of regularly-spaced interacting monomers this chain can fold into structures characterized by multiple distinct domains of consecutive segments. In each domain, attractive interactions are balanced by the entropy cost of forming loops. We show by advanced replica-exchange simulations that adding disorder in the position of the interacting monomers further stabilizes these domains. The model suggests that the partitioning of the chain into well-defined domains of consecutive monomers is a spontaneous property of heteropolymers. In the case of chromosomes, evolution could have acted on the spacing of interacting monomers to modulate in a simple way the underlying domains for functional reasons.
q-bio.BM:Classical simulations of protein flexibility remain computationally expensive, especially for large proteins. A few years ago, we developed a fast method for predicting protein structure fluctuations that uses a single protein model as the input. The method has been made available as the CABS-flex web server and applied in numerous studies of protein structure-function relationships. Here, we present a major update of the CABS-flex web server to version 2.0. The new features include: extension of the method to significantly larger and multimeric proteins, customizable distance restraints and simulation parameters, contact maps and a new, enhanced web server interface. CABS-flex 2.0 is freely available at http://biocomp.chem.uw.edu.pl/CABSflex2
q-bio.BM:Pathogenic Gram-negative bacteria have developed resistance to antibiotics due to their ability in creating an envelope on the outer layer of lipooligosaccharides (LOS). The cationic phosphoethanolamine (PEA) decoration of LOS lipid A is regulated by lipid A-PEA transferase (EptA) which may serve as a prominent target for developing new antibiotics. The structural characterization of Neisserial EptA has provided a structural basis to its catalytic mechanisms and ligand recognition that are crucial for inhibitor development. In this study, a combination of pharmacophore- and ligand-based approach has been employed to explore novel potent EptA inhibitors among millions of commercially-available compounds and approved drugs. A total of 8166 hit molecules obtained from ZincPharmer pharmacophore-based screening and PubMed ligand similarity search were further examined through individual two-step semi-flexible docking simulation performed in MOE. Best hits were therefore selected based on their docking score and consensus of the two docking validations. Free energy of binding calculation suggests that the best 20 consensus compounds have a stronger binding affinity than EptA natural substrate PEA. Further interaction analyses of selected eight ligands demonstrate that these ligands have overall more effective interactions with catalytically-essential residues and metal cofactors of EptA. Selected hits can be further analyzed in vitro and examined through a pre-clinical trial. This study provides an insight into drug repurposing which may serve as an initial step to develop novel potent EptA inhibitors to combat the virulence of multi-drug resistant Gram-negative bacteria.
q-bio.BM:The recent rise of cryo-EM and X-ray high-throughput techniques is providing a wealth of new structures trapped in different conformations. Understanding how proteins transition between different conformers, and how they relate to each other in terms of function is not straightforward, and highly depends on the choice of the right set of degrees of freedom. Here we present eBDIMS server, an online tool and software for automatic classification of structural ensembles and reconstruction of transition pathways using coarse-grained (CG) simulations. The server generates CG-pathways between two protein conformations along with a representation in a simplified 2D-motion landscape based on the Principal Components (PCs) from experimental structures. For a conformationally rich ensemble, the PCs provide powerful reaction coordinates for automatic structure classification, detection of on-pathway intermediates and validation of in silico pathways. When the number of available structures is low or sampling is limited, Normal Modes (NMs) provide alternative motion axes for trajectory analysis. The path-generation eBDIMS method is available at a user-friendly website: https://login.biophysics.kth.se/eBDIMS/ or as standalone software. The server incorporates a powerful interactive graphical interface for simultaneous visualization of transition pathways in 2D-motion space and 3D-molecular graphics, which greatly facilitates the exploration of the relationships between different conformations.
q-bio.BM:The folding dynamics of proteins at the single molecule level has been studied with single-molecule force spectroscopy (SMFS) experiments for twenty years, but a common standardized method for the analysis of the collected data and for the sharing among the scientific community members is still not available. We have developed a new open source tool, Fodis, for the analysis of the Force-distance curves obtained in SMFS experiments, providing an almost automatic processing, analysis and classification of the obtained data. Our method provides also a classification of the possible unfolding pathways and structural heterogeneity, present during the unfolding of proteins.
q-bio.BM:MOTIVATION: Proteins fold into complex structures that are crucial for their biological functions. Experimental determination of protein structures is costly and therefore limited to a small fraction of all known proteins. Hence, different computational structure prediction methods are necessary for the modelling of the vast majority of all proteins. In most structure prediction pipelines, the last step is to select the best available model and to estimate its accuracy. This model quality estimation problem has been growing in importance during the last decade, and progress is believed to be important for large scale modelling of proteins. The current generation of model quality estimation programs performs well at separating incorrect and good models, but fails to consistently identify the best possible model. State-of-the-art model quality assessment methods use a combination of features that describe a model and the agreement of the model with features predicted from the protein sequence.   RESULTS: We first introduce a deep neural network architecture to predict model quality using significantly fewer input features than state-of-the-art methods. Thereafter, we propose a methodology to train the deep network that leverages the comparative structure of the problem. We also show the possibility of applying transfer learning on databases of known protein structures. We demonstrate its viability by reaching state-of-the-art performance using only a reduced set of input features and a coarse description of the models.   AVAILABILITY: The code will be freely available for download at github.com/ElofssonLab/ProQ4.
q-bio.BM:Advanced mathematics, such as multiscale weighted colored graph and element specific persistent homology, and machine learning including deep neural networks were integrated to construct mathematical deep learning models for pose and binding affinity prediction and ranking in the last two D3R grand challenges in computer-aided drug design and discovery. D3R Grand Challenge 2 (GC2) focused on the pose prediction and binding affinity ranking and free energy prediction for Farnesoid X receptor ligands. Our models obtained the top place in absolute free energy prediction for free energy Set 1 in Stage 2. The latest competition, D3R Grand Challenge 3 (GC3), is considered as the most difficult challenge so far. It has 5 subchallenges involving Cathepsin S and five other kinase targets, namely VEGFR2, JAK2, p38-$\alpha$, TIE2, and ABL1. There is a total of 26 official competitive tasks for GC3. Our predictions were ranked 1st in 10 out of 26 official competitive tasks.
q-bio.BM:Interest in equilibrium-based sampling methods has grown with recent advances in computational hardware and Markov state modeling (MSM) methods, yet outstanding questions remain that hinder widespread adoption. Namely, how do sampling strategies explore conformational space and how might this influence predictions? Here, we seek to answer these questions for four commonly used sampling methods: 1) a long simulation, 2) many short simulations, 3) adaptive sampling, and 4) FAST. We first develop a theoretical framework for analytically calculating the probability of discovering states and uncover the drastic effects of varying the number and length of simulations. We then use kinetic Monte Carlo simulations on a variety of physically inspired landscapes to characterize state discovery and transition pathways. Consistently, we find that FAST simulations discover target states with the highest probability and traverse realistic pathways. Furthermore, we uncover the pathology that short parallel simulations sometimes predict an incorrect transition pathway by crossing large energy barriers that long simulations would typically circumnavigate, which we refer to as pathway tunneling. To protect against tunneling, we introduce FAST-string, which samples along the highest-flux transition paths to refine an MSMs transition probabilities and discriminate between competing pathways. Additionally, we compare MSM estimators in describing thermodynamics and kinetics. For adaptive sampling, we recommend normalizing the transition counts out of each state after adding pseudo-counts to avoid creating sources or sinks. Lastly, we evaluate our insights from simple landscapes with all-atom molecular dynamics simulations of the folding of the {\lambda}-repressor protein. We find that FAST-contacts predicts the same folding pathway as long simulations but with orders of magnitude less simulation time.
q-bio.BM:Experiments measuring currents through single protein channels show unstable currents, a phenomena called the gating of a single channel. Channels switch between an 'open' state with a well defined single amplitude of current and 'closed' states with nearly zero current. The existing mean-field theory of ion channels focuses almost solely on the open state. The physical modeling of the dynamical features of ion channels is still in its infancy, and does not describe the transitions between open and closed states, nor the distribution of the duration times of open states. One hypothesis is that gating corresponds to noise-induced fast transitions between multiple steady (equilibrium) states of the underlying system. In this work, we aim to test this hypothesis. Particularly, our study focuses on the (high order) steric Poisson-Nernst-Planck-Cahn-Hilliard model since it has been successful in predicting permeability and selectivity of ionic channels in their open state, and since it gives rise to multiple steady states. We show that this system gives rise to a gating-like behavior, but that important features of this switching behavior are different from the defining features of gating in biological systems. Furthermore, we show that noise prohibits switching in the system of study. The above phenomena are expected to occur in other PNP-type models, strongly suggesting that one has to go beyond over-damped (gradient flow) Nernst-Planck type dynamics to explain the spontaneous gating of single channels.
q-bio.BM:Proteins employ the information stored in the genetic code and translated into their sequences to carry out well-defined functions in the cellular environment. The possibility to encode for such functions is controlled by the balance between the amount of information supplied by the sequence and that left after that the protein has folded into its structure. We developed a computational algorithm to evaluate the amount of information necessary to specify the protein structure, keeping into account the thermodynamic properties of protein folding. We thus show that the information remaining in the protein sequence after encoding for its structure (the 'information gap') is very close to what needed to encode for its function and interactions. Then, by predicting the information gap directly from the protein sequence, we show that it may be possible to use these insights from information theory to discriminate between ordered and disordered proteins, to identify unknown functions, and to optimize designed proteins sequences.
q-bio.BM:The Protein Data Bank (PDB) contains more than 135 000 entries today. From these, relatively few amyloid structures can be identified, since amyloids are insoluble in water. Therefore, mostly solid state NMR-recorded amyloid structures are deposited in the PDB. Based on the geometric analysis of these deposited structures we have prepared an automatically updated webserver, which generates the list of the deposited amyloid structures, and, additionally, those globular protein entries, which have amyloid-like substructures of a given size and characteristics. We have found that applying only the properly chosen geometric conditions, it is possible to identify the deposited amyloid structures, and a number of globular proteins with amyloid-like substructures. We have analyzed these globular proteins and have found that many of them are known to form amyloids more easily than many other globular proteins. Our results relate to the method of (Stankovic, I. et al. (2017): Construction of Amyloid PDB Files Database. Transactions on Internet Research. 13 (1): 47-51), who have applied a hybrid textual-search and geometric approach for finding amyloids in the PDB.   If one intends to identify a subset of the PDB for some applications, the identification algorithm needs to be re-run periodically, since in 2017, on average, every day 30 new entries were deposited in the data bank. Our webserver is updated regularly and automatically, and the identified amyloid- and partial amyloid structures can be viewed or their list can be downloaded from the site https://pitgroup.org/amyloid.
q-bio.BM:Eicosanoids and related species are critical, small bioactive mediators of human physiology and inflammation. While ~1100 distinct eicosanoids have been predicted to exist, to date, less than 150 of these molecules have been measured in humans, limiting our understanding of eicosanoids and their role in human biology. Using a directed non-targeted mass spectrometry approach in conjunction with computational chemical networking of spectral fragmentation patterns, we find over 500 discrete chemical signals highly consistent with known and putative eicosanoids in human plasma, including 46 putative novel molecules not previously described, thereby greatly expanding the breath of prior analytical strategies. In plasma samples from 1500 individuals, we find members of this expanded eicosanoid library hold close association with markers of inflammation, as well as clinical characteristics linked with inflammation, including advancing age and obesity. These experimental and computational approaches enable discovery of new chemical entities and will shed important insight into the role of bioactive molecules in human disease.
q-bio.BM:Membrane transporters contribute to the regulation of the internal environment of cells by translocating substrates across cell membranes. Like all physical systems, the behaviour of membrane transporters is constrained by the laws of thermodynamics. However, many mathematical models of transporters, especially those incorporated into whole-cell models, are not thermodynamically consistent, leading to unrealistic behaviour. In this paper we use a physics-based modelling framework, in which the transfer of energy is explicitly accounted for, to develop thermodynamically consistent models of transporters. We then apply this methodology to model two specific transporters: the cardiac sarcoplasmic/endoplasmic Ca$^{2+}$ ATPase (SERCA) and the cardiac Na$^+$/K$^+$ ATPase.
q-bio.BM:In the present work, we review the fundamental methods which have been developed in the last few years for classifying into families and clans the distribution of amino acids in protein databases. This is done through functions of random variables, the Entropy Measures of probabilities of occurrence of the amino acids. An intensive study of the Pfam databases is presented with restrictions to families which could be represented by rectangular arrays of amino acids with m rows (protein domains) and n columns (amino acids). This work is also an invitation to scientific research groups worldwide to undertake the statistical analysis with different numbers of rows and columns since we believe in the mathematical characterization of the distribution of amino acids as a fundamental insight on the determination of protein structure and evolution.
q-bio.BM:A central goal of protein-folding theory is to predict the stochastic dynamics of transition paths --- the rare trajectories that transit between the folded and unfolded ensembles --- using only thermodynamic information, such as a low-dimensional equilibrium free-energy landscape. However, commonly used one-dimensional landscapes typically fall short of this aim, because an empirical coordinate-dependent diffusion coefficient has to be fit to transition-path trajectory data in order to reproduce the transition-path dynamics. We show that an alternative, first-principles free-energy landscape predicts transition-path statistics that agree well with simulations and single-molecule experiments without requiring dynamical data as an input. This 'topological configuration' model assumes that distinct, native-like substructures assemble on a timescale that is slower than native-contact formation but faster than the folding of the entire protein. Using only equilibrium simulation data to determine the free energies of these coarse-grained intermediate states, we predict a broad distribution of transition-path transit times that agrees well with the transition-path durations observed in simulations. We further show that both the distribution of finite-time displacements on a one-dimensional order parameter and the ensemble of transition-path trajectories generated by the model are consistent with the simulated transition paths. These results indicate that a landscape based on transient folding intermediates, which are often hidden by one-dimensional projections, can form the basis of a predictive model of protein-folding transition-path dynamics.
q-bio.BM:Markov State Models (MSMs) are a powerful framework to reproduce the long-time conformational dynamics of biomolecules using a set of short Molecular Dynamics (MD) simulations. However, precise kinetics predictions of MSMs heavily rely on the features selected to describe the system. Despite the importance of feature selection for large system, determining an optimal set of features remains a difficult unsolved problem. Here, we introduce an automatic approach to optimize feature selection based on genetic algorithms (GA), which adaptively evolves the most fitted solution according to natural selection laws. The power of the GA-based method is illustrated on long atomistic folding simulations of four proteins, varying in length from 28 to 80 residues. Due to the diversity of tested proteins, we expect that our method will be extensible to other proteins and drive MSM building to a more objective protocol.
q-bio.BM:The design of novel proteins has many applications but remains an attritional process with success in isolated cases. Meanwhile, deep learning technologies have exploded in popularity in recent years and are increasingly applicable to biology due to the rise in available data. We attempt to link protein design and deep learning by using variational autoencoders to generate protein sequences conditioned on desired properties. Potential copper and calcium binding sites are added to non-metal binding proteins without human intervention and compared to a hidden Markov model. In another use case, a grammar of protein structures is developed and used to produce sequences for a novel protein topology. One candidate structure is found to be stable by molecular dynamics simulation. The ability of our model to confine the vast search space of protein sequences and to scale easily has the potential to assist in a variety of protein design tasks.
q-bio.BM:The Protein Data Bank (PDB) contains the atomic structures of over 105 biomolecules with better than 2.8A resolution. The listing of the identities and coordinates of the atoms comprising each macromolecule permits an analysis of the slow-time vibrational response of these large systems to minor perturbations. 3D video animations of individual modes of oscillation demonstrate how regions interdigitate to create cohesive collective motions, providing a comprehensive framework for and familiarity with the overall 3D architecture. Furthermore, the isolation and representation of the softest, slowest deformation coordinates provide opportunities for the development of me- chanical models of enzyme function. The eigenvector decomposition, therefore, must be accurate, reliable as well as rapid to be generally reported upon. We obtain the eigenmodes of a 1.2A 34kDa PDB entry using either exclusively heavy atoms or partly or fully reduced atomic sets; Cartesian or internal coordinates; interatomic force fields derived either from a full Cartesian potential, a reduced atomic potential or a Gaussian distance-dependent potential; and independently devel- oped software. These varied technologies are similar in that each maintains proper stereochemistry either by use of dihedral degrees of freedom which freezes bond lengths and bond angles, or by use of a full atomic potential that includes realistic bond length and angle restraints. We find that the shapes of the slowest eigenvectors are nearly identical, not merely similar.
q-bio.BM:Let Sn denote the network of all RNA secondary structures of length n, in which undirected edges exist between structures s, t such that t is obtained from s by the addition, removal or shift of a single base pair. Using context-free grammars, generating functions and complex analysis, we show that the asymptotic average degree is O(n) and that the asymptotic clustering coeffcient is O(1/n), from which it follows that the family Sn, n = 1,2,3,... of secondary structure networks is not small-world.
q-bio.BM:We construct a one-bead-per-residue coarse-grained dynamical model to describe intrinsically disordered proteins at significantly longer timescales than in the all-atom models. In this model, inter-residue contacts form and disappear during the course of the time evolution. The contacts may arise between the sidechains, the backbones or the sidechains and backbones of the interacting residues. The model yields results that are consistent with many all-atom and experimental data on these systems. We demonstrate that the geometrical properties of various homopeptides differ substantially in this model. In particular, the average radius of gyration scales with the sequence length in a residue-dependent manner.
q-bio.BM:All known terrestrial proteins are coded as continuous strings of ~20 amino acids. The patterns formed by the repetitions of elements in groups of finite sequences describes the natural architectures of protein families. We present a method to search for patterns and groupings of patterns in protein sequences using a mathematically precise definition for 'repetition', an efficient algorithmic implementation and a robust scoring system with no adjustable parameters. We show that the sequence patterns can be well-separated into disjoint classes according to their recurrence in nested structures. The statistics of pattern occurrences indicate that short repetitions are enough to account for the differences between natural families and randomized groups by more than 10 standard deviations, while patterns shorter than 5 residues are effectively random. A small subset of patterns is sufficient to account for a robust ''familiarity'' definition of arbitrary sets of sequences.
q-bio.BM:Protein-peptide interactions play essential roles in many cellular processes and their structural characterization is the major focus of current experimental and theoretical research. Two decades ago, it was proposed to employ the steered molecular dynamics to assess the strength of protein-peptide interactions. The idea behind using steered molecular dynamics simulations is that the mechanical stability can be used as a promising and an efficient alternative to computationally highly demanding estimation of binding affinity. However, mechanical stability defined as a peak in force-extension profile depends on the choice of the pulling direction. Here we propose an uncommon choice of the pulling direction along resultant dipole moment vector, which has not been explored in simulations so far. Using explicit solvent all-atom MD simulations, we apply steered molecular dynamics technique to probe mechanical resistance of protein-peptide system pulled along two different vectors. A novel pulling direction, along the resultant dipole moment vector, results in stronger forces compared to commonly used peptide unbinding along center of masses vector. Our results demonstrate that resultant dipole moment is one of the factors influencing the mechanical stability of protein-peptide complex.
q-bio.BM:Beta-turn prediction is useful in protein function studies and experimental design. Although recent approaches using machine-learning techniques such as SVM, neural networks, and K-NN have achieved good results for beta-turn pre-diction, there is still significant room for improvement. As previous predictors utilized features in a sliding window of 4-20 residues to capture interactions among sequentially neighboring residues, such feature engineering may result in incomplete or biased features, and neglect interactions among long-range residues. Deep neural networks provide a new opportunity to address these issues. Here, we proposed a deep dense inception network (DeepDIN) for beta-turn prediction, which takes advantages of the state-of-the-art deep neural network design of the DenseNet and the inception network. A test on a recent BT6376 benchmark shows that the DeepDIN outperformed the previous best BetaTPred3 significantly in both the overall prediction accuracy and the nine-type beta-turn classification. A tool, called MUFold-BetaTurn, was developed, which is the first beta-turn prediction tool utilizing deep neural networks. The tool can be downloaded at http://dslsrv8.cs.missouri.edu/~cf797/MUFoldBetaTurn/download.html.
q-bio.BM:Peroxisome proliferator-activated receptors gamma (PPAR{\gamma}) are ligand-activated controllers of various metabolic actions and insulin sensitivity. PPAR{\gamma} is thus considered as an important target to treat type 2 diabetes. Available PPAR{\gamma} drugs (full agonists) have robust insulin-sensitizing properties but are accompanied by severe side effects leading to complicated health problems. Here, we have used molecular docking and a molecular dynamics simulation study to find a novel PPAR{\gamma} ligand from a natural product. Our study suggests that the inhibition of ceramicine B in the PPAR{\gamma} ligand-binding domain (LBD) could act as a partial agonist and block cdk5-mediated phosphorylation. This result may provide an opportunity for the development of new anti-diabetic drugs by targeting PPAR{\gamma} while avoiding the side effects associated with full agonists.
q-bio.BM:Auxin is considered one of the cardinal hormones in plant growth and development. It regulates a wide range of processes throughout the plant. Synthetic auxins exploit the auxin-signalling pathway and are valuable as herbicidal agrochemicals. Currently, despite a diversity of chemical scaffolds all synthetic auxins have a carboxylic acid as the active core group. By applying bio-isosteric replacement we discovered that indole-3-tetrazole was active by surface plasmon resonance (SPR) spectrometry, showing that the tetrazole could initiate assembly of the TIR1 auxin co-receptor complex. We then tested the tetrazole's efficacy in a range of whole plant physiological assays and in protoplast reporter assays which all confirmed auxin activity, albeit rather weak. We then tested indole-3-tetrazole against the AFB5 homologue of TIR1, finding that binding was selective against TIR1, absent with AFB5. The kinetics of binding to TIR1 are contrasted to those for the herbicide picloram, which shows the opposite receptor preference as it binds to AFB5 with far greater affinity than to TIR1. The basis of the preference of indole-3-tetrazole for TIR1 was revealed to be a single residue substitution using molecular docking, and assays using tir1 and afb5 mutant lines confirmed selectivity in vivo. Given the potential that a TIR1-selective auxin might have for unmasking receptor-specific actions, we followed a rational design, lead optimisation campaign and a set of chlorinated indole-3-tetrazoles was synthesised. Improved affinity for TIR1 and the preference for binding to TIR1 was maintained for 4- and 6-chloroindole-3-tetrazoles, coupled with improved efficacy in vivo. This work expands the range of auxin chemistry for the design of receptor-selective synthetic auxins.
q-bio.BM:We present a new method that combines alchemical transformation with physical pathway to accurately and efficiently compute the absolute binding free energy of receptor-ligand complex. Currently, the double decoupling method (DDM) and the potential of mean force approach (PMF) methods are widely used to compute the absolute binding free energy of biomolecules. The DDM relies on alchemically decoupling the ligand from its environments, which can be computationally challenging for large ligands and charged ligands because of the large magnitude of the decoupling free energies involved. On the other hand, the PMF approach uses physical pathway to extract the ligand out of the binding site, thus avoids the alchemical decoupling of the ligand. However, the PMF method has its own drawback because of the reliance on a ligand binding/unbinding pathway free of steric obstruction from the receptor atoms. Therefore, in the presence of deeply buried ligand functional groups the convergence of the PMF calculation can be very slow leading to large errors in the computed binding free energy. Here we develop a new method called AlchemPMF by combining alchemical transformation with physical pathway to overcome the major drawback in the PMF method. We have tested the new approach on the binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20 ns of simulation per umbrella sampling window, the new method yields absolute binding free energies within ~1 kcal/mol from the experimental result, whereas the standard PMF approach and the DDM calculations result in errors of ~5 kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy computed using the new method is associated with smaller statistical error compared with those obtained from the existing methods.
q-bio.BM:Hydrogen bonding between nucleobases produces diverse DNA structural motifs, including canonical duplexes, guanine (G) quadruplexes and cytosine (C) i-motifs. Incorporating metal-mediated base pairs into nucleic acid structures can introduce new functionalities and enhanced stabilities. Here we demonstrate, using mass spectrometry (MS), ion mobility spectrometry (IMS) and fluorescence resonance energy transfer (FRET), that parallel-stranded structures consisting of up to 20 G-Ag(I)-G contiguous base pairs are formed when natural DNA sequences are mixed with silver cations in aqueous solution. FRET indicates that duplexes formed by poly(cytosine) strands with 20 contiguous C-Ag(I)-C base pairs are also parallel. Silver-mediated G-duplexes form preferentially over G-quadruplexes, and the ability of Ag+ to convert G-quadruplexes into silver-paired duplexes may provide a new route to manipulating these biologically relevant structures. IMS indicates that G-duplexes are linear and more rigid than B-DNA. DFT calculations were used to propose structures compatible with the IMS experiments. Such inexpensive, defect-free and soluble DNA-based nanowires open new directions in the design of novel metal-mediated DNA nanotechnology.
q-bio.BM:Due to the low density of envelope (Env) spikes on the surface of HIV-1, neutralizing IgG antibodies rarely bind bivalently using both antigen-binding arms (Fabs) to crosslink between spikes (inter-spike crosslinking), instead resorting to weaker monovalent binding that is more sensitive to Env mutations. Synthetic antibodies designed to bivalently bind a single Env trimer (intra-spike crosslinking) were previously shown to exhibit increased neutralization potencies. In initial work, diFabs joined by varying lengths of rigid double-stranded DNA (dsDNA) were considered. Anticipating future experiments to improve synthetic antibodies, we investigate whether linkers with different rigidities could enhance diFab potency by modeling DNA-Fabs containing different combinations of rigid dsDNA and flexible single-stranded DNA (ssDNA) and characterizing their neutralization potential. Model predictions suggest that while a long flexible polymer may be capable of bivalent binding, it exhibits weak neutralization due to the large loss in entropic degrees of freedom when both Fabs are bound. In contrast, the strongest neutralization potencies are predicted to require a rigid linker that optimally spans the distance between two Fab binding sites on an Env trimer, and avidity can be further boosted by incorporating more Fabs into these constructs. These results inform the design of multivalent anti-HIV-1 therapeutics that utilize avidity effects to remain potent against HIV-1 in the face of the rapid mutation of Env spikes.
q-bio.BM:Accurate prediction of protein stability changes upon single-site variations (DDG) is important for protein design, as well as our understanding of the mechanism of genetic diseases. The performance of high-throughput computational methods to this end is evaluated mostly based on the Pearson correlation coefficient between predicted and observed data, assuming that the upper bound would be 1 (perfect correlation). However, the performance of these predictors can be limited by the distribution and noise of the experimental data. Here we estimate, for the first time, a theoretical upper-bound to the DDG prediction performances imposed by the intrinsic structure of currently available DDG data. Given a set of measured DDG protein variations, the theoretically best predictor is estimated based on its similarity to another set of experimentally determined DDG values. We investigate the correlation between pairs of measured DDG variations, where one is used as a predictor for the other. We analytically derive an upper bound to the Pearson correlation as a function of the noise and distribution of the DDG data. We also evaluate the available datasets to highlight the effect of the noise in conjunction with DDG distribution. We conclude that the upper bound is a function of both uncertainty and spread of the DDG values, and that with current data the best performance should be between 0.7-0.8, depending on the dataset used; higher Pearson correlations might be indicative of overtraining. It also follows that comparisons of predictors using different datasets are inherently misleading.
q-bio.BM:In this study, we introduced a new unit, named "protein token", as a dynamic protein structural unit for protein-protein interactions. Unlike the conventional structural units, protein token is not based on the sequential or spatial arrangement of residues, but comprises remote residues involved in cooperative conformational changes during protein interactions. Application of protein token on Ras GTPases revealed various tokens present in the superfamily. Distinct token combinations were found in H-Ras interacting with its various regulators and effectors, directing to a possible clue for the multiplexer property of Ras superfamily. Thus, this protein token theory may provide a new approach to study protein-protein interactions in broad applications.
q-bio.BM:Biorefinery sector has become a serious dispute for cleaner and sustainable development in recent years. In the present study, pretreatment of pineapple peel waste was carried out in high pressure reactor using various pretreatment-enhancers. The type and concentration effect of each enhancer on hemicellulose solubilization was systematically investigated. The binary acid (phenol + sulfuric acid) at 180 {\deg}C was found to be superior amongst other studied enhancers, giving 81.17% (w/v) hemicellulose solubilization in liquid-fraction under optimized conditions. Solid residue thus obtained was subjected to enzymatic hydrolysis that resulted into 24.50% (w/v) cellulose breakdown. Treated solid residue was further characterized by scanning electron microscopy and Fourier transform infrared spectroscopy to elucidate structural changes. The pooled fractions (acid treated and enzymatically hydrolyzed) were fermented using Clostridium acetobutylicum NRRL B 527 which resulted in butanol production of 5.18 g/L with yield of 0.13 g butanol/g sugar consumed. Therefore, pretreatment of pineapple peel waste evaluated in this study can be considered as milestone in utilization of low cost feedstock, for bioenergy production.
q-bio.BM:We consider self-assembly of proteins into a virus capsid by the methods of molecular dynamics. The capsid corresponds either to SPMV or CCMV and is studied with and without the RNA molecule inside. The proteins are flexible and described by the structure-based coarse-grained model augmented by electrostatic interactions. Previous studies of the capsid self-assembly involved solid objects of a supramolecular scale, e.g. corresponding to capsomeres, with engineered couplings and stochastic movements. In our approach, a single capsid is dissociated by an application of a high temperature for a variable period and then the system is cooled down to allow for self-assembly. The restoration of the capsid proceeds to various extent, depending on the nature of the dissociated state, but is rarely complete because some proteins depart too far unless the process takes place in a confined space.
q-bio.BM:Contact-assisted protein folding has made very good progress, but two challenges remain. One is accurate contact prediction for proteins lack of many sequence homologs and the other is that time-consuming folding simulation is often needed to predict good 3D models from predicted contacts. We show that protein distance matrix can be predicted well by deep learning and then directly used to construct 3D models without folding simulation at all. Using distance geometry to construct 3D models from our predicted distance matrices, we successfully folded 21 of the 37 CASP12 hard targets with a median family size of 58 effective sequence homologs within 4 hours on a Linux computer of 20 CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot fold any of them in the absence of folding simulation and the best CASP12 group folded 11 of them by integrating predicted contacts into complex, fragment-based folding simulation. The rigorous experimental validation on 15 CASP13 targets show that among the 3 hardest targets of new fold our distance-based folding servers successfully folded 2 large ones with <150 sequence homologs while the other servers failed on all three, and that our ab initio folding server also predicted the best, high-quality 3D model for a large homology modeling target. Further experimental validation in CAMEO shows that our ab initio folding server predicted correct fold for a membrane protein of new fold with 200 residues and 229 sequence homologs while all the other servers failed. These results imply that deep learning offers an efficient and accurate solution for ab initio folding on a personal computer.
q-bio.BM:Prediction of metabolism in cytochrome P450s remains to be a crucial yet challenging topic in discovering and designing drugs, agrochemicals and nutritional supplements. The problem is challenging because the rate of P450 metabolism depends upon both the intrinsic chemical reactivity of the site and the protein-ligand geometry that is energetically accessible in the active site of a given P450 isozyme. We have addressed this problem using a two-level screening system. The first level implements an empirical QSAR-based scoring function employing the local chemical motifs to characterize the intrinsic reactivity. The second level uses molecular docking and molecular mechanics to account for the geometrical effects, including induced-fit effects in the protein which can be very important in P450 interactions with ligands. This approach has achieved high accuracy for both the P450 3A4 and 2D6 isoforms. In identifying at least one metabolic site in the top two ranked positions, the prediction rate can reach as high as 92.7% for the test set of isoform 3A4. For the 2D6 isoform, 100% accuracy is achieved on this basic evaluation metric, and, because this active site is considerably smaller and more selective than 3A4, very high precision is attained for full prediction of all metabolic sites. The method also requires considerably less CPU time than our previous efforts, which involved a large number of expensive simulations for each ligand to be evaluated. After screening using the empirical score function, only a few best candidates are left for each ligand, making the number of necessary estimations in the second level very small, which significantly reduces the computation time.
q-bio.BM:Machine learning (ML)-guided directed evolution is a new paradigm for biological design that enables optimization of complex functions. ML methods use data to predict how sequence maps to function without requiring a detailed model of the underlying physics or biological pathways. To demonstrate ML-guided directed evolution, we introduce the steps required to build ML sequence-function models and use them to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to using ML for protein engineering as well as the current literature and applications of this new engineering paradigm. ML methods accelerate directed evolution by learning from information contained in all measured variants and using that information to select sequences that are likely to be improved. We then provide two case studies that demonstrate the ML-guided directed evolution process. We also look to future opportunities where ML will enable discovery of new protein functions and uncover the relationship between protein sequence and function.
q-bio.BM:The inapplicability of amino acid covariation methods to small protein families has limited their use for structural annotation of whole genomes. Recently, deep learning has shown promise in allowing accurate residue-residue contact prediction even for shallow sequence alignments. Here we introduce DMPfold, which uses deep learning to predict inter-atomic distance bounds, the main chain hydrogen bond network, and torsion angles, which it uses to build models in an iterative fashion. DMPfold produces more accurate models than two popular methods for a test set of CASP12 domains, and works just as well for transmembrane proteins. Applied to all Pfam domains without known structures, confident models for 25% of these so-called dark families were produced in under a week on a small 200 core cluster. DMPfold provides models for 16% of human proteome UniProt entries without structures, generates accurate models with fewer than 100 sequences in some cases, and is freely available.
q-bio.BM:Proteins display generic properties that are challenging to explain by direct selection, notably allostery, the capacity to be regulated through long-range effects, and evolvability, the capacity to adapt to new selective pressures. An evolutionary scenario is proposed where proteins acquire these two features indirectly as a by-product of their selection for a more fundamental property, exquisite discrimination, the capacity to bind discriminatively very similar ligands. Achieving this task is shown to typically require proteins to undergo a conformational change. We argue that physical and evolutionary constraints impel this change to be controlled by a group of sites extending from the binding site. Proteins can thus acquire a latent potential for allosteric regulation and evolutionary adaptation because of long-range effects that initially arise as evolutionary spandrels. This scenario accounts for the groups of conserved and coevolving residues observed in multiple sequence alignments. However, we propose that most pairs of coevolving and contacting residues inferred from such alignments have a different origin, related to thermal stability. A physical model is presented that illustrates this evolutionary scenario and its implications. The scenario can be implemented in experiments of protein evolution to directly test its predictions.
q-bio.BM:We present a detailed heuristic method to quantify the degree of local energetic frustration manifested by protein molecules. Current applications are realized in computational experiments where a protein structure is visualized highlighting the energetic conflicts or the concordance of the local interactions in that structure. Minimally frustrated linkages highlight the stable folding core of the molecule. Sites of high local frustration, in contrast, often indicate functionally relevant regions such as binding, active or allosteric sites.
q-bio.BM:While G-protein coupled receptors (GPCRs) constitute the largest class of membrane proteins, structures and endogenous ligands of a large portion of GPCRs remain unknown. Due to the involvement of GPCRs in various signaling pathways and physiological roles, the identification of endogenous ligands as well as designing novel drugs is of high interest to the research and medical communities. Along with highlighting the recent advances in structure-based ligand discovery, including docking and molecular dynamics, this article focuses on the latest advances for automating the discovery of bioactive ligands using machine learning. Machine learning is centered around the development and applications of algorithms that can learn from data automatically. Such an approach offers immense opportunities for bioactivity prediction as well as quantitative structure-activity relationship studies. This review describes the most recent and successful applications of machine learning for bioactive ligand discovery, concluding with an outlook on deep learning methods that are capable of automatically extracting salient information from structural data as a promising future direction for rapid and efficient bioactive ligand discovery.
q-bio.BM:Proteins are only moderately stable. It has long been debated whether this narrow range of stabilities is solely a result of neutral drift towards lower stability or purifying selection against excess stability is also at work - for which no experimental evidence was found so far. Here we show that mutations outside the active site in the essential E. coli enzyme adenylate kinase result in stability-dependent increase in substrate inhibition by AMP, thereby impairing overall enzyme activity at high stability. Such inhibition caused substantial fitness defects not only in the presence of excess substrate but also under physiological conditions. In the latter case, substrate inhibition caused differential accumulation of AMP in the stationary phase for the inhibition prone mutants. Further, we show that changes in flux through Adk could accurately describe the variation in fitness effects. Taken together, these data suggest that selection against substrate inhibition and hence excess stability may have resulted in a narrow range of optimal stability observed for modern proteins.
q-bio.BM:Although algebraic graph theory based models have been widely applied in physical modeling and molecular studies, they are typically incompetent in the analysis and prediction of biomolecular properties when compared with other quantitative approaches. There is a need to explore the capability and limitation of algebraic graph theory for molecular and biomolecular modeling, analysis, and prediction. In this work, we propose novel algebraic graph learning (AGL) models that encode high-dimensional physical and biological information into intrinsically low-dimensional representations. The proposed AGL model introduces multiscale weighted colored subgraphs to describe crucial molecular and biomolecular interactions via graph invariants associated with the graph Laplacian, its pseudo-inverse, and adjacent matrix. Additionally, the AGL models are incorporated with an advanced machine learning algorithm to connect the low-dimensional graph representation of biomolecular structures with their macroscopic properties. Three popular protein-ligand binding affinity benchmarks, namely CASF-2007, CASF-2013, and CASF-2016, are employed to validate the accuracy, robustness, and reliability of the present AGL model. Numerical results indicate that the proposed AGL method outperforms the other state-of-the-art methods in the binding affinity predictions of the protein-ligand complexes.
q-bio.BM:The intricate three-dimensional geometries of protein tertiary structures underlie protein function and emerge through a folding process from one-dimensional chains of amino acids. The exact spatial sequence and configuration of amino acids, the biochemical environment and the temporal sequence of distinct interactions yield a complex folding process that cannot yet be easily tracked for all proteins. To gain qualitative insights into the fundamental mechanisms behind the folding dynamics and generic features of the folded structure, we propose a simple model of structure formation that takes into account only fundamental geometric constraints and otherwise assumes randomly paired connections. We find that despite its simplicity, the model results in a network ensemble consistent with key overall features of the ensemble of Protein Residue Networks we obtained from more than 1000 biological protein geometries as available through the Protein Data Base. Specifically, the distribution of the number of interaction neighbors a unit (amino acid) has, the scaling of the structure's spatial extent with chain length, the eigenvalue spectrum and the scaling of the smallest relaxation time with chain length are all consistent between model and real proteins. These results indicate that geometric constraints alone may already account for a number of generic features of protein tertiary structures.
q-bio.BM:Direct comparison of three-dimensional (3D) objects is computationally expensive due to the need for translation, rotation, and scaling of the objects to evaluate their similarity. In applications of 3D object comparison, often identifying specific local regions of objects is of particular interest. We have recently developed a set of 2D moment invariants based on discrete orthogonal Krawtchouk polynomials for comparison of local image patches. In this work, we extend them to 3D and construct 3D Krawtchouk descriptors (3DKD) that are invariant under translation, rotation, and scaling. The new descriptors have the ability to extract local features of a 3D surface from any region-of-interest. This property enables comparison of two arbitrary local surface regions from different 3D objects. We present the new formulation of 3DKD and apply it to the local shape comparison of protein surfaces in order to predict ligand molecules that bind to query proteins.
q-bio.BM:Exploring and understanding the protein-folding problem has been a long-standing challenge in molecular biology. Here, using molecular dynamics simulation, we reveal how parallel distributed adjacent planar peptide groups of unfolded proteins fold reproducibly following explicit physical folding codes in aqueous environments due to electrostatic attractions. Superfast folding of protein is found to be powered by the contribution of the formation of hydrogen bonds. Temperature-induced torsional waves propagating along unfolded proteins break the parallel distributed state of specific amino acids, inferred as the beginning of folding. Electric charge and rotational resistance differences among neighboring side-chains are used to decipher the physical folding codes by means of which precise secondary structures develop. We present a powerful method of decoding amino acid sequences to predict native structures of proteins. The method is verified by comparing the results available from experiments in the literature.
q-bio.BM:Proteins are essential for maintaining life. For example, knowing the structure of a protein, cell regulatory mechanisms of organisms can be modeled, supporting the development of disease treatments or the understanding of relationships between protein structures and food attributes. However, discovering the structure of a protein can be a difficult and expensive task, since it is hard to explore the large search to predict even a small protein. Template-based methods (coarse-grained, homology, threading etc) depend on Prior Knowledge (PK) of proteins determined using other methods as X-Ray Crystallography or Nuclear Magnetic Resonance. On the other hand, template-free methods (full-atom and ab initio) rely on atoms physical-chemical properties to predict protein structures. In comparison with other approaches, the Estimation of Distribution Algorithms (EDAs) can require significant less PK, suggesting that it could be adequate for proteins of low-level of PK. Finding an EDA able to handle both prediction quality and computational time is a difficult task, since they are strong inversely correlated. We developed an EDA specific for the ab initio Protein Structure Prediction (PSP) problem using full-atom representation. We developed one univariate and two bivariate probabilistic models in order to design a proper EDA for PSP. The bivariate models make relationships between dihedral angles $\phi$ and $\psi$ within an amino acid. Furthermore, we compared the proposed EDA with other approaches from the literature. We noticed that even a relatively simple algorithm such as Random Walk can find the correct solution, but it would require a large amount of prior knowledge (biased prediction). On the other hand, our EDA was able to correctly predict with no prior knowledge at all, characterizing such a prediction as pure ab initio.
q-bio.BM:Since the largest 2014-2016 Ebola virus disease outbreak in West Africa, understanding of Ebola virus infection has improved, notably the involvement of innate immune mediators. Amongst them, collectins are important players in the antiviral innate immune defense. A screening of Ebola glycoprotein (GP)-collectins interactions revealed the specific interaction of human surfactant protein D (hSP-D), a lectin expressed in lung and liver, two compartments where Ebola was found in vivo. Further analyses have demonstrated an involvement of hSP-D in the enhancement of virus infection in several in vitro models. Similar effects were observed for porcine SP-D (pSP-D). In addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a novel partner of Ebola GP that may participate to enhance viral spread.
q-bio.BM:Artificial RNA molecules with novel functionality have many applications in synthetic biology, pharmacy and white biotechnology. The de novo design of such devices using computational methods and prediction tools is a resource-efficient alternative to experimental screening and selection pipelines. In this review, we describe methods common to many such computational approaches, thoroughly dissect these methods and highlight open questions for the individual steps. Initially, it is essential to investigate the biological target system, the regulatory mechanism that will be exploited, as well as the desired components in order to define design objectives. Subsequent computational design is needed to combine the selected components and to obtain novel functionality. This process can usually be split into constrained sequence sampling, the formulation of an optimization problem and an in silico analysis to narrow down the number of candidates with respect to secondary goals. Finally, experimental analysis is important to check whether the defined design objectives are indeed met in the target environment and detailed characterization experiments should be performed to improve the mechanistic models and detect missing design requirements.
q-bio.BM:Studying evolutionary correlations in alignments of homologous sequences by means of an inverse Potts model has proven useful to obtain residue-residue contact energies and to predict contacts in proteins. The quality of the results depend much on several choices of the detailed model and on the algorithms used. We built, in a very controlled way, synthetic alignments with statistical properties similar to those of real proteins, and used them to assess the performance of different inversion algorithms and of their variants. Realistic synthetic alignments display typical features of low--temperature phases of disordered systems, a feature that affects the inversion algorithms. We showed that a Boltzmann--learning algorithm is computationally feasible and performs well in predicting the energy of native contacts. However, all algorithms suffer of false positives quite equally, making the quality of the prediction of native contacts with the different algorithm much system--dependent.
q-bio.BM:G protein-coupled receptors (GPCRs) are a large superfamily of membrane proteins that are activated by extracellular small molecules or photons. Neurotensin receptor 1 (NTSR1) is a GPCR that is activated by neurotensin, i.e. a 13 amino acid peptide. Binding of neurotensin induces conformational changes in the receptor that trigger the intracellular signaling processes. While recent single-molecule studies have reported a dynamic monomer - dimer equilibrium of NTSR1 in vitro, a biophysical characterization of the oligomerization status of NTSR1 in living mammalian cells is complicated. Here we report on the oligomerization state of the human NTSR1 tagged with mRuby3 by dissolving the plasma membranes of living HEK293T cells into 10 nm-sized soluble lipid nanoparticles by addition of styrene-maleic acid copolymers (SMALPs). Single SMALPs were analyzed one after another in solution by multi-parameter single molecule spectroscopy including brightness, fluorescence lifetime and anisotropy for homoFRET. Brightness analysis was improved using single SMALP detection in a confocal ABELtrap for extended observation times in solution. A bimodal brightness distribution indicated a significant fraction of dimeric NTSR1 in SMALPs or in the plasma membrane, respectively, before addition of neurotensin.
q-bio.BM:Hsp70 molecular chaperones are abundant ATP-dependent nanomachines that actively reshape non-native, misfolded proteins and assist a wide variety of essential cellular processes. Here we combine complementary computational/theoretical approaches to elucidate the structural and thermodynamic details of the chaperone-induced expansion of a substrate protein, with a particular emphasis on the critical role played by ATP hydrolysis. We first determine the conformational free-energy cost of the substrate expansion due to the binding of multiple chaperones using coarse-grained molecular simulations. We then exploit this result to implement a non-equilibrium rate model which estimates the degree of expansion as a function of the free energy provided by ATP hydrolysis. Our results are in quantitative agreement with recent single-molecule FRET experiments and highlight the stark non-equilibrium nature of the process, showing that Hsp70s are optimized to convert effectively chemical energy into mechanical work close to physiological conditions.
q-bio.BM:Membrane Proteins (MPs) account for around 15-39% of the human proteome and assume a critical role in a vast set of cellular and physiological mechanisms, including molecular transport, nutrient uptake, toxin and waste product clearance, respiration, and signaling. While roughly 60% of all FDA-approved drugs target MPs, there is a shortage of structural and biochemical data on them mainly hindered by their localization in the lipid bilayer. We present here MEmbrane protein dimer Novel Structure Analyser database (MENSAdb), a real time web-application exposing a broad array of fundamental features about MPs surface and their interfacial regions. In particular, we present conservation, four distinctive Accessible Solvent Area (ASA) descriptors, average and environment-specific B-factors, intermolecular contacts at 2.5 and 4.0 angstroms distance cutoffs, salt-bridges, hydrogen-bonds, hydrophobic, pi-pi interactions, t-stacking and cation-pi interactions. Additionally, users can closely inspect differences in values between three distinctive residues classes: i) non-surface, ii) surface and non-interfacial and iii) interfacial. The database is freely available at www.moreiralab.com/resources/mensadb.
q-bio.BM:Shape had been intuitively recognized to play a dominant role in determining the global motion patterns of bio-molecular assemblies. However, it is not clear exactly how shape determines the motion patterns. What about the local interactions that hold a structure together to a certain shape? The contributions of global shape and local interactions usually mix together and are difficult to tease part. In this work, we use symmetry to elucidate the distinct roles of global shape and local interactions in protein dynamics. Symmetric complexes provide an ideal platform for this task since in them the effects of local interactions and global shape are separable, allowing their distinct roles to be identified. Our key findings based on symmetric assemblies are: (i) the motion patterns of each subunit are determined primarily by intra-subunit interactions (IRSi), and secondarily by inter-subunit interactions (IESi); (ii) the motion patterns of the whole assembly are fully dictated by the global symmetry/shape and have nothing to do with local iESi or IRSi. This is followed by a discussion on how the findings may be generalized to complexes in any shape, with or without symmetry.
q-bio.BM:To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning in the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine learning models trained on tested variants provide a fast method for testing sequence space computationally. We validate this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (stereodivergence) of a new-to-nature carbene Si-H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93% and 79% ee. By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.
q-bio.BM:Computation of biological processes creates great promise for everyday life and great challenges for physical scientists. Simulations of molecular dynamics appeal to biologists as a natural extension of structural biology. Once biologists see a structure, they want to see it move. Molecular biology shows that a few atoms, often messenger ions like Ca$^{2+}$, control biological function on the scale of cells, sometimes organisms. Enormously concentrated ions (~20 M) in protein channels and enzymes can control important characteristics of living systems, just as highly concentrated ions near electrodes control important characteristics of electrochemical systems. The scale differences needed to simulate all the atoms of biological cells are $10^7$ in linear dimension, $10^{21}$ in three dimensions, $10^{9}$ in resolution, $10^{11}$ in time, and $10^{13}$ in particle number (to deal with concentrations of Ca$^{2+}$). $\mathbf{These}$ $\mathbf{scales}$ $\mathbf{must}$ $\mathbf{be}$ $\mathbf{dealt}$ $\mathbf{with}$ $\mathbf{simultaneously}$ if the simulation is to deal with most biological functions. We suggest a computational approach using explicit multiscale analysis instead of implicit simulation of all scales. Successful computation of ions concentrated in special places may be a significant step to understanding the defining characteristics of biological and electrochemical systems.
q-bio.BM:Ligands entering a protein binding pocket essentially compete with water molecules for binding to the protein. Hence, the location and thermodynamic properties of water molecules in protein structures have gained increased attention in the drug design community. Including corresponding data into 3D pharmacophore modeling is essential for efficient high throughput virtual screening. Here, we present PyRod, a free and open-source python software that allows for visualization of pharmacophoric binding pocket characteristics, identification of hot spots for ligand binding and subsequent generation of pharmacophore features for virtual screening. The implemented routines analyze the protein environment of water molecules in molecular dynamics (MD) simulations and can differentiate between hydrogen bonded waters as well as waters in a protein environment of hydrophobic, charged or aromatic atom groups. The gathered information is further processed to generate dynamic molecular interaction fields (dMIFs) for visualization and pharmacophoric features for virtual screening. The described software was applied to 5 therapeutically relevant drug targets and generated pharmacophores were evaluated using DUD-E benchmarking sets. The best performing pharmacophore was found for the HIV1 protease with an early enrichment factor of 54.6. PyRod adds a new perspective to structure-based screening campaigns by providing easy-to-interpret dMIFs and purely protein-based 3D pharmacophores that are solely based on tracing water molecules in MD simulations. Since structural information about co-crystallized ligands is not needed, screening campaigns can be followed, for which less or no ligand information is available. PyRod is freely available at https://github.com/schallerdavid/pyrod.
q-bio.BM:The double-helical structure of DNA results from canonical base pairing and stacking interactions. However, variations from steady-state conformations result from mechanical perturbations in cells. These different topologies have physiological relevance but their dependence on sequence remains unclear. Here, we use molecular dynamics simulations to show that sequence differences result in markedly different structural motifs upon physiological twisting and stretching. We simulated overextension on four different sequences of DNA ((AA)12, (AT)12, (GG)12 and (GC)12) with supercoiling densities within the physiological range. We found that DNA denatures in the majority of stretching simulations, surprisingly including those with overtwisted DNA. GC-rich sequences were observed to be more stable than AT-rich, with the specific response dependent on base pair ordering. Furthermore, we found that (AT)12 forms stable periodic structures with non-canonical hydrogen bonds in some regions and non-canonical stacking in others, whereas (GC)12 forms a stacking motif of four base pairs independent of supercoiling density. Our results demonstrate that 20-30% DNA extension is sufficient for breaking B-DNA around and significantly above cellular supercoiling, and that the DNA sequence is crucial for understanding structural changes under mechanical stress. Our findings have important implications for the activities of protein machinery interacting with DNA in all cells.
q-bio.BM:Here, we show by solution nuclear magnetic resonance measurements that the urea-unfolded protein apoazurin becomes elongated when the synthetic crowding agent dextran 20 is present, in contrast to the prediction from the macromolecular crowding effect based on the argument of volume exclusion. To explore the complex interactions beyond volume exclusion, we employed coarse-grained molecular dynamics simulations to explore the conformational ensemble of apoazurin in a box of monodisperse crowders under strong chemically denaturing conditions. The elongated conformation of unfolded apoazurin appears to result from the interplay of the effective attraction between the protein and crowders and the shape of the crowders. With a volume-conserving crowder model, we show that the crowder shape provides an anisotropic direction of the depletion force, in which a bundle of surrounding rod-like crowders stabilize an elongated conformation of unfolded apoazurin in the presence of effective attraction between the protein and crowders.
q-bio.BM:14-3-3 proteins are a group of seven dimeric adapter proteins that exert their biological function by interacting with hundreds of phosphorylated proteins, thus influencing their sub-cellular localization, activity or stability in the cell. Due to this remarkable interaction network, 14-3-3 proteins have been associated with several pathologies and the protein-protein interactions established with a number of partners are now considered promising drug targets. The activity of 14-3-3 proteins is often isoform specific and to our knowledge only one out of seven isoforms, 14-3-3$\zeta$, has been assigned. Despite the availability of the crystal structures of all seven isoforms of 14-3-3, the additional NMR assignments of 14-3-3 proteins are important for both biological mechanism studies and chemical biology approaches. Herein, we present a robust backbone assignment of 14-3-3$\sigma$, which will allow advances in the discovery of potential therapeutic compounds. This assignment is now being applied to the discovery of both inhibitors and stabilizers of 14-3-3 protein-protein interactions.
q-bio.BM:A growing number of experimental evidence shows that it is general for a ligand binding protein to have a potential for allosteric regulation and for further evolution. In addition, such proteins generically change their conformation upon binding. O. Rivoire has recently proposed an evolutionary scenario that explains these properties as a generic byproduct of selection for exquisite discrimination between very similar ligands. The initial claim was supported by two classes of basic examples: continuous protein models with small numbers of degrees of freedom, on which the development of a conformational switch was established, and a 2-dimensional spin glass model supporting the rest of the statement. This work aimed to clarify the implication of the exquisite discrimination for smooth models with large number of degrees of freedom, the situation closer to real biological systems. With the help of differential geometry, jet-space analysis, and transversality theorems, it is shown that the claim holds true for any generic flexible system that can be described in terms of smooth manifolds. The result suggests that, indeed, evolutionary solutions to the exquisite discrimination problem, if exist, are located near a codimension-1 subspace of the appropriate genotypical space. This constraint, in turn, gives rise to a potential for the allosteric regulation of the discrimination via generic conformational changes upon binding.
q-bio.BM:The coding space of protein sequences is shaped by evolutionary constraints set by requirements of function and stability. We show that the coding space of a given protein family--the total number of sequences in that family--can be estimated using models of maximum entropy trained on multiple sequence alignments of naturally occuring amino acid sequences. We analyzed and calculated the size of three abundant repeat proteins families, whose members are large proteins made of many repetitions of conserved portions of ~ 30 amino acids. While amino acid conservation at each position of the alignment explains most of the reduction of diversity relative to completely random sequences, we found that correlations between amino acid usage at different positions significantly impact that diversity. We quantified the impact of different types of correlations, functional and evolutionary, on sequence diversity. Analysis of the detailed structure of the coding space of the families revealed a rugged landscape, with many local energy minima of varying sizes with a hierarchical structure, reminiscent of fustrated energy landscapes of spin glass in physics. This clustered structure indicates a multiplicity of subtypes within each family, and suggests new strategies for protein design.
q-bio.BM:Comprehensive and unambiguous identification of small molecules in complex samples will revolutionize our understanding of the role of metabolites in biological systems. Existing and emerging technologies have enabled measurement of chemical properties of molecules in complex mixtures and, in concert, are sensitive enough to resolve even stereoisomers. Despite these experimental advances, small molecule identification is inhibited by (i) chemical reference libraries representing <1% of known molecules, limiting the number of possible identifications, and (ii) the lack of a method to generate candidate matches directly from experimental features (i.e. without a library). To this end, we developed a variational autoencoder (VAE) to learn a continuous numerical, or latent, representation of molecular structure to expand reference libraries for small molecule identification. We extended the VAE to include a chemical property decoder, trained as a multitask network, in order to shape the latent representation such that it assembles according to desired chemical properties. The approach is unique in its application to small molecule identification, with its focus on m/z and CCS, paired with its training paradigm, which involved a cascade of transfer learning iterations. This allows the network to learn as much as possible at each stage, enabling success with progressively smaller datasets without overfitting. Once trained, the network can rapidly predict chemical properties directly from structure, as well as generate candidate structures with desired chemical properties. Additionally, the ability to generate novel molecules along manifolds, defined by chemical property analogues, positions DarkChem as highly useful in a number of application areas, including metabolomics and small molecule identification, drug discovery and design, chemical forensics, and beyond.
q-bio.BM:Atomic Force Microscopy was utilized to study the morphology of Gag, {\Psi}RNA, and their binding complexes with lipids in a solution environment with 0.1{\AA} vertical and 1nm lateral resolution. TARpolyA RNA was used as a RNA control. The lipid used was phospha-tidylinositol-(4,5)-bisphosphate (PI(4,5)P2). The morphology of specific complexes Gag-{\Psi}RNA, Gag-TARpolyA RNA, Gag-PI(4,5)P2 and PI(4,5)P2-{\Psi}RNA-Gag were studied. They were imaged on either positively or negatively charged mica substrates depending on the net charges carried. Gag and its complexes consist of monomers, dimers and tetramers, which was confirmed by gel electrophoresis. The addition of specific {\Psi}RNA to Gag is found to increase Gag multimerization. Non-specific TARpolyA RNA was found not to lead to an increase in Gag multimerization. The addition PI(4,5)P2 to Gag increases Gag multimerization, but to a lesser extent than {\Psi}RNA. When both {\Psi}RNA and PI(4,5)P2 are present Gag undergoes comformational changes and an even higher degree of multimerization.
q-bio.BM:Native contacts between residues could be predicted from the amino acid sequence of proteins, and the predicted contact information could assist the de novo protein structure prediction. Here, we present a novel pipeline of a residue contact predictor AmoebaContact and a contact-assisted folder GDFold for rapid protein structure prediction. Unlike mainstream contact predictors that utilize human-designed neural networks, AmoebaContact adopts a set of network architectures that are found as optimal for contact prediction through automatic searching and predicts the residue contacts at a series of cutoffs. Different from conventional contact-assisted folders that only use top-scored contact pairs, GDFold considers all residue pairs from the prediction results of AmoebaContact in a differentiable loss function and optimizes the atom coordinates using the gradient descent algorithm. Combination of AmoebaContact and GDFold allows quick reconstruction of the protein structure, with comparable model quality to the state-of-the-art protein structure prediction methods.
q-bio.BM:Given native 2D contact map, protein 3D structure could be reconstructed with accuracy of 2A or better, and such reconstruction is a feasible computational approach for protein folding problem. The prediction accuracy from traditional methods is generally too poor to useful, but the recent deep learning model has significantly improved the accuracy. In this study, we proposed a neural network model comprising a bi-directional recurrent neural network and artificial neural network. Over the non-redundant database of all available protein 3D structures in Protein Data Bank, this deep learning model achieved an accuracy of 0.80, much higher than those of previous models. This study represents a major breakthrough in protein 2D contact map prediction and likely a major step forward for the protein folding problem.
q-bio.BM:The outcome of an epidemic is closely related to the network of interactions between the individuals. Likewise, protein functions depend on the 3D arrangement of their residues and on the underlying energetic interaction network. Borrowing ideas from the theoretical framework that has been developed to address the spreading of real diseases, we study the diffusion of a fictitious epidemic inside the protein non-bonded interaction network. Our approach allowed to probe the overall stability and the capability to propagate information in the complex 3D-structures and proved to be very efficient in addressing different problems, from the assessment of thermal stability to the identification of allosteric sites.
q-bio.BM:A small fraction of all protein structures characterized so far are entangled. The challenge of understanding the properties of these knotted proteins, and the why and the how of their natural folding process, has been taken up in the past decade with different approaches, such as structural characterization, in vitro experiments, and simulations of protein models with varying levels of complexity. The simplest among these are the lattice G\=o models, which belong to the class of structure-based models, i.e., models that are biased to the native structure by explicitly including structural data. In this review we highlight the contributions to the field made in the scope of lattice G\=o models, putting them into perspective in the context of the main experimental and theoretical results and of other, more realistic, computational approaches.
q-bio.BM:The strength or weakness of an algorithm is ultimately governed by the confidence of its result. When the domain of the problem is large (e.g. traversal of a high-dimensional space), a perfect solution cannot be obtained, so approximations must be made. These approximations often lead to a reported quantity of interest (QOI) which varies between runs, decreasing the confidence of any single run. When the algorithm further computes this final QOI based on uncertain or noisy data, the variability (or lack of confidence) of the final QOI increases. Unbounded, these two sources of uncertainty (algorithmic approximations and uncertainty in input data) can result in a reported statistic that has low correlation with ground truth.   In biological applications, this is especially applicable, as the search space is generally approximated at least to some degree (e.g. a high percentage of protein structures are invalid or energetically unfavorable) and the explicit conversion from continuous to discrete space for protein representation implies some uncertainty in the input data. This research applies uncertainty quantification techniques to the difficult protein-protein docking problem, first showing the variability that exists in existing software, and then providing a method for computing probabilistic certificates in the form of Chernoff-like bounds. Finally, this paper leverages these probabilistic certificates to accurately bound the uncertainty in docking from two docking algorithms, providing a QOI that is both robust and statistically meaningful.
q-bio.BM:The problem of determining which nucleotides of an RNA sequence are paired or unpaired in the secondary structure of an RNA, which we call RNA state inference, can be studied by different machine learning techniques. Successful state inference of RNA sequences can be used to generate auxiliary information for data-directed RNA secondary structure prediction. Bidirectional long short-term memory (LSTM) neural networks have emerged as a powerful tool that can model global nonlinear sequence dependencies and have achieved state-of-the-art performances on many different classification problems. This paper presents a practical approach to RNA secondary structure inference centered around a deep learning method for state inference. State predictions from a deep bidirectional LSTM are used to generate synthetic SHAPE data that can be incorporated into RNA secondary structure prediction via the Nearest Neighbor Thermodynamic Model (NNTM). This method produces predicted secondary structures for a diverse test set of 16S ribosomal RNA that are, on average, 25 percentage points more accurate than undirected MFE structures. These improvements range from several percentage points for some sequences to nearly 50 percentage points for others. Accuracy is highly dependent on the success of our state inference method, and investigating the global features of our state predictions reveals that accuracy of both our state inference and structure inference methods are highly dependent on the similarity of the sequence to the dataset. This paper presents a deep learning state inference tool, trained and tested on 16S ribosomal RNA. Converting these state predictions into synthetic SHAPE data with which to direct NNTM can result in large improvements in secondary structure prediction accuracy, as shown on a test set of 16S rRNA.
q-bio.BM:The total phenols and flavonoids content of different polar solvent extracts from Aurea Helianthus flowers, and their antioxidant activity were determined. The ethanol extract of Aurea Helianthus flowers were suspended in water and fractionated using different polar solvents; hexane, chloroform, ethyl acetate, butanol and water. The parameters of each extract mentioned above were determined using Floin-ciocalteu reagent(FCR) method, AlCl3 colorimetry method, ferric reducing ability of plasma(FRAP) assay, total antioxidant activity(TAA) assay and DPPH radical scavenging assay. The highest total phenols content(516.21 mg GAE/g) and flavonoids content(326.06 mg QCE/g) were obtained in ethyl acetate extract, the correlation between TPC and TFC assay was founded to be 0.967. All polar solvent extracts of Aurea Helianthus flowers showed significant antioxidant effects, the hightest inhibition was obtained in ethyl acetate and choroform extracts and the lowest inhibition in the water extract. There is a good correlation of total phenols and flavonoids content with antioxidant activity. This work indicated that the polar solvent extracts of Aurea Helianthus flowers contain high phenols and flavonoids content and exhibited antioxidant activities in vitro, therefore, could be candidates for use as natural antioxidant.
q-bio.BM:The effects of purification methods and its hypolipidemic function on the total flavonoids of Aurea Helianthus flower were investigated. Liquid-liquid extraction of ethanol extract from Aurea Helianthus flower was carried out by using different polar solvents. The extract with the highest total flavonoid content was selected, and the optimal conditions for purification of total flavonoids were determined by purification with macroporous resin. The human digestive environment was simulated in vitro, and the binding ability of different flavonoid samples to three kinds of cholate was compared. The results showed that the purity of total flavonoids in ethanol extract was 27.8%, the purity of total flavonoids in ethyl acetate extract was 46.4%, and the purity was increased by 18.6%. Subsequent purification with AB-8 macroporous resin; loading of total flavonoids at a concentration of 5.5 mg/mL, flow rate of 1.5 mL/min, 110 mL; use of 75% ethanol, 80 mL as eluent at a flow rate of 1.5 mL The elution at /min resulted in a total flavonoid purity of 83.5 % and an increase of 37.1%, and a good purification effect was obtained. The binding rate of total flavonoids purified by AB-8 macroporous resin to sodium taurocholate, sodium glycocholate and sodium cholate was 88.2%, 73.2% and 75.8 %, respectively. The binding ability was the strongest, and the others were ethyl acetate. Extract, ethanol extract. The purity of total flavonoids showed a good correlation with the binding capacity of cholate, and the correlation coefficient was between 0.963 and 0.988. The total flavonoids of Aurea Helianthus flower have good bile acid binding ability and can be used as the focus of natural hypolipidemic substances.
q-bio.BM:Current methods for investigation of receptor - ligand interactions in drug discovery are based on three-dimensional complementarity of receptor and ligand surfaces, and they include pharmacophore modelling, QSAR, molecular docking etc. Those methods only consider short-range molecular interactions (distances <5A), and not include long-range interactions (distances >5A) which are essential for kinetic of biochemical reactions because they influence the number of productive collisions between interacting molecules. Previously was shown that the electron-ion interaction potential (EIIP) represents the physical property which determines the long-range properties of biological molecules. This molecular descriptor served as a base for development of the informational spectrum method (ISM), a virtual spectroscopy method for investigation of protein-protein interactions. In this paper, we proposed a new approach to treat small molecules as linear entities, allowing study of the small molecule - protein interaction by ISM. We analyzed here 21 sets of KEGG drug-protein interactions and showed that this new approach allows an efficient discrimination between biologically active and inactive ligands, and consistence with AA regions of their binding site on the target protein.
q-bio.BM:Estrogen receptors (ERs) are a group of proteins activated by 17$\beta$-estradiol. The endocrine-disrupting chemicals (EDCs) mimic estrogen action by bind directly to the ligand binding domain of ER. From this perspective, ER represent a good model for identifying and assessing the health risk of potential EDCs. This ability is best reflected by the ligand-ER binding energy. Multilayer fragment molecular orbital (MFMO) calculations were performed which allowed us to obtain the binding energy using a calculation scheme that considers the molecular interactions that occur on the following model systems: the bound and free receptor, 17$\beta$-estradiol and a water cluster. The bound and free receptor and 17$\beta$-estradiol were surrounded by a water shell containing the same number of molecules as the water cluster. The structures required for MFMO calculations were obtained from molecular dynamics simulations and cluster analysis. Attractive dispersion interactions were observed between 17$\beta$-estradiol and the binding site hydrophobic residues. In addition, strong electrostatic interactions were found between 17$\beta$-estradiol and the following charged/polarized residues: Glu 353, His 524 and Arg 394. The FMO2-RHF/STO-3G:MP2/6-31G(d) weighted binding energy was of -67.2 kcal/mol. We hope that the model developed in this study can be useful for identifying and assessing the health risk of potential EDCs.
q-bio.BM:Significant progress in computer hardware and software have enabled molecular dynamics (MD) simulations to model complex biological phenomena such as protein folding. However, enabling MD simulations to access biologically relevant timescales (e.g., beyond milliseconds) still remains challenging. These limitations include (1) quantifying which set of states have already been (sufficiently) sampled in an ensemble of MD runs, and (2) identifying novel states from which simulations can be initiated to sample rare events (e.g., sampling folding events). With the recent success of deep learning and artificial intelligence techniques in analyzing large datasets, we posit that these techniques can also be used to adaptively guide MD simulations to model such complex biological phenomena. Leveraging our recently developed unsupervised deep learning technique to cluster protein folding trajectories into partially folded intermediates, we build an iterative workflow that enables our generative model to be coupled with all-atom MD simulations to fold small protein systems on emerging high performance computing platforms. We demonstrate our approach in folding Fs-peptide and the $\beta\beta\alpha$ (BBA) fold, FSD-EY. Our adaptive workflow enables us to achieve an overall root-mean squared deviation (RMSD) to the native state of 1.6$~\AA$ and 4.4~$\AA$ respectively for Fs-peptide and FSD-EY. We also highlight some emerging challenges in the context of designing scalable workflows when data intensive deep learning techniques are coupled to compute intensive MD simulations.
q-bio.BM:Cryo-Electron Microscopy (cryo-EM) has become an extremely powerful method for resolving structural details of large biomolecular complexes. However, challenging problems in single-particle methods remain open because of (1) the low signal-to-noise ratio in EM; and (2) the potential anisotropy and lack of coverage of projection directions relative to the body-fixed coordinate system for some complexes. Whereas (1) is usually addressed by class averaging (and increasingly due to rapid advances in microscope and sensor technology), (2) is an artifact of the mechanics of interaction of biomolecular complexes and the vitrification process. In the absence of tilt series, (2) remains a problem, which is addressed here by supplementing EM data with Small-Angle X-Ray Scattering (SAXS). Whereas SAXS is of relatively low resolution and contains much lower information content than EM, we show that it is nevertheless possible to use SAXS to fill in blind spots in EM in difficult cases where the range of projection directions is limited.
q-bio.BM:We present the assembly category assessment in the 13th edition of the CASP community-wide experiment. For the second time, protein assemblies constitute an independent assessment category. Compared to the last edition we see a clear uptake in participation, more oligomeric targets released, and consistent, albeit modest, improvement of the predictions quality. Looking at the tertiary structure predictions we observe that ignoring the oligomeric state of the targets hinders modelling success. We also note that some contact prediction groups successfully predicted homomeric interfacial contacts, though it appears that these predictions were not used for assembly modelling. Homology modelling with sizeable human intervention appears to form the basis of the assembly prediction techniques in this round of CASP. Future developments should see more integrated approaches to modelling where multiple subunits are a natural part of the modelling process, which would benefit the structure prediction field as a whole.
q-bio.BM:Virtual reality is a powerful tool with the ability to immerse a user within a completely external environment. This immersion is particularly useful when visualizing and analyzing interactions between small organic molecules, molecular inorganic complexes, and biomolecular systems such as redox proteins and enzymes. A common tool used in the biomedical community to analyze such interactions is the APBS software, which was developed to solve the equations of continuum electrostatics for large biomolecular assemblages. Numerous applications exist for using APBS in the biomedical community including analysis of protein ligand interactions and APBS has enjoyed widespread adoption throughout the biomedical community. Currently, typical use of the full APBS toolset is completed via the command line followed by visualization using a variety of two-dimensional external molecular visualization software. This process has inherent limitations: visualization of three-dimensional objects using a two-dimensional interface masks important information within the depth component. Herein, we have developed a single application, UnityMol-APBS, that provides a dual experience where users can utilize the full range of the APBS toolset, without the use of a command line interface, by use of a simple \ac{GUI} for either a standard desktop or immersive virtual reality experience.
q-bio.BM:Evidence accumulated over the past decade provides support for liquid-liquid phase separation as the mechanism underlying the formation of biomolecular condensates, which include not only membraneless organelles such as nucleoli and RNA granules, but additional assemblies involved in transcription, translation and signaling. Understanding the molecular mechanisms of condensate function requires knowledge of the structures of their constituents. Current knowledge suggests that structures formed via multivalent domain-motif interactions remain largely unchanged within condensates. Two different viewpoints exist regarding structures of disordered low-complexity domains within condensates; one argues that low-complexity domains remain largely disordered in condensates and their multivalency is encoded in short motifs called stickers, while the other argues that the sequences form cross-beta structures resembling amyloid fibrils. We review these viewpoints and highlight outstanding questions that will inform structure-function relationships for biomolecular condensates.
q-bio.BM:Grid diagrams with their relatively simple mathematical formalism provide a convenient way to generate and model projections of various knots. It has been an open question whether these 2D diagrams can be used to model a complex 3D process such as the topoisomerase-mediated preferential unknotting of DNA molecules. We model here topoisomerase-mediated passages of double-stranded DNA segments through each other using the formalism of grid diagrams. We show that this grid diagram-based modelling approach captures the essence of the preferential unknotting mechanism, based on topoisomerase selectivity of hooked DNA juxtapositions as the sites of intersegmental passages. We show that grid diagram-based approach provide an important, new and computationally convenient framework for investigating entanglement in biopolymers.
q-bio.BM:Filament formation by non-cytoskeletal enzymes has been known for decades, yet only relatively recently has its wide-spread role in enzyme regulation and biology come to be appreciated. This comprehensive review summarizes what is known for each enzyme confirmed to form filamentous structures in vitro, and for the many that are known only to form large self-assemblies within cells. For some enzymes, studies describing both the in vitro filamentous structures and cellular self-assembly formation are also known and described. Special attention is paid to the detailed structures of each type of enzyme filament, as well as the roles the structures play in enzyme regulation and in biology. Where it is known or hypothesized, the advantages conferred by enzyme filamentation are reviewed. Finally, the similarities, differences, and comparison to the SgrAI system are also highlighted.
q-bio.BM:Peptide nucleic acids (PNAs) are artificial nucleic acids with a peptide backbone instead of sugar phosphate backbone of DNA or RNA. Their resistance to degradation, selectivity and greater binding affinity in comparison to usual nucleic acids led to consideration of their great potential for different applications. For example, they can be used in molecular diagnostics and antisense therapeutics such as antimicrobial agents or gene regulatory tools. On the other hand, large hydrophilic property of PNA molecules, which inhibit them to cross cell membranes readily, is an obstacle for their delivery to considered target cells and a limiting criterion for their applications. Therefore, PNA delivery technologies have been developing to hurdle this limitation. For example, addition of lysine residues, charged membrane penetrating peptide sequences, PNAs conjugated with antibodies or steroids, cationic liposomes as carriers of PNA conjugates, protective peptides and technology of photochemical internalization (PCI) as well as by the recent technology of nanoparticle-based delivery have been employed. In this article we compared different delivery technologies which can be applicable to PNAs. As a result nanoparticle-based delivery showed more advantages in comparison to others and its application is growing fast. Keywords: Target cell delivery, Peptide nucleic acid, PNA, Delivery.
q-bio.BM:The ability of homologous chromosomes (or selected chromosomal loci) to pair specifically in the apparent absence of DNA breakage and recombination represents a prominent feature of eukaryotic biology. The mechanism of homology recognition at the basis of such recombination-independent pairing has remained elusive. A number of studies have supported the idea that sequence homology can be sensed between intact DNA double helices in vivo. In particular, recent analyses of the two silencing phenomena in fungi, known as repeat-induced point mutation (RIP) and meiotic silencing by unpaired DNA (MSUD), have provided genetic evidence for the existence of the direct homologous dsDNA-dsDNA pairing. Both RIP and MSUD likely rely on the same search strategy, by which dsDNA segments are matched as arrays of interspersed base-pair triplets. This process is general and very efficient, yet it proceeds normally without the RecA/Rad51/Dmc1 proteins. Further studies of RIP and MSUD may yield surprising insights into the function of DNA in the cell.
q-bio.BM:Intrinsically disordered proteins (IDPs) are important for biological functions. In contrast to folded proteins, molecular recognition among certain IDPs is "fuzzy" in that their binding and/or phase separation are stochastically governed by the interacting IDPs' amino acid sequences while their assembled conformations remain largely disordered. To help elucidate a basic aspect of this fascinating yet poorly understood phenomenon, the binding of a homo- or hetero-dimeric pair of polyampholytic IDPs is modeled statistical mechanically using cluster expansion. We find that the binding affinities of binary fuzzy complexes in the model correlate strongly with a newly derived simple "jSCD" parameter readily calculable from the pair of IDPs' sequence charge patterns. Predictions by our analytical theory are in essential agreement with coarse-grained explicit-chain simulations. This computationally efficient theoretical framework is expected to be broadly applicable to rationalizing and predicting sequence-specific IDP-IDP polyelectrostatic interactions.
q-bio.BM:One algorithm to predict protein structure is the residual dipolar coupling based residue assembly and filter tool (REDCRAFT). This algorithm exploits an exponential reduction of the search space of all possible structures to find a structure that best fits a set of experimental residual dipolar couplings. However, the minimum amount of data required to successfully determine a protein's structure using REDCRAFT has not been previously investigated. Here we explore the effect of reducing the amount of data used to fold proteins. Our goal is to reduce experimental data collection times while retaining the accuracy levels previously achieved with larger amounts of data. We also investigate incorporating a priori secondary structure information into REDCRAFT to improve its structure prediction ability.
q-bio.BM:It remains a challenging task to generate a vast variety of novel compounds with desirable pharmacological properties. In this work, a generative network complex (GNC) is proposed as a new platform for designing novel compounds, predicting their physical and chemical properties, and selecting potential drug candidates that fulfill various druggable criteria such as binding affinity, solubility, partition coefficient, etc. We combine a SMILES string generator, which consists of an encoder, a drug-property controlled or regulated latent space, and a decoder, with verification deep neural networks, a target-specific three-dimensional (3D) pose generator, and mathematical deep learning networks to generate new compounds, predict their drug properties, construct 3D poses associated with target proteins, and reevaluate druggability, respectively. New compounds were generated in the latent space by either randomized output, controlled output, or optimized output. In our demonstration, 2.08 million and 2.8 million novel compounds are generated respectively for Cathepsin S and BACE targets. These new compounds are very different from the seeds and cover a larger chemical space. For potentially active compounds, their 3D poses are generated using a state-of-the-art method. The resulting 3D complexes are further evaluated for druggability by a championing deep learning algorithm based on algebraic topology, differential geometry, and algebraic graph theories. Performed on supercomputers, the whole process took less than one week. Therefore, our GNC is an efficient new paradigm for discovering new drug candidates.
q-bio.BM:Recently, molecular fingerprints extracted from three-dimensional (3D) structures using advanced mathematics, such as algebraic topology, differential geometry, and graph theory have been paired with efficient machine learning, especially deep learning algorithms to outperform other methods in drug discovery applications and competitions. This raises the question of whether classical 2D fingerprints are still valuable in computer-aided drug discovery. This work considers 23 datasets associated with four typical problems, namely protein-ligand binding, toxicity, solubility and partition coefficient to assess the performance of eight 2D fingerprints. Advanced machine learning algorithms including random forest, gradient boosted decision tree, single-task deep neural network and multitask deep neural network are employed to construct efficient 2D-fingerprint based models. Additionally, appropriate consensus models are built to further enhance the performance of 2D-fingerprintbased methods. It is demonstrated that 2D-fingerprint-based models perform as well as the state-of-the-art 3D structure-based models for the predictions of toxicity, solubility, partition coefficient and protein-ligand binding affinity based on only ligand information. However, 3D structure-based models outperform 2D fingerprint-based methods in complex-based protein-ligand binding affinity predictions.
q-bio.BM:Aligning multiple protein structures can yield valuable information about structural similarities among related proteins, as well as provide insight into evolutionary relationships between proteins in a family. We have developed an algorithm (msTALI) for aligning multiple protein structures using biochemical and biophysical properties, including torsion angles, secondary structure, hydrophobicity, and surface accessibility. The algorithm is a progressive alignment algorithm motivated by popular techniques from multiple sequence alignment. It has demonstrated success in aligning the major structural regions of a set of proteins from the s/r kinase family. The algorithm was also successful at aligning functional residues of these proteins. In addition, the algorithm was also successful in aligning seven members of the acyl carrier protein family, including both experimentally derived as well as computationally modeled structures.
q-bio.BM:Hydrostatic pressure is a common perturbation to probe the conformations of proteins. There are two common forms of pressure dependent potentials of mean force (PMFs) derived from hydrophobic molecules available for the coarse grained molecular simulations of protein folding and unfolding under hydrostatic pressure. Although both PMF includes a desolvation barrier separating the well of a direct contact and the well of a solvent mediated contact, how these features vary with hydrostatic pressure is still debated. There is a need of a systematic comparison of these two PMFs on a protein. We investigated the two different pressure dependencies on the desolvation potential in a structure based protein model using coarse grained molecular simulations. We compared them to the known behavior a real protein based on experimental evidence. We showed that the protein s folding transition curve on the pressure temperature phase diagram depends on the relationship between the potential well minima and pressure. For protein that reduces the total volume under pressure, it is essential for the PMF to carry the feature that the direct contact well is essential less stable than the water mediated contact well at high pressure. We also comment on the practicality and importance of structure based minimalist models for understanding the phenomenological behavior of a protein under a wide range of phase space.
q-bio.BM:The ability to switch on the activity of an enzyme through its spontaneous reconstitution has proven to be a valuable tool in fundamental studies of enzyme structure/reactivity relationships or in the design of artificial signal transduction systems in bioelectronics, synthetic biology, or bioanalytical applications. In particular, those based on the spontaneous reconstitution/activation of the apo-PQQ-dependent soluble glucose dehydrogenase (sGDH) from Acinetobacter calcoaceticus were widely developed. However, the reconstitution mechanism of sGDH with its two cofactors, i.e. the pyrroloquinoline quinone (PQQ) and Ca2+, remains unknown. The objective here is to elucidate this mechanism by stopped-flow kinetics under single-turnover conditions. The reconstitution of sGDH exhibited biphasic kinetics, characteristic of a square reaction scheme associated to two activation pathways. From a complete kinetic analysis, we were able to fully predict the reconstitution dynamic, but also to demonstrate that when PQQ first binds to the apo-sGDH, it strongly impedes the access of Ca2+ to its enclosed position at the bottom of the enzyme binding site, thereby greatly slowing down the reconstitution rate of sGDH. This slow calcium insertion may purposely be accelerated by providing more flexibility to the Ca2+ binding loop through the specific mutation of the calcium coordinating P248 proline residue, reducing thus the kinetic barrier to calcium ion insertion. The dynamic nature of the reconstitution process is also supported by the observation of a clear loop shift and a reorganization of the hydrogen bonding network and van der Waals interactions observed in both active sites of the apo and holo forms, a structural change modulation that was revealed from the refined X-ray structure of apo-sGDH (PDB:5MIN).
q-bio.BM:Bacterial infectious diseases are a major threat to human health. Timely and sensitive pathogenic bacteria detection is crucial in identifying the bacterial contaminations and preventing the spread of infectious diseases. Due to limitations of conventional bacteria detection techniques there have been concerted research efforts towards development of new biosensors. Biosensors offering label free, whole bacteria detection are highly desirable over those relying on label based or pathogenic molecular components detection. The major advantage is eliminating the additional time and cost required for labeling or extracting the desired bacterial components. Here, we demonstrate rapid, sensitive and label free E. coli detection utilizing interferometric reflectance imaging enhancement allowing for visualizing individual pathogens captured on the surface. Enabled by our ability to count individual bacteria on a large sensor surface, we demonstrate a limit of detection of 2.2 CFU/ml from a buffer solution with no sample preparation. To the best of our knowledge, this high level of sensitivity for whole E. coli detection is unprecedented in label free biosensing. The specificity of our biosensor is validated by comparing the response to target bacteria E. coli and non target bacteria S. aureus, K. pneumonia and P. aeruginosa. The biosensor performance in tap water also proves that its detection capability is unaffected by the sample complexity. Furthermore, our sensor platform provides high optical magnification imaging and thus validation of recorded detection events as the target bacteria based on morphological characterization. Therefore, our sensitive and label free detection method offers new perspectives for direct bacterial detection in real matrices and clinical samples.
q-bio.BM:The second domain of gelsolin (G2) hosts mutations responsible for a hereditary form of amyloidosis. The active form of gelsolin is Ca2+-bound; it is also a dynamic protein, hence structural biologists often rely on the study of the isolated G2. However, the wild type G2 structure that have been used so far in comparative studies is bound to a crystallographic Cd2+, in lieu of the physiological calcium. Here, we report the wild type structure of G2 in complex with Ca2+ highlighting subtle ion-dependent differences. Previous findings on different G2 mutations are also briefly revised in light of these results.
q-bio.BM:Mutations in the gelsolin protein are responsible for a rare conformational disease known as AGel amyloidosis. Four of these mutations are hosted by the second domain of the protein (G2): D187N/Y, G167R and N184K. The impact of the latter has been so far evaluated only by studies on the isolated G2. Here we report the characterization of full-length gelsolin carrying the N184K mutation and compare the findings with those obtained on the wild type and the other variants. The crystallographic structure of the N184K variant in the Ca2+-free conformation shows remarkable similarities with the wild type protein. Only minimal local rearrangements can be observed and the mutant is as efficient as the wild type in severing filamentous actin. However, thermal stability of the pathological variant is compromised in the Ca2+-free conditions. These data suggest that the N to K substitution causes a local disruption of the H-bond network in the core of the G2 domain. Such a subtle rearrangement of the connections does not lead to significant conformational changes but severely affects the stability of the protein.
q-bio.BM:Fatty acid sugar esters represent an important class of non-ionic bio-based surfactants. They can be synthesized from vinyl fatty acids and sugars with enzyme as a catalyst. Herein, the influence of the solvent, the lipase and the temperature on a model reaction between vinyl palmitate and glucose via enzymatic catalysis has been investigated and the reaction conditions optimized. Full conversion into 6-O-glucose palmitate was reached in 40 hours in acetonitrile starting from a reactant ratio 1:1, at only 5%-wt loading of lipase from Candida antarctica B (CALB) without the presence of molecular sieves.
q-bio.BM:Predicting the three-dimensional (3D) functional structures of proteins remains an important computational milestone in molecular biology to be achieved. This feat is hinged on a clear understanding of the mechanism which proteins use to fold into their native structures. Since Levinthal's paradox, there has been a lot of progress in understanding this mechanism. Most of the earlier attempts were caught between assigning either hydrophobic interactions or hydrogen bonding as the dominant folding force. However, a consensus now seems to be emerging about hydrogen bonding being a stronger force. Interestingly, a view from chaperone action may further throw some light on the nature of the folding mechanism. Thus the very mechanisms which prevent protein aggregation and misfolding, could help us have a better understanding of the folding mechanism itself.
q-bio.BM:Mikhail Shapiro's lab investigated a promising new method for non-invasive control of calcium currents in individual cells in the nervous system by the selective heating of nanoparticles and show that simple physical laws, properly applied, explain what is happening, and so can be a foundation for constructing improved methods and techniques. They use custom built instrumentation and detailed quantitative measurement to show that special surface properties are not needed to explain their results. Their work is taken as an example of the need for quantitative measurement in biophysics and biology in general. Classical examples are cited and the argument is made that the success of structural and molecular biology has hidden the need for quantitative measurements and controls. Semiconductor and computational electronics is cited as a science even more successful than structural and molecular biology that depends on quantitative measurement, controls, and analysis.
q-bio.BM:A novel technique was demonstrated that overcome important drawbacks to crosslink cells by irradiation with ultrashort UV laser pulses (L-crosslinking). To use this technique coupled to Chromatin ImmunoPrecipitation (ChIP) in a high throughput context, a pre-screening fast method needs to be implemented to set up suitable irradiation conditions of the cell sample for efficient L-crosslinking with no final and long ChIP analysis. Here a fast method is reported where living human cells have been first transfected with a vector coding for Estrogen Receptor {\alpha} (ER{\alpha}), linked to Green Florescent protein (ER{\alpha}-GFP), so that the well-known interaction between the Estrogen Receptor Elements (ERE) region of the cell DNA and the ER{\alpha} protein can be detected by studying the fluorometric response of the irradiated cells. The damage induced to cells by UV irradiation is characterized by looking at DNA integrity, proteins stability and cellular viability. A second novel approach is presented to analyze or re-visit DNA and RNA sequences and their molecular configurations. This approach is based on methods derived from Chern-Simons super-gravity adapted to describe mutations in DNA/RNA strings, as well as interactions between nucleic acids. As a preliminary case we analyze the KRAS human gene sequence and some of its mutations. Interestingly, our model shows how the Chern-Simons current are capable to characterize the mutations within a sequence, in particular giving a quantitative indication of the mutation likelihood.
q-bio.BM:The analysis of coevolution of residues in homologous proteins is a powerful tool to predict their native conformation. The standard framework in which coevolutionary analysis is usually worked out is that of equilibrium Potts models, assuming that proteins have evolved for enough time to reach thermodynamic equilibrium in sequence space. Here we propose a model to describe correlations in sequences based on an explicit description of the evolutionary kinetics of proteins. We show that this procedure improves the correct prediction of native contacts with respect to equilibrium--based models.
q-bio.BM:All stem cell fate transitions, including the metabolic reprogramming of stem cells and the somatic reprogramming of fibroblasts into pluripotent stem cells, can be understood from a unified theoretical model of cell fates. Each cell fate transition can be regarded as a phase transition in DNA supercoiling. However, there has been a dearth of quantitative biophysical models to explain and predict the behaviors of these phase transitions. The generalized Ising model is proposed to define such phase transitions. The model predicts that, apart from temperature-induced phase transitions, there exists DNA torsion frequency-induced phase transitions. Major transitions in epigenetic states, from stem cell activation to differentiation and reprogramming, can be explained by such torsion frequency-induced phase transitions, with important implications for regenerative medicine and medical diagnostics in the future.
q-bio.BM:The 2019 novel coronavirus (2019-nCoV) is currently causing a widespread outbreak centered on Hubei province, China and is a major public health concern. Taxonomically 2019-nCoV is closely related to SARS-CoV and SARS-related bat coronaviruses, and it appears to share a common receptor with SARS-CoV (ACE-2). Here, we perform structural modeling of the 2019-nCoV spike glycoprotein. Our data provide support for the similar receptor utilization between 2019-nCoV and SARS-CoV, despite a relatively low amino acid similarity in the receptor binding module. Compared to SARS-CoV, we identify an extended structural loop containing basic amino acids at the interface of the receptor binding (S1) and fusion (S2) domains, which we predict to be proteolytically-sensitive. We suggest this loop confers fusion activation and entry properties more in line with MERS-CoV and other coronaviruses, and that the presence of this structural loop in 2019-nCoV may affect virus stability and transmission.
q-bio.BM:Ions in channels have been imagined as hard balls in a macroscopic mechanical model, for a very long time. Hard balls interact by collisions in such models, randomly knocking each other on and off `binding' sites in thermal motion. But ions have large charge, and the hard balls of classical models do not. The electrodynamics of charge guarantee strong correlations between the movements of ions on all time scales, even those of atomic scale thermal motion. Correlations are present whenever Maxwell's equations apply, so they are present in individual trajectories, not just averages. Indeed, in a series system like an idealized narrow channel, the correlation is perfect (within the accuracy of Maxwell's equations) because of conservation of total current (that includes Maxwell's displacement current, the ethereal ${\varepsilon }_0{\partial \boldsymbol{\mathrm{E}}}/{\partial t}$).The ethereal component of current prevents spatial variation of total current in a series system. The stochastic complexity of spatial thermal motion disappears for current }in a series system. Total current does not depend on location in a series system like a narrow ion channel on any time scale. Spatial variables are not needed in the description of total current in a one dimensional channel on any time scale, according to the Maxwell equations. Removing the spatial dependence of total current should dramatically simplify theories and simulations, one might imagine.
q-bio.BM:We have applied a computational strategy, based on the synergy of virtual screening, docking and molecular dynamics techniques, aimed at identifying possible lead compounds for the non-covalent inhibition of the main protease 3CL-pro of the SARS-Cov2 Coronavirus. Based on the recently resolved 6LU7 PDB structure, ligands were generated using a multimodal structure-based design and then optimally docked to the 6LU7 monomer. Docking calculations show that ligand-binding is strikingly similar in SARS-CoV and SARS-CoV2 main proteases, irrespectively of the protonation state of the catalytic CYS-HIS dyad. The most potent docked ligands are found to share a common binding pattern with aromatic moieties connected by rotatable bonds in a pseudo-linear arrangement. Molecular dynamics calculations fully confirm the stability in the 3CL-pro binding pocket of the most potent binder identified by docking, namely a chlorophenyl-pyridyl-carboxamide derivative.
q-bio.BM:Studying the conformations involved in the dimerization of cadherins is highly relevant to understand the development of tissue and its failure, which is associated with tumors and metastases. Experimental techniques, like X-ray crystallography, can usually report only the most stable conformations, missing minority states that could nonetheless be important for the recognition mechanism. Computer simulations could be a valid complement to the experimental approach. However, standard all-atom protein models in explicit solvent are computationally too demanding to search thoroughly the conformational space of multiple chains composed of several hundreds of amino acids. To reach this goal, we resorted to a coarse-grained model in implicit solvent. The standard problem with this kind of models is to find a realistic potential to describe their interactions. We used coevolutionary information from cadherin alignments, corrected by a statistical potential, to build an interaction potential which is agnostic of the experimental conformations of the protein. Using this model, we explored the conformational space of multi-chain systems and validated the results comparing with experimental data. We identified dimeric conformations that are sequence-specific and that can be useful to rationalize the mechanism of recognition between cadherins.
q-bio.BM:Both molecular mechanical and quantum mechanical calculations play an important role in describing the behavior and structure of molecules. In this work, we compare for the same peptide systems the results obtained from folding molecular dynamics simulations with previously reported results from quantum mechanical calculations. More specifically, three molecular dynamics simulations of 5 $\mu$s each in explicit water solvent were carried out for three Asn-Gly-containing heptapeptides, in order to study their folding and dynamics. Previous data, based on quantum mechanical calculations and the DFT methods have shown that these peptides adopt $\beta$-turn structures in aqueous solution, with type I' $\beta$-turn being the most preferred motif. The results from our analyses indicate that for the given system the two methods diverge in their predictions. The possibility of a force field-dependent deficiency is examined as a possible source of the observed discrepancy.
q-bio.BM:Oligonucleotide-based agents have the potential to treat or cure almost any disease, and are one of the key therapeutic drug classes of the future. Bioconjugated oligonucleotides, a subset of this class, are emerging from basic research and being successfully translated to the clinic. In this review, we first briefly describe two approaches for inhibiting specific genes using oligonucleotides -- antisense DNA (ASO) and RNA interference (RNAi) -- followed by a discussion on delivery to cells. We then summarize and analyze recent developments in bioconjugated oligonucleotides including those possessing GalNAc, cell penetrating peptides, $\alpha$-tocopherol, aptamers, antibodies, cholesterol, squalene, fatty acids, or nucleolipids. These novel conjugates provide a means to enhance tissue targeting, cell internalization, endosomal escape, target binding specificity, resistance to nucleases, and more. We next describe those bioconjugated oligonucleotides approved for patient use or in clinical trials. Finally, we summarize the state of the field, describe current limitations, and discuss future prospects. Biocon-jugation chemistry is at the centerpiece of this therapeutic oligonucleotide revolution, and significant opportunities exist for development of new modification chemistries, for mechanistic studies at the chemical-biology interface, and for translating such agents to the clinic.
q-bio.BM:Recent advances in distance-based protein folding have led to a paradigm shift in protein structure prediction. Through sufficiently precise estimation of the inter-residue distance matrix for a protein sequence, it is now feasible to predict the correct folds for new proteins much more accurately than ever before. Despite the exciting progress, a dedicated visualization system that can dynamically capture the distance-based folding process is still lacking. Most molecular visualizers typically provide only a static view of a folded protein conformation, but do not capture the folding process. Even among the selected few graphical interfaces that do adopt a dynamic perspective, none of them are distance-based. Here we present PolyFold, an interactive visual simulator for dynamically capturing the distance-based protein folding process through real-time rendering of a distance matrix and its compatible spatial conformation as it folds in an intuitive and easy-to-use interface. PolyFold integrates highly convergent stochastic optimization algorithms with on-demand customizations and interactive manipulations to maximally satisfy the geometric constraints imposed by a distance matrix. PolyFold is capable of simulating the complex process of protein folding even on modest personal computers, thus making it accessible to the general public for fostering citizen science. Open source code of PolyFold is freely available for download at https://github.com/Bhattacharya-Lab/PolyFold. It is implemented in cross-platform Java and binary executables are available for macOS, Linux, and Windows.
q-bio.BM:Motivation: The coronavirus disease 2019 (COVID-19) caused by a new type of coronavirus has been emerging from China and led to thousands of death globally since December 2019. Despite many groups have engaged in studying the newly emerged virus and searching for the treatment of COVID-19, the understanding of the COVID-19 target-ligand interactions represents a key chal-lenge. Herein, we introduce COVID-19 Docking Server, a web server that predicts the binding modes between COVID-19 targets and the ligands including small molecules, peptides and anti-bodies. Results: Structures of proteins involved in the virus life cycle were collected or constructed based on the homologs of coronavirus, and prepared ready for docking. The meta platform provides a free and interactive tool for the prediction of COVID-19 target-ligand interactions and following drug discovery for COVID-19.
q-bio.BM:Mass spectrometry has experienced a rapid development since its first application for protein analysis in the 1980s. While the most common use of mass spectrometry for protein analysis is identification and quantification workflows on peptides (digested from their parent protein), there is also a rapidly growing use of mass spectrometry for structural proteomics. One example is the analysis of cross-linked proteins that can give valuable structural information, complementing the information gained by classical protein structure determination methods, useful for integrated methods of structure determination and modeling. For a broad and reproducible application of cross-linking mass spectrometry a standardized representation of cross-linking experimental results is necessary. This paper describes the developing and release of the xlmod ontology from the HUPO-PSI. xlmod contains terms for the description of reagents used in cross-linking experiments and of cross-linker related chemical modifications together with their main properties relevant for planning and performing cross-linking experiments. We also describe how xlmod is used within the new release of HUPO-PSI-s mzIdentML data standard, for reporting the used cross-linking reagents and results in a consistent manner. In addition xlmod contains terms for GC-MS and LC-MS derivatization reagents for specifying them in the upcoming mzTab-M and mzTab-L formats.
q-bio.BM:The Protein Data Bank (PDB) today contains more than 153,000 entries with the 3-dimensional structures of biological macromolecules. Using the rich resources of this repository, it is possible identifying subsets with specific, interesting properties for different applications. Our research group prepared an automatically updated list of amyloid- and probably amyloidogenic molecules, the PDB\_Amyloid collection, which is freely available at the address \url{http://pitgroup.org/amyloid}. This resource applies exclusively the geometric properties of the steric structures for identifying amyloids. In the present contribution, we analyze the starting (i.e., prefix) subsequences of the characteristic, parallel beta-sheets of the structures in the PDB\_Amyloid collection, and identify further appearances of these length-5 prefix subsequences in the whole PDB data set. We have identified this way numerous proteins, whose normal or irregular functions involve amyloid formation, structural misfolding, or anti-coagulant properties, simply by containing these prefixes: including the T-cell receptor (TCR), bound with the major histocompatibility complexes MHC-1 and MHC-2; the p53 tumor suppressor protein; a mycobacterial RNA polymerase transcription initialization complex; the human bridging integrator protein BIN-1; and the tick anti-coagulant peptide TAP.
q-bio.BM:Coronavirus (COVID-19) outbreak in late 2019 and 2020 comprises a serious and more likely a pandemic threat worldwide. Given that the disease has not approved vaccines or drugs up to now, any efforts for drug design and or clinical trails of old drugs based on their mechanism of action are worthy and creditable in such circumstances. Experienced docking experiments using the newly released coordinate structure for COVID-19 protease as a receptor and thoughtfully selected chemicals among antiviral and antibiotics drugs as ligands may be leading in this context. We selected nine drugs from HIV-1 protease inhibitors and twenty-one candidates from anti bronchitis drugs based on their chemical structures and enrolled them in blind and active site-directed dockings in different modes and in native-like conditions of interactions. Our findings suggest the binding capacity and the inhibitory potency of candidates are as follows Tipranavir>Indinavir>Atazanavir>Darunavir>Ritonavir>Amprenavir for HIV-1 protease inhibitors and Cefditoren>Cefixime>Erythromycin>Clarithromycin for anti bronchitis medicines. The drugs bioavailability, their hydrophobicity and the hydrophobic properties of their binding sites and also the rates of their metabolisms and deactivations in the human body are the next determinants for their overall effects on viral infections, the net results that should survey by clinical trials to assess their therapeutic usefulness for coronavirus infections.
q-bio.BM:A library of substituted pyrimidines was synthesized and evaluated for free radical scavenging, and in vitro cytotoxic activity in 3T3 cells. All compounds showed good free radical scavenging activity with IC50 values in the range of 42.9 + 0.31 to 438.3 3.3 {\mu}M as compared to the standard butylated hydroxytoluene having IC50 value of 128.83 2. 1 {\mu}M. The structure activity-relationship was also established. Selected analogues 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 15, 19, 20, 21, 24, 25, 26 and 28 were tested for cytotoxicity in mouse fibroblast 3T3 cell line using MTT assay, and most of the analogues showed cytotoxicity. This study has identified a number of cytotoxic novel substituted pyrimidines having free radical scavenging activities that can be used as inhibitory compounds for those cancer cells whose growth is mediated by reactive oxygen species.
q-bio.BM:Melanoma is one of the most dangerous skin malignancies due to its high metastatic tendency and high mortality. Activation of key signaling pathways enforcing melanoma progression depends on phosphorylation of tyrosine kinases, and oxidative stress. We here investigated the effect of the new bis-coumarin derivative (3,5-DCPBC) on human melanoma cell survival, growth, proliferation, migration, and intracellular redox state, and deciphered associated signal pathways. This novel derivative was found to be toxic for melanoma cells, and non-toxic for their benign counterparts, melanocytes and fibroblasts. 3,5-DCPBC inhibited cell survival, migration and proliferation of different metastatic, and non-metastatic melanoma cell lines through the profound suppression of phosphorylation of the Epidermal Growth Factor receptor, and related downstream pathways. Suppression of phosphorylation of key downstream transcription factors and different tyrosine kinases comprise JAK/STAT, SRC kinases, ERK and MAP kinases (p38alpha), all involved in melanoma progression. Simultaneous and specific targeting of multiple tyrosine kinases and corresponding key genes in melanoma cells makes 3,5-DCPBC a highly interesting anti-melanoma, and anti-metastatic drug candidate which may in the long term hold promise in the therapy of advanced melanoma.
q-bio.BM:COVID-19, a member of corona virus family is spreading its tentacles across the world due to lack of drugs at present. However, the main viral proteinase (Mpro/3CLpro) has recently been regarded as a suitable target for drug design against SARS infection due to its vital role in polyproteins processing necessary for coronavirus reproduction. The present in silico study was designed to evaluate the effect of Jensenone, a essential oil component from eucalyptus oil, on Mpro by docking study. In the present study, molecular docking studies were conducted by using 1-click dock and swiss dock tools. Protein interaction mode was calculated by Protein Interactions Calculator.The calculated parameters such as binding energy, and binding site similarity indicated effective binding of Jensenone to COVID-19 proteinase. Active site prediction further validated the role of active site residues in ligand binding. PIC results indicated that, Mpro/ Jensenone complexes forms hydrophobic interactions, hydrogen bond interactions and strong ionic interactions. Therefore, Jensenone may represent potential treatment potential to act as COVID-19 Mpro inhibitor. However, further research is necessary to investigate their potential medicinal use.
q-bio.BM:Cotranslational folding depends on the folding speed and stability of the nascent protein. It remains difficult, however, to predict which proteins cotranslationally fold. Here, we simulate evolution of model proteins to investigate how native structure influences evolution of cotranslational folding. We developed a model that connects protein folding during and after translation to cellular fitness. Model proteins evolved improved folding speed and stability, with proteins adopting one of two strategies for folding quickly. Low contact order proteins evolve to fold cotranslationally. Such proteins adopt native conformations early on during the translation process, with each subsequently translated residue establishing additional native contacts. On the other hand, high contact order proteins tend not to be stable in their native conformations until the full chain is nearly extruded. We also simulated evolution of slowly translating codons, finding that slower translation speeds at certain positions enhances cotranslational folding. Finally, we investigated real protein structures using a previously published dataset that identified evolutionarily conserved rare codons in E. coli genes and associated such codons with cotranslational folding intermediates. We found that protein substructures preceding conserved rare codons tend to have lower contact orders, in line with our finding that lower contact order proteins are more likely to fold cotranslationally. Our work shows how evolutionary selection pressure can cause proteins with local contact topologies to evolve cotranslational folding.
q-bio.BM:Here after performing docking and molecular dynamics of various small molecules derived as a secondary metabolite from fungi, we propose Flaviolin to act as potent inhibitor of 3-chymotrypsin (3C) like protease (3CLpro) of noval corona virus SARS-CoV2 responsible for pandemic condition caused by coronavirus disease 2019 (COVID-19).
q-bio.BM:Presently, India bears amongst the highest burden of non-communicable diseases such as diabetes mellitus (DM), hypertension (HT), and cardio vascular disease (CVD) and thus represents a vulnerable target to the SARS-CoV-2/COVID-19 pandemic. Involvement of the angiotensin converting enzyme 2 (ACE2) in susceptibility to infection and pathogenesis by SARS-CoV-2 is currently an actively pursued research area. An increased susceptibility to infection in individuals with DM, HT and CVD together with higher levels of circulating ACE2 in these settings presents a scenario where interaction with soluble ACE2 may result in disseminated virus-receptor complexes that could enhance virus acquisition and pathogenesis. Thus, understanding the SARS-CoV-2 receptor binding domain-ACE2 interaction, both membrane bound and in the cell free context may contribute to elucidating the role of co-morbidities in increased susceptibility to infection and pathogenesis. Both Azithromycin and Hydroxychloroquine (HCQ) have shown efficacy in mitigating viral carriage in infected individuals. Furthermore, each of these compounds generate active metabolites which in turn may also modulate virus-receptor interaction and thus influence clinical outcomes. In this study, we model the structural interaction of S1 with both full-length and soluble ACE2. Additionally, therapeutic drugs and their active metabolites were docked with soluble ACE2 protein. Our results show that S1 from either of the reported Indian sequences can bind both full-length and soluble ACE2, albeit with varying affinity that can be attributed to a reported substitution in the RBD. Furthermore, both Azythromycin and HCQ together with their active metabolites can allosterically affect, to a range of extents, binding of S1 to ACE2.
q-bio.BM:The emergence of COVID-19 has severely compromised the arsenal of antiviral and antibiotic drugs. Drug discovery is a multistep process with a high failure rate, high cost and it takes approximately 10-12 years for the development of new molecules into the clinical candidate. On the other side, drug repurposing also called old drugs for new uses, is an attractive alternative approach for a new application of marketed FDA approved or investigational drugs. In the current pandemic situation raised due to COVID-19, repurposing of existing FDA approved drugs are emerging as the first line of the treatment. The causative viral agent of this highly contagious disease and acute respiratory syndrome coronavirus (SARS-CoV) shares high nucleotide similarity. Therefore, many existing viral targets are structurally expected to be similar to SARS-CoV and likely to be inhibited by the same compounds. Here, we selected three viral key proteins based on their vital role in viral life cycle: ACE2 (helps in entry into the human host), viral nonstructural proteins RNA-dependent RNA polymerase (RdRp) NSP12, and NSP16 which helps in replication, and viral latency (invasion from immunity). The FDA approved drugs chloroquine (CQ), hydroxychloroquine (HCQ), remdesivir (RDV) and arbidol (ABD) are emerging as promising agents to combat COVID-19. Our hypothesis behind the docking studies is to determine the binding affinities of these drugs and identify the key amino acid residues playing a key role in their mechanism of action. The docking studies were carried out through Autodock and online COVID-19 docking server. Further studies on a broad range of FDA approved drugs including few more protein targets, molecular dynamics studies, in-vitro and in-vivo biological evaluation are required to identify the combination therapy targeting various stages of the viral life cycle.
q-bio.BM:The pandemic prevalence of COVID-19 has become a very serious global health issue. Scientists all over the world have been heavily invested in the discovery of a drug to combat SARS-CoV-2. It has been found that RNA-dependent RNA Polymerase (RdRp) plays a crucial role in SARS-CoV-2 replication, and thus could be a potential drug target. Here, comprehensive computational approaches including drug repurposing and molecular docking were employed to predict an effective drug candidate targeting RdRp of SARS-CoV-2. This study revealed that Rifabutin, Rifapentine, Fidaxomicin, 7-methyl-guanosine-5'-triphosphate-5'-guanosine and Ivermectin have a potential inhibitory interaction with RdRp of SARS-CoV-2, and could be effective drugs for COVID-19. In addition, virtual screening of the compounds from ZINC database also allowed the prediction of two compounds (ZINC09128258 and ZINC 09883305) with pharmacophore features that interact effectively with RdRp of SARS-CoV-2; indicating their potentiality as effective inhibitors of the enzyme. Furthermore, ADME analysis along with analysis of toxicity was also investigated to check the pharmacokinetics and drug-likeness properties of the two compounds. Comparative structural analysis of protein-inhibitor complexes revealed that positions of the amino acid Y32, K47, Y122, Y129, H133, N138, D140, T141, S709 and N781 are crucial for drug surface hotspot in the RdRp of SARS-CoV-2.
q-bio.BM:Messenger RNA (mRNA) vaccines are being used for COVID-19, but still suffer from the critical issue of mRNA instability and degradation, which is a major obstacle in the storage, distribution, and efficacy of the vaccine. Previous work showed that optimizing secondary structure stability lengthens mRNA half-life, which, together with optimal codons, increases protein expression. Therefore, a principled mRNA design algorithm must optimize both structural stability and codon usage to improve mRNA efficiency. However, due to synonymous codons, the mRNA design space is prohibitively large, e.g., there are $\sim\!10^{632}$ mRNAs for the SARS-CoV-2 Spike protein, which poses insurmountable challenges to previous methods. Here we provide a surprisingly simple solution to this hard problem by reducing it to a classical problem in computational linguistics, where finding the optimal mRNA is akin to finding the most likely sentence among similar sounding alternatives. Our algorithm, named LinearDesign, takes only 11 minutes for the Spike protein, and can jointly optimize stability and codon usage. Experimentally, without chemical modification, our designs substantially improve mRNA half-life and protein expression in vitro, and dramatically increase antibody response by up to 23$\times$ in vivo, compared to the codon-optimized benchmark. Our work enables the exploration of highly stable and efficient designs that are previously unreachable and is a timely tool not only for vaccines but also for mRNA medicine encoding all therapeutic proteins (e.g., monoclonal antibodies and anti-cancer drugs).
q-bio.BM:We report the results of our study of approved drugs as potential treatments for COVID 19, based on the application of various bioinformatics predictive methods. The drugs studied include chloroquine, ivermectin, remdesivir, sofosbuvir, boceprevir, and {\alpha}-difluoromethylornithine (DMFO). Our results indicate that these small molecules selectively bind to stable, kinetically active residues and residues adjoining them on the surface of proteins and inside protein pockets and that some prefer hydrophobic over other active sites. Our approach is not restricted to viruses and can facilitate rational drug design, as well as improve our understanding of molecular interactions, in general.
q-bio.BM:The primary cell surface receptor for SARS-CoV-2 is the angiotensin-converting enzyme 2 (ACE2). Recently it has been noticed that the viral Spike protein has an RGD motif, suggesting that cell surface integrins may be co-receptors. We examined the sequences of ACE2 and integrins with the Eukaryotic Linear Motif resource, ELM, and were presented with candidate short linear motifs (SLiMs) in their short, unstructured, cytosolic tails with potential roles in endocytosis, membrane dynamics, autophagy, cytoskeleton and cell signalling. These SLiM candidates are highly conserved in vertebrates. They suggest potential interactions with the AP2 mu2 subunit as well as I-BAR, LC3, PDZ, PTB and SH2 domains found in signalling and regulatory proteins present in epithelial lung cells. Several motifs overlap in the tail sequences, suggesting that they may act as molecular switches, often involving tyrosine phosphorylation status. Candidate LIR motifs are present in the tails of ACE2 and integrin beta3, suggesting that these proteins can directly recruit autophagy components. We also noticed that the extracellular part of ACE2 has a conserved MIDAS structural motif, which are commonly used by beta integrins for ligand binding, potentially supporting the proposal that integrins and ACE2 share common ligands. The findings presented here identify several molecular links and testable hypotheses that might help uncover the mechanisms of SARS-CoV-2 attachment, entry and replication, and strengthen the possibility that it might be possible to develop host-directed therapies to dampen the efficiency of viral entry and hamper disease progression. The strong sequence conservation means that these putative SLiMs are good candidates: Nevertheless, SLiMs must always be validated by experimentation before they can be stated to be functional.
q-bio.BM:The hydrogen peroxide is present in the living cell at small concentrations that increase under the action of the heavy ion beams in the process of anticancer therapy. The interactions of hydrogen peroxide with DNA, proteins and other biological molecules are poorly understood. In the present work the competitive binding of the hydrogen peroxide and water molecules with the DNA double helix backbone has been studied using the molecular dynamics method. The simulations have been carried out for the DNA double helix in a water solution with hydrogen peroxide molecules and Na$^{+}$ counterions. The obtained radial distribution functions of counterions, H$_2$O$_2$ and H$_2$O molecules with respect to the oxygen atoms of DNA phosphate groups have been used for the analysis of the formation of different complexes. The calculated mean residence times show that a hydrogen peroxide molecule stays at least twice as long near the phosphate group (up to 7 ps) than a water molecule (about 3 ps). The hydrogen peroxide molecules form more stable complexes with the phosphate groups of the DNA backbone than water molecules do.
q-bio.BM:The COVID-19 pandemic triggered by SARS-CoV-2 is a worldwide health disaster. Main protease is an attractive drug target among coronaviruses, due to its vital role in processing the polyproteins that are translated from the viral RNA. There is presently no exact drug or treatment for this diseases caused by SARS-CoV-2. In the present study, we report the potential inhibitory activity of some FDA approved drugs against SARS-CoV-2 main protease by molecular docking study to investigate their binding affinity in protease active site. Docking studies revealed that drug Oseltamivir (anti-H1N1 drug), Rifampin (anti-TB drug), Maraviroc, Etravirine, Indinavir, Rilpivirine (anti-HIV drugs) and Atovaquone, Quinidine, Halofantrine, Amodiaquine, Tetracylcine, Azithromycin, hydroxycholoroquine (anti-malarial drugs) among others binds in the active site of the protease with similar or higher affinity. However, the in-silico abilities of the drug molecules tested in this study, further needs to be validated by carrying out in vitro and in vivo studies. Moreover, this study spreads the potential use of current drugs to be considered and used to comprise the fast expanding SARS-CoV-2 infection.
q-bio.BM:Smokers being witnessed with the mild adverse clinical symptoms of SARS-CoV-2, the in-silico study is intended to explore the effect of nicotine binding to the soluble angiotensin converting enzyme II (ACE2) receptor with or without SARS-CoV-2 binding. Nicotine established a stable interaction with the conserved amino acid residues: Asp382, Gly405, His378 and Tyr385 through His401 of the soluble ACE2 that seals its interaction with the INS1. Also, nicotine binding has significantly reduced the affinity score of ACE2 with INS1 to -12.6 kcal/mol (versus -15.7 kcal/mol without nicotine) and the interface area to 1933.6 square Angstrom (versus 2057.3 square Angstrom without nicotine). Nicotine exhibited a higher binding affinity score with ACE2-SARS-CoV-2 complex with -6.33 kcal/mol (Vs -5.24 kcal/mol without SARS-CoV-2) and a lowered inhibitory contant value of 22.95 micromolar (Vs 151.69 micromolar without SARS-CoV). Eventhough ACE2 is not a potential receptor for nicotine binding in the healthy people, in COVID19 patients, it may exhibit better binding affinity with the ACE2 receptor. In overall, nicotines strong preference for ACE2-SARS-CoV-2 complex might drastically reduce the SARS-CoV-2 virulence by intervening the ACE2 conserved residues interaction with the spike (S1) protein of SARS-CoV-2.
q-bio.BM:Ubiquinone is an important component of the electron transfer chains in proteobacteria and eukaryotes. The biosynthesis of ubiquinone requires multiple steps, most of which are common to bacteria and eukaryotes. Whereas the enzymes of the mitochondrial pathway that produces ubiquinone are highly similar across eukaryotes, recent results point to a rather high diversity of pathways in bacteria. This review focuses on ubiquinone in bacteria, highlighting newly discovered functions and detailing the proteins that are known to participate to its biosynthetic pathways. Novel results showing that ubiquinone can be produced by a pathway independent of dioxygen suggest that ubiquinone may participate to anaerobiosis, in addition to its well established role for aerobiosis. We also discuss the supramolecular organization of ubiquinone biosynthesis proteins and we summarize the current understanding of the evolution of the ubiquinone pathways relative to those of other isoprenoid quinones like menaquinone and plastoquinone.
q-bio.BM:The recent improvements in cryo-electron microscopy (cryo-EM) in the past few years are now allowing to observe molecular complexes at atomic resolution. As a consequence, numerous structures derived from cryo-EM are now available in the Protein Data Bank. However, if for some complexes atomic resolution is reached, this is not true for all. This is also the case in cryo-electron tomography where the achievable resolution is still limited. Furthermore the resolution in a cryo-EM map is not a constant, with often outer regions being of lower resolution, possibly linked to conformational variability. Although those low to medium resolution EM maps (or regions thereof) cannot directly provide atomic structure of large molecular complexes, they provide valuable information to model the individual components and their assembly into them. Most approaches for this kind of modelling are performing rigid fitting of the individual components into the EM density map. While this would appear an obvious option, they ignore key aspects of molecular recognition, the energetics and flexibility of the interfaces. Moreover, these often restricts the modelling to a unique source of data, the EM density map. In this chapter, we describe a protocol where an EM map is used as restraint in HADDOCK to guide the modelling process.
q-bio.BM:Efficient syntheses of some new substituted pyrazoline derivatives linked to substituted benzimidazole scaffold were performed by multistep reaction sequences. All the synthesized compounds were characterized using elemental analysis and spectral studies (IR, 1D/2D NMR techniques and mass spectrometry). The synthesized compounds were screened for their antimicrobial activity against selected Gram-positive and Gram-negative bacteria, and fungi strain. The compounds with halo substituted phenyl group at C5 of the 1-phenyl pyrazoline ring (15, 16 and 17) showed significant antibacterial activity. Among the screened compounds, 17 showed most potent inhibitory activity (MIC = 64 {\mu}g mL-1) against a bacterial strain. The tested compounds werefound to be almost inactive against the fungal strain C. albicans, apart from pyrazoline-1-carbothiomide 21, which was moderately active.
q-bio.BM:In the recent years, therapeutic use of antibodies has seen a huge growth, due to their inherent proprieties and technological advances in the methods used to study and characterize them. Effective design and engineering of antibodies for therapeutic purposes are heavily dependent on knowledge of the structural principles that regulate antibody-antigen interactions. Several experimental techniques such as X-ray crystallography, cryo-electron microscopy, NMR or mutagenesis analysis can be applied, but these are usually expensive and time consuming. Therefore computational approaches like molecular docking may offer a valuable alternative for the characterisation of antibody-antigen complexes.   Here we describe a protocol for the prediction of the 3D structure of antibody-antigen complexes using the integrative modelling platform HADDOCK. The protocol consists of: 1) The identification of the antibody residues belonging to the hyper variable loops which are known to be crucial for the binding and can be used to guide the docking; 2) The detailed steps to perform docking with the HADDOCK 2.4 webserver following different strategies depending on the availability of information about epitope residues.
q-bio.BM:Antimicrobial peptides (AMPs) are anti-infectives that have potential as a novel and untapped class of biotherapeutics. Modes of action of antimicrobial peptides imply interaction with cell envelope. Comprehensive understanding of peculiarities of interactions of antimicrobial peptides with cell envelope is necessary to perform the task-oriented design of new biotherapeutics, against which for microbes it is hard to work out resistance. In order to enable a de novo design with low costs and in high throughput, in silico predictive models have to be required. To develop the performant predictive model, comprehensive knowledge on mechanisms of action of AMPs has to be possessed. The last knowledge will allow us to encode amino acid sequences expressively and to get success to the choosing of the accurate classifier of AMPs. A shared protective layer of microbial cells is inner, plasmatic membrane. The interaction of AMP with a biological membrane (native and/or artificial) is the most comprehensively studied. We provide a review of mechanisms and results of interaction of AMP with the cell membrane, relying on the survey of physicochemical, aggregative and structural features of AMPs. Potency and mechanism of action of AMP have presented in the terms of amino acid compositions and distributions of the polar and apolar residues along the chain, that is in such physicochemical features of peptides as the hydrophobicity, hydrophilicity, and amphiphilicity. Many different approaches were used to classify AMPs. The survey of the knowledge on sequences, structures, and modes of actions of AMP, allows concluding that, only the physicochemical features of AMPs give the capability to perform the unambiguous classification. Comprehensive knowledge of physicochemical features of AMP is necessary to develop task-oriented methods of design of peptide-based antibiotics de novo.
q-bio.BM:Geometry and topology are the main factors that determine the functional properties of proteins. In this work, we show how to use the Gauss linking integral (GLN) in the form of a matrix diagram - for a pair of a loop and a tail - to study both the geometry and topology of proteins with closed loops e.g. lassos. We show that the GLN method is a significantly faster technique to detect entanglement in lasso proteins in comparison with other methods. Based on the GLN technique, we conduct comprehensive analysis of all proteins deposited in the PDB and compare it to the statistical properties of the polymers. We found that there are significantly more lassos with negative crossings than those with positive ones in proteins, the average value of maxGLN (maximal GLN between loop and pieces of tail) depends logarithmically on the length of a tail similarly as in the polymers. Next, we show the how high and low GLN values correlate with the internal exibility of proteins, and how the GLN in the form of a matrix diagram can be used to study folding and unfolding routes. Finally, we discuss how the GLN method can be applied to study entanglement between two structures none of which are closed loops. Since this approach is much faster than other linking invariants, the next step will be evaluation of lassos in much longer molecules such as RNA or loops in a single chromosome.
q-bio.BM:An active loop-extrusion mechanism is regarded as the main out--of--equilibrium mechanism responsible for the structuring of megabase-sized domains in chromosomes. We developed a model to study the dynamics of the chromosome fibre by solving the kinetic equations associated with the motion of the extruder. By averaging out the position of the extruder along the chain, we build an effective equilibrium model capable of reproducing experimental contact maps based solely on the positions of extrusion--blocking proteins. We assessed the quality of the effective model using numerical simulations of chromosomal segments and comparing the results with explicit-extruder models and experimental data.
q-bio.BM:Proteins play a key role in facilitating the infectiousness of the 2019 novel coronavirus. A specific spike protein enables this virus to bind to human cells, and a thorough understanding of its 3-dimensional structure is therefore critical for developing effective therapeutic interventions. However, its structure may continue to evolve over time as a result of mutations. In this paper, we use a data science perspective to study the potential structural impacts due to ongoing mutations in its amino acid sequence. To do so, we identify a key segment of the protein and apply a sequential Monte Carlo sampling method to detect possible changes to the space of low-energy conformations for different amino acid sequences. Such computational approaches can further our understanding of this protein structure and complement laboratory efforts.
q-bio.BM:Virtual screening of phytochemicals was performed through molecular docking, simulation, in silico ADMET and drug-likeness prediction to identify the potential hits that can inhibit the effects of SARS-CoV-2. Considering the published literature on medicinal importance, total 154 phytochemicals with analogous structure from limonoids and triterpenoids were selected to search potential inhibitors for the five therapeutic protein targets of SARS-CoV-2, i.e., 3CLpro (main protease), PLpro (papain-like protease), SGp-RBD (spike glycoprotein-receptor binding domain), RdRp (RNA dependent RNA polymerase) and ACE2 (angiotensin-converting enzyme 2). The in silico computational results revealed that the phytochemicals such as glycyrrhizic acid, limonin, 7-deacetyl-7-benzoylgedunin, maslinic acid, corosolic acid, obacunone and ursolic acid were found to be effective against the target proteins of SARS-CoV-2. The protein-ligand interaction study revealed that these phytochemicals bind with the amino acid residues at the active site of the target proteins. Therefore, the core structure of these potential hits can be used for further lead optimization to design drugs for SARS-CoV-2. Also, the medicinal plants containing these phytochemicals like licorice, neem, tulsi, citrus and olives can be used to formulate suitable therapeutic approaches in traditional medicines.
q-bio.BM:Recent computational advances in the accurate prediction of protein three-dimensional (3D) structures from amino acid sequences now present a unique opportunity to decipher the interrelationships between proteins. This task entails--but is not equivalent to--a problem of 3D structure comparison and classification. Historically, protein domain classification has been a largely manual and subjective activity, relying upon various heuristics. Databases such as CATH represent significant steps towards a more systematic (and automatable) approach, yet there still remains much room for the development of more scalable and quantitative classification methods, grounded in machine learning. We suspect that re-examining these relationships via a Deep Learning (DL) approach may entail a large-scale restructuring of classification schemes, improved with respect to the interpretability of distant relationships between proteins. Here, we describe our training of DL models on protein domain structures (and their associated physicochemical properties) in order to evaluate classification properties at CATH's "homologous superfamily" (SF) level. To achieve this, we have devised and applied an extension of image-classification methods and image segmentation techniques, utilizing a convolutional autoencoder model architecture. Our DL architecture allows models to learn structural features that, in a sense, 'define' different homologous SFs. We evaluate and quantify pairwise 'distances' between SFs by building one model per SF and comparing the loss functions of the models. Hierarchical clustering on these distance matrices provides a new view of protein interrelationships--a view that extends beyond simple structural/geometric similarity, and towards the realm of structure/function properties.
q-bio.BM:Objective: Total 186 biologically important phenylpropanoids and polyketides compounds from different Indian medicinal plants and dietary sources were screened to filter potential compounds that bind at the active site of the therapeutic target proteins of SARS-CoV-2. Method: The molecular docking studies were carried out by using the Autodock Vina. The in silico ADMET and drug-likeness properties of the compounds were predicted from SwissADME server. Result: The molecular docking study of the 186 compounds with the therapeutic target proteins (Mpro, PLpro, RdRp, SGp and ACE2) of SARS-CoV-2 resulted 40 compounds that bind at the active site with dock score above -8.0 kcal/mol. Conclusion: Based on the in silico ADMET study and drug-likeness prediction of 40 compounds, we proposed petunidin, baicalein, cyanidin, 7-hydroxy-3',4'-methylenedioxyflavan, quercetin and ellagic acid among the 186 biologically important phenylpropanoids and polyketides as potential lead compounds, which can further be investigated pharmacologically and clinically to formulate therapeutic approaches for the COVID-19.
q-bio.BM:We propose a mesoscale model structure for the coronavirus nucleocapsid, assembled from the high resolution structures of the basic building blocks of the N-protein, CryoEM imaging and mathematical constraints for an overall quasi-spherical particle. The structure is a truncated octahedron that accommodates two layers: an outer shell composed of triangular and quadrangular lattices of the N-terminal domain and an inner shell of equivalent lattices of coiled parallel helices of the C-terminal domain. The model is consistent with the dimensions expected for packaging large viral genomes and provides a rationale to interpret the apparent pleomorphic nature of coronaviruses.
q-bio.BM:Using a beta-hairpin protein as a representative example of two-state folders, we studied how the exploration of native-like states affects the folding kinetics. It has been found that the first-passage time (FPT) distributions are essentially single-exponential not only for the times to overcome the free energy barrier that separates unfolded and native-like states but also for the times to find the native state among the native-like ones. If the protein explores native-like states for a time much longer than the time to overcome the free energy barrier, which was found to be characteristic of high temperatures, the resulting FPT distribution to reach the native state remains close to exponential but the mean FPT (MFPT) is determined not by the height of the free energy barrier but by the time to explore native-like states. The mean time to overcome the free energy barrier is found to be in reasonable agreement with the Kramers rate formula and generally far shorter than the MFPT to reach the native state. The time to find the native state among native-like ones increases with temperature, which explains the known U-shape dependence of the MFPTs on temperature.
q-bio.BM:Antibodies with high titer and affinity to small molecule are critical in the field for the development of vaccines against drugs of abuse, antidotes to toxins and immunoassays for compounds. However, little is known regarding how properties of small molecule influence and which chemical descriptor could indicate the degree of the antibody response. Based on our previous study, we designed and synthesized two groups of small molecules, called haptens, with varied hydrophobicities to investigate the relationship between properties of small molecules and antibody response in term of titer and affinity. We found that the magnitude of the antibody response is positively correlated with the degree of molecular hydrophobicity and related chemical descriptors. This study provides insight into the immunological characteristics of small molecules themselves and useful clues to produce high quality antibodies against small molecules.
q-bio.BM:SARS-CoV-2, the causative agent of the disease known as Covid-19, has so far reported around 3,435,000 cases of human infections, including more than 239,000 deaths in 187 countries, with no effective treatment currently available. For this reason, it is necessary to explore new approaches for the development of a drug capable of inhibiting the entry of the virus into the host cell. Therefore, this work includes the exploration of potential inhibitory compounds for the Spike protein of SARS-CoV-2 (PDB ID: 6VSB), which were obtained from The Patogen Box. Later, they were filtered through virtual screening and molecular docking techniques, thus obtaining a top of 1000 compounds, which were used against a binding site located in the Receptor Binding Domain (RBD) and a cryptic site located in the N-Terminal Domain (NTD), resulting in good pharmaceutical targets for the blocking the infection. From the top 1000, the best compound (TCMDC-124223) was selected for the binding site. It interacts with specific residues that intervene in the recognition and subsequent entry into the host cell, resulting in a more favorable binding free energy in comparison to the control compounds (Hesperidine and Emodine). In the same way, the compound TCMDC-133766 was selected for the cryptic site. These identified compounds are potential inhibitors that can be used for the development of new drugs that allow effective treatment for the disease.
q-bio.BM:A new coronavirus identified as SARS-CoV-2 virus has brought the world to a state of crisis, causing a major pandemic, claiming more than 433,000 lives and instigating major financial damage to the global economy. Despite current efforts, developing safe and effective treatments remains a major challenge. Moreover, new strains of the virus are likely to emerge in the future. To prevent future pandemics, several drugs with various mechanisms of action are required. Drug discovery efforts against the virus fall into two main categories: (a) monoclonal antibodies targeting the spike protein of the virus and blocking it from entry; (b) small molecule inhibitors targeting key proteins of the virus, interfering with replication and translation of the virus. In this study, we are presenting a computational investigation of a potential drug candidate that targets SARS-CoV-2 protease, a viral protein critical for replication and translation of the virus.
q-bio.BM:Deep learning methods have permeated into the research area of computer-aided drug design. The deep learning generative model and classical algorithm can be simultaneously used for three-dimensional (3D) drug design in the 3D pocket of the receptor. Here, three aspects of MolAICal are illustrated for drug design: in the first part, the MolAICal uses the genetic algorithm, Vinardo score and deep learning generative model trained by generative adversarial net (GAN) for drug design. In the second part, the deep learning generative model is trained by drug-like molecules from the drug database such as ZINC database. The MolAICal invokes the deep learning generative model and molecular docking for drug virtual screening automatically. In the third part, the useful drug tools are added for calculating the relative properties such as Pan-assay interference compounds (PAINS), Lipinski's rule of five, synthetic accessibility (SA), and so on. Besides, the structural similarity search and quantitative structure-activity relationship (QSAR), etc are also embedded for the calculations of drug properties in the MolAICal. MolAICal will constantly optimize and develop the current and new modules for drug design. The MolAICal can help the scientists, pharmacists and biologists to design the rational 3D drugs in the receptor pocket through the deep learning model and classical programming. MolAICal is free of charge for any academic and educational purposes, and it can be downloaded from the website https://molaical.github.io.
q-bio.BM:Under the global health emergency caused by coronavirus disease 2019 (COVID-19), efficient and specific therapies are urgently needed. Compared with traditional small-molecular drugs, antibody therapies are relatively easy to develop and as specific as vaccines in targeting severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and thus attract much attention in the past few months. This work reviews seven existing antibodies for SARS-CoV-2 spike (S) protein with three-dimensional (3D) structures deposited in the Protein Data Bank. Five antibody structures associated with SARS-CoV are evaluated for their potential in neutralizing SARS-CoV-2. The interactions of these antibodies with the S protein receptor-binding domain (RBD) are compared with those of angiotensin-converting enzyme 2 (ACE2) and RBD complexes. Due to the orders of magnitude in the discrepancies of experimental binding affinities, we introduce topological data analysis (TDA), a variety of network models, and deep learning to analyze the binding strength and therapeutic potential of the aforementioned fourteen antibody-antigen complexes. The current COVID-19 antibody clinical trials, which are not limited to the S protein target, are also reviewed.
q-bio.BM:Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the strain of coronavirus that causes coronavirus disease 2019 (COVID-19), the respiratory illness responsible for the COVID-19 pandemic. Currently there is no known vaccine or specific antiviral treatment for COVID-19 and so, there is an urgent need for expedite discovery of new therapeutics to combat the disease until a vaccine will be available worldwide. Drug repurposing is a strategy for identifying new uses for approved drugs that has the advantage (over conventional approaches that attempt to develop a drug from scratch) that time frame of the overall process can be significantly reduced because of the few number of clinical trial required. In this work, a virtual screening of FDA-approved drugs was performed for repositioning as potential inhibitors of the main protease Mpro of SARS-CoV-2. As a result of this study, 12 drugs are proposed as candidates for inhibitors of the Mpro enzyme. Some of the selected compounds are antiviral drugs that are already being tested in COVID-19 clinical trials (i.e. ribavirin) or are used to alleviate symptoms of the disease (i.e. codeine). Surprisingly, the most promising candidate is the naturally occurring broad spectrum antibiotic oxytetracycline. This compound has largely outperformed the remaining selected candidates along all filtering steps of our virtual screening protocol. If the activity of any of these drugs is experimentally corroborated, they could be used directly in clinical trials without the need for pre-clinical testing or safety evaluation since they are already used as drugs for other diseases.
q-bio.BM:The recent appearance of COVID-19 virus has created a global crisis due to unavailability of any vaccine or drug that can effectively and deterministically work against it. Naturally, different possibilities (including herbal medicines having known therapeutic significance) have been explored by the scientists. The systematic scientific study (beginning with in silico study) of herbal medicines in particular and any drug in general is now possible as the structural components (proteins) of COVID-19 are already characterized. The main protease of COVID-19 virus is $\rm{M^{pro}}$ or $\rm{3CL^{pro}}$ which is a key CoV enzyme and an attractive drug target as it plays a pivotal role in mediating viral replication and transcription. In the present study, $\rm{3CL^{pro}}$ is used to study drug:3CLpro interactions and thus to investigate whether all or any of the main chemical constituents of Tinospora cordifolia (e.g., berberine $\rm{(C_{20}H_{18}NO_{4})}$, $\beta$-sitosterol $\rm{(C_{29}H_{50}O)}$, choline $\rm{(C_{5}H_{14}NO)}$, tetrahydropalmatine $\rm{(C_{21}H_{25}NO_{4})}$ and octacosanol $\rm{(C_{28}H_{58}O))}$ can be used as an anti-viral drug against SARS-CoV-2. The in silico study performed using tools of network pharmacology, molecular docking including molecular dynamics have revealed that among all considered phytochemicals in Tinospora cordifolia, berberine can regulate $\rm{3CL^{pro}}$ protein's function due to its easy inhibition and thus can control viral replication. The selection of Tinospora cordifolia was motivated by the fact that the main constituents of it are known to be responsible for various antiviral activities and the treatment of jaundice, rheumatism, diabetes, etc.
q-bio.BM:Computational prediction of RNA structures is an important problem in computational structural biology. Studies of RNA structure formation often assume that the process starts from a fully synthesized sequence. Experimental evidence, however, has shown that RNA folds concurrently with its elongation. We investigate RNA secondary structure formation, including pseudoknots, that takes into account the cotranscriptional effects. We propose a single-nucleotide resolution kinetic model of the folding process of RNA molecules, where the polymerase-driven elongation of an RNA strand by a new nucleotide is included as a primitive operation, together with a stochastic simulation method that implements this folding concurrently with the transcriptional synthesis. Numerical case studies show that our cotranscriptional RNA folding model can predict the formation of conformations that are favored in actual biological systems. Our new computational tool can thus provide quantitative predictions and offer useful insights into the kinetics of RNA folding.
q-bio.BM:Protein tertiary structure prediction has improved dramatically in recent years. A considerable fraction of various proteomes can be modelled in the absence of structural templates. We ask whether our DMPfold method can model all the proteins without templates in the JCVI-syn3.0 minimal genome, which contains 438 proteins. We find that a useful tertiary structure annotation can be provided for all but 10 proteins. The models may help annotate function in cases where it is unknown, and provide coverage for 29 predicted protein-protein interactions which lacked monomer models. We also show that DMPfold performs well on proteins with structures released since initial publication. It is likely that the minimal genome will have complete structural coverage within a few years.
q-bio.BM:Despite the huge effort to contain the infection, the novel SARS-CoV-2 coronavirus has rapidly become pandemics, mainly due to its extremely high human-to-human transmission capability, and a surprisingly high viral charge of symptom-less people. While the seek of a vaccine is still ongoing, promising results have been obtained with antiviral compounds. In particular, lactoferrin is found to have beneficial effects both in preventing and soothing the infection. Here, we explore the possible molecular mechanisms with which lactoferrin interferes with SARS-CoV-2 cell invasion, preventing attachment and/or entry of the virus. To this aim, we search for possible interactions lactoferrin may have with virus structural proteins and host receptors. Representing the molecular iso-electron surface of proteins in terms of 2D-Zernike descriptors, we (i) identified putative regions on the lactoferrin surface able to bind sialic acid receptors on the host cell membrane, sheltering the cell from the virus attachment; (ii) showed that no significant shape complementarity is present between lactoferrin and the ACE2 receptor, while (iii) two high complementarity regions are found on the N- and C-terminal domains of the SARS-CoV-2 spike protein, hinting at a possible competition between lactoferrin and ACE2 for the binding to the spike protein.
q-bio.BM:3CL-Pro (or M-Pro) is the SARS-CoV-2 main protease, acting as a homodimer, is responsible for the cleavage of the large polyprotein 1ab transcript in proteins acting on viral growth and replication. 3CL-Pro has been one of the most studied SARS-CoV-2 proteins and the subject of therapeutic interventions, targeting its catalytic domain. A number of drug candidates have been reported, including some natural products. Here, we investigated in silico, through binding and molecular dynamics simulations, the natural product space for the identification of candidates of 3CL-Pro dimerization inhibitors. We report that fortunellin (acacetin 7-O-neohesperidoside), a natural flavonoid O-glycoside, is a potent inhibitor of 3CL-Pro dimerization. A search of the ZINC natural products database identified another 16 related molecules, including apilin and rhoifolin, with interesting pharmacological properties. We propose that fortunellin and its structural analogs might be the basis of novel pharmaceuticals and dietary supplements against SARS-CoV-2 induced COVID-19 disease.
q-bio.BM:Coronaviruses are enveloped, non-segmented positive-sense RNA viruses that have the largest genome among RNA viruses. The genome contains a large replicase ORF encodes nonstructural proteins (NSPs), structural and accessory genes. NSP15 is a nidoviral RNA uridylate-specific endoribonuclease (NendoU) has C-terminal catalytic domain. The endoribonuclease activity of NSP15 interferes with the innate immune response of the host. Here, we screened Selleckchem Natural product database of compounds against the NSP15, Thymopentin and Oleuropein showed highest binding energies. The binding of these molecules was further validated by Molecular dynamic simulation and found very stable complexes. These drugs might serve as effective counter molecules in the reduction of virulence of this virus. Future validation of both these inhibitors are worth consideration for patients being treated for COVID -19.
q-bio.BM:Liquid-liquid phase separation is the mechanism underlying the formation of biomolecular condensates. Disordered protein regions often drive phase separation, but molecular interactions of disordered protein regions are not well understood, sometimes leading to the conflation that all disordered protein regions drive phase separation. Given the critical role of phase separation in many cellular processes, and that dysfunction of phase separation can lead to debilitating diseases, it is important that we understand the interactions and sequence properties underlying phase behavior. A conceptual framework that divides IDRs into interacting and solvating regions has proven particularly useful, and analytical instantiations and coarse-grained models can test our understanding of the driving forces against experimental phase behavior. Validated simulation paradigms enable the exploration of sequence space to help our understanding of how disordered protein regions can encode phase behavior, which IDRs may mediate phase separation in cells, and which IDRs are in contrast highly soluble.
q-bio.BM:Inspired by recent work on anti-covid-19 drugs \cite{2} here we study the Quantitative-structure property relationships(QSPR) of phytochemicals screened against SARS-CoV-2 $3CL^{pro}$ with the help of topological indices like the first Zagreb index $M_{1}$, second Zagreb index $M_{2}$, Randi$\acute{c}$ index $R$, Balban index $J$ and sum-connectivity index $SCI(G)$. Our study has raveled that the sum-connectivity index $(SCI)$ and the first Zagreb index $(M_{1})$ are two important parameters to predict the molecular weight and the topological polar surface area of phytochemicals respectively.
q-bio.BM:Prediction of protein-ligand complexes for flexible proteins remains still a challenging problem in computational structural biology and drug design. Here we present two novel deep neural network approaches with significant improvement in efficiency and accuracy of binding mode prediction on a large and diverse set of protein systems compared to standard docking. Whereas the first graph convolutional network is used for re-ranking poses the second approach aims to generate and rank poses independent of standard docking approaches. This novel approach relies on the prediction of distance matrices between ligand atoms and protein C_alpha atoms thus incorporating side-chain flexibility implicitly.
q-bio.BM:Spike (S) glycoproteins mediate the coronavirus entry into the host cell. The S1 subunit of S-proteins contains the receptor-binding domain (RBD) that is able to recognize different host receptors, highlighting its remarkable capacity to adapt to their hosts along the viral evolution. While RBD in spike proteins is determinant for the virus-receptor interaction, the active residues lie at the receptor-binding motif (RBM), a region located in RBD that plays a fundamental role binding the outer surface of their receptors. Here, we address the hypothesis that SARS-CoV and SARS-CoV-2 strains able to use angiotensin-converting enzyme 2 (ACE2) proteins have adapted their RBM along the viral evolution to explore specific conformational topology driven by the residues YGF to infect host cells. We also speculate that this YGF-based mechanism can act as a protein signature located at the RBM to distinguish coronaviruses able to use ACE2 as a cell entry receptor.
q-bio.BM:COVID-19 presents a great threat to public health worldwide and the infectious agent SARS-CoV-2 is currently the target of much research aiming at inhibition. The virus' main protease is a dimeric enzyme that has only recently begun to be thoroughly described, opening the door for virtual screening more broadly. Here, a PAIN-filtered flavonoid database was screened against four sites of the protease: a free (normal) conformation of the Substrate Binding Site (NSBS), an induced-fit state of the SBS (ISBS), a Dimerization Site (DS) and a Cryptic Site (CS). The mean binding energies of the top five ligands from each site were -9.52, -11.512, -7.042 and -10.348 kcal/mol for the NSBS, the ISBS, the DS and the CS, respectively. For the DS and CS, these top five compounds were selected as candidates to bind their respective site. In the case of SBS, the top 30 ligands with the lowest binding energies from NSBS and ISBS were contrasted and the ones present in both lists were selected as the final candidates. The final list was: Dorsilurin E (FL3FQUNP0001), Euchrenone a11 (FL2FALNP0014), Kurziflavolactone C (FL2FA9NC0016), Licorice glycoside E (FL2F1AGSN001) and Taxifolin 3'- (6"-phenyl- acetylglucoside) (FL4DACGS0020) for the SBS; Sanggenol O (FL2FALNP0020), CHEMBL2171573, Kanzonol E (FL3F1ANP0001), CHEMBL2171584 and Abyssynoflavanone VI (FL2FACNP0014) for DS and CHEMBL2171598, CHEMBL2171577, Denticulaflavanol (FL5FAANR0001), Kurzichalcolactone (FL1CA9NC0001) and CHEMBL2171578 for CS. Virtual screening integrated several confirmation methods, including cross-docking assays and positive and negative controls. All 15 compounds are currently subjected to molecular dynamics so as to theoretically validate their binding to the protease.
q-bio.BM:There remains an urgent need to identify existing drugs that might be suitable for treating patients suffering from COVID-19 infection. Drugs rarely act at a single molecular target, with off target effects often being responsible for undesirable side effects and sometimes, beneficial synergy between targets for a specific illness. Off target activities have also led to blockbuster drugs in some cases, e.g. Viagra for erectile dysfunction and Minoxidil for male pattern hair loss. Drugs already in use or in clinical trials plus approved natural products constitute a rich resource for discovery of therapeutic agents that can be repurposed for existing and new conditions, based on the rationale that they have already been assessed for safety in man. A key question then is how to rapidly and efficiently screen such compounds for activity against new pandemic pathogens such as COVID-19. Here we show how a fast and robust computational process can be used to screen large libraries of drugs and natural compounds to identify those that may inhibit the main protease of SARS-Cov-2 (3CL pro, Mpro). We show how the resulting shortlist of candidates with strongest binding affinities is highly enriched in compounds that have been independently identified as potential antivirals against COVID-19. The top candidates also include a substantial number of drugs and natural products not previously identified as having potential COVID-19 activity, thereby providing additional targets for experimental validation. This in silico screening pipeline may also be useful for repurposing of existing drugs and discovery of new drug candidates against other medically important pathogens and for use in future pandemics.
q-bio.BM:The main reasons for the ongoing COVID-19 (coronavirus disease 2019) pandemic are the unavailability of recommended efficacious drugs or vaccines along with the human to human transmission nature of SARS-CoV-2 virus. So, there is urgent need to search appropriate therapeutic approach by repurposing approved drugs. In this communication, molecular docking analyses of two influenza antiviral drugs baloxavir acid (BXA) and baloxavir marboxil (BXM) were performed with the three therapeutic target proteins of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), i.e., main protease (Mpro), papain-like protease (PLpro) and RNA-dependent RNA polymerase (RdRp). The molecular docking results of both the drugs BXA and BXM were analysed and compared. The investigational drug BXA binds at the active site of Mpro and RdRp, whereas the approved drug BXM binds only at the active site of RdRp. Also, comparison of dock score revealed that BXA is binding more effectively at the active site of RdRp than BXM. The computational molecular docking revealed that the drug BXA may be more effective against COVID-19 as compared to BXM.
q-bio.BM:Protein folding is a phenomenon that has been studied for about 50 years and still remains as an unsolved problem. The main feature of this process is that it occurs as an all or none process, so a protein, can jump directly between folded and unfolded states. It is proposed in this manuscript that the cooperative phenomena associated to protein folding has its origin in the synchronization oscillating phases of the instantaneously induced dipoles that gives rise to the van der Waals London dispersion interaction. When the oscillation of the induced dipoles is synchronized an enhanced interaction regime is triggered and the protein enters a into a folding process. The propagation of this regime throughout the molecular structure complete the folding process. If a desynchronizing strong enough perturbation is introduced at any component cooperative network the system enters in a weakened -- or even a repulsive -- van der Waals dispersion interaction regime that propagate itself throughout the structure, that triggers a phase transition that finally unfolds the protein.
q-bio.BM:Exploring the protein-folding problem has been a long-standing challenge in molecular biology. Protein folding is highly dependent on folding of secondary structures as the way to pave a native folding pathway. Here, we demonstrate that a feature of a large hydrophobic surface area covering most side-chains on one side or the other side of adjacent $\beta$-strands of a $\beta$-sheet is prevail in almost all experimentally determined $\beta$-sheets, indicating that folding of $\beta$-sheets is most likely triggered by multistage hydrophobic interactions among neighbored side-chains of unfolded polypeptides, enable $\beta$-sheets fold reproducibly following explicit physical folding codes in aqueous environments. $\beta$-turns often contain five types of residues characterized with relatively small exposed hydrophobic proportions of their side-chains, that is explained as these residues can block hydrophobic effect among neighbored side-chains in sequence. Temperature dependence of the folding of $\beta$-sheet is thus attributed to temperature dependence of the strength of the hydrophobicity. The hydrophobic-effect-based mechanism responsible for $\beta$-sheets folding is verified by bioinformatics analyses of thousands of results available from experiments. The folding codes in amino acid sequence that dictate formation of a $\beta$-hairpin can be deciphered through evaluating hydrophobic interaction among side-chains of an unfolded polypeptide from a $\beta$-strand-like thermodynamic metastable state.
q-bio.BM:Natural isolates of Bacillus subtilis are known for their ability to produce a large panel of bioactive compounds. Unfortunately, their recalcitrance to conventional molecular techniques limits their transcript studies. In this work, diYculties to isolate RNA attributed to the cell wall were overcome, Wnally authorising powerful RT-PCR's.
q-bio.BM:This work is related to the setup of overflowing exponential fed-batch cultures (O-EFBC) derived from carbon limited EFBC dedicated to the production of mycosubtilin, an antifungal lipopeptide belonging to the iturin family. O-EFBC permits the continuous removal of the product from the bioreactor achieving a complete extraction of mycosubtilin. This paper also provides a dynamical Monod-based growth model of this process that is accurate enough to simulate the evolution of the specific growth rate and to correlate it to the mycosubtilin specific productivity. Two particular and dependant phenomena related to the foam overflow are taken into account by the model: the outgoing flow rate of a broth volume and the loss of biomass. Interestingly, the biomass concentration in the foam was found to be lower than the biomass concentration in the bioreactor relating this process to a recycling one. Parameters of this model are the growth yield on substrate and the maximal specific growth rate estimated from experiments led at feed rates of 0.062, 0.071 and 0.086 h --1. The model was extrapolated to five additional experiments carried out at feed rates of 0.008, 0.022, 0.040, 0.042 and 0.062 h --1 enabling the correlation of the mean specific growth rates with productivity results. Finally, a feed rate of 0.086 h --1 corresponding to a mean specific growth rate of 0.070 h --1 allowed a specific productivity of 1.27 mg of mycosubtilin g --1 of dried biomass h --1 .
q-bio.BM:Bacillus subtilis ATCC6633 produces mycosubtilin, a non-ribosomally synthesized lipopeptide of the iturin family which presents antagonistic activities toward various phytopathogens. Different homologues with fatty acid moiety varying from C 15 to C 17 are usually co-produced, with their biological activities increasing with the number of carbons in the fatty acid chain. In the present report, we highlight that growth temperature modulates both the extent of mycosubtilin production and the relative abundance of the different homologues. A 30-fold increase in mycosubtilin production was observed when the temperature was decreased from 37 C to 25 C for both strain ATCC6633 and its derivative BBG100, a constitutive mycosubtilin overproducer. However, no significant difference in either the expression of the mycosubtilin synthetase encoding genes or in the intracellular synthetase concentration could be found, suggesting that the observed phenotype originated from a higher mycosubtilin synthetase turnover at lower temperature. We also point out that lower growth temperature leads to an increased proportion of odd-numbered fatty acid homologues as a consequence of de novo synthesis of C 17 anteiso fatty acid following cell adaptation to low temperatures.
q-bio.BM:A Bacillus subtilis derivative was obtained from strain ATCC 6633 by replacement of the native promoter of the mycosubtilin operon by a constitutive promoter originating from the replication gene repU of the Staphylococcus aureus plasmid pUB110. The recombinant strain, designated BBG100, produced up to 15-fold more mycosubtilin than the wild type produced. The overproducing phenotype was related to enhancement of the antagonistic activities against several yeasts and pathogenic fungi. Hemolytic activities were also clearly increased in the modified strain. Mass spectrometry analyses of enriched mycosubtilin extracts showed similar patterns of lipopeptides for BBG100 and the wild type. Interestingly, these analyses also revealed a new form of mycosubtilin which was more easily detected in the BBG100 sample. When tested for its biocontrol potential, wild-type strain ATCC 6633 was almost ineffective for reducing a Pythium infection of tomato seedlings. However, treatment of seeds with the BBG100 overproducing strain resulted in a marked increase in the germination rate of seeds. This protective effect afforded by mycosubtilin overproduction was also visualized by the significantly greater fresh weight of emerging seedlings treated with BBG100 compared to controls or seedlings inoculated with the wild-type strain.
q-bio.BM:Due to SARS-CoV-2 (Severe Acute Respiratory Syndrome Coronavirus 2) being a novel virus, there are currently no known effective antiviral drugs capable of slowing its progress. To accelerate the discovery of potential drug candidates, bioinformatics based in silico drug discovery can be applied as a very robust tool. In the present study, more than 60 antiviral drugs already available on the market, were chosen after literature survey. These can be used in clinical trials for the treatment of COVID-19. In this study, these candidate drugs were ranked based on their potential to interact with the Spike protein and RdRp (RNA-dependent RNA polymerase) of SARS-CoV-2. Additionally, the mechanism of their action as well as how the virus infection can utilize Hemoglobin to decrease the oxygen level in blood is explained. Moreover, multiple sequence alignments of the Spike protein with 75 sequences of different viruses from the Orthocoronavirinae subfamily were performed. This gives insight into the evolutionarily conserved domains that can be targeted using drug or antibody treatment. This multidimensional study opens a new layer of understanding about the most effective drug-targetable sites on the Spike protein of SARS-CoV-2.
q-bio.BM:In this work, we developed an efficient approach to compute ensemble averages in systems with pairwise-additive energetic interactions between the entities. Methods involving full enumeration of the configuration space result in exponential complexity. Sampling methods such as Markov Chain Monte Carlo (MCMC) algorithms have been proposed to tackle the exponential complexity of these problems; however, in certain scenarios where significant energetic coupling exists between the entities, the efficiency of the such algorithms can be diminished. We used a strategy to improve the efficiency of MCMC by taking advantage of the cluster structure in the interaction energy matrix to bias the sampling. We pursued two different schemes for the biased MCMC runs and show that they are valid MCMC schemes. We used both synthesized and real-world systems to show the improved performance of our biased MCMC methods when compared to the regular MCMC method. In particular, we applied these algorithms to the problem of estimating protonation ensemble averages and titration curves of residues in a protein.
q-bio.BM:Antibody therapeutics and vaccines are among our last resort to end the raging COVID-19 pandemic. They, however, are prone to over 5,000 mutations on the spike (S) protein uncovered by a Mutation Tracker based on over 200,000 genome isolates. It is imperative to understand how mutations would impact vaccines and antibodies in the development. In this work, we study the mechanism, frequency, and ratio of mutations on the S protein. Additionally, we use 56 antibody structures and analyze their 2D and 3D characteristics. Moreover, we predict the mutation-induced binding free energy (BFE) changes for the complexes of S protein and antibodies or ACE2. By integrating genetics, biophysics, deep learning, and algebraic topology, we reveal that most of 462 mutations on the receptor-binding domain (RBD) will weaken the binding of S protein and antibodies and disrupt the efficacy and reliability of antibody therapies and vaccines. A list of 31 vaccine escape mutants is identified, while many other disruptive mutations are detailed as well. We also unveil that about 65\% existing RBD mutations, including those variants recently found in the United Kingdom (UK) and South Africa, are binding-strengthen mutations, resulting in more infectious COVID-19 variants. We discover the disparity between the extreme values of RBD mutation-induced BFE strengthening and weakening of the bindings with antibodies and ACE2, suggesting that SARS-CoV-2 is at an advanced stage of evolution for human infection, while the human immune system is able to produce optimized antibodies. This discovery implies the vulnerability of current vaccines and antibody drugs to new mutations. Our predictions were validated by comparison with more than 1,400 deep mutations on the S protein RBD. Our results show the urgent need to develop new mutation-resistant vaccines and antibodies and to prepare for seasonal vaccinations.
q-bio.BM:Computational docking methods can provide structural models of protein-protein complexes, but protein backbone flexibility upon association often thwarts accurate predictions. In recent blind challenges, medium or high accuracy models were submitted in less than 20% of the "difficult" targets (with significant backbone change or uncertainty). Here, we describe recent developments in protein-protein docking and highlight advances that tackle backbone flexibility. In molecular dynamics and Monte Carlo approaches, enhanced sampling techniques have reduced time-scale limitations. Internal coordinate formulations can now capture realistic motions of monomers and complexes using harmonic dynamics. And machine learning approaches adaptively guide docking trajectories or generate novel binding site predictions from deep neural networks trained on protein interfaces. These tools poise the field to break through the longstanding challenge of correctly predicting complex structures with significant conformational change.
q-bio.BM:When the hydration shell of a protein is filled with at least 0.6 gram of water per gram of protein, a significant anti-correlation between the vibrational free energy and the potential energy of energy-minimized conformers is observed. This means that low potential energy, well-hydrated, protein conformers tend to be more rigid than high-energy ones. On the other hand, in the case of CASP target 624, when its hydration shell is filled, a significant average energy gap is observed between the crystal structure and the best conformers proposed during the prediction experiment, strongly suggesting that including explicit water molecules may help identifying unlikely conformers among good-looking ones.
q-bio.BM:The antioxidant activity of baked foods is of utmost interest when envisioning enhancing their health benefits. Incorporating functional ingredients is challenging since their bioactivity naturally declines during baking. In this study, 3D food printing and design of experiments are employed to clarify how the antioxidant activity of cookies enriched with encapsulated polyphenols can be maximized. A synergistic effect between encapsulation, time, temperature, number of layers, and infill of the printed cookies was observed on the moisture and antioxidant activity. Four-layer cookies with 30 % infill provided the highest bioactivity and phenolic content if baked for 10 min and at 180 {\deg}C. The bioacitivity and total phenolic content improved by 115 % and 173 %, respectively, comparing to free extract cookies. Moreover, the proper combination of the design and baking variables allowed to vary the bioactivity of cooked cookies (moisture 3-5 %) between 300 to 700 {\mu}molTR/gdry. The additive manufacture of foods with interconnected pores could accelerate baking and browning, or reduce thermal degradation. This represents a potential approach to enhance the functional and healthy properties of cookies or other thermal treated bioactive food products.
q-bio.BM:The dynamics of the structured water molecules in the hydration shell of the DNA double helix is of paramount importance for the understanding of many biological mechanisms. In particular, the vibrational dynamics of a water spine that is formed in the DNA minor groove is the aim of the present study. Within the framework of the developed phenomenological model, based on the approach of DNA conformational vibrations, the modes of H-bond stretching, backbone vibrations, and water translational vibrations have been established. The calculated frequencies of translation vibrations of water molecules vary from 167 to 205 cm$^{-1}$ depending on the nucleotide sequence. The mode of water vibrations higher than the modes of internal conformational vibrations of DNA. The calculated frequencies of water vibrations have shown a sufficient agreement with the experimental low-frequency vibrational spectra of DNA. The obtained modes of water vibrations are observed in the same region of the vibrational spectra of DNA as translation vibrations of water molecules in the bulk phase. To distinguish the vibrations of water molecules in the DNA minor groove from those in the bulk water, the dynamics of DNA with heavy water was also considered. The results have shown that in the case of heavy water the frequencies of vibrations decrease for about 10 cm$^{-1}$ that may be used in the experiment to identify the mode of water vibrations in the spine of hydration in DNA minor groove.
q-bio.BM:The protein-protein interactions (PPIs) of 14-3-3 proteins are a model system for studying PPI stabilization. The complex natural product Fusicoccin A stabilizes many 14-3-3 PPIs but is not amenable for use in SAR studies, motivating the search for more drug-like chemical matter. However, drug-like 14-3-3 PPI stabilizers enabling such study have remained elusive. An X-ray crystal structure of a PPI in complex with an extremely low potency stabilizer uncovered an unexpected non-protein interacting, ligand-chelated Mg 2+ leading to the discovery of metal ion-dependent 14-3-3 PPI stabilization potency. This originates from a novel chelation-controlled bioactive conformation stabilization effect. Metal chelation has been associated with pan-assay interference compounds (PAINS) and frequent hitter behavior, but chelation can evidently also lead to true potency gains and find use as a medicinal chemistry strategy to guide compound optimization. To demonstrate this, we exploited the effect to design the first potent, selective and drug-like 14-3-3 PPI stabilizers.
q-bio.BM:The amyloid state of proteins is widely studied with relevancy in neurology, biochemistry, and biotechnology. In contrast with amorphous aggregation, the amyloid state has a well-defined structure, consisting of parallel and anti-parallel $\beta$-sheets in a periodically repeated formation. The understanding of the amyloid state is growing with the development of novel molecular imaging tools, like cryogenic electron microscopy. Sequence-based amyloid predictors were developed by using mostly artificial neural networks (ANNs) as the underlying computational techniques. From a good neural network-based predictor, it is a very difficult task to identify those attributes of the input amino acid sequence, which implied the decision of the network. Here we present a Support Vector Machine (SVM)-based predictor for hexapeptides with correctness higher than 84\%, i.e., it is at least as good as the published ANN-based tools. Unlike the artificial neural networks, the decision of the SVMs are much easier to analyze, and from a good predictor, we can infer rich biochemical knowledge.   Availability and Implementation: The Budapest Amyloid Predictor webserver is freely available at https://pitgroup.org/bap.
q-bio.BM:Generally, carbohydrate-active enzymes are studied using chromogenic substrates that provide quick and easy color-based detection of enzyme-mediated hydrolysis. In the case of feruloyl esterases, commercially available chromogenic ferulate derivatives are both costly and limited in terms of their experimental application. In this study, we describe solutions for these two issues, using a chemoenzymatic approach to synthesize different ferulate compounds. The overall synthetic routes towards commercially available 5-bromo-4-chloro-3-indolyl and 4-nitrophenyl O-5-feruloyl-$\alpha$-l-arabinofuranosides 1a and 1b were significantly shortened (7-8 steps reduced to 4-6) and transesterification yields enhanced (from 46 to 73% for 1a and 47 to 86 % for 1b). This was achieved using enzymatic (immobilized Lipolase 100T from Thermomyces lanuginosus) transesterification of unprotected vinyl ferulate to the primary hydroxyl group of $\alpha$-l-arabinofuranosides. Moreover, a novel feruloylated-butanetriol 4-nitrocatechol-1-yl analog 12, containing a cleavable hydroxylated linker was also synthesized in 29% overall yield in 3 steps (convergent synthesis). The latter route combined regioselective functionalization of 4-nitrocatechol and enzymatic transferuloylation. The use of 12 as a substrate to characterize type A feruloyl esterase from Aspergillus niger reveals the advantages of this substrate for the characterizations of feruloyl esterases.
q-bio.BM:Efficient molecular featurization is one of the major issues for machine learning models in drug design. Here we propose persistent Ricci curvature (PRC), in particular Ollivier persistent Ricci curvature (OPRC), for the molecular featurization and feature engineering, for the first time. Filtration process proposed in persistent homology is employed to generate a series of nested molecular graphs. Persistence and variation of Ollivier Ricci curvatures on these nested graphs are defined as Ollivier persistent Ricci curvature. Moreover, persistent attributes, which are statistical and combinatorial properties of OPRCs during the filtration process, are used as molecular descriptors, and further combined with machine learning models, in particular, gradient boosting tree (GBT). Our OPRC-GBT model is used in the prediction of protein-ligand binding affinity, which is one of key steps in drug design. Based on three most-commonly used datasets from the well-established protein-ligand binding databank, i.e., PDBbind, we intensively test our model and compare with existing models. It has been found that our model are better than all machine learning models with traditional molecular descriptors.
q-bio.BM:A variety of missense mutations and a stop mutation in the gene coding for transmembrane protein 240 (TMEM240) have been reported to be the causative mutations of spinocerebellar ataxia 21 (SCA21). We aimed to investigate the expression of TMEM240 protein in mouse brain at the tissue, cellular, and subcellular levels. Immunofluorescence labeling showed TMEM240 to be expressed in various areas of the brain, with the highest levels in the hippocampus, isocortex, and cerebellum. In the cerebellum, TMEM240 was detected in the deep nuclei and the cerebellar cortex. The protein was expressed in all three layers of the cortex and various cerebellar neurons. TMEM240 was localized to climbing, mossy, and parallel fiber afferents projecting to Purkinje cells, as shown by coimmunostaining with VGLUT1 and VGLUT2. Co-immunostaining with synaptophysin, post-synaptic fractionation, and confirmatory electron microscopy showed TMEM240 to be localized to the post-synaptic side of synapses near the Purkinje-cell soma. Similar results were obtained in human cerebellar sections. These data suggest that TMEM240 may be involved in the organization of the cerebellar network, particularly in synaptic inputs converging on Purkinje cells. This study is the first to describe TMEM240 expression in the normal mouse brain.
q-bio.BM:For fast development of COVID-19, it is only feasible to use drugs (off label use) or approved natural products that are already registered or been assessed for safety in previous human trials. These agents can be quickly assessed in COVID-19 patients, as their safety and pharmacokinetics should already be well understood. Computational methods offer promise for rapidly screening such products for potential SARS-CoV-2 activity by predicting and ranking the affinities of these compounds for specific virus protein targets. The RNA-dependent RNA polymerase (RdRP) is a promising target for SARS-CoV-2 drug development given it has no human homologs making RdRP inhibitors potentially safer, with fewer off-target effects that drugs targeting other viral proteins. We combined robust Vina docking on RdRP with molecular dynamic (MD) simulation of the top 80 identified drug candidates to yield a list of the most promising RdRP inhibitors. Literature reviews revealed that many of the predicted inhibitors had been shown to have activity in in vitro assays or had been predicted by other groups to have activity. The novel hits revealed by our screen can now be conveniently tested for activity in RdRP inhibition assays and if conformed testing for antiviral activity invitro before being tested in human trials
q-bio.BM:SARS-CoV-2 (COVID-19), a positive single stranded RNA virus, member of corona virus family, is spreading its tentacles across the world due to lack of drugs at present. Being associated with cough, fever, and respiratory distress, this disease caused more than 15 % mortality worldwide. Due to its vital role in virus replication, Mpro/3CLpro has recently been regarded as a suitable target for drug design. The current study focused on the inhibitory activity of Calotropin, a component from milk of Calotropis gigantean, against Mpro protein from SARS-CoV-2. Till date there is no work is undertaken on in-silico analysis of this compound against Mpro of COVID-19 protein. In the present study, molecular docking studies were conducted by using Patchdock tool. Protein Interactions tool was used for protein interactions. The calculated parameters such as docking score indicated effective binding of Calotropin to Mpro protein. Interactions results indicated that, Mpro/ Calotropin complexes forms hydrophobic interactions. Therefore, Calotropin may represent potential herbal treatment to act as COVID-19 Mpro inhibitor. However, further research is necessary to investigate their potential medicinal use.
q-bio.BM:The outer hair cell (OHC) membrane harbors a voltage-dependent protein, prestin (SLC26a5), in high density, whose charge movement is evidenced as a nonlinear capacitance (NLC). NLC is bell-shaped, with its peak occurring at a voltage, Vh, where sensor charge is equally distributed across the plasma membrane. Thus, Vh provides information on the conformational state of prestin. Vh is sensitive to membrane tension, shifting to positive voltage as tension increases and is the basis for considering prestin piezoelectric (PZE). NLC can be deconstructed into real and imaginary components that report on charge movements in phase or 90 degrees out of phase with AC voltage. Here we show in membrane macro-patches of the OHC that there is a partial trade-off in the magnitude of real and imaginary components as interrogation frequency increases, as predicted by a recent PZE model (Rabbitt, 2020). However, we find similar behavior in a simple kinetic model of prestin that lacks piezoelectric coupling, the meno presto model. At a particular frequency, Fis, the complex component magnitudes intersect. Using this metric, Fis, which depends on the frequency response of each complex component, we find that initial Vh influences Fis; thus, by categorizing patches into groups of different Vh, (above and below -30 mV) we find that Fis is lower for the negative Vh group. We also find that the effect of membrane tension on complex NLC is dependent, but differentially so, on initial Vh. Whereas the negative group exhibits shifts to higher frequencies for increasing tension, the opposite occurs for the positive group. Despite complex component trade-offs, the low-pass roll-off in absolute magnitude of NLC, which varies little with our perturbations and is indicative of diminishing total charge movement, poses a challenge for a role of voltage-driven prestin in cochlear amplification at very high frequencies.
q-bio.BM:Cataract is one of the most prevalent protein aggregation disorders and still the biggest cause of vision loss worldwide. The human lens, in its core region, lacks turnover of any cells or cellular components; it has therefore evolved remarkable mechanisms for resisting protein aggregation for a lifetime. We now report that one such mechanism relies on an unusually abundant metabolite, myo-inositol, to suppress light-scattering aggregation of lens proteins. We quantified aggregation suppression by in vitro turbidimetry and characterized both macroscopic and microscopic mechanisms of myo-inositol action using negative-stain electron microscopy, differential scanning fluorometry, and a thermal scanning Raman spectroscopy apparatus. Given recent metabolomic evidence that it is dramatically depleted in human cataractous lenses compared to age-matched controls, we suggest that maintaining or restoring healthy levels of myo-inositol in the lens may be a simple, safe, and widely available strategy for reducing the global burden of cataract.
q-bio.BM:One of the most common mutations in the serine protease inhibitor Kazal type 1 (SPINK1) gene is the N34S variant which is strongly associated with chronic pancreatitis. Although it is assumed that N34S mutation constitutes a high-risk factor, the underlying pathologic mechanism is still unknown. In the present study, we investigated the impact of physiological stress factors on SPINK1 protein structure and trypsin inhibitor function using biophysical methods. Our circular dichroism spectroscopy data revealed differences in the secondary structure of SPINK1 and N34S mutant suggesting protein structural changes induced by the mutation as an impairment that could be disease-relevant. We further confirmed that both SPINK1 (KD of 0.15 +/- 0.06 nM) and its N34S variant (KD of 0.08 +/- 0.02 nM) have similar binding affinity and inhibitory effect towards trypsin as shown by surface plasmon resonance and trypsin inhibition assay studies, respectively. We found that stress conditions such as altered ion concentrations (i.e. potassium, calcium), temperature shifts, as well as environmental pH lead to insignificant differences in trypsin inhibition between SPINK1 and N34S mutant. However, we have shown that the environmental pH induces structural changes in both SPINK1 constructs in a different manner. Our findings suggest protein structural changes in the N34S variant as an impairment of SPINK1 and environmental pH shift as a trigger that could play a role in disease progression of pancreatitis.
q-bio.BM:The estrogen receptor is a nuclear hormone receptor activated by the natural steroid hormone 17$\beta$-estradiol (E2). Fragment molecular orbital (FMO) calculations were performed which allowed us to obtain the interaction energy ($E_{int}$) between E2, 17$\alpha$-estradiol (17$\alpha$-E2) and the human estrogen receptor $\alpha$ ligand-binding domain. In aqueous media the MP2/6-31G(d) $E_{int}$ was of -88.52 kcal/mol for E2 and -78.73 kcal/mol for 17$\alpha$-E2. Attractive dispersion interactions were observed between ligands and all surrounding hydrophobic residues. Water molecules were found at the binding site and strong attractive electrostatic interactions were observed between the ligands and the Glu 353 and His 524 residues. The essential dynamics revealed that E2 adapts to the binding site and its motion, in a sense, synchronizes with the whole receptor; while 17$\alpha$-E2, with its motion of greater amplitude compared to E2, disturbs the binding site. Perhaps this feature of the normal substrate is a necessary condition for biological function. Another important requirement relates to the number of water molecules at the binding site. Therefore, negative values in $E_{int}$ is a necessary but not sufficient condition since, it is also necessary to consider the conformers population that fulfill all the requirements that ensure a biological response.
q-bio.BM:Background: Coronavirus disease 2019 (COVID-19) and Influenza A are common disease caused by viral infection. The clinical symptoms and transmission routes of the two diseases are similar. However, there are no relevant studies on laboratory diagnostic models to discriminate COVID-19 and influenza A. This study aims at establishing a signature of laboratory findings to tell patients with COVID-19 apart from those with influenza A perfectly. Materials: In this study, 56 COVID-19 patients and 54 influenza A patients were included. Laboratory findings, epidemiological characteristics and demographic data were obtained from electronic medical record databases. Elastic network models, followed by a stepwise logistic regression model were implemented to identify indicators capable of discriminating COVID-19 and influenza A. A nomogram is diagramed to show the resulting discriminative model. Results: The majority of hematological and biochemical parameters in COVID-19 patients were significantly different from those in influenza A patients. In the final model, albumin/globulin (A/G), total bilirubin (TBIL) and erythrocyte specific volume (HCT) were selected as predictors. Using an external dataset, the model was validated to perform well. Conclusion: A diagnostic model of laboratory findings was established, in which A/G, TBIL and HCT were included as highly relevant indicators for the segmentation of COVID-19 and influenza A, providing a complimentary means for the precise diagnosis of these two diseases.
q-bio.BM:Many of the building blocks of life such as amino acids and nucleotides are chiral, i.e., different from their mirror image. Contemporary life selects and synthesizes only one of two possible handednesses. In an abiotic environment, however, there are usually equally many left- and right-handed molecules. If homochirality was a prerequisite of life, there must have been physical or chemical circumstances that led to the selection of a certain preference. Conversely, if it was a consequence of life, we must identify possible pathways for accomplishing a transition from a racemic to a homochiral chemistry. After a discussion of the observational evidence, I will review ideas where homochirality of any handedness could emerge as a consequence of the first polymerization events of nucleotides in an emerging RNA world. These mechanisms are not limited to nucleotides, but can also occur for peptides, as a precursor to the RNA world. The question of homochirality is, in this sense, intimately tied to the origin of life. Future Mars missions may be able to detect biomolecules of extant or extinct life. We will therefore also discuss possible experimental setups for determining the chirality of primitive life forms in situ on Mars.
q-bio.BM:Summary: Coarse-grained normal mode analysis (NMA) is a fast computational technique to study the dynamics of biomolecules. Here we present the Najmanovich Research Group Toolkit for Elastic Networks (NRGTEN). NRGTEN is a Python toolkit that implements four different NMA models in addition to popular and novel metrics to benchmark and measure properties from these models. Furthermore, the toolkit is available as a public Python package and is easily extensible for the development or implementation of additional NMA models. The inclusion of the ENCoM model (Elastic Network Contact Model) developed in our group within NRGTEN is noteworthy, owing to its account for the specific chemical nature of atomic interactions. This makes possible some unique predictions of the effect of mutations, such as on stability (via changes in vibrational entropy differences), on the transition probability between different conformational states or on the flexibility profile of the whole macromolecule/complex (to study allostery and signalling). In addition, all NMA models can be used to generate conformational ensembles from a starting structure to aid in protein-protein, protein-ligand or other docking studies among applications. NRGTEN is freely available via a public Python package which can be easily installed on any modern machine and includes a detailed user guide hosted online. Availability and implementation: https://github.com/gregorpatof/nrgten_package/ Contact: rafael.najmanovich@umontreal.ca
q-bio.BM:The present Health Crisis tests the response of modern science and medicine to finding treatment for a new COVID-19 disease. The presentation on the world stage of antivirals such as remdesivir, obeys to the continuous investigation of biologically active molecules with multiple theoretical, computational and experimental tools. Diseases such as COVID:19 remind us that research into active ingredients for therapeutic purposes should cover all available sources, such as plants. In the present work, in silico tools, specifically docking study, were used to evaluate the binding and inhibition capacity of an antiviral such as remdesivir on the NSP-12 protein of SARS-CoV, a polymerase that is key in the replication of the SARS-COV virus. The results are then compared with a docking analysis of two natural products (Alpha-Bisabolol and betalain) with SARS-CoV protein, in order to find more candidates for COVID-19 virus replication inhibitors. in addition to increasing studies that help explain the specific mechanisms of the SARs-CoV-2 virus, remembering that we will have to live with the virus for an indefinite time from now on. Finally, natural products such as betalains may have inhibitory effects of a small order but in conjunction with other synergistic active ingredients they may increase their inhibition effect on NSP-12 protein of SARS-CoV.
q-bio.BM:The design of antibacterial-releasing coatings or wrapping materials with controlled drug release capability is a promising strategy to minimise risks of infection and medical device failure in vivo. Collagen fibres have been employed as medical device building block, although they still fail to display controlled release capability, competitive wet-state mechanical properties, and retained triple helix organisation. We investigated this challenge by pursuing a multiscale design approach integrating drug encapsulation, in-situ covalent crosslinking and fibre spinning. By selecting ciprofloxacin (Cip) as a typical antibacterial drug, wet spinning was selected as a triple helix-friendly route towards Cip-encapsulated collagen fibres; whilst in situ crosslinking of fibre-forming triple helices with 1,3 phenylenediacetic acid (Ph) was hypothesised to yield Ph-Cip {\pi}-{\pi} stacking aromatic interactions and enable controlled drug release. Higher tensile modulus and strength were measured in Ph crosslinked fibres compared to state-of-the-art carbodiimide crosslinked controls. Cip-encapsulated Ph-crosslinked fibres revealed decreased elongation at break and significantly-enhanced drug retention in vitro with respect to Cip-free variants and carbodiimide-crosslinked controls, respectively. This multiscale manufacturing strategy provides new insight aiming at wet spun collagen triple helices with nanoscale-regulated tensile properties and drug release capability.
q-bio.BM:SARS-CoV-2 is what has caused the COVID-19 pandemic. Early viral infection is mediated by the SARS-CoV-2 homo-trimeric Spike (S) protein with its receptor binding domains (RBDs) in the receptor-accessible state. We performed molecular dynamics simulation on the S protein with a focus on the function of its N-terminal domains (NTDs). Our study reveals that the NTD acts as a "wedge" and plays a crucial regulatory role in the conformational changes of the S protein. The complete RBD structural transition is allowed only when the neighboring NTD that typically prohibits the RBD's movements as a wedge detaches and swings away. Based on this NTD "wedge" model, we propose that the NTD-RBD interface should be a potential drug target.
q-bio.BM:Sulfur mustard (SM), a chemical warfare agent, is a strong alkylating compound that readily reacts with numerous biomolecules. The goal of the present work was to define and validate new biomarkers of exposure to SM that could be easily accessible in urine or plasma. Because investigations using SM are prohibited by the Organization for the Prohibition of Chemical Weapons, we worked with 2-chloroethyl ethyl sulfide (CEES), a monofunctional analog of SM. We developed an ultra-high-pressure liquid chromatography - tandem mass spectrometry approach (UHPLC-MS/MS) to the conjugate of CEES to glutathione and two of its metabolites, the cysteine and the N-acetyl-cysteine conjugates. The N7-guanine adduct of CEES (N7Gua-CEES) was also targeted. After synthesizing the specific biomarkers, a solid phase extraction protocol and a UHPLC-MS/MS method with isotopic dilution were optimized. We were able to quantify N7Gua-CEES in the DNA of HaCaT keratinocytes and of explants of human skin exposed to CEES. N7Gua-CEES was also detected in the culture medium of these two models, together with the glutathione and the cysteine conjugates. In contrast, the N-acetyl-cysteine conjugate was not detected. The method was then applied to plasma from mice cutaneously exposed to CEES. All four markers could be detected. Our present results thus validate both the analytical technique and the biological relevance of new, easily quantifiable biomarkers of exposure to CEES. Because CEES behaves very similarly to SM, the results are promising for application to this toxic of interest.
q-bio.BM:Engineering simple, artificial models of living cells allows synthetic biologists to study cellular functions under well-controlled conditions. Reconstituting multicellular behaviors with synthetic cell-mimics is still a challenge because it requires efficient communication between individual compartments in large populations. This protocol presents a microfluidic method to produce large quantities of cell-mimics with highly porous, stable and chemically modifiable polymer membranes that can be programmed on demand with nucleus-like DNA-hydrogel compartments for gene expression. We describe expression of genes encoded in the hydrogel compartment and communication between neighboring cell-mimics through diffusive protein signals.
q-bio.BM:Mesoscale molecular assemblies on the cell surface, such as cilia and filopodia, integrate information, control transport and amplify signals. Synthetic devices mimicking these structures could sensitively monitor these cellular functions and direct new ones. The challenges in creating such devices, however are that they must be integrated with cells in a precise kinetically controlled process and a device's structure and its precisely structured cell interface must then be maintained during active cellular function. Here we report the ability to integrate synthetic micro-scale filaments, DNA nanotubes, into a cell's architecture by anchoring them by their ends to specific receptors on the surfaces of mammalian cells. These filaments can act as shear stress meters: how anchored nanotubes bend at the cell surface quantitatively indicates the magnitude of shear stresses between 0-2 dyn per cm2, a regime important for cell signaling. Nanotubes can also grow while anchored to cells, thus acting as dynamic components of cells. This approach to cell surface engineering, in which synthetic biomolecular assemblies are organized within existing cellular architecture, could make it possible to build new types of sensors, machines and scaffolds that can interface with, control and measure properties of cells.
q-bio.BM:We investigated progestin and corticosteroid activation of the progesterone receptor (PR) from elephant shark (Callorhinchus milii), a cartilaginous fish belonging to the oldest group of jawed vertebrates. Comparison with human PR experiments provides insights into the evolution of steroid activation of human PR. At 1 nM steroid, elephant shark PR is activated by progesterone, 17-hydroxy-progesterone, 20beta-hydroxy-progesterone, 11-deoxycorticosterone (21-hydroxyprogesterone) and 11-deoxycortisol. At 1 nM steroid, human PR is activated only by progesterone and11-deoxycorticosterone indicating increased specificity for progestins and corticosteroids during the evolution of human PR. RU486, an important clinical antagonist of human PR, did not inhibit progesterone activation of elephant shark PR. Cys-528 in elephant shark PR corresponds to Gly-722 in human PR, which is essential for RU486 inhibition of human PR. Confirming the importance of this site on elephant shark PR, RU486 inhibited progesterone activation of the Cys528Gly mutant PR. There also was a decline in activation of elephant shark Cys528Gly PR by 11-deoxycortisol, 17-hydroxy-progesterone and 20beta-hydroxy-progesterone and an increase in activation of human Gly722Cys PR by 11-deoxycortisol and decreased activation by corticosterone. One or more of these changes may have selected for the mutation corresponding to human glycine-722 PR that first evolved in platypus PR, a basal mammal.
q-bio.BM:The deadly coronavirus disease 2019 (COVID-19) pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has gone out of control globally. Despite much effort by scientists, medical experts, and society in general, the slow progress on drug discovery and antibody therapeutic development, the unknown possible side effects of the existing vaccines, and the high transmission rate of the SARS-CoV-2, remind us of the sad reality that our current understanding of the transmission, infectivity, and evolution of SARS-CoV-2 is unfortunately very limited. The major limitation is the lack of mechanistic understanding of viral-host cell interactions, the viral regulation, protein-protein interactions, including antibody-antigen binding, protein-drug binding, host immune response, etc. This limitation will likely haunt the scientific community for a long time and have a devastating consequence in combating COVID-19 and other pathogens. Notably, compared to the long-cycle, highly cost, and safety-demanding molecular-level experiments, the theoretical and computational studies are economical, speedy, and easy to perform. There exists a tsunami of the literature on molecular modeling, simulation, and prediction of SARS-CoV-2 that has become impossible to fully be covered in a review. To provide the reader a quick update about the status of molecular modeling, simulation, and prediction of SARS-CoV-2, we present a comprehensive and systematic methodology-centered narrative in the nick of time. Aspects such as molecular modeling, Monte Carlo (MC) methods, structural bioinformatics, machine learning, deep learning, and mathematical approaches are included in this review. This review will be beneficial to researchers who are looking for ways to contribute to SARS-CoV-2 studies and those who are assessing the current status in the field.
q-bio.BM:We study local conformational biases in the dynamics of {\alpha}-synuclein by using all-atom simulations with explicit and implicit solvents. The biases are related to the frequency of the specific contact formation. In both approaches, the protein is intrinsically disordered, and its strongest bias is to make bend and turn local structures. The explicit-solvent conformations can be substantially more extended which allows for formation of transient trefoil knots, both deep and shallow, that may last for up to 5 {\mu}s. The two-chain self-association events, both short- and long-lived, are dominated by formation of contacts in the central part of the sequence. This part tends to form helices when bound to a micelle.
q-bio.BM:Biomolecular condensates such as membraneless organelles, underpinned by liquid-liquid phase separation (LLPS), are important for physiological function, with electrostatics -- among other interaction types -- being a prominent force in their assembly. Charge interactions of intrinsically disordered proteins (IDPs) and other biomolecules are sensitive to the aqueous dielectric environment. Because the relative permittivity of protein is significantly lower than that of water, the interior of an IDP condensate is a relatively low-dielectric regime, which, aside from its possible functional effects on client molecules, should facilitate stronger electrostatic interactions among the scaffold IDPs. To gain insight into this LLPS-induced dielectric heterogeneity, addressing in particular whether a low-dielectric condensed phase entails more favorable LLPS than that posited by assuming IDP electrostatic interactions are uniformly modulated by the higher dielectric constant of the pure solvent, we consider a simplified multiple-chain model of polyampholytes immersed in explicit solvents that are either polarizable or possess a permanent dipole. Notably, simulated phase behaviors of these systems exhibit only minor to moderate differences from those obtained using implicit-solvent models with a uniform relative permittivity equals to that of pure solvent. Buttressed by theoretical treatments developed here using random phase approximation and polymer field-theoretic simulations, these observations indicate a partial compensation of effects between favorable solvent-mediated interactions among the polyampholytes in the condensed phase and favorable polyampholyte-solvent interactions in the dilute phase, often netting only a minor enhancement of overall LLPS propensity from the very dielectric heterogeneity that arises from the LLPS itself. Further ramifications of this principle are discussed.
q-bio.BM:Protein fold classification is a classic problem in structural biology and bioinformatics. We approach this problem using persistent homology. In particular, we use alpha shape filtrations to compare a topological representation of the data with a different representation that makes use of knot-theoretic ideas. We use the statistical method of Angle-based Joint and Individual Variation Explained (AJIVE) to understand similarities and differences between these representations.
q-bio.BM:DNA replication in all organisms must overcome nucleoprotein blocks to complete genome duplication. Accessory replicative helicases in Escherichia coli, Rep and UvrD, help replication machinery overcome blocks by removing incoming nucleoprotein complexes or aiding the re-initiation of replication. Mechanistic details of Rep function have emerged from recent live cell studies, however, the activities of UvrD in vivo remain unclear. Here, by integrating biochemical analysis and super-resolved single-molecule fluorescence microscopy, we discovered that UvrD self-associates into a tetramer and, unlike Rep, is not recruited to a specific replisome protein despite being found at approximately 80% of replication forks. By deleting rep and DNA repair factors mutS and uvrA, perturbing transcription by mutating RNA polymerase, and antibiotic inhibition; we show that the presence of UvrD at the fork is dependent on its activity. This is likely mediated by the very high frequency of replication blocks due to DNA bound proteins, including RNA polymerase, and DNA damage. UvrD is recruited to sites of nucleoprotein blocks via distinctly different mechanisms to Rep and therefore plays a more important and complementary role than previously realised in ensuring successful DNA replication.
q-bio.BM:Preliminary epidemiologic, phylogenetic and clinical findings suggest that several novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants have increased transmissibility and decreased efficacy of several existing vaccines. Four mutations in the receptor-binding domain (RBD) of the spike protein that are reported to contribute to increased transmission. Understanding physical mechanism responsible for the affinity enhancement between the SARS-CoV-2 variants and ACE2 is the "urgent challenge" for developing blockers, vaccines and therapeutic antibodies against the coronavirus disease 2019 (COVID-19) pandemic. Based on a hydrophobic-interaction-based protein docking mechanism, this study reveals that the mutation N501Y obviously increased the hydrophobic attraction and decrease hydrophilic repulsion between the RBD and ACE2 that most likely caused the transmissibility increment of the variants. By analyzing the mutation-induced hydrophobic surface changes in the attraction and repulsion at the binding site of the complexes of the SARS-CoV-2 variants and antibodies, we found out that all the mutations of N501Y, E484K, K417N and L452R can selectively decrease or increase their binding affinity with some antibodies.
q-bio.BM:The wide spread of coronavirus disease 2019 (COVID-19) has declared a global health emergency. As one of the most important targets for antibody and drug developments, Spike RBD-ACE2 interface has received extensive attention. Here, using molecular dynamics simulations, we explicitly evaluated the binding energetic features of the RBD-ACE2 complex of both SARS-CoV and SARS-CoV-2 to find the key residues. Although the overall ACE2-binding mode of the SARS-CoV-2 RBD is nearly identical to that of the SARS-CoV RBD, the difference in binding affinity is as large as -16.35 kcal/mol. Energy decomposition analyses identified three binding patches in the SARS-CoV-2 RBD and eleven key residues (Phe486, Tyr505, Asn501, Tyr489, Gln493, Leu455 and etc) which are believed to be the main targets for drug development. The dominating forces are from van der Waals attractions and dehydration of these residues. It is also worth mention that we found seven mutational sites (Lys417, Leu455, Ala475, Gly476, Glu484, Gln498 and Val503) on SARS-CoV-2 which unexpectedly weakened the RBD-ACE2 binding. Very interestingly, the most repulsive residue at the RBD-ACE2 interface (E484), is found to be mutated in the latest UK variant, B1.1.7, cause complete virus neutralization escapes from highly neutralizing COVID-19 convalescent plasma. Our present results indicate that at least from the energetic point of view such E484 mutation may have beneficial effects on ACE2 binding. The present study provides a systematical understanding, from the energetic point of view, of the binding features of SARS-CoV-2 RBD with ACE2 acceptor. We hope that the present findings of three binding patches, key attracting residues and unexpected mutational sites can provide insights to the design of SARS-CoV-2 drugs and identification of cross-active antibodies.
q-bio.BM:Proteins play crucial roles in every cellular process by interacting with each other, with nucleic acids, metabolites, and other molecules. The resulting assemblies can be very large and intricate and pose challenges to experimental methods. In the current era of integrative modeling, it is often only by a combination of various experimental techniques and computations that 3D models of those molecular machines can be obtained. Among the various computational approaches available, molecular docking is often the method of choice when it comes to predicting 3D structures of complexes. Docking can generate particularly accurate models when taking into account the available information on the complex of interest. We review here the use of experimental and bioinformatics data in protein-protein docking, describing recent software developments and highlighting applications for the modeling of antibody-antigen complexes and membrane protein complexes, and the use of evolutionary and shape information.
q-bio.BM:Heparin has been found to have antiviral activity against SARS-CoV-2. Here, by means of sliding window docking, molecular dynamics simulations and biochemical assays, we investigate the binding mode of heparin to the virus spike glycoprotein and the molecular basis for its antiviral activity. The simulations show that heparin binds at long, mostly positively charged patches on the spike, thereby masking the basic residues of the receptor binding domain and of the S1/S2 site. Experiments corroborated the simulation results by showing that heparin inhibits the cleavage of spike by furin by binding to the basic S1/S2 site. Our results indicate that heparin exerts its antiviral activity by both direct and allosteric mechanisms. Furthermore, the simulations provide insights into how heparan sulfate proteoglycans on the host cell can facilitate viral infection. Our results will aid the rational optimization of heparin derivatives for SARS-CoV-2 antiviral therapy.
q-bio.BM:Protein-protein interactions are the basis of many important physiological processes and are currently promising, yet difficult, targets for drug discovery. In this context, inhibitor of apoptosis proteins (IAPs)-mediated interactions are pivotal for cancer cell survival; the interaction of the BIR1 domain of cIAP2 with TRAF2 was shown to lead the recruitment of cIAPs to the TNF receptor, promoting the activation of the NF-\kappa B survival pathway. In this work, using a combined in silico-in vitro approach, we identified a drug-like molecule, NF023, able to disrupt cIAP2 interaction with TRAF2. We demonstrated in vitro its ability to interfere with the assembly of the cIAP2-BIR1/TRAF2 complex and performed a thorough characterization of the compound's mode of action through 248 parallel unbiased molecular dynamics simulations of 300 ns (totaling almost 75 {\mu}s of all-atom sampling), which identified multiple binding modes to the BIR1 domain of cIAP2 via clustering and ensemble docking. NF023 is, thus, a promising protein-protein interaction disruptor, representing a starting point to develop modulators of NF-\kappa B-mediated cell survival in cancer. This study represents a model procedure that shows the use of large-scale molecular dynamics methods to typify promiscuous interactors.
q-bio.BM:Domain motions involved in the function of proteins can often be well described as a combination of motions along a handfull of low-frequency modes, that is, with the values of a few normal coordinates. This means that, when the functional motion of a protein is unknown, it should prove possible to predict it, since it amounts to guess a few values. However, without the help of additional experimental data, using normal coordinates for generating accurate conformers far away from the initial one is not so straightforward. To do so, a new approach is proposed: instead of building conformers directly with the values of a subset of normal coordinates, they are built in two steps, the conformer built with normal coordinates being just used for defining a set of distance constraints, the final conformer being built so as to match them. Note that this approach amounts to transform the problem of generating accurate protein conformers using normal coordinates into a better known one: the distance-geometry problem, which is herein solved with the help of the ROSETTA software. In the present study, this approach allowed to rebuild accurately six large amplitude conformational changes, using at most six low-frequency normal coordinates. As a consequence of the low-dimensionality of the corresponding subspace, random exploration also proved enough for generating low-energy conformers close to the known end-point of the conformational change of the LAO binding protein, lysozyme T4 and adenylate kinase.
q-bio.BM:FoF1-ATP synthases are ubiquitous membrane-bound, rotary motor enzymes that can catalyze ATP synthesis and hydrolysis. Their enzyme kinetics are controlled by internal subunit rotation, by substrate and product concentrations, by mechanical inhibitory mechanisms, but also by the electrochemical potential of protons across the membrane. Single-molecule F\"orster resonance energy transfer (smFRET) has been used to detect subunit rotation within FoF1-ATP synthases embedded in freely diffusing liposomes. We now report that kinetic monitoring of functional rotation can be prolonged from milliseconds to seconds by utilizing an Anti-Brownian electrokinetic trap (ABEL trap). These extended observation times allowed us to observe fluctuating rates of functional rotation for individual FoF1-liposomes in solution. Broad distributions of ATP-dependent catalytic rates were revealed. The buildup of an electrochemical potential of protons was confirmed to limit the maximum rate of ATP hydrolysis. In the presence of ionophores or uncouplers, the fastest subunit rotation speeds measured in single reconstituted FoF1-ATP synthases were 180 full rounds per second. This was much faster than measured by biochemical ensemble averaging, but not as fast as the maximum rotational speed reported previously for isolated single F1 complexes uncoupled from the membrane-embedded Fo complex. Further application of ABEL trap measurements should help resolve the mechanistic causes of such fluctuating rates of subunit rotation.
q-bio.BM:In this study, the mechanical behavior of single tau, dimerized tau, and tau-microtubule interface subjected to high strain rate is investigated by molecular dynamics simulation.
q-bio.BM:Hyperactivity is one of the hallmakrs of ADHD. Aberrant dopamine signaling is a major theme in ADHD and dopamine production is directly linked to the intensity and persistence of hyperactive conduct. The strength and persistence of hyperactivity responses in Drosophila to startle stimuli were measured in a study to determine the effects of sugar on dopamine development. A total of four experimental groups, namely 1%,3%, and 5% glucose, as well as a control group were taken for the diet of Drosophila, and these four different amounts of glucose were introduced to the growth medium where Drosophila was cultured. The movements of Drosophila in the four treatment groups were captured using a camera. This experiment was carried out five times, each time using a different batch of Drosophila. Each group's average velocity over time was also reported. The web adaptation of Drosophila Activity Monitor(DAM) was used to analyze the captured movies from the camera. Furthermore, when it came to hyperactivity persistence, all four treatment classes were statistically different (p0.05). Since the strength and persistence of hyperactive behavior are directly correlated to dopamine output, this study shows that higher glucose intake is associated with more hyperactivity, for both the intensity({\Delta}V) and persistence.
q-bio.BM:The anticancer potential of monoamine oxidase (MAO) was observed in pre-clinical assays conducted with cell cultures and animals. L-Deprenyl (DEP) causes apoptosis in melanoma, leukemia and mammary cells. High-dose DEP has shown toxicity in mammary and pituitary cancers, as well as in monoblastic leukemia, in rats. DEP accounts for immune-stimulant effect capable of increasing natural killer cell activity, IL-2 generation, as well as of inhibiting tumor growth. DEP administration in old female rats has increased IL-2 generation and inverted the age-related depletion of IFN-{\gamma} generation in the spleen. Co-adjuvant DEP administration helped preventing/mitigating symptoms associated with peripheral neuropathy in cancer treatment. It also enhanced the cytotoxic effects of antineoplastic drugs - such as doxorubicin, cisplatin, among others - in cancer cells while they protected healthy cells from being damaged. DEP presented effect against dysfunctions such as debilitating hormone imbalance triggered by pituitary gland tumor; this gland produces the stimulatory hormone of adrenocorticotropic hormone which was related to the exacerbation of this disease. Thus, DEP emerges as an excellent potential drug against several cancer types and it also presents low toxicity in Parkinson`s disease patients subjected to long treatment with it.
q-bio.BM:The endogenous tridecapeptide neurotensin (NT) has emerged as an important inhibitory modulator of pain transmission, exerting its analgesic action through the activation of the G protein-coupled receptors, NTS1 and NTS2. Whereas both NT receptors mediate the analgesic effects of NT, NTS1 activation also produces hypotension and hypothermia, which may represent obstacles for the development of new pain medications. In the present study, we implemented various chemical strategies to improve the metabolic stability of the biologically active fragment NT(8-13) and assessed their NTS1/NTS2 relative binding affinities. We then determined their ability to reduce the nociceptive behaviors in acute, tonic, and chronic pain models and to modulate blood pressure and body temperature. To this end, we synthesized a series of NT(8-13) analogs carrying a reduced amide bond at Lys8-Lys9 and harboring site-selective modifications with unnatural amino acids, such as silaproline (Sip) and trimethylsilylalanine (TMSAla). Incorporation of Sip and TMSAla respectively in positions 10 and 13 of NT(8-13) combined with the Lys8-Lys9 reduced amine bond (JMV5296) greatly prolonged the plasma half-life time over 20 hours. These modifications also led to a 25-fold peptide selectivity toward NTS2. More importantly, central delivery of JMV5296 was able to induce a strong antinociceptive effect in acute (tail-flick), tonic (formalin), and chronic inflammatory (CFA) pain models without inducing hypothermia. Altogether, these results demonstrate that the chemically-modified NT(8-13) analog JMV5296 exhibits a better therapeutic profile and may thus represent a promising avenue to guide the development of new stable NT agonists and improve pain management.
q-bio.BM:In recent years, fossil-based fuels have been used to supply the energy needs of the world. Fossil-based fuels induce accumulation of the atmospheric CO2 which causes global warming. One of CO2 source is flue gas emission from the power plant. The microalgae have been considered excellent biological materials for reduction CO2 with ability to photosynthesis. In this study, air, 10% CO2, 15% CO2 and simulated flue gas (containing 15% CO2) feed were used to observe effect CO2 concentration and flue gas on cell growth and lipid content of Chlorella protothecoides. The highest dry cell weight (1,5 g/L) and lipid content (45%) values were obtained with 15% CO2 feed while the highest growth rate (1,10), biomass productivity (0,125 g/L/day), and lipid weight (0,63 g/g) were observed in 10% CO2 feed. Cultures fed with flue gas did not inhibited C. protothecoides growth and showed similar results with those fed with 15% CO2 gas in terms of growth rate, dry cell weight, biomass productivity and lipid content. These results showed that C. protothecoides has great potential for reducing CO2 emission from flue gas.
q-bio.BM:To generate drug molecules of desired properties with computational methods is the holy grail in pharmaceutical research. Here we describe an AI strategy, retro drug design, or RDD, to generate novel small molecule drugs from scratch to meet predefined requirements, including but not limited to biological activity against a drug target, and optimal range of physicochemical and ADMET properties. Traditional predictive models were first trained over experimental data for the target properties, using an atom typing based molecular descriptor system, ATP. Monte Carlo sampling algorithm was then utilized to find the solutions in the ATP space defined by the target properties, and the deep learning model of Seq2Seq was employed to decode molecular structures from the solutions. To test feasibility of the algorithm, we challenged RDD to generate novel drugs that can activate {\mu} opioid receptor (MOR) and penetrate blood brain barrier (BBB). Starting from vectors of random numbers, RDD generated 180,000 chemical structures, of which 78% were chemically valid. About 42,000 (31%) of the valid structures fell into the property space defined by MOR activity and BBB permeability. Out of the 42,000 structures, only 267 chemicals were commercially available, indicating a high extent of novelty of the AI-generated compounds. We purchased and assayed 96 compounds, and 25 of which were found to be MOR agonists. These compounds also have excellent BBB scores. The results presented in this paper illustrate that RDD has potential to revolutionize the current drug discovery process and create novel structures with multiple desired properties, including biological functions and ADMET properties. Availability of an AI-enabled fast track in drug discovery is essential to cope with emergent public health threat, such as pandemic of COVID-19.
q-bio.BM:This paper presents the methods that have participated in the SHREC 2021 contest on retrieval and classification of protein surfaces on the basis of their geometry and physicochemical properties. The goal of the contest is to assess the capability of different computational approaches to identify different conformations of the same protein, or the presence of common sub-parts, starting from a set of molecular surfaces. We addressed two problems: defining the similarity solely based on the surface geometry or with the inclusion of physicochemical information, such as electrostatic potential, amino acid hydrophobicity, and the presence of hydrogen bond donors and acceptors. Retrieval and classification performances, with respect to the single protein or the existence of common sub-sequences, are analysed according to a number of information retrieval indicators.
q-bio.BM:As the only thiol-bearing amino acid, cysteine (Cys) residues in proteins have the reactive thiol side chain, which is susceptible to a series of post-translational modifications (PTMs). These PTMs participate in a wide range of biological activities including the alteration of enzymatic reactions, protein-protein interactions and protein stability. Here we summarize the advance of cysteine PTM identification technologies and the features of the various kinds of the PTMs. We also discuss in silico approaches for the prediction of the different types of cysteine modified sites, giving directions for future study.
q-bio.BM:Cryo-EM reconstruction algorithms seek to determine a molecule's 3D density map from a series of noisy, unlabeled 2D projection images captured with an electron microscope. Although reconstruction algorithms typically model the 3D volume as a generic function parameterized as a voxel array or neural network, the underlying atomic structure of the protein of interest places well-defined physical constraints on the reconstructed structure. In this work, we exploit prior information provided by an atomic model to reconstruct distributions of 3D structures from a cryo-EM dataset. We propose Cryofold, a generative model for a continuous distribution of 3D volumes based on a coarse-grained model of the protein's atomic structure, with radial basis functions used to model atom locations and their physics-based constraints. Although the reconstruction objective is highly non-convex when formulated in terms of atomic coordinates (similar to the protein folding problem), we show that gradient descent-based methods can reconstruct a continuous distribution of atomic structures when initialized from a structure within the underlying distribution. This approach is a promising direction for integrating biophysical simulation, learned neural models, and experimental data for 3D protein structure determination.
q-bio.BM:There is great interest to develop artificial intelligence-based protein-ligand affinity models due to their immense applications in drug discovery. In this paper, PointNet and PointTransformer, two pointwise multi-layer perceptrons have been applied for protein-ligand affinity prediction for the first time. Three-dimensional point clouds could be rapidly generated from the data sets in PDBbind-2016, which contain 3 772 and 11 327 individual point clouds derived from the refined or/and general sets, respectively. These point clouds were used to train PointNet or PointTransformer, resulting in protein-ligand affinity prediction models with Pearson correlation coefficients R = 0.831 or 0.859 from the larger point clouds respectively, based on the CASF-2016 benchmark test. The analysis of the parameters suggests that the two deep learning models were capable to learn many interactions between proteins and their ligands, and these key atoms for the interaction could be visualized in point clouds. The protein-ligand interaction features learned by PointTransformer could be further adapted for the XGBoost-based machine learning algorithm, resulting in prediction models with an average Rp of 0.831, which is on par with the state-of-the-art machine learning models based on PDBbind database. These results suggest that point clouds derived from the PDBbind datasets are useful to evaluate the performance of 3D point clouds-centered deep learning algorithms, which could learn critical protein-ligand interactions from natural evolution or medicinal chemistry and have wide applications in studying protein-ligand interactions.
q-bio.BM:Spike proteins, 1200 amino acids, are divided into two nearly equal parts, S1 and S2. We review here phase transition theory, implemented quantitatively by thermodynamic scaling. The theory explains the evolution of Coronavirus extremely high contagiousness caused by a few mutations from CoV2003 to CoV2019 identified among hundreds in S1. The theory previously predicted the unprecedented success of spike-based vaccines. Here we analyze impressive successes by McClellan et al., 2020, in stabilizing their original S2P vaccine to Hexapro. Hexapro has expanded the two proline mutations of S2P, 2017, to six combined proline mutations in S2. Their four new mutations are the result of surveying 100 possibilities in their detailed structure-based context Our analysis, based on only sparse publicly available data, suggests new proline mutations could improve the Hexapro combination to Octapro or beyond.
q-bio.BM:Many factors influence biomolecules binding, and its assessment constitutes an elusive challenge in computational structural biology. In this respect, the evaluation of shape complementarity at molecular interfaces is one of the main factors to be considered. We focus on the particular case of antibody-antigen complexes to quantify the complementarities occurring at molecular interfaces. We relied on a method we recently developed, which employs the 2D Zernike descriptors, to characterize investigated regions with an ordered set of numbers summarizing the local shape properties. Collected a structural dataset of antibody-antigen complexes, we applied this method and we statistically distinguished, in terms of shape complementarity, pairs of interacting regions from non-interacting ones. Thus, we set up a novel computational strategy based on \textit{in-silico} mutagenesis of antibody binding site residues. We developed a Monte Carlo procedure to increase the shape complementarity between the antibody paratope and a given epitope on a target protein surface. We applied our protocol against several molecular targets in SARS-CoV-2 spike protein, known to be indispensable for viral cell invasion. We, therefore, optimized the shape of template antibodies for the interaction with such regions. As the last step of our procedure, we performed an independent molecular docking validation of the results of our Monte Carlo simulations.
q-bio.BM:Proteins tend to bury hydrophobic residues inside their core during the folding process to provide stability to the protein structure and to prevent aggregation. Nevertheless, proteins do expose some 'sticky' hydrophobic residues to the solvent. These residues can play an important functional role, for example in protein-protein and membrane interactions. Here, we investigate how hydrophobic protein surfaces are by providing three measures for surface hydrophobicity: the total hydrophobic surface area, the relative hydrophobic surface area, and - using our MolPatch method - the largest hydrophobic patch. Secondly, we analyse how difficult it is to predict these measures from sequence: by adapting solvent accessibility predictions from NetSurfP2.0, we obtain well-performing prediction methods for the THSA and RHSA, while predicting LHP is more difficult. Finally, we analyse implications of exposed hydrophobic surfaces: we show that hydrophobic proteins typically have low expression, suggesting cells avoid an overabundance of sticky proteins.
q-bio.BM:Recently a technique based on the interaction between adhesion proteins extracted from Streptococcus pyogenes, known as SpyRing, has been widely used to improve the thermal resilience of enzymes, the assembly of biostructures, cancer cell recognition and other fields. In SpyRing, the two termini of the target enzyme are respectively linked to the peptide SpyTag and its protein partner SpyCatcher. SpyTag spontaneously reacts with SpyCatcher to form an isopeptide bond, with which the target enzyme forms a close ring structure. It was believed that the covalent cyclization of protein skeleton caused by SpyRing reduces the conformational entropy of biological structure and improves its rigidity, thus improving the thermal resilience of the target enzyme. However, the effects of SpyTag/ SpyCatcher interaction with this enzyme are poorly understood, and their regulation of enzyme properties remains unclear. Here, for simplicity, we took the single domain enzyme lichenase from Bacillus subtilis 168 as an example, studied the interface interactions in the SpyRing system by molecular dynamics simulations, and examined the effects of the changes of electrostatic interaction and van der Waals interaction on the thermal resilience of target enzyme. The simulations showed that the interface between SpyTag/SpyCatcher and lichenase is different from that found by geometric matching method and highlighted key mutations that affect the intensity of interactions at the interface and might have effect on the thermal resilience of the enzyme. Our calculations provided new insights into the rational designs in the SpyRing.
q-bio.BM:In several previous works, I presented the mirror symmetry in the set of protein amino acids, expressed through the number of atoms. Here, however, the same thing is shown but over the number of nucleons and molecules mass. Compared to the previous version of the paper, minimal changes have been made, and Display 2 as well as Figures 3 and 4 have been added.
q-bio.BM:While many good textbooks are available on Protein Structure, Molecular Simulations, Thermodynamics and Bioinformatics methods in general, there is no good introductory level book for the field of Structural Bioinformatics. This book aims to give an introduction into Structural Bioinformatics, which is where the previous topics meet to explore three dimensional protein structures through computational analysis. We provide an overview of existing computational techniques, to validate, simulate, predict and analyse protein structures. More importantly, it will aim to provide practical knowledge about how and when to use such techniques. We will consider proteins from three major vantage points: Protein structure quantification, Protein structure prediction, and Protein simulation & dynamics.   The main emphasis of this work is to provide a background on experimental techniques for protein structure determination. The focus is set on X-ray crystallography and Nuclear Magnetic Resonance spectroscopy (NMR), which are by far the main methods used to determine the structure of soluble proteins. We will also introduce cryogenic Electron Microscopy (cryo-EM) and electron diffraction which are more suited to analyze membrane proteins and larger protein complexes. At the end, more qualitative techniques are summarized that are used to obtain insight on the overall structure and dynamics of proteins. Note that this introduction to protein structure determination aims at familiarizing the reader to different experimental techniques, their benefits and bottlenecks, but that a thorough mathematical and technical description of the concept is beyond the scope of this work.
q-bio.BM:Antibiotic resistance is a topical problem for both humans and animals and has been the subject of special monitoring for two decades. Several recent studies, including ours, have shown that this phenomenon is accentuated by the transfer of certain genetic elements and cross resistance acquisition, the latter or both resulting from the misuse of these antimicrobial drugs. A more careful use of antimicrobials, the search for new antibacterial compounds (including probiotics and phages) are the most recommended alternatives to overcome this situation. However, tests for modulations of antimicrobial activity can also play a major role. The main goal of synergy studies is to assess whether substances with antibacterial properties can improve the effectiveness of existing antimicrobials or give them a second life against resistant germs. Moreover, recent studies have demonstrated the ability of silver nanoparticles and extracts of certain plants to boost the effectiveness of certain antibiotics. such as ampicillin, benzylpenicillin, cefazolin, ciprofloxacin, nitrofurantoin, and kanamycin. Yet, from these studies we found that there was a serious problem with the interpretation of the results when using the disk method with determination of the increase in fold area. Indeed, the use of this method firstly requires the determination of the diameter of inhibition of the antibiotic alone; follow by the determination of the combination of antibiotic + modulating substance (MS) or extract.....
q-bio.BM:All human diseases involve proteins, yet our current tools to characterize and quantify them are limited. To better elucidate proteins across space, time, and molecular composition, we provide provocative projections for technologies to meet the challenges that protein biology presents. With a broad perspective, we discuss grand opportunities to transition the science of proteomics into a more propulsive enterprise. Extrapolating recent trends, we offer potential futures for a next generation of disruptive approaches to define, quantify and visualize the multiple dimensions of the proteome, thereby transforming our understanding and interactions with human disease in the coming decade.
q-bio.BM:The global burden of neurological diseases, the second leading cause of death after heart dis-eases constitutes one of the major challenges of modern medicine. Ayurveda, the traditional Indian medicinal systemenrooted in the Vedic literature and considered as a schema for the holistic management of health, characterizes various neurological diseases disorders (NDDs) and prescribes several herbs, formulations, and bio-cleansing regimes for their care and cure. In this work, we examined neuro-phytoregulatory potential of 34,472 phytochemicals among 3,038 herbs (including their varieties) mentioned in Ayurveda using network pharmacology approach and found that 45% of these Ayurvedic phytochemicals (APCs) have regulatory associations with 1,643 approved protein targets. Metabolite interconversion enzymes and protein modifying enzymes were found to be the major target classes of APCs against NDDs. The study further suggests that the actions of Ayurvedic herbs in managing NDDs were majorly via regulating signalling processes, like, G-protein signaling, acetylcholine signaling, chemokine signaling pathway and GnRH signaling. A high confidence network specific to 219 pharmaceutically relevant neuro-phytoregulators (NPRs) from 1,197 Ayurvedic herbs against 102 approved protein-targets involved in NDDs was developed and analyzed for gaining mechanistic insights. The key protein targets of NPRs to elicit their neuro-regulatory effect were highlighted as CYP and TRPA, while estradiol and melatonin were identified as the NPRs with high multi-targeting ability. 32 herbs enriched in NPRs were identified that include some of the well-known Ayurvedic neurological recommendations, like, Papaver somniferum, Glycyrrhiza glabra, Citrus aurantium, Cannabis sativa etc. Herbs enriched in NPRs may be used as a chemical source library for drug-discovery against NDDs from systems medicine perspectives.
q-bio.BM:More than 198 million cases of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been reported that result in no fewer than 4.2 million deaths globally. The rapid spread of the disease coupled with the lack of specific registered drugs for its treatment pose a great challenge that necessitate the development of therapeutic agents from a variety of sources. In this study, we employed an in-silico method to screen natural compounds with a view to identify inhibitors of the human transmembrane protease serine type 2 (TMPRSS2). The activity of this enzyme is essential for viral access into the host cells via angiotensin-converting enzyme 2 (ACE-2). Inhibiting the activity of this enzyme is therefore highly crucial for preventing viral fusion with ACE-2 thus shielding SARS-CoV-2 infectivity. 3D model of TMPRSS2 was constructed using I-TASSER, refined by GalaxyRefine, validated by Ramachandran plot server and overall model quality was checked by ProSA. 95 natural compounds from microalgae were virtually screened against the modeled protein that led to the identification 17 best leads capable of binding to TMPRSS2 with a good binding score comparable, greater or a bit lower than that of the standard inhibitor (camostat). Physicochemical properties, ADME (absorption, distribution, metabolism, excretion) and toxicity analysis revealed top 4 compounds including the reference drug with good pharmacokinetic and pharmacodynamic profiles. These compounds bind to the same pocket of the protein with a binding energy of -7.8 kcal/mol, -7.6 kcal/mol, -7.4 kcal/mol and -7.4 kcal/mol each for camostat, apigenin, catechin and epicatechin respectively. This study shed light on the potential of microalgal compounds against SARS-CoV-2. In vivo and invitro studies are required to developed SARS-CoV-2 drugs based on the structures of the compounds identified in this study.
q-bio.BM:Dual graphs have been applied to model RNA secondary structures with pseudoknots, or intertwined base pairs. In previous works, a linear-time algorithm was introduced to partition dual graphs into maximally connected components called blocks and determine whether each block contains a pseudoknot or not. As pseudoknots can not be contained into two different blocks, this characterization allow us to efficiently isolate smaller RNA fragments and classify them as pseudoknotted or pseudoknot-free regions, while keeping these sub-structures intact. Moreover we have extended the partitioning algorithm by classifying a pseudoknot as either recursive or non-recursive in order to continue with our research in the development of a library of building blocks for RNA design by fragment assembly. In this paper we present a methodology that uses our previous results and classify pseudoknots into the classical H,K,L, and M types, based upon a novel representation of RNA secondary structures as dual directed graphs (i.e., digraphs). This classification would help the systematic analysis of RNA structure and prediction as for example the implementation of more accurate folding algorithms.
q-bio.BM:The multidomain protein gelsolin (GSN) is composed of six homologous modules, sequentially named G1 to G6. Single point substitutions in this protein are responsible for AGel amyloidosis, a hereditary disease characterized by progressive corneal lattice dystrophy, cutis laxa, and polyneuropathy. Several different amyloidogenic variants of GSN have been identified over the years, but only the most common D187N/Y mutants, in G2, have been thoroughly characterized, and the underlying functional mechanistic link between mutation, altered protein structure, susceptibility to aberrant furin cleavage and aggregative potential resolved. Little is known about the recently identified mutations A551P, E553K and M517R hosted at the interface between G4 and G5, whose aggregation process likely follows an alternative pathway. We demonstrate that these three substitutions impair temperature and pressure stability of GSN but do not increase its susceptibility to furin cleavage, the first event of the canonical aggregation pathway. The variants are also characterized by a higher tendency to aggregate in the unproteolysed forms and show a higher proteotoxicity in a C. elegans-based assay. Structural studies point to a destabilization of the interface between G4 and G5 due to three different structural determinants: beta-strand breaking, steric hindrance and/or charge repulsion, all implying the impairment of interdomain contacts. All available evidence suggests that the rearrangement of the protein global architecture triggers a furin-independent aggregation of the protein, supporting the existence of a non-canonical pathway of gelsolin amyloidosis pathogenesis.
q-bio.BM:There is the need to represent in a standard manner all the possible variations of a protein or peptide primary sequence, including both artefactual and post-translational modifications of peptides and proteins. With that overall aim, here, the Human Proteome Organization (HUPO) Proteomics Standards Initiative (PSI) has developed a notation, called ProForma 2.0, which is a substantial extension of the original ProForma notation, developed by the Consortium for Top-Down Proteomics (CTDP). ProForma 2.0 aims to unify the representation of proteoforms and peptidoforms. Therefore, this notation supports use cases needed for bottom-up and middle/topdown proteomics approaches and allows the encoding of highly modified proteins and peptides using a human and machine-readable string. ProForma 2.0 covers encoding protein modification names and accessions, cross-linking reagents including disulfides, glycans, modifications encoded using mass shifts and/or via chemical formulas, labile and C or N-terminal modifications, ambiguity in the modification position and representation of atomic isotopes, among other use cases. Notational conventions are based on public controlled vocabularies and ontologies. Detailed information about the notation and existing implementations are available at http://www.psidev.info/proforma and at the corresponding GitHub repository (https://github.com/HUPO-PSI/proforma).
q-bio.BM:Cancer ranks as one of the deadliest diseases worldwide. The high mortality rate associated with cancer is partially due to the lack of reliable early detection methods and/or inaccurate diagnostic tools such as certain protein biomarkers. Cell-free nucleic acids (cfNA) such as circulating long non-coding RNAs (lncRNAs) have recently been proposed as a new class of potential biomarkers that could improve cancer diagnosis. The reported correlation between circulating lncRNA levels and the presence of tumors has triggered a great amount of interest among clinicians and scientists who have been actively investigating their potentials as reliable cancer biomarkers. In this report, we review the progress achieved (the Good) and challenges encountered (the Bad) in the development of circulating lncRNAs as potential biomarkers for early cancer diagnosis. We report and discuss the specificity and sensitivity issues of blood-based lncRNAs currently considered as promising biomarkers for various cancers such as hepatocellular carcinoma, colorectal cancer, gastric cancer and prostate cancer. We also emphasize the potential clinical applications (the Beauty) of circulating lncRNAs both as therapeutic targets and agents, on top of diagnostic and prognostic capabilities. Based on different published works, we finally provide recommendations for investigators who seek to investigate and compare the levels of circulating lncRNAs in the blood of cancer patients compared to healthy subjects by RT-qPCR or Next Generation Sequencing.
q-bio.BM:This chapter describes the application of constrained geometric simulations for prediction of antibody structural dynamics. We utilize constrained geometric simulations method FRODAN, which is a low computational complexity alternative to Molecular Dynamics (MD) simulations that can rapidly explore flexible motions in protein structures. FRODAN is highly suited for conformational dynamics analysis of large proteins, complexes, intrinsically disordered proteins and dynamics that occurs on longer biologically relevant time scales which are normally inaccessible to classical MD simulations. This approach predicts protein dynamics at an all-atom scale while retaining realistic covalent bonding, maintaining dihedral angles in energetically good conformations while avoiding steric clashes in addition to performing other geometric and stereochemical criteria checks. In this chapter, we apply FRODAN to showcase its applicability for probing functionally relevant dynamics of IgG2a, including large amplitude domain-domain motions and motions of complementarity determining region (CDR) loops. As was suggested in previous experimental studies, our simulations show that antibodies can explore a large range of conformational space.
q-bio.BM:This study included isolate petroleum hydrocarbons degradable bacteria and develops a consortium or a mixture of bacteria with high biodegradation capabilities which can be used in biological treatment units of the contaminated soils before release. In this studyt ten bacterial strains were isolated from soils contaminated with crude oil, by primary and secondary screening, and by using the sterile saline solution with 1% of crude oil, five bacterial isolates that can degrade oil were identified. The bacterial isolates were isolated from polluted soils with crude oil, the samples were collected from different areas of the Baiji refinery, and samples of heavy crude oil extracted from Qayyarah fields were used in designing the biodegradation experiments, the five isolates were diagnosed based on phenotypic, culture and biochemical characterizes, and detection of the gene sequence of bacterial isolates (16SrRNA). The (16SrRNA) gene sequence of the bacterial isolates was recorded under the accession numbers LC596402, LC596403, LC596406, LC596404, LC596405, for the genus (AM-I-1, AM-I-2, AM-I-5) and the species (AM-I-3, AM-I-4) that following Bacillus Sp respectively, in the NCBI's GenBank, the efficiency of the five isolates were examined for utilizing petroleum hydrocarbons using a sterile mineral salt medium MS supplemented with crude oil as the sole source of carbon with different concentrations, the results showed that the decomposition of petroleum hydrocarbons at the concentrations (0.5, 1, 1.5, 2, 3) % reached (57.32, 69.36,63.71, 75.20, 68.60) %, respectively, for mixed isolates, depending on the results of the gas chromatography (GC) analysis.
q-bio.BM:AlphaFold predicts protein structures from the amino acid sequence at or near experimental resolution, solving the 50-year-old protein folding challenge, leading to progress by transforming large-scale genomics data into protein structures. AlphaFold will also greatly change the scientific research model from low-throughput to high-throughput manner. The AlphaFold framework is a mixture of two types of workloads: MSA construction based on CPUs and model inference on GPUs. The first CPU stage dominates the overall runtime, taking hours for a single protein due to the large database sizes and I/O bottlenecks. However, GPUs in this CPU stage remain idle, resulting in low GPU utilization and restricting the capacity of large-scale structure predictions. Therefore, we proposed ParaFold, an open-source parallel version of AlphaFold for high throughput protein structure predictions. ParaFold separates the CPU and GPU parts to enable large-scale structure predictions. ParaFold also effectively reduces the CPU and GPU runtime with two optimizations without compromising the quality of prediction results: using multi-threaded parallelism on CPUs and using optimized JAX compilation on GPUs. We evaluated ParaFold with three datasets of different size and protein lengths. We evaluated the accuracy and efficiency of optimizations on CPUs and GPUs, and showed the large-scale prediction capability by running ParaFold inferences of 19,704 small proteins in five hours on one NVIDIA DGX-2. Using the JAX compile optimization, ParaFold attained a 13.8X average speedup over AlphaFold. ParaFold offers a rapid and effective approach for high-throughput structure predictions, leveraging the predictive power by running on supercomputers, with shorter time, and at a lower cost. The development of ParaFold will greatly speed up high-throughput studies and render the protein "structure-omics" feasible.
q-bio.BM:There is much concern about disruption of endocrine physiology regulated by steroid hormones in humans, other terrestrial vertebrates and fish by industrial chemicals, such as bisphenol A, and pesticides, such as DDT. These endocrine-disrupting chemicals influence steroid-mediated physiology in humans and other vertebrates by competing with steroids for receptor binding sites, disrupting diverse responses involved in reproduction, development and differentiation. Here I discuss that due to evolution of the progesterone receptor (PR) and mineralocorticoid receptor (MR) after ray-finned fish and terrestrial vertebrates diverged from a common ancestor, each receptor evolved to respond to different steroids in ray-finned fish and terrestrial vertebrates. In elephant shark, a cartilaginous fish, ancestral to ray-finned fish and terrestrial vertebrates, both progesterone and 17,20dihydroxyprogesterone activate the PR. During the evolution of ray-finned fish and terrestrial vertebrates, the PR in terrestrial vertebrates continued responding to progesterone and evolved to weakly respond to 17,20dihydroxyprogesterone. In contrast, the physiological progestin for the PR in zebrafish and other ray-finned fish is 17,20dihydroxyprogesterone, and ray-finned fish PR responds weakly to progesterone. The MR in fish and terrestrial vertebrates also diverged to have different responses to progesterone.
q-bio.BM:An effective model for protein structures is important for the study of protein geometry, which, to a large extent, determine the functions of proteins. There are a number of approaches for modelling; one might focus on the conformation of the backbone or H-bonds, and the model may be based on the geometry or the topology of the structure in focus. We focus on the topology of H-bonds in proteins, and explore the link between the topology and the geometry of protein structures. More specifically, we take inspiration from CASP Evaluation of Model Accuracy and investigate the extent to which structural similarities, via GDT_TS, can be estimated from the topology of H-bonds. We report on two experiments; one where we attempt to mimic the computation of GDT_TS based solely on the topology of H-bonds, and the other where we perform linear regression where the independent variables are various scores computed from the topology of H-bonds. We achieved an average $\Delta\text{GDT}$ of 6.45 with 54.5% of predictions inside 2 $\Delta\mathrm{GDT}$ for the first method, and an average $\Delta\mathrm{GDT}$ of 4.41 with 72.7% of predictions inside 2 $\Delta\mathrm{GDT}$ for the second method.
q-bio.BM:We introduce a new, simplified model of proteins, which we call protein metastructure. The metastructure of a protein carries information about its secondary structure and $\beta$-strand conformations. Furthermore, protein metastructure allows us to associate an object called a fatgraph to a protein, and a fatgraph in turn gives rise to a topological surface. It becomes thus possible to study the topological invariants associated to a protein. We discuss the correspondence between protein metastructures and fatgraphs, and how one can compute topological invariants, such as genus and the number of boundary components, from fatgraphs. We then describe an algorithm for generating likely candidate metastructures using the information obtained from topology of protein fatgraphs. This algorithm is further developed to predict $\beta$-sheet topology of proteins, with a possibility to combine it with an existing algorithm. We demonstrate the algorithm on the data from PDB, and improve the performance of and existing algorithm by combining with it.
q-bio.BM:Structural DNA nanotechnology has advanced to the extent that extremely complex structures can be designed. Much of this advancement has been due to the development of automated DNA design and simulation tools. Typically, the tools (e.g. NUPAK, cadnano, OxDNA) are created for a specific task. Ideally, there would be an environment that can integrate all such DNA tools, also with non-DNA tools - for example for modelling electromagnetic field along a zero-mode waveguide made of gold nanoparticles organized on a DNA breadboard. Such an environment would streamline design in DNA nanotechnology and enable applying DNA nanotechnology principles to construct high performance materials and devices from non-DNA components.   Here we present small a programmatic tool that is a step towards building such an environment for designing arbitrary nanostructures. In particular we showcase how small has been used to create an integrated computational materials engineering (ICME) framework for DNA nanotechnology, allowing the hierarchical design, simulation and visualization of arbitrary DNA nanostructures. Furthermore we demonstrate the design and modeling of the mode profiles and band structure of hybrid DNA-nanoparticle materials through the integration of small with Maxwell solvers.
q-bio.BM:The 21st century is presenting humankind with unprecedented environmental and medical challenges. The ability to design novel proteins tailored for specific purposes could transform our ability to respond timely to these issues. Recent advances in the field of artificial intelligence are now setting the stage to make this goal achievable. Protein sequences are inherently similar to natural languages: Amino acids arrange in a multitude of combinations to form structures that carry function, the same way as letters form words and sentences that carry meaning. Therefore, it is not surprising that throughout the history of Natural Language Processing (NLP), many of its techniques have been applied to protein research problems. In the last few years, we have witnessed revolutionary breakthroughs in the field of NLP. The implementation of Transformer pre-trained models has enabled text generation with human-like capabilities, including texts with specific properties such as style or subject. Motivated by its considerable success in NLP tasks, we expect dedicated Transformers to dominate custom protein sequence generation in the near future. Finetuning pre-trained models on protein families will enable the extension of their repertoires with novel sequences that could be highly divergent but still potentially functional. The combination of control tags such as cellular compartment or function will further enable the controllable design of novel protein functions. Moreover, recent model interpretability methods will allow us to open the 'black box' and thus enhance our understanding of folding principles. While early initiatives show the enormous potential of generative language models to design functional sequences, the field is still in its infancy. We believe that protein language models are a promising and largely unexplored field and discuss their foreseeable impact on protein design.
q-bio.BM:The tetanus neurotoxin (TeNT) is one of the most toxic proteins known to man, which prior to the use of the vaccine against the TeNT producing bacteria Clostridium tetani, resulted in a 20 % mortality rate upon infection. The clinical detrimental effects of tetanus have decreased immensely since the introduction of global vaccination programs, which depend on sustainable vaccine production. One of the major critical points in the manufacturing of these vaccines is the stable and reproducible production of high levels of toxin by the bacterial seed strains. In order to minimize time loss, the amount of TeNT is often monitored during and at the end of the bacterial culturing. The different methods that are currently available to assess the amount of TeNT in the bacterial medium suffer from variability, lack of sensitivity, and/or require specific antibodies. In accordance with the consistency approach and the three Rs (3Rs), both aiming to reduce the use of animals for testing, in-process monitoring of TeNT production could benefit from animal and antibody-free analytical tools. In this paper, we describe the development and validation of a new and reliable antibody free targeted LC-MS/MS method that is able to identify and quantify the amount of TeNT present in the bacterial medium during the different production time points up to the harvesting of the TeNT just prior to further upstream purification and detoxification. The quantitation method, validated according to ICH guidelines and by the application of the total error approach, was utilized to assess the amount of TeNT present in the cell culture medium of two TeNT production batches during different steps in the vaccine production process prior to the generation of the toxoid. The amount of TeNT generated under different physical stress conditions applied during bacterial culture was also monitored.
q-bio.BM:Performing full-resolution atomistic simulations of nucleic acid folding has remained a challenge for biomolecular modeling. Understanding how nucleic acids fold and how they transition between different folded structures as they unfold and refold has important implications for biology. This paper reports a theoretical model and computer simulation of the ab initio folding of DNA inverted repeat sequences. The formulation is based on an all-atom conformational model of the sugar-phosphate backbone via chain closure, and it incorporates three major molecular-level driving forces - base stacking, counterion-induced backbone self-interactions and base pairing - via separate analytical theories designed to capture and reproduce the effects of the solvent without requiring explicit water and ions in the simulation. To accelerate computational throughput, a mixed numerical/analytical algorithm for the calculation of the backbone conformational volume is incorporated into the Monte Carlo simulation, and special stochastic sampling techniques were employed to achieve the computational efficiency needed to fold nucleic acids from scratch. This paper describes implementation details, benchmark results and the advantages and technical challenges with this approach.
q-bio.BM:When considering large sets of molecules, it is helpful to place them in the context of a "chemical space" - a multidimensional space defined by a set of descriptors that can be used to visualize and analyze compound grouping as well as identify regions that might be void of valid structures. The chemical space of all possible molecules in a given biological or environmental sample can be vast and largely unexplored, mainly due to current limitations in processing of 'big data' by brute force methods (e.g., enumeration of all possible compounds in a space). Recent advances in artificial intelligence (AI) have led to multiple new cheminformatics tools that incorporate AI techniques to characterize and learn the structure and properties of molecules in order to generate plausible compounds, thereby contributing to more accessible and explorable regions of chemical space without the need for brute force methods. We have used one such tool, a deep-learning software called DarkChem, which learns a representation of the molecular structure of compounds by compressing them into a latent space. With DarkChem's design, distance in this latent space is often associated with compound similarity, making sparse regions interesting targets for compound generation due to the possibility of generating novel compounds. In this study, we used 1 million small molecules (less than 1000 Da) to create a representative chemical space (defined by calculated molecular properties) of all small molecules. We identified regions with few or no compounds and investigated their location in DarkChem's latent space. From these spaces, we generated 694,645 valid molecules, all of which represent molecules not found in any chemical database to date. These molecules filled 50.8% of the probed empty spaces in molecular property space. Generated molecules are provided in the supporting information.
q-bio.BM:Surface assays such as ELISA are pervasive in clinics and research and predominantly standardized in microtiter plates (MTP). MTPs provide many advantages but are often detrimental to surface assay efficiency due to inherent mass transport limitations. Microscale flows can overcome these and largely improve assay kinetics. However, the disruptive nature of microfluidics with existing labware and protocols has narrowed its transformative potential. We present WellProbe, a novel microfluidic concept compatible with MTPs. With it, we show that immunoassays become more sensitive at low concentrations (up to 9-fold signal improvement in 12-fold less time), richer in information with 3-4 different kinetic conditions, and can be used to estimate kinetic parameters, minimize washing steps and non-specific binding, and identify compromised results. We further multiplex single-well assays combining WellProbes kinetic regions with tailored microarrays. Finally, we demonstrate our system in a context of immunoglobulin subclass evaluation, increasingly regarded as clinically relevant.
q-bio.BM:The potential of virtual reality (VR) to contribute to drug design and development has been recognised for many years. Hardware and software developments now mean that this potential is beginning to be realised, and VR methods are being actively used in this sphere. A recent advance is to use VR not only to visualise and interact with molecular structures, but also to interact with molecular dynamics simulations of 'on the fly' (interactive molecular dynamics in VR, IMD-VR), which is useful not only for flexible docking but also to examine binding processes and conformational changes. iMD-VR has been shown to be useful for creating complexes of ligands bound to target proteins, e.g., recently applied to peptide inhibitors of the SARS-CoV-2 main protease. In this review, we use the term 'interactive VR' to refer to software where interactivity is an inherent part of the user VR experience e.g., in making structural modifications or interacting with a physically rigorous molecular dynamics (MD) simulation, as opposed to simply using VR controllers to rotate and translate the molecule for enhanced visualisation. Here, we describe these methods and their application to problems relevant to drug discovery, highlighting the possibilities that they offer in this arena. We suggest that the ease of viewing and manipulating molecular structures and dynamics, and the ability to modify structures on the fly (e.g., adding or deleting atoms) makes modern interactive VR a valuable tool to add to the armoury of drug development methods.
q-bio.BM:We have studied the effect of high hydrostatic pressure and temperature on the steady state fluorescence anisotropy of Green Fluorescent Protein (GFP). We find that the fluorescence anisotropy of GFP at a constant temperature decreases with increasing pressure. At atmospheric pressure, anisotropy decreases with increasing temperature but exhibits a maximum with temperature for pressure larger than 20 MPa. The temperature corresponding to the maximum of anisotropy increases with increasing pressure. By taking into account of the rotational correlation time changes of GFP with the pressure-temperature dependent viscosity of the solvent, we argue that viscosity increase with pressure is not a major contributing factor to the decrease in anisotropy with pressure. Our results suggest that the decrease of fluorescence anisotropy with pressure may result from changes in H-bonding environment around the chromophore.
q-bio.BM:Protein-protein binding enables orderly and lawful biological self-organization, and is therefore considered a miracle of nature. Protein-protein binding is steered by electrostatic forces, hydrogen bonding, van der Waals force, and hydrophobic interactions. Among these physical forces, only the hydrophobic interactions can be considered as long-range intermolecular attractions between proteins in intracellular and extracellular fluid. Low-entropy regions of hydration shells around proteins drive hydrophobic attraction among them that essentially coordinate protein-protein docking in rotational-conformational space of mutual orientations at the guidance stage of the binding. Here, an innovative method was developed for identifying the low-entropy regions of hydration shells of given proteins, and we discovered that the largest low-entropy regions of hydration shells on proteins typically cover the binding sites. According to an analysis of determined protein complex structures, shape matching between the largest low-entropy hydration shell region of a protein and that of its partner at the binding sites is revealed as a regular pattern. Protein-protein binding is thus found to be mainly guided by hydrophobic collapse between the shape-matched low-entropy hydration shells that is verified by bioinformatics analyses of hundreds of structures of protein complexes. A simple algorithm is developed to precisely predict protein binding sites.
q-bio.BM:Hexapeptides are widely applied as a model system for studying amyloid-forming properties of polypeptides, including proteins. Recently, large experimental databases have become publicly available with amyloidogenic labels. Using these datasets for training and testing purposes, one may build artificial intelligence (AI)-based classifiers for predicting the amyloid state of peptides. In our previous work (Biomolecules, 11(4) 500, (2021)) we described the Support Vector Machine (SVM)-based Budapest Amyloid Predictor (\url{https://pitgroup.org/bap}). Here we apply the Budapest Amyloid Predictor for discovering numerous amyloidogenic and non-amyloidogenic hexapeptide patterns with accuracy between 80\% and 84\%, as surprising and succinct novel rules for further understanding the amyloid state of peptides. For example, we have shown that for any independently mutated residue (position marked by ``x''), the patterns CxFLWx, FxFLFx, or xxIVIV are predicted to be amyloidogenic, while those of PxDxxx, xxKxEx, and xxPQxx non-amyloidogenic at all. We note that each amyloidogenic pattern with two x's (e.g.,CxFLWx) describes succinctly $20^2=400$ hexapeptides, while the non-amyloidogenic patterns comprising four point mutations (e.g.,PxDxxx) gives $20^4=160,000$ hexapeptides in total. To our knowledge, no similar applications of artificial intelligence tools or succinct amyloid patterns were described before the present work.
q-bio.BM:Emerging severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants have compromised existing vaccines and posed a grand challenge to coronavirus disease 2019 (COVID-19) prevention, control, and global economic recovery. For COVID-19 patients, one of the most effective COVID-19 medications is monoclonal antibody (mAb) therapies. The United States Food and Drug Administration (U.S. FDA) has given the emergency use authorization (EUA) to a few mAbs, including those from Regeneron, Eli Elly, etc. However, they are also undermined by SARS-CoV-2 mutations. It is imperative to develop effective mutation-proof mAbs for treating COVID-19 patients infected by all emerging variants and/or the original SARS-CoV-2. We carry out a deep mutational scanning to present the blueprint of such mAbs using algebraic topology and artificial intelligence (AI). To reduce the risk of clinical trial-related failure, we select five mAbs either with FDA EUA or in clinical trials as our starting point. We demonstrate that topological AI-designed mAbs are effective to variants of concerns and variants of interest designated by the World Health Organization (WHO), as well as the original SARS-CoV-2. Our topological AI methodologies have been validated by tens of thousands of deep mutational data and their predictions have been confirmed by results from tens of experimental laboratories and population-level statistics of genome isolates from hundreds of thousands of patients.
q-bio.BM:Objective: To predict structure and function of Glutamine synthetase (GS) from Pseudoalteromonas sp. by bioinformatics technology, and to provide a theoretical basis for further study. Methods: Open reading frame (ORF) of GS sequence from Pseudoalteromonas sp. was obtained by ORF finder and was translated into amino acid residue. The structure domain was analyzed by Blast. By the method of analysis tools: Protparam, ProtScale, SignalP-4.0, TMHMM, SOPMA, SWISS-MODEL, NCBI SMART-BLAST and MAGA 7.0, the structure and function of the protein were predicted and analyzed. Results: The results showed that the sequence was GS with 468 amino acid residues, theoretical molecular weight was 51986.64 Da. The protein has the closest evolutionary status with Shewanella oneidensis. Then it had no signal peptide site and transmembrane domain. Secondary structure of GS contained 35.04% alpha-helix, 16.67% Extended chain, 5.34% beta-turn, 42.95% RandomCoil. Conclusions: This GU was a variety of biological functions of protein that may be used as a molecular samples of microbial nitrogen metabolism in extreme environments.
q-bio.BM:The COVID-19 pandemic has presented many challenges that have spurred biotechnological research to address specific problems. Diagnostics is one area where biotechnology has been critical. Diagnostic tests play a vital role in managing a viral threat by facilitating the detection of infected and/or recovered individuals. From the perspective of what information is provided, these tests fall into two major categories, molecular and serological. Molecular diagnostic techniques assay whether a virus is present in a biological sample, thus making it possible to identify individuals who are currently infected. Additionally, when the immune system is exposed to a virus, it responds by producing antibodies specific to the virus. Serological tests make it possible to identify individuals who have mounted an immune response to a virus of interest and therefore facilitate the identification of individuals who have previously encountered the virus. These two categories of tests provide different perspectives valuable to understanding the spread of SARS-CoV-2. Within these categories, different biotechnological approaches offer specific advantages and disadvantages. Here we review the categories of tests developed for the detection of the SARS-CoV-2 virus or antibodies against SARS-CoV-2 and discuss the role of diagnostics in the COVID-19 pandemic.
q-bio.BM:The infectivity of SARS-CoV-2 depends on the binding affinity of the receptor-binding domain (RBD) of the spike protein with the angiotensin converting enzyme 2 (ACE2) receptor. The calculated RBD-ACE2 binding energies indicate that the difference in transmission efficiency of SARS-CoV-2 variants cannot be fully explained by electrostatic interactions, hydrogen-bond interactions, van der Waals interactions, internal energy, and nonpolar solvation energies. Here, we demonstrate that low-entropy regions of hydration shells around proteins drive hydrophobic attraction between shape-matched low-entropy regions of the hydration shells, which essentially coordinates protein-protein binding in rotational-configurational space of mutual orientations and determines the binding affinity. An innovative method was used to identify the low-entropy regions of the hydration shells of the RBDs of multiple SARS-CoV-2 variants and the ACE2. We observed integral low-entropy regions of hydration shells covering the binding sites of the RBDs and matching in shape to the low-entropy region of hydration shell at the binding site of the ACE2. The RBD-ACE2 binding is thus found to be guided by hydrophobic collapse between the shape-matched low-entropy regions of the hydration shells. A measure of the low-entropy of the hydration shells can be obtained by counting the number of hydrophilic groups expressing hydrophilicity within the binding sites. The low-entropy level of hydration shells at the binding site of a spike protein is found to be an important indicator of the contagiousness of the coronavirus.
q-bio.BM:Molecular surface representations have been advertised as a great tool to study protein structure and functions, including protein-ligand binding affinity modeling. However, the conventional surface-area-based methods fail to deliver a competitive performance on the energy scoring tasks. The main reason is the lack of crucial physical and chemical interactions encoded in the molecular surface generations. We present novel molecular surface representations embedded in different scales of the element interactive manifolds featuring the dramatically dimensional reduction and accurately physical and biological properties encoders. Those low-dimensional surface-based descriptors are ready to be paired with any advanced machine learning algorithms to explore the essential structure-activity relationships that give rise to the element interactive surface area-based scoring functions (EISA-score). The newly developed EISA-score has outperformed many state-of-the-art models, including various well-established surface-related representations, in standard PDBbind benchmarks.
q-bio.BM:This paper presents the methods that have participated in the SHREC 2022 contest on protein-ligand binding site recognition. The prediction of protein-ligand binding regions is an active research domain in computational biophysics and structural biology and plays a relevant role for molecular docking and drug design. The goal of the contest is to assess the effectiveness of computational methods in recognizing ligand binding sites in a protein based on its geometrical structure. Performances of the segmentation algorithms are analyzed according to two evaluation scores describing the capacity of a putative pocket to contact a ligand and to pinpoint the correct binding region. Despite some methods perform remarkably, we show that simple non-machine-learning approaches remain very competitive against data-driven algorithms. In general, the task of pocket detection remains a challenging learning problem which suffers of intrinsic difficulties due to the lack of negative examples (data imbalance problem).
q-bio.BM:Posttranslational modifications (PTMs) are an integral component to how cells respond to perturbation. While experimental advances have enabled improved PTM identification capabilities, the same throughput for characterizing how structural changes caused by PTMs equate to altered physiological function has not been maintained. In this Perspective, we cover the history of computational modeling and molecular dynamics simulations which have characterized the structural implications of PTMs. We distinguish results from different molecular dynamics studies based upon the timescales simulated and analysis approaches used for PTM characterization. Lastly, we offer insights into how opportunities for modern research efforts on in silico PTM characterization may proceed given current state-of-the-art computing capabilities and methodological advancements.
q-bio.BM:The proteins involved in cells' mechanobiological processes have evolved specialized and surprising responses to applied forces. Biochemical transformations that show catch-to-slip switching and force-induced pathway switching serve important functions in cell adhesion, mechano-sensing and signaling, and protein folding. We show that these switching behaviors are generated by singularities in the flow field that describes force-induced deformation of bound and transition states. These singularities allow for a complete characterization of switching mechanisms in 2-dimensional (2D) free energy landscapes, and provide a path toward elucidating novel forms of switching in higher dimensional models. Remarkably, the singularity that generates a catch-slip switch occurs in almost every 2D free energy landscape, implying that almost any bond admitting a 2D model will exhibit catch-slip behavior under appropriate force. We apply our analysis to models of P-selectin and antigen extraction to illustrate how these singularities provide an intuitive framework for explaining known behaviors and predicting new behaviors.
q-bio.BM:A theory for sequence dependent liquid-liquid phase separation (LLPS) of intrinsically disordered proteins (IDPs) in the study of biomolecular condensates is formulated by extending the random phase approximation (RPA) and field-theoretic simulation (FTS) of heteropolymers with spatially long-range Coulomb interactions to include the fundamental effects of short-range, hydrophobic-like interactions between amino acid residues. To this end, short-range effects are modeled by Yukawa interactions between multiple nonelectrostatic charges derived from an eigenvalue decomposition of pairwise residue-residue contact energies. Chain excluded volume is afforded by incompressibility constraints. A mean-field approximation leads to an effective Flory $\chi$ parameter, which, in conjunction with RPA, accounts for the contact-interaction effects of amino acid composition and the sequence-pattern effects of long-range electrostatics in IDP LLPS, whereas FTS based on the formulation provides full sequence dependence for both short- and long-range interactions. This general approach is illustrated here by applications to variants of a natural IDP in the context of several different amino-acid interaction schemes as well as a set of different model hydrophobic-polar sequences sharing the same composition. Effectiveness of the methodology is verified by coarse-grained explicit-chain molecular dynamics simulations.
q-bio.BM:Viroporins are small viral proteins that oligomerize in the membrane of host cells and induce the formation of hydrophilic pores in these membranes, thus altering the physiological properties of the host cells. Due to their significance for viral pathogenicity, they have become targets for pharmaceutical intervention, especially through compounds that block their pore-forming activity. Here we add to the growing literature concerning the structure and function of viroporins by studying and comparing -- through molecular dynamics simulations -- the folding of the transmembrane domain peptides of viroporins derived from four viruses : influenza A, influenza B, and the coronaviruses MERS-Cov-2 and SARS-CoV-2. Through a total of more than 50 {\mu}s of simulation time in explicit solvent (TFE) and with full electrostatics, we characterize the folding behavior, helical stability and helical propensity of these transmembrane peptides in their monomeric state and we identify common motifs that may reflect their quaternary organization and/or biological function. We show that the two influenza-derived peptides are significantly different in peptide sequence and secondary structure from the two coronavirus-derived peptides, and that they are organized in two structurally distinct parts : a significantly more stable N-terminal half, and a fast converting C-terminal half that continuously folds and unfolds between $\alpha$-helical structures and non-canonical structures which are mostly turns. In contrast, the two coronavirus-derived transmembrane peptides are much more stable and fast helix formers when compared with the influenza ones. We discuss possible interpretations of these findings and their putative connection to the structural characteristics of the respective viroporins.
q-bio.BM:In recent years, extracellular vesicles such have become promising carriers as the next-generation drug delivery platforms. Effective loading of exogenous cargos without compromising the extracellular vesicle membrane is a major challenge. Rapid squeezing through nanofluidic channels is a widely used approach to load exogenous cargoes into the EV through the nanopores generated temporarily on the membrane. However, the exact mechanism and dynamics of nanopores opening, as well as cargo loading through nanopores during the squeezing process remains unknown and is impossible to be visualized or quantified experimentally due to the small size of the EV and the fast transient process. This paper developed a systemic algorithm to simulate nanopore formation and predict drug loading during extracellular vesicle (EV) squeezing by leveraging the power of coarse-grain (CG) molecular dynamics simulations with fluid dynamics. The EV CG beads are coupled with implicit Fluctuating Lattice Boltzmann solvent. Effects of EV property and various squeezing test parameters, such as EV size, flow velocity, channel width, and length, on pore formation and drug loading efficiency are analyzed. Based on the simulation results, a phase diagram is provided as a design guidance for nanochannel geometry and squeezing velocity to generate pores on membrane without damaging the EV. This method can be utilized to optimize the nanofluidic device configuration and flow setup to obtain desired drug loading into EVs
q-bio.BM:Binding kinetic parameters can be correlated with drug efficacy, which led to the development of various computational methods for predicting binding kinetic rates and gaining insight into protein-drug binding paths and mechanisms in recent years. In this review, we introduce and compare computational methods recently developed and applied to two systems, trypsin-benzamidine and kinase-inhibitor complexes. Methods involving enhanced sampling in molecular dynamics simulations or machine learning can be used not only to predict kinetic rates, but also to reveal factors modulating the duration of residence times, selectivity and drug resistance to mutations. Methods which require less computational time to make predictions are highlighted, and suggestions to reduce the error of computed kinetic rates are presented.
q-bio.BM:Opioid use disorder (OUD) is a chronic and relapsing condition that involves the continued and compulsive use of opioids despite harmful consequences. The development of medications with improved efficacy and safety profiles for OUD treatment is urgently needed. Drug repurposing is a promising option for drug discovery due to its reduced cost and expedited approval procedures. Computational approaches based on machine learning enable the rapid screening of DrugBank compounds, identifying those with the potential to be repurposed for OUD treatment. We collected inhibitor data for four major opioid receptors and used advanced machine learning predictors of binding affinity that fuse the gradient boosting decision tree algorithm with two natural language processing (NLP)-based molecular fingerprints and one traditional 2D fingerprint. Using these predictors, we systematically analyzed the binding affinities of DrugBank compounds on four opioid receptors. Based on our machine learning predictions, we were able to discriminate DrugBank compounds with various binding affinity thresholds and selectivities for different receptors. The prediction results were further analyzed for ADMET (absorption, distribution, metabolism, excretion, and toxicity), which provided guidance on repurposing DrugBank compounds for the inhibition of selected opioid receptors. The pharmacological effects of these compounds for OUD treatment need to be tested in further experimental studies and clinical trials. Our machine learning studies provide a valuable platform for drug discovery in the context of OUD treatment.
q-bio.BM:The well known phenomenon of phase separation in synthetic polymers and proteins has become a major topic in biophysics because it has been invoked as a mechanism of compartment formation in cells, without the need for membranes. Most of the coacervates (or condensates) are composed of Intrinsically Disordered Proteins (IDPs) or regions that are structureless, often in interaction with RNA and DNA. One of the more intriguing IDPs is the 526-residue RNA binding protein, Fused In Sarcoma (FUS), whose monomer conformations and condensates exhibit unusual behavior that are sensitive to solution conditions. By focussing principally on the N-terminus low complexity domain (FUS-LC comprising residues 1-214) and other truncations, we rationalize the findings of solid state NMR experiments, which show that FUS-LC adopts a non-polymorphic fibril (core-1) involving residues 39-95, flanked by fuzzy coats on both the N- and C- terminal ends. An alternate structure (core-2), whose free energy is comparable to core-1, emerges only in the truncated construct (residues 110-214). Both core-1 and core-2 fibrils are stabilized by a Tyrosine ladder as well as hydrophilic interactions. The morphologies (gels, fibrils, and glass-like behavior) adopted by FUS seem to vary greatly, depending on the experimental conditions. The effect of phosphorylation is site specific and affects the stability of the fibril depending on the sites that are phosphorylated. Many of the peculiarities associated with FUS may also be shared by other IDPs, such as TDP43 and hnRNPA2. We outline a number of problems for which there is no clear molecular understanding.
q-bio.BM:Cell-free protein synthesis (CFPS) systems are an attractive to complement the usual cell-based synthesis of proteins, especially for screening approaches. The literature describes a wide variety of CFPS systems, but their performance is difficult to compare since the reaction components are often used at different concentrations. Therefore, we have developed a calculation tool based on amino acid balancing to evaluate the performance of CFPS by determining the fractional yield as the ratio between theoretically achievable and achieved protein molar concentration. This tool was applied to a series of experiments from our lab and to various systems described in the literature to identify systems that synthesize proteins very efficiently and those that still have potential for higher yields. The well-established Escherichia coli system showed a high efficiency in the utilization of amino acids, but interestingly, less-attention-paid systems, such as the one based on Streptomyces lividans, also demonstrated exceptional fractional yields of 100%, meaning complete amino acid conversion. The methods and tools described here can quickly identify when a system has reached its maximum or has other limitations. We believe that the approach described here will facilitate the evaluation and optimization of existing CFPS systems and provide the basis for the systematic development of new CFPS systems.
q-bio.BM:Biomolecular communication demands that interactions between parts of a molecular system act as scaffolds for message transmission. It also requires an evolving and organized system of signs - a communicative agency - for creating and transmitting meaning. Here I explore the need to dissect biomolecular communication with retrodiction approaches that make claims about the past given information that is available in the present. While the passage of time restricts the explanatory power of retrodiction, the use of molecular structure in biology offsets information erosion. This allows description of the gradual evolutionary rise of structural and functional innovations in RNA and proteins. The resulting chronologies can also describe the gradual rise of molecular machines of increasing complexity and computation capabilities. For example, the accretion of rRNA substructures and ribosomal proteins can be traced in time and placed within a geological timescale. Phylogenetic, algorithmic and theoretical-inspired accretion models can be reconciled into a congruent evolutionary model. Remarkably, the time of origin of enzymes, functional RNA, non-ribosomal peptide synthetase (NRPS) complexes, and ribosomes suggest they gradually climbed Chomsky's hierarchy of formal grammars, supporting the gradual complexification of machines and communication in molecular biology. Future retrodiction approaches and in-depth exploration of theoretical models of computation will need to confirm such evolutionary progression.
q-bio.BM:Understanding the base pairing of an RNA sequence provides insight into its molecular structure.By mining suboptimal sampling data, RNAprofiling 1.0 identifies the dominant helices in low-energy secondary structures as features, organizes them into profiles which partition the Boltzmann sample, and highlights key similarities/differences among the most informative, i.e. selected, profiles in a graphical format. Version 2.0 enhances every step of this approach. First, the featured substructures are expanded from helices to stems. Second, profile selection includes low-frequency pairings similar to featured ones. In conjunction, these updates extend the utility of the method to sequences up to length 600, as evaluated over a sizable dataset. Third, relationships are visualized in a decision tree which highlights the most important structural differences. Finally, this cluster analysis is made accessible to experimental researchers in a portable format as an interactive webpage, permitting a much greater understanding of trade-offs among different possible base pairing combinations.
q-bio.BM:The molecular origins of proteins' functions are a combinatorial search problem in the proteins' sequence space, which requires enormous resources to solve. However, evolution has already solved this optimization problem for us, leaving behind suboptimal solutions along the way. Comparing suboptimal proteins along the evolutionary pathway, or ancestors, with more optimal modern proteins can lead us to the exact molecular origins of a particular function. In this paper, we study the long-standing question of the selectivity of Imatinib, an anti-cancer kinase inhibitor drug. We study two related kinases, Src and Abl, and four of their common ancestors, to which Imatinib has significantly different affinities. Our results show that the orientation of the N-lobe with respect to the C-lobe varies between the kinases along their evolutionary pathway and is consistent with Imatinib's inhibition constants as measured experimentally. The conformation of the DFG-motif (Asp-Phe-Gly) and the structure of the P-loop also seem to have different stable conformations along the evolutionary pathway, which is aligned with Imatinib's affinity.
q-bio.BM:The Shiga toxins comprise a family of related protein toxins secreted by certain types of bacteria. Shigella dysenteriae, some strain of Escherichia coli and other bacterias can express toxins which caused serious complication during the infection. Shiga toxin and the closely related Shiga-like toxins represent a group of very similar cytotoxins that may play an important role in diarrheal disease and hemolytic-uremic syndrome. The outbreaks caused by this toxin raised serious public health crisis and caused economic losses. These toxins have the same biologic activities and according to recent studies also share the same binding receptor, globotriosyl ceramide (Gb3). Rapid detection of food contamination is therefore relevant for the containment of food-borne pathogens. The conventional methods to detect pathogens, such as microbiological and biochemical identification are time-consuming and laborious. The immunological or nucleic acid-based techniques require extensive sample preparation and are not amenable to miniaturization for on-site detection. In the present are necessary of techniques of rapid identification, simple and sensitive which can be employed in the countryside with minimally-sophisticated instrumentation. Biosensors have shown tremendous promise to overcome these limitations and are being aggressively studied to provide rapid, reliable and sensitive detection platforms for such applications.
q-bio.BM:The activation of the dodecameric Ca2+/calmodulin dependent kinase II (CaMKII) holoenzyme is critical for the formation of memory. We now report that CaMKII has a remarkable property, which is that activation of the holoenzyme triggers the exchange of subunits with other holoenzymes, including unactivated ones, enabling the calcium-independent phosphorylation and activation of new subunits. We show, using a single-molecule TIRF microscopy technique, that the exchange process is triggered by the activation of CaMKII, with the exchange rate being modulated by phosphorylation of two residues in the calmodulin-binding segment, Thr 305 and Thr 306. Based on these results, and on the analysis of molecular dynamics simulations, we suggest that the phosphorylated regulatory segment of CaMKII interacts with the central hub of the holoenzyme and weakens its integrity, thereby promoting exchange. Our results have implications for an earlier idea that subunit exchange in CaMKII may have relevance for the storage of information resulting from brief coincident stimuli during neuronal signaling.
q-bio.BM:A simple way to get insights about the possible functional motions of a protein is to perform a normal mode analysis (NMA). Indeed, it has been shown that low-frequency modes thus obtained are often closely related to domain motions involved in protein function. Moreover, because protein low-frequency modes are known to be robust, NMA can be performed using coarse-grained models. As a consequence, it can be done for large ensembles of conformations as well as for large systems, like the ribosome, whole virus capsids, etc. Unexpectedly, on the high-frequency side, modes obtained with cutoff-based coarse-grained models also seem able to provide useful insights on protein dynamical properties.
q-bio.BM:In addition to the biologically active monomer of the protein Insulin circulating in human blood, the molecule also exists in dimeric and hexameric forms that are used as storage. The Insulin monomer contains two distinct surfaces, namely the dimer forming surface (DFS) and the hexamer forming surface (HFS) that are specifically designed to facilitate the formation of the dimer and the hexamer, respectively. In order to characterize the structural and dynamical behaviour of interfacial water molecules near these two surfaces (DFS and HFS), we performed atomistic molecular dynamics simulations of Insulin with explicit water. Dynamical characterization reveals that the structural relaxation of the hydrogen bonds formed between the residues of DFS and the interfacial water molecules is faster than those formed between water and that of the HFS. Furthermore, the residence times of water molecules in the protein hydration layer for both the DFS and HFS are found to be significantly higher than those for some of the other proteins studied so far, such as HP-36 and lysozyme. The surface topography and the arrangement of amino acid residues work together to organize the water molecules in the hydration layer in order to provide them with a preferred orientation. HFS having a large polar solvent accessible surface area and a convex extensive nonpolar region, drives the surrounding water molecules to acquire predominantly a clathrate-like structure. In contrast, near the DFS, the surrounding water molecules acquire an inverted orientation owing to the flat curvature of hydrophobic surface and interrupted hydrophilic residual alignment. We have followed escape trajectory of several such quasi-bound water molecules from both the surfaces and constructed free energy surfaces of these water molecules.These free energy surfaces reveal the differences between the two hydration layers.
q-bio.BM:Pyrrolizidine Alkaloids (PAs) are a group of naturally occurring alkaloids that are produced by plants as a defense mechanism against insect herbivores. The analytical methodologies employed for their detection have come a long way since the first analytical experiment and in the last 30 years had an enormous development, both technological and experimental. It is notorious that before the generalization of certain technologies, especially in a post-war atmosphere, most scientific researches relied on what it is today thin-layer chromatography. Nevertheless this technique was not sufficient for accurately measure quantities and unambiguously identify compounds, therefore spectroscopic techniques arose as well as chromatographic techniques. While the first never really coped with PAs analysis requirements the latter, either as gas or liquid chromatography allowed the analysis of complex sample matrices. Simultaneously, nuclear magnetic resonance also suffered significant developments while mass spectrometry has become an attractive technique due to increasingly higher maximum resolutions. The observed tendency in recent years, in pyrrolizidine detection and quantification, as well as in many other areas, is that hyphenated techniques are the chosen methods. A large number of papers report multihyphenated methodologies, and the overwhelming majority relies on gas or liquid chromatography.
q-bio.BM:X-ray crystallography is the predominant method for obtaining atomic-scale information about biological macromolecules. Despite the success of the technique, obtaining well diffracting crystals still critically limits going from protein to structure. In practice, the crystallization process proceeds through knowledge-informed empiricism. Better physico-chemical understanding remains elusive because of the large number of variables involved, hence little guidance is available to systematically identify solution conditions that promote crystallization. To help determine relationships between macromolecular properties and their crystallization propensity, we have trained statistical models on samples for 182 proteins supplied by the Northeast Structural Genomics consortium. Gaussian processes, which capture trends beyond the reach of linear statistical models, distinguish between two main physico-chemical mechanisms driving crystallization. One is characterized by low levels of side chain entropy and has been extensively reported in the literature. The other identifies specific electrostatic interactions not previously described in the crystallization context. Because evidence for two distinct mechanisms can be gleaned both from crystal contacts and from solution conditions leading to successful crystallization, the model offers future avenues for optimizing crystallization screens based on partial structural information. The availability of crystallization data coupled with structural outcomes analyzed through state-of-the-art statistical models may thus guide macromolecular crystallization toward a more rational basis.
q-bio.BM:We discuss the approach to investigation of molecular machines using systems of integro--differential ultrametric (p-adic) reaction--diffusion equations with drift. This approach combines the features of continuous and discrete dynamic models. We apply this model to investigation of actomyosin molecular motor.   The introduced system of equations is solved analytically using p-adic wavelet theory. We find explicit stationary solutions and behavior in the relaxation regime.
q-bio.BM:Transformation of protein 3D structures from their all-atom representation (AAR) to the double-centroid reduced representation (DCRR) is a prerequisite to the implementation of both the tetrahedral three-dimensional search motif (3D SM) method for predicting specific ligand binding sites (LBS) in proteins, and the 3D interface search motif tetrahedral pair (3D ISMTP) method for predicting binary protein-protein interaction (PPI) partners (Reyes, V.M., 2015a & c, 2015b, 2009a, b & c). Here we describe results demonstrating the efficacy of the set of FORTRAN 77 and 90 source codes used in the transformation from AAR to DCRR and the implementation of the 3D SM and 3D ISMTP methods. Precisely, we show here the construction of the 3D SM for the biologically important ligands, GTP and sialic acid, from a training set composed of experimentally solved structures of proteins complexed with the pertinent ligand, and their subsequent use in the screening for potential receptor proteins of the two ligands. We also show here the construction of the 3D ISMTP for the binary complexes, RAC:P67PHOX and KAP:phospho-CDK2, from a training set composed of the experimentally solved complexes, and their subsequent use in the screening for potential protomers of the two complexes. The 15 FORTRAN programs used in the AAR to DCRR transformation and the implementation of the two said methods are: get_bbn.f, get_sdc.f, res2cm_bbn.f, res2cm_sdc.f, nrst_ngbr.f, find_Hbonds.f, find_VDWints.f, find_clusters.f90, find_trees.f90, find_edgenodes.f90, match_nodes.f, fpBS.f90, Gen_Chain_Separ.f, remove_H_atoms.f and resd_num_reduct.f. Two flowcharts - one showing how to implement the tetrahedral 3D SM method to find LBSs in proteins, and another how to implement the 3D ISMTP method to find binary PPI partners - are presented in our two companion papers (Fig. 2, Reyes, V.M., 2015a, Fig. 1 & 2, Reyes, V.M., 2015c).
q-bio.BM:Ligand burial depth is an indicator of protein flexibility, as the extent of receptor conformational change required to bind a ligand in general varies directly with its depth of burial. In a companion paper (Reyes, V.M. 2015a), we report on the Tangent Sphere (TS) and Cutting Plane (CP) methods -- complementary methods to quantify, independent of protein size, the degree of ligand burial in a protein receptor. In this report, we present results that demonstrate the effectiveness of a set of FORTRAN 77 and 90 source codes used in the implementation of the two related procedures, as well as the precise implementation of the procedures. Particularly, we show here that application of the TS and CP methods on a theoretical model protein in the form of a spherical grid of points accurately portrays the behavior of the TS and CP indices, the predictive parameters obtained from the two methods. We also show that results of the implementation of the TS and CP methods on six protein receptors (Laskowski et al. 1996) are inagreement with their findings regarding cavity sizes in these proteins. The six FORTRAN programs we present here are: find_molec_centr.f, tangent_sphere.f, find_CP_coeffs.f, CPM_Neg_Side.f, CPM_Pos_Side.f and CPM_Zero_Side.f. The first program calculates the x-, y- and z-coordinates of the molecular geometric centroid of the protein (global centroid, GC), the center of the TS. Its radius is the distance between the GC and the local centroid (LC), the centroid of the bound ligand or a portion of its binding site. The second program finds the number of protein atoms inside, outside and on the TS. The third determines the four coefficients A, B, C and D of the equation of the CP, Ax + By + Cz + D = 0. The CP is tangent to the TS at GC. The fourth, fifth and sixth programs determine the number of protein atoms lying on the negative side, positive side, and on the CP.
q-bio.BM:We previously described the representation of protein 3D structures in spherical coordinates (rho, phi, theta) and two of its applications: separation of the outer layer (OL) from the inner core (IC) of proteins, and assessment of protein surface protrusions and invaginations (Reyes, V.M., 2011& 2009). Here we present results demonstrating the performance success of the FORTRAN 77 and 90 programs used in the implementation of the two said applications, and how to implement both applications. In particular, we show here data that demonstrate the success of our OL-IC separation procedure using a subset of the Laskowski et al. (1996) dataset. Using a theoretical model protein in the form of a scalene ellipsoid grid of points with and without an artificially constructed protrusion or invagination, we also show results demonstrating that protrusions and invaginations on the protein surface maybe predicted. The nine programs we present here and their respective functions are: find_molec_centr.f: finds the x-, y- and z-coordinates of the protein molecular geometric centroid, cart2sphere_degrees.f90: converts PDB protein coordinates to spherical, with phi and theta in degrees, cart2sphere_radians.f90: does the same thing as the second program, but with phi and theta in radians, spher2cart_degrees.f90: converts the coordinates from spherical back to PDB, where input phi and theta are in degrees, spher2cart_radians.f90: does the same thing as the fourth program, but with phi and theta in radians, find_rho_cutoff.f: determines the rho cut-off for finding the boundary between OL and IC, phi6_theta8_binning.f90: performs the binning of phi in six- and theta in eight-degree increments, phi10_theta10_binning.f90: performs the binning of phi and theta both in ten-degree increments, and bin_rho.f90: performs the binning of rho values for plotting the frequency distribution of maximum rho values.
q-bio.BM:Crystallization is a key step in macromolecular structure determination by crystallography. While a robust theoretical treatment of the process is available, due to the complexity of the system, the experimental process is still largely one of trial and error. In this article, efforts in the field are discussed together with a theoretical underpinning using a solubility phase diagram. Prior knowledge has been used to develop tools that computationally predict the crystallization outcome and define mutational approaches that enhance the likelihood of crystallization. For the most part these tools are based on binary outcomes (crystal or no crystal), and the full information contained in an assembly of crystallization screening experiments is lost. The potential of this additional information is illustrated by examples where new biological knowledge can be obtained and where a target can be sub-categorized to predict which class of reagents provides the crystallization driving force. Computational analysis of crystallization requires complete and correctly formatted data. While massive crystallization screening efforts are under way, the data available from many of these studies are sparse. The potential for this data and the steps needed to realize this potential are discussed.
q-bio.BM:Wide-angle x-ray scattering (WAXS) is emerging as a powerful tool for increasing the resolution of solution structure measurements of biomolecules. Compared to its better known complement, small angle x-ray scattering (SAXS), WAXS targets higher scattering angles and can enhance structural studies of molecules by accessing finer details of solution structures. Although the extension from SAXS to WAXS is easy to implement experimentally, the computational tools required to fully harness the power of WAXS are still under development. Currently, WAXS is employed to study structural changes and ligand binding in proteins; however the methods are not as fully developed for nucleic acids. Here, we show how WAXS can qualitatively characterize nucleic acid structures as well as the small but significant structural changes driven by the addition of multivalent ions. We show the potential of WAXS to test all-atom molecular dynamics (MD) simulations and to provide insight in understanding how the trivalent ion cobalt(III) hexammine (CoHex) affects the structure of RNA and DNA helices. We find that MD simulations capture the RNA structural change that occurs due to addition of CoHex.
q-bio.BM:Multivariate regression is a widespread computational technique that may give meaningless results if the explanatory variables are too numerous or highly collinear. Tikhonov regularization, or ridge regression, is a popular approach to address this issue. We reveal here a formal analogy between ridge regression and statistical mechanics, where the objective function is comparable to a free energy and the ridge parameter plays the role of temperature. This analogy suggests two new criteria to select a suitable ridge parameter: the specific-heat (Cv) and the maximum penalty (MP) fits. We apply these methods to the calibration of the force constant in elastic network models (ENM). This key parameter determines the amplitude of the predicted atomic fluctuations, and is commonly obtained by fitting crystallographic B-factors. However, rigid-body motions are often partially neglected in such fits, even though their importance has been repeatedly stressed. Considering the full set of rigid-body and internal degrees of freedom bears significant risks of overfitting, due to strong correlations between explanatory variables, and requires thus careful regularization. Using simulated data, we show that ridge regression with the Cv or MP criterion markedly reduces the error of the estimated force constant, its across-protein variation, and the number of proteins with unphysical values of the fit parameters, in comparison with popular regularization schemes such as generalized cross-validation. When applied to protein crystals, the new methods provide a more robust calibration of ENM force constants, even though rigid-body motions account on average for more than 80% of the amplitude of B-factors. While MP emerges as the optimal choice for fitting crystallographic B-factors, the Cv fit is more robust to the nature of the data, and is thus an interesting candidate for other applications.
q-bio.BM:In this paper, we systematically review weighted persistent homology (WPH) models and their applications in biomolecular data analysis. Essentially, the weight value, which reflects physical, chemical and biological properties, can be assigned to vertices (atom centers), edges (bonds), or higher order simplexes (cluster of atoms), depending on the biomolecular structure, function, and dynamics properties. Further, we propose the first localized weighted persistent homology (LWPH). Inspired by the great success of element specific persistent homology (ESPH), we do not treat biomolecules as an inseparable system like all previous weighted models, instead we decompose them into a series of local domains, which may be overlapped with each other. The general persistent homology or weighted persistent homology analysis is then applied on each of these local domains. In this way, functional properties, that are embedded in local structures, can be revealed. Our model has been applied to systematically studying DNA structures. It has been found that our LWPH based features can be used to successfully discriminate the A-, B-, and Z-types of DNA. More importantly, our LWPH based PCA model can identify two configurational states of DNA structure in ion liquid environment, which can be revealed only by the complicated helical coordinate system. The great consistence with the helical-coordinate model demonstrates that our model captures local structure variations so well that it is comparable with geometric models. Moreover, geometric measurements are usually defined in very local regions. For instance, the helical-coordinate system is limited to one or two basepairs. However, our LWPH can quantitatively characterize structure information in local regions or domains with arbitrary sizes and shapes, where traditional geometrical measurements fail.
q-bio.BM:Several technological limitations of traditional silicon based computing are leading towards the paradigm shift, from silicon to carbon, in computational world. Among the unconventional modes of computing evolved in past several decades, DNA computing has been considered to be quite promising in solving computational and reasoning problems by using DNA strands. Along with the sequential operations, the huge parallelism of DNA computing methodologies engaging numerous numbers of DNA strands induce the consideration of concurrent high-level formalisms. In this paper we have reviewed the algebraic explanation of concurrent DNA processes using DNA strand algebra, process calculus and DNA strand graph. We have demonstrated the application of syntax and semantics of the illustrated methodologies in the domains of reasoning and theorem proving with resolution refutation. Finally, we have presented DNA cryptography as one of the prominent areas for the future scope of research work where DNA strand algebra can be used as formal modelling tool to authenticate the security, logic and reasoning of the existing protocols.
q-bio.BM:The primary building block of the body is collagen, which is found in the extracellular matrix and in many stress-bearing tissues such as tendon and cartilage. It provides elasticity and support to cells and tissues while influencing biological pathways including cell signaling, motility and differentiation. Collagen's unique triple helical structure is thought to impart mechanical stability. However, detailed experimental studies on its molecular mechanics have been only recently emerging. Here, we review the treatment of the triple helix as a homogeneous flexible rod, including bend (standard worm-like chain model), twist, and stretch deformations, and the assumption of backbone linearity. Additionally, we discuss protein-specific properties of the triple helix including sequence dependence, and relate single-molecule mechanics to collagen's physiological context.
q-bio.BM:AGel amyloidosis, formerly known as familial amyloidosis of the Finnish-type, is caused by pathological aggregation of proteolytic fragments of plasma gelsolin. So far, four mutations in the gelsolin gene have been reported as responsible for the disease. Although D187N is the first identified variant and the best characterized, its structure has been hitherto elusive. Exploiting a recently-developed nanobody targeting gelsolin, we were able to stabilize the G2 domain of the D187N protein and obtained, for the first time, its high-resolution crystal structure. In the nanobody-stabilized conformation, the main effect of the D187N substitution is the impairment of the calcium binding capability, leading to a destabilization of the C-terminal tail of G2. However, molecular dynamics simulations show that in the absence of the nanobody, D187N-mutated G2 further misfolds, ultimately exposing its hydrophobic core and the furin cleavage site. The nanobody's protective effect is based on the enhancement of the thermodynamic stability of different G2 mutants (D187N, G167R and N184K). In particular, the nanobody reduces the flexibility of dynamic stretches, and most notably decreases the conformational entropy of the C-terminal tail, otherwise stabilized by the presence of the Ca2+ ion. A Caenorhabditis elegans-based assay was also applied to quantify the proteotoxic potential of the mutants and determine whether nanobody stabilization translates into a biologically relevant effect. Successful protection from G2 toxicity in vivo points to the use of C. elegans as a tool for investigating the mechanisms underlying AGel amyloidosis and rapidly screen new therapeutics.
q-bio.BM:To understand the effects of fluctuations on achieving homochirality, we employ a Monte-Carlo method where autocatalysis and enantiomeric cross-inhibition, as well as racemization and deracemization reactions are included. The results of earlier work either without autocatalysis or without cross-inhibition are reproduced. Bifurcation diagrams and the dependencies of the number of reaction steps on parameters are studied. In systems with 30,000 molecules, for example, up to a billion reaction steps may be needed to achieve homochirality without autocatalysis.
q-bio.BM:Although non-canonical residues, caps, crosslinks, and nicks play an important role in the function of many DNA, RNA, proteins, and complexes, we do not fully understand how networks of non-canonical macromolecules generate behavior. One barrier is our limited formats, such as IUPAC, for abstractly describing macromolecules. To overcome this barrier, we developed BpForms and BcForms, a toolkit of ontologies, grammars, and software for abstracting the primary structure of polymers and complexes as combinations of residues, caps, crosslinks, and nicks. The toolkit can help quality control, exchange, and integrate information about the primary structure of macromolecules into fine-grained global networks of intracellular biochemistry.
q-bio.BM:Accumulation of plastic waste is a major environmental problem. Enzymes, particularly esterases, play an important role in the biodegradation of polyesters. These enzymes are usually only active on aliphatic polyesters, but a few have showed catalytic activity for semi-aromatic polyesters. Due to the importance of these processes, an atomic level characterization of how common polyesters are degraded by esterases is necessary. Hereby, we present a Molecular dynamics (MD) and Quantum Mechanics/Molecular Mechanics (QM/MM) MD study of the hydrolysis of a model of polycaprolactone (PCL), one of the most widely used biomaterials, by the thermophilic esterase from the archaeon Archaeoglobus fulgidus (AfEST). This enzyme is particularly interesting because it can withstand temperatures well above the glass transition of many polyesters. Our insights about the reaction mechanism are important for the design of customized enzymes able to degrade different synthetic polyesters.
q-bio.BM:Neopetrosia exigua has received great attention in natural product chemistry. The diversity of N. exigua constituents has been demonstrated by the continued discovery of novel bioactive metabolites such as antimicrobial metabolites. In this study, in order to localise the active component of N. exigua biomass according to the polarity, a sequential gradient partition with different solvents (nhexane, carbon tetrachloride, dichloromethane, n-butanol, and water) was performed to obtain fractions containing metabolites distributed according to their polarity. The antimicrobial activities of N. exigua fractions were then evaluated using disc diffusion and microdilution methods (influence on the growth curve, Minimum Inhibitory Concentration (MIC) and Minimum Bactericidal Concentration (MBC)). The results showed that the active metabolites were present in n-hexane, CH2Cl2, nBuOH, and water fractions. n-hexane, CH2Cl2, and n-BuOH fractions were the most effective fractions. Among microbes tested, Staphylococcus aureus was the most susceptible microbe evaluated. The obtained results are considered sufficient for further study to isolate the compounds represent the antimicrobial activity.
q-bio.BM:Transport of ions and small molecules across the cell membrane against electrochemical gradients is catalyzed by integral membrane proteins that use a source of free energy to drive the energetically uphill flux of the transported substrate. Secondary active transporters couple the spontaneous influx of a "driving" ion such as Na$^+$ or H$^+$ to the flux of the substrate. The thermodynamics of such cyclical non-equilibrium systems are well understood and recent work has focused on the molecular mechanism of secondary active transport. The fact that these transporters change their conformation between an inward-facing and outward-facing conformation in a cyclical fashion, called the alternating access model, is broadly recognized as the molecular framework in which to describe transporter function. High resolution structures and detailed computer simulations lead to the recognition of common molecular-level principles between disparate transporter families. Inverted repeat symmetry in secondary active transporters has shed light on how protein structures can encode a bi-stable two-state system. Three broad classes of alternating access transitions have been described as rocker-switch, rocking-bundle, and elevator mechanisms. Transporters can be understood as gated pores with at least two coupled gates that map to distinct parts of the transporter protein. Enumerating all distinct gate states naturally includes occluded states in the alternating access picture and suggests observable protein conformations. By connecting the possible conformational states and ion/substrate bound states in a kinetic model, a unified picture emerges in which symporter, antiporter, and uniporter function are extremes in a continuum of functionality. We briefly discuss how biological complexity may be integrated in quantitative kinetic models to provide a bridge from structure to function.
q-bio.BM:Predicting protein structure from the amino acid sequence has been a challenge with theoretical and practical significance in biophysics. Despite the recent progresses elicited by improved residue-residue contact prediction, contact-based structure prediction has gradually reached the performance ceiling. New methods have been proposed to predict the residue-residue distance, but unanimously by simplifying the real-valued distance prediction into a multiclass classification problem. Here we show a regression-based distance prediction method, which adopts the generative adversarial network to capture the delicate geometric relationship between residue pairs and thus could predict the continuous, real-valued residue-residue distance satisfactorily. The predicted residue distance map allows rapid structure modeling by the CNS suite, and the constructed models approach at least the same level of quality as the other state-of-the-art protein structure prediction methods when tested on available CASP13 targets. Moreover, this method can be used directly for the structure prediction of membrane proteins without transfer learning.
q-bio.BM:Phosphate groups are naturally present in starch at C3- or C6-position of the glucose residues and impact the structure of starch granules. Their precise quantification is necessary for understanding starch physicochemical properties and metabolism. Nevertheless, reliable quantification of Glc-3-P remains laborious and time consuming. Here we describe a capillary electrophoresis method for simultaneous measurement of both Glc-6-P and Glc-3-P after acid hydrolysis of starch. The sensitivity threshold was estimated at the fg scale, which is compatible with the analysis of less than a $\mu$g of sample. The method was validated by analyzing antisense potato lines deficient in SBEs, GWD or GBSS. We show that Glc-3-P content is altered in the latter and that these variations do not correlate with modifications in Glc-6-P content. We anticipate the method reported here to be an efficient tool for high throughput study of starch phosphorylation at both C3- and C6-position.
q-bio.BM:The DNA double helix is a polyanionic macromolecule that in water solutions is neutralized by metal ions (counterions). The property of the counterions to stabilize the water network (positive hydration) or to make it friable (negative hydration) is important in terms of the physical mechanisms of stabilization of the DNA double helix. In the present research, the effects of positive hydration of Na$^{+}$ counterions and negative hydration of K$^{+}$ and Cs$^{+}$ counterions, incorporated into the hydration shell of the DNA double helix have been studied using molecular dynamics simulations. The results have shown that the dynamics of the hydration shell of counterions depends on region of the double helix: minor groove, major groove, and outside the macromolecule. The longest average residence time has been observed for water molecules contacting with the counterions, localized in the minor groove of the double helix (about 50 ps for Na$^{+}$, and lower than 10 ps for K$^{+}$ and Cs$^{+}$). The estimated potentials of mean force for the hydration shells of the counterions show that the water molecules are constrained too strong, and, consequently, the effect of negative hydration for K$^{+}$ and Cs$^{+}$ counterions has not been observed in the simulations. The analysis has shown that the effects of counterion hydration can be described more accurately with water models having lower dipole moments.
q-bio.BM:Icosahedral viruses have their infectious genome encapsulated by a shell assembled by a multiscale process, starting from an integer multiple of 60 viral capsid or coat protein (VP) monomers.   We predict and validate inter-atomic hotspot interactions between VP monomers that are important for the assembly of 3 icosahedral viral capsids: Adeno Associated Virus serotype 2 (AAV2) and Minute Virus of Mice (MVM), both T=1 single stranded DNA viruses, and Bromo Mosaic Virus (BMV), a T=3 single stranded RNA virus. Experimental validation is by in-vitro, site-directed mutagenesis data found in literature.   We combine ab-initio predictions at two scales: at the interface-scale, we predict the importance (cruciality) of an interaction for successful subassembly across each interface between VP monomers; and at the capsid-scale, we predict the cruciality of an interface for successful capsid assembly.   At the interface-scale, we measure cruciality by changes in the capsid free-energy landscape partition function when an interaction is removed. The partition function computation uses atlases of interface subassembly landscapes, rapidly generated by a novel geometric method and curated opensource software EASAL (efficient atlasing and search of assembly landscapes). At the capsid-scale, cruciality of an interface for successful assembly of the capsid is based on combinatorial entropy.   Our study goes from resource-light, multiscale computational predictions of crucial hotspot inter-atomic interactions to validation using data on site-directed mutagenesis' effect on capsid assembly. By reliably and rapidly narrowing down target interactions, (no more than 1.5 hours per interface on a laptop with Intel Core i5-2500K 3.2Ghz CPU and 8GB of RAM) our predictions can inform and reduce time-consuming in-vitro and in-vivo experiments, or more computationally intensive in-silico analyses.
q-bio.BM:Hydrogenase occupy a central place in the energy metabolism of anaerobic bacteria. Although the structure of mitochondrial complex I is similar to that of hydrogenase, whether it has hydrogen metabolic activity remain unclear. Here, we show that a H2 evolving activity exists in higher plants mitochondria and is closely related to complex I, especially around ubiquinone binding site. The H2 production could be inhibited by rotenone and ubiquinone. Hypoxia could simultaneously promote H2 evolution and succinate accumulation. Redox properties of quinone pool, adjusted by NADH or succinate according to oxygen concentration, acts as a valve to control the flow of protons and electrons and the production of H2. The coupling of H2 evolving activity of mitochondrial complex I with metabolic regulation reveals a more effective redox homeostasis regulation mechanism. Considering the ubiquity of mitochondria in eukaryotes, H2 metabolism might be the innate function of higher organisms. This may serve to explain, at least in part, the broad physiological effects of H2.
q-bio.BM:Traditional approaches to elucidation of protein structures by NMR spectroscopy rely on distance restraints also know as nuclear Overhauser effects (NOEs). The use of NOEs as the primary source of structure determination by NMR spectroscopy is time consuming and expensive. Residual Dipolar Couplings (RDCs) have become an alternate approach for structure calculation by NMR spectroscopy. In this work we report our results for structure calculation of the novel protein PF2048.1 from RDC data and establish the minimum data requirement for successful structure calculation using the software package REDCRAFT. Our investigations start with utilizing four sets of synthetic RDC data in two alignment media and proceed by reducing the RDC data to the final limit of {CN, NH} and {NH} from two alignment media respectively. Our results indicate that structure elucidation of this protein is possible with as little as {CN, NH} and {NH} to within 0.533{\AA} of the target structure.
q-bio.BM:In this paper, we investigate potential biases in datasets used to make drug binding predictions using machine learning. We investigate a recently published metric called the Asymmetric Validation Embedding (AVE) bias which is used to quantify this bias and detect overfitting. We compare it to a slightly revised version and introduce a new weighted metric. We find that the new metrics allow to quantify overfitting while not overly limiting training data and produce models with greater predictive value.
q-bio.BM:Intrinsically disordered proteins (IDPs) constitute a broad set of proteins with few uniting and many diverging properties. IDPs-and intrinsically disordered regions (IDRs) interspersed between folded domains-are generally characterized as having no persistent tertiary structure; instead they interconvert between a large number of different and often expanded structures. IDPs and IDRs are involved in an enormously wide range of biological functions and reveal novel mechanisms of interactions, and while they defy the common structure-function paradigm of folded proteins, their structural preferences and dynamics are important for their function. We here discuss open questions in the field of IDPs and IDRs, focusing on areas where machine learning and other computational methods play a role. We discuss computational methods aimed to predict transiently formed local and long-range structure, including methods for integrative structural biology. We discuss the many different ways in which IDPs and IDRs can bind to other molecules, both via short linear motifs, as well as in the formation of larger dynamic complexes such as biomolecular condensates. We discuss how experiments are providing insight into such complexes and may enable more accurate predictions. Finally, we discuss the role of IDPs in disease and how new methods are needed to interpret the mechanistic effects of genomic variants in IDPs.
q-bio.BM:Health benefits of EGCG are well established; however, the mechanisms of EGCG action are not completely understood. In our previous study we discovered solvatochromism of EGCG fluorescence and described the dependence of fluorescence maxima on solvent polarity. We also noted that fluorescence intensity (FI) depends on solvent. The goal of this study was to gain insights into how the protonic properties of the environment affect FI. We demonstrate that 1) FI of EGCG inversely correlates with the autoprotolysis constant (Kap) of the solvents, 2) HCl decreases, and NaOH increases FI of EGCG in water, and 3) NaOH evokes slow (~10 min time scale) transient changes in FI of EGCG. We conclude that EGCG fluorescence depends on Kap, i.e. protonicity of the environment, that is useful for differentiating EGCG in protic aqueous environment at physiological pH~7.0{\div}7.4, where EGCG fluorescence is substantially quenched, from EGCG in aprotic environments, where EGCG fluorescence significantly increases. Thus, this property of EGCG fluorescence is useful for investigation of specific EGCG interactions within more aprotic protein binding sites.
q-bio.BM:Since the epidemic began in November 2019, no viable medicine against SARS-CoV-2 has been discovered. The typical medication discovery strategy requires several years of rigorous research and development as well as a significant financial commitment, which is not feasible in the face of the current epidemic. Through molecular docking and dynamic simulation studies, we used the FDA-approved drug mezonavir against the most important viral targets, including spike (S) glycoprotein, Transmembrane serine protease 2 (TMPRSS2), RNA-dependent RNA polymerase (RdRp), Main protease (Mpro), human angiotensin-converting enzyme 2 (ACE-2), and furin. These targets are critical for viral replication and infection propagation because they play a key role in replication/transcription and host cell recognition. Molecular docking revealed that the antiviral medication mozenavir showed a stronger affinity for SARS-CoV-2 target proteins than reference medicines in this investigation. We discovered that mozenavir increases the complex's stability and validates the molecular docking findings using molecular dynamics modelling. Furin, a target protein of COVID-19, has a greater binding affinity (-12.04 kcal/mol) than other COVID-19 target proteins, forming different hydrogen bonds and polar and hydrophobic interactions, suggesting that it might be used as an antiviral treatment against SARS-CoV-2. Overall, the present in silico results will be valuable in identifying crucial targets for subsequent experimental investigations that might help combat COVID-19 by blocking the protease furin's proteolytic activity.
q-bio.BM:Aldosterone, the main physiological mineralocorticoid in humans and other terrestrial vertebrates, first appears in lungfish, which are lobe-finned fish that are forerunners of terrestrial vertebrates. Aldosterone activation of the MR regulates internal homeostasis of water, sodium and potassium, which was critical in the conquest of land by vertebrates. We studied transcriptional activation of the slender African lungfish MR by aldosterone, other corticosteroids and progesterone and find that aldosterone, 11-deoxycorticosterone, 11-deoxycortisol and progesterone have half-maximal responses (EC50s) below 1 nM and are potential physiological mineralocorticoids. In contrast, EC50s for corticosterone and cortisol were 23 nM and 66 nM, respectively. Unexpectedly, truncated lungfish MR, consisting of the DNA-binding, hinge and steroid-binding domains, had a stronger response to corticosteroids and progesterone than full-length lungfish MR, indicating that the N-terminal domain represses steroid activation of lungfish MR, unlike human MR in which the N-terminal domain contains an activation function. BLAST searches of GenBank did not retrieve a GR ortholog, leading us to test dexamethasone and triamcinolone for activation of lungfish MR. At 10 nM, both synthetic glucocorticoids are about 4-fold stronger than 10 nM aldosterone in activating full-length lungfish MR, leading us to propose that lungfish MR also functions as a GR.
q-bio.BM:Relative lifetimes of inherent double stranded DNA openings with lengths up to ten base pairs are presented for different gene promoters and corresponding mutants that either increase or decrease transcriptional activity, in the framework of the Peyrard-Bishop-Dauxois model. Extensive microcanonical simulations are used, with energies corresponding to physiological temperature. The bubble lifetime profiles along the DNA sequences demonstrate a significant reduction of the average lifetime at the mutation sites when the mutated promoter decreases transcription, while a corresponding enhancement of the bubble lifetime is observed in the case of mutations leading to increased transcription. The relative difference of bubble lifetimes between the mutated and the wild type promoters at the position of mutation varies from 20% to more than 30% as the bubble length is decreasing
q-bio.BM:Increased Atmospheric CO2 to over 400 ppm has prompted global climate irregularities. Reducing the released CO2 from biotechnological processes could remediate these phenomena. In this study, we sought to find a solution to reduce the amount of CO2 in the process of growth and reproduction by preventing the conversion of formic acid into CO2. The (bio)chemical conversion of formic acid to CO2 is a key reaction. Therefore, we compared the growth of BL21, being a subfamily of K12, alongside two strains in which two different genes related to the formate metabolism were deleted, in complex and simple media. Experimental results were entirely consistent with metabolic predictions. Subsequently, the knockout bacteria grew more efficiently than BL21. Interestingly, TsFDH, a formate dehydrogenase with the tendency of converting CO2 to formate, increased the growth of all strains compared with cells without the TsFDH. Most mutants grew in a simple medium containing glycerol, which showed that glycerol is the preferred carbon source compared to glucose for the growth of E. coli. These results explain the reasons for the inconsistency of predictions in previous metabolic models that declared glycerol as a suitable carbon source for the growth of E. coli but failed to achieve it in practice. To conduct a more mechanistic evaluation of our observations, RNA sequencing data analysis was conducted on an E. coli RNA-seq dataset. The gene expression correlation outcome revealed the increased expression levels of several genes related to protein biosynthesis and glycerol degradation as a possible explanation of our observations.
q-bio.BM:This paper proposes an iterative algorithm for choosing gradient directions use to reconstruct white matter fibers in the brain. The present study is not focusing on data acquisition where scanning is performed. The Adaptive Gradient Directions (AGD) approach is extended to refine the position and area of the grid, resulting in an admissible reduction in angular error. We begin with the gradient directions distributed uniformly inside a grid of bigger size and with larger spacing between the points. Both (size of the grid and spacing between the points) reduce iteratively. The proposed algorithm ensures that the actual position of fiber comes inside the grid at each iteration, unlike as in the AGD approach. As a result, the solution tends to actual orientation in each iteration followed by better estimation of fibers. The proposed algorithm is validated by associating it with mixture of Gaussian diffusion and mixture of non-central Wishart distribution models. The proposed approach significantly reduce the angular error for multiple computer-generated experiments on synthetic simulations and real data. Moreover, we have also performed simulations with fibers not residing in the XY-plane. For this set-up also, the proposed work outperforms, giving lesser angular error with both the models. Synthetic simulations have been performed with Rician distributed (R-D) noise of standard deviation ranging from 0.02-0.1. This work helps in better understanding of the anatomy of the brain using the MRI signal data.
physics.optics:A numerical study of the properties of Gaussian pulses propagating in planar waveguide under the combined effect of positive Kerr-type nonlinearity, diffraction in planar waveguides and anomalous or normal dispersion, is presented. It is demonstrated how the relative strength of dispersion and diffraction, the strength of nonlinearity and the initial spatial and temporal pulse chirps effect on the parameters of pulse compression, such as the maximal compression factor and the distance to the point of maximal compression.
physics.optics:We report on a theoretical and numerical investigation of the switching of power in new hybrid models of nonlinear coherent couplers consisting of optical slab waveguides with various orders of nonlinearity. The first model consists of two guides with second-order instead of the usual third-order susceptibilities as typified by the Jensen coupler. This second-order system is shown to have a power self-trapping transition at a critical power greater than the third-order susceptibility coupler. Next, we consider a mixed coupler composed of a second-order guide coupled to a third-order guide and show that, although it does not display a rigorous self-trapping transition, for a particular choice of parameters it does show a fairly abrupt trapping of power at a lower power than in the third-order coupler. By coupling this mixed nonlinear pair to a third, purely linear guide, the power trapping can be brought to even lower levels and in this way a satisfactory switching profile can be achieved at less than one sixth the input power needed in the Jensen coupler.
physics.optics:It is noted that the Jones-matrix formalism for polarization optics is a six-parameter two-by-two representation of the Lorentz group. It is shown that the four independent Stokes parameters form a Minkowskian four-vector, just like the energy-momentum four-vector in special relativity. The optical filters are represented by four-by-four Lorentz-transformation matrices. This four-by-four formalism can deal with partial coherence described by the Stokes parameters. A four-by-four matrix formulation is given for decoherence effects on the Stokes parameters, and a possible experiment is proposed. It is shown also that this Lorentz-group formalism leads to optical filters with a symmetry property corresponding to that of two-dimensional Euclidean transformations.
physics.optics:We study the electromagnetic scattering by multilayered biperiodic aggregates of dielectric layers and gratings of conducting plates. We show that the characteristic lengths of such structures provide a good control of absorption bands. The influence of the physical parameters of the problem (sizes, impedances) is discussed.
physics.optics:The propagation of an electromagnetic pulse in a plasma is studied for pulse durations that are comparable to the plasma period. When the carrier frequency of the incident pulse is much higher than the plasma frequency, the pulse propagates without distortion at its group speed. When the carrier frequency is comparable to the plasma frequency, the pulse is distorted and leaves behind it an electromagnetic wake.
physics.optics:We present scattering from many body systems in a new light. In place of the usual van Hove treatment, (applicable to a wide range of scattering processes using both photons and massive particles) based on plane waves, we calculate the scattering amplitude as a space-time integral over the scattering sample for an incident wave characterized by its correlation function which results from the shaping of the wave field by the apparatus. Instrument resolution effects - seen as due to the loss of correlation caused by the path differences in the different arms of the instrument are automatically included and analytic forms of the resolution function for different instruments are obtained. The intersection of the moving correlation volumes (those regions where the correlation functions are significant) associated with the different elements of the apparatus determines the maximum correlation lengths (times) that can be observed in a sample, and hence, the momentum (energy) resolution of the measurement. This geometrical picture of moving correlation volumes derived by our technique shows how the interaction of the scatterer with the wave field shaped by the apparatus proceeds in space and time. Matching of the correlation volumes so as to maximize the intersection region yields a transparent, graphical method of instrument design. PACS: 03.65.Nk, 3.80 +r, 03.75, 61.12.B
physics.optics:In this paper we extend for the case of Maxwell equations the "X-shaped" solutions previously found in the case of scalar (e.g., acoustic) wave equations. Such solutions are localized in theory, i.e., diffraction-free and particle-like (wavelets), in that they maintain their shape as they propagate. In the electromagnetic case they are particularly interesting, since they are expected to be Superluminal. We address also the problem of their practical, approximate production by finite (dynamic) radiators. Finally, we discuss the appearance of the X-shaped solutions from the purely geometric point of view of the Special Relativity theory.   [PACS nos.: 03.50.De; 1.20.Jb; 03.30.+p; 03.40.Kf; 14.80.-j.   Keywords: X-shaped waves; localized solutions to Maxwell equations; Superluminal waves; Bessel beams; Limited-dispersion beams; electromagnetic wavelets; Special Relativity; Extended Relativity].
physics.optics:This paper has been withdrawn by the authors until some changes are made.
physics.optics:The effect of dispersion or diffraction on zero-velocity solitons is studied for the generalized massive Thirring model describing a nonlinear optical fiber with grating or parallel-coupled planar waveguides with misaligned axes. The Thirring solitons existing at zero dispersion/diffraction are shown numerically to be separated by a finite gap from three isolated soliton branches. Inside the gap, there is an infinity of multi-soliton branches. Thus, the Thirring solitons are structurally unstable. In another parameter region (far from the Thirring limit), solitons exist everywhere.
physics.optics:We theoretically study reflection of light by a phase-conjugating mirror preceded by a partially reflecting normal mirror. The presence of a suitably chosen normal mirror in front of the phase conjugator is found to greatly enhance the total phase-conjugate reflected power, even up to an order of magnitude. Required conditions are that the phase-conjugating mirror itself amplifies upon reflection and that constructive interference of light in the region between the mirrors takes place. We show that the phase-conjugate reflected power then exhibits a maximum as a function of the transmittance of the normal mirror.
physics.optics:Reliable control of the deposition process of optical films and coatings frequently requires monitoring of the refractive index profile throughout the layer. In the present work a simple in situ approach is proposed which uses a WKBJ matrix representation of the optical transfer function of a single thin film on a substrate. Mathematical expressions are developed which represent the minima and maxima envelopes of the curves transmittance-vs-time and reflectance-vs-time. The refractive index and extinction coefficient depth profiles of different films are calculated from simulated spectra as well as from experimental data obtained during PECVD of silicon-compound films. Variation of the deposition rate with time is also evaluated from the position of the spectra extrema as a function of time. The physical and mathematical limitations of the method are discussed.
physics.optics:A new definition for the electromagnetic field velocity is proposed. The velocity depends on the physical fields.
physics.optics:We have fabricated light emitting diodes (LEDs) with Schottky contacts on Si-nanocrystals formed by simple techniques as used for standard Si devices. Orange electroluminescence (EL) from these LEDs could be seen with the naked eye at room temperature when a reverse bias voltage was applied. The EL spectrum has a major peak with a photon energy of 1.9 eV and a minor peak with a photon energy of 2.2 eV. Since the electrons and holes are injected into the radiative recombination centers related to nanocrystals through avalanche breakdown, the voltage needed for a visible light emission is reduced to 4.0 - 4.5 V, which is low enough to be applied by a standard Si transistor.
physics.optics:A general model is presented for coupling of high-$Q$ whispering-gallery modes in optical microsphere resonators with coupler devices possessing discrete and continuous spectrum of propagating modes. By contrast to conventional high-Q optical cavities, in microspheres independence of high intrinsic quality-factor and controllable parameters of coupling via evanescent field offer variety of regimes earlier available in RF devices. The theory is applied to the earlier-reported data on different types of couplers to microsphere resonators and complemented by experimental demonstration of enhanced coupling efficiency (about 80%) and variable loading regimes with Q>10^8 fused silica microspheres.
physics.optics:The mechanism of DC-Electric-Field-Induced Second-Harmonic (EFISH) generation at weakly nonlinear buried Si(001)-SiO$_2$ interfaces is studied experimentally in planar Si(001)-SiO$_2$-Cr MOS structures by optical second-harmonic generation (SHG) spectroscopy with a tunable Ti:sapphire femtosecond laser. The spectral dependence of the EFISH contribution near the direct two-photon $E_1$ transition of silicon is extracted. A systematic phenomenological model of the EFISH phenomenon, including a detailed description of the space charge region (SCR) at the semiconductor-dielectric interface in accumulation, depletion, and inversion regimes, has been developed. The influence of surface quantization effects, interface states, charge traps in the oxide layer, doping concentration and oxide thickness on nonlocal screening of the DC-electric field and on breaking of inversion symmetry in the SCR is considered. The model describes EFISH generation in the SCR using a Green function formalism which takes into account all retardation and absorption effects of the fundamental and second harmonic (SH) waves, optical interference between field-dependent and field-independent contributions to the SH field and multiple reflection interference in the SiO$_2$ layer. Good agreement between the phenomenological model and our recent and new EFISH spectroscopic results is demonstrated. Finally, low-frequency electromodulated EFISH is demonstrated as a useful differential spectroscopic technique for studies of the Si-SiO$_2$ interface in silicon-based MOS structures.
physics.optics:The new mechanism for obtaining a nonlinear phase shift has been proposed and the schemes are described for its implementation. As it is shown, the interference of two waves with intensity-dependent amplitude ratio coming from the second harmonic generation should produce the nonlinear phase shift. The sign and amount of nonlinear distortion of a beam wavefront is dependent of the relative phase of the waves that is introduced by the phase element. Calculated value of $n_2^{eff}$ exceeds that connected with cascaded quadratic nonlinearity, at the same conditions.
physics.optics:We analyze the guiding problem in a realistic photonic crystal fiber using a novel full-vector modal technique, a biorthogonal modal method based on the nonselfadjoint character of the electromagnetic propagation in a fiber. Dispersion curves of guided modes for different fiber structural parameters are calculated along with the 2D transverse intensity distribution of the fundamental mode. Our results match those achieved in recent experiments, where the feasibility of this type of fiber was shown.
physics.optics:A new method for investigation of x-ray propagation in a rough narrow dielectric waveguide is proposed on the basis of the numerical integration of the quazioptical equation. In calculations a model rough surface was used with the given statistical properties. It was shown that the losses in the narrow waveguides strongly depend on the wall roughness and on the input angle. The losses are not zero even at zero input angle if the width of the waveguide is smaller or about 1 mkm. The effect is accounted for as the influence of diffraction. The angular spread of the transmitted X-ray radiation is much more narrow than the Fresnel angle of the total external reflection.
physics.optics:We study a generalized notion of two-mode squeezing for the Stokes and anti-Stokes fields in a model of a cavity Raman laser, which leads to a significant reduction in decoherence or quantum noise. The model comprises a loss-less cavity with classical pump, unsaturated medium and arbitrary homogeneous broadening and dispersion. Allowing for arbitrary linear combinations of the two modes in the definition of quadrature variables, we find that there always exists a combination of the two output modes which exhibits quadrature squeezing with noise reduction below the vacuum level. The number of noise photons for this combination mode is proportional to the square root of the number of Stokes noise photons.
physics.optics:We study the effects of higher order transversal modes in a model of a singly-resonant OPO, using both numerical solutions and mode expansions including up to two radial modes. The numerical and two-mode solutions predict lower threshold and higher conversion than the single-mode solution at negative dispersion. Relative power in the zero order radial mode ranges from about 88% at positive and small negative dispersion to 48% at larger negative dispersion, with most of the higher mode content in the first mode, and less than 2% in higher modes.
physics.optics:This paper presents a detailed numerical study of the effect of focusing on the conversion efficiency of low-loss singly-resonant parametric oscillators with collinear focusing of pump and signal. Results are given for the maximal pump depletion and for pump power levels required for various amounts of depletion, as functions of pump and signal confocal parameters, for kI/kP=0.33 and 0.50. It is found that the ratio of pump depletion to maximal depletion as a function of the ratio of pump power to threshold power agrees with the plane-wave prediction to within 5%, for a wide range of focusing conditions. The observed trends are explained as resulting from intensity and phase dependent mechanisms.
physics.optics:Via solution of appropriate variational problem it is shown that light beams with Gaussian spatial profile and sufficiently short duration provide maximal destruction of global coherence under nonlinear self-modulation.
physics.optics:The dynamics of Fabry-Perot cavity with suspended mirrors is described. The suspended mirrors are nonlinear oscillators interacting with each other through the laser circulating in the cavity. The degrees of freedom decouple in normal coordinates, which are the position of the center of mass and the length of the cavity. We introduce two parameters and study how the dynamics changes with respect to these parameters. The first parameter specifies how strong the radiation pressure is. It determines whether the cavity is multistable or not. The second parameter is the control parameter, which determines location of the cavity equilibrium states. The equilibrium state shows hysteresis if the control parameter varies within a wide range. We analyze stability of the equilibrium states and identify the instability region. The instability is explained in terms of the effective potential: the stable states correspond to local minima of the effective potential and unstable states correspond to local maxima. The minima of the effective potential defines the resonant frequencies for the oscillations of the cavity length. We find the frequencies, and analyze how to tune them. Multistability of the cavity with a feedback control system is analyzed in terms of the servo potential. The results obtained in this paper are general and apply to all Fabry-Perot cavities with suspended mirrors.
physics.optics:We show that an azimuthally-periodically-modulated bright ring "necklace" beam can self-trap in self-focusing Kerr media and can exhibit stable propagation for very large distances. These are the first bright (2+1) D beams to exhibit stable self-trapping in a system described by the cubic (2+1) D Nonlinear Schrodinger Equation (NLSE).
physics.optics:We present a new class of micro lasers based on nanoporous molecular sieve host-guest systems. Organic dye guest molecules of 1-Ethyl-4-(4-(p-Dimethylaminophenyl)-1,3-butadienyl)-pyridinium Perchlorat were inserted into the 0.73-nm-wide channel pores of a zeolite AlPO$_4$-5 host. The zeolitic micro crystal compounds where hydrothermally synthesized according to a particular host-guest chemical process. The dye molecules are found not only to be aligned along the host channel axis, but to be oriented as well. Single mode laser emission at 687 nm was obtained from a whispering gallery mode oscillating in a 8-$\mu$m-diameter monolithic micro resonator, in which the field is confined by total internal reflection at the natural hexagonal boundaries inside the zeolitic microcrystals.
physics.optics:We report a quantum ring-like toroidal cavity naturally formed in a vertical-cavity-like active microdisk plane due to Rayleigh's band of whispering gallery modes. The $\sqrt{T}$-dependent redshift and a square-law property of microampere-range threshold currents down to 2 $\mu$A are consistent with a photonic quantum wire view, due to whispering gallery mode-induced dimensional reduction.
physics.optics:The effect of capture of X-ray beam into narrow submicron capillary was investigated with account for diffraction and decay of coherency by roughness scattering in transitional boundary layer. In contrast to well-known Andronov-Leontovich approach the losses do not vanish at zero gliding angle and scale proportional to the first power of roughness amplitude for small gliding angles. It was shown that for small correlation radius of roughness the scattering decay of coherency can be made of the same order as absorption decay of lower channeling modes to produce angular collimation of X-ray beams. Estimates were given for optimum capillary length at different roughness amplitudes for angular sensitivity of X-ray transmission and chenneling effects that can be usefull for designing of detector systems.
physics.optics:We report the measurement of the photons flux produced in parametric down-conversion, performed in photon counting regime with actively quenched silicon avalanche photodiodes as single photon detectors. Measurements are done with the detector in a well defined geometrical and spectral situation. By comparison of the experimental data with the theory, a value for the second order susceptibilities of the non linear crystal can be inferred.
physics.optics:In a frame of quasi-crystal approximation the dispersion equations are obtained for the wave vector of a coherent electromagnetic wave propagating in a media which contains a random set of parallel dielectric cylinders with possible overlapping. The results are compared with that for the case when a regularity at the cylinder placement exists.
physics.optics:Accurate calculation of internal and surface scattering losses in fused silica microspheres is done. We show that in microspheres internal scattering is partly inhibited as compared to losses in the bulk material. We pay attention on the effect of frozen thermodynamical capillary waves on surface roughness. We calculate also the value of mode splitting due to backscattering and other effects of this backscattering.
physics.optics:We analytically compute a localization criterion in double scattering approximation for a set of dielectric spheres or perfectly conducting disks uniformly distributed in a spatial volume which can be either spherical or layered. For every disordered medium, we numerically investigate a localization criterion, and examine the influence of the system parameters on the wavelength localization domains.
physics.optics:The use of specific symmetry properties of the optical second-harmonic generation (the s,s-exclusion rule) has allowed us to observe high-contrast hyper-Rayleigh interference patterns in a completely diffuse light - an effect having no analog in case of linear (Rayleigh) scattering.
physics.optics:We report detailed measurements of the pump-current dependency of the self-pulsating frequency of semiconductor CD lasers. A distinct kink in this dependence is found and explained using rate-equation model. The kink denotes a transition between a region where the self-pulsations are weakly sustained relaxation oscillations and a region where Q-switching takes place. Simulations show that spontaneous emission noise plays a crucial role for the cross-over.
physics.optics:A new method is proposed to produce population inversion on transitions involving the ground state of atoms. The method is realized experimentally with sodium atoms. Lasing at the frequency corresponding to the sodium D_2 line is achieved in the presence of pump radiation resonant to the D_1 line with helium as a buffer gas.
physics.optics:A moving dielectric appears to light as an effective gravitational field. At low flow velocities the dielectric acts on light in the same way as a magnetic field acts on a charged matter wave. We develop in detail the geometrical optics of moving dispersionless media. We derive a Hamiltonian and a Lagrangian to describe ray propagation. We elucidate how the gravitational and the magnetic model of light propagation are related to each other. Finally, we study light propagation around a vortex flow. The vortex shows an optical Aharonov--Bohm effect at large distances from the core, and, at shorter ranges, the vortex may resemble an optical black hole.
physics.optics:We report the first observation of a nonlinear mode in a cylindrical nonlinear Fabry-Perot cavity. The field enhancement from cavity buildup, as well as the large chi3 optical nonlinearity due to resonantly-excited Rb-85 vapor, allows the nonlinear mode to form at low incident optical powers of less than a milliwatt. The mode is observed to occur for both the self-focusing and self-defocusing nonlinearity.
physics.optics:Optical second harmonic generation (SHG) is used as a noninvasive probe of two-dimensional (2D) ferroelectricity in Langmuir-Blodgett (LB) films of copolymer vinylidene fluoride with trifluorethylene. The surface 2D ferroelectric-paraelectric phase transition in the topmost layer of LB films and a thickness independent (almost 2D) transition in the bulk of these films are observed in temperature studies of SHG.
physics.optics:An all optical set-reset flip flop is presented that is based on two coupled identical laser diodes. The lasers are coupled so that when one of the lasers lases it quenches lasing in the other laser. The state of the flip flop is determined by which laser is currently lasing. Rate equations are used to model the flip flop and obtain steady state characteristics. The flip flop is experimentally demonstrated by use of antireflection coated laser diodes and free space optics.
physics.optics:We report on the fabrication of what we believe is the first example of a two dimensional nonlinear periodic crystal\cite{berger}, where the refractive index is constant but in which the 2nd order nonlinear susceptibility is spatially periodic. Such a crystal allows for efficient quasi-phase matched 2nd harmonic generation using multiple reciprocal lattice vectors of the crystal lattice. External 2nd harmonic conversion efficiencies > 60% were measured with picosecond pulses. The 2nd harmonic light can be simultaneously phase matched by multiple reciprocal lattice vectors, resulting in the generation of multiple coherent beams. The fabrication technique is extremely versatile and allows for the fabrication of a broad range of 2-D crystals including quasi-crystals.
physics.optics:The influence of linearly and circularly polarized laser fields on the dynamics of fast electron-impact excitation in atomic helium is discussed. A detailed analysis is made in the excitation of 2^1S, 3^1S and 3^1D dressed states of helium target.
physics.optics:The nonlinear dynamics of dissipative quantum systems in incoherent laser fields is studied in the framework of master equation with random model describing the laser noise and Markovian approximation for dealing with the system-bath couplings.
physics.optics:We present a theoretical study of strong laser-atom interactions, when the laser field parameters are subjected to random processes. The atom is modelled by a two-level and three-level systems, while the statistical fluctuations of the laser field are described by a pre-Gaussian model.
physics.optics:We consider the effect of spatial correlations on sources of polarized electromagnetic radiation. The sources, assumed to be monochromatic, are constructed out of dipoles aligned along a line such that their orientation is correlated with their position. In one representative example, the dipole orientations are prescribed by a generalized form of the standard von Mises distribution for angular variables such that the azimuthal angle of dipoles is correlated with their position. In another example the tip of the dipole vector traces a helix around the symmetry axis of the source, thereby modelling the DNA molecule. We study the polarization properties of the radiation emitted from such sources in the radiation zone. For certain ranges of the parameters we find a rather striking angular dependence of polarization. This may find useful applications in certain biological systems as well as in astrophysical sources.
physics.optics:We study the dynamics of the reduced density matrix(RDM) of the field in the micromaser. The resonator is pumped by N-atomic clusters of two-level atoms. At each given instant there is only one cluster in the cavity. We find the conditions of the independent evolution of the matrix elements of RDM belonging to a (sub)diagonal of the RDM, i.e. conditions of the diagonal invariance for the case of pumping by N-atomic clusters. We analyze the spectrum of the evolution operator of the RDM and discover the existence of the quasitrapped states of the field mode. These states exist for a wide range of number of atoms in the cluster as well as for a broad range of relaxation rates. We discuss the hierarchy of dynamical processes in the micromaser and find an important property of the field states corresponding to the quasi-equilibrium: these states are close to either Fock states or to a superposition of the Fock states. A possibility to tune the distribution function of photon numbers is discussed.
physics.optics:Temporal and angular correlations in atom-mediated photon-photon scattering are measured. Good agreement is found with the theory presented in Part~I.
physics.optics:The mediated photon-photon interaction due to the resonant Kerr nonlinearity in an inhomogeneously broadened atomic vapor is considered. The time-scale for photon-photon scattering is computed and found to be determined by the inhomogeneous broadening and the magnitude of the momentum transfer. This time can be shorter than the atomic relaxation time. Effects of atom statistics are included and the special case of small-angle scattering is considered. In the latter case the time-scale of the nonlinear response remains fast, even though the linear response slows as the inverse of the momentum transfer.
physics.optics:A simple variation of the traditional Young's double slit experiment can demonstrate several subtleties of interference with polarized light, including Berry and Pancharatnam's phase. Since the position of the fringes depends on the polarization state of the light at the input, the apparatus can also be used to measure the light's polarization without a quarter-wave plate or an accurate measurement of the light's intensity. In principle this technique can be used for any wavelength of photon as long as one can effectively polarize the incoming radiation.
physics.optics:The combination of charge separation induced by the formation of a single photorefractive screening soliton and an applied external bias field in a paraelectric is shown to lead to a family of useful electro-optic guiding patterns and properties.
physics.optics:The nonlinear pulse propagation in an optical fibers with varying parameters is investigated. The capture of moving in the frequency domain femtosecond colored soliton by a dispersive trap formed in an amplifying fiber makes it possible to accumulate an additional energy and to reduce significantly the soliton pulse duration. Nonlinear dynamics of the chirped soliton pulses in the dispersion managed systems is also investigated. The methodology developed does provide a systematic way to generate infinite ``ocean'' of the chirped soliton solutions of the nonlinear Schr\"odinger equation (NSE) with varying coefficients.
physics.optics:The Jaynes-Cummings model describing the interaction of a single linearly- polarized mode of the quantized electromagnetic field with an isolated two- level atom is generalized to the case of atomic levels degenerate in the projections of the angular momenta on the quantization axis, which is a usual case in the experiments. This generalization, like the original model, obtains the explicit solution. The model is applied to calculate the dependence of the atomic level populations on the angle between the polarization of cavity field mode and that of the laser excitation pulse in the experiment with one-atom micromaser.
physics.optics:We deduce the simplest form for an axicon Gaussian laser beam, i.e., one with radial polarization of the electric field.
physics.optics:We consider a linearly polarized electromagnetic wave incident on an opaque screen with square aperture of edge a. An application of Faraday's law to a loop parallel to the screen, on the side away from the source, shows that the wave must have longitudinal components there. The ratio of the longitudinal to transverse field is a measure of the diffraction angle.
physics.optics:We show that a time-reversed formulation of Huygens-Kirchhoff diffraction can be used to deduce the transverse and longitudinal fields of a Gaussian laser beam, starting from a simple assumption of a Gaussian beam profile in the far field. An attempt to apply this technique to the far fields of a Hertzian dipole shows how the laws of diffraction do not permit a wave to be focused to a volume smaller than a cubic wavelength in a charge-free region.
physics.optics:Slow light generated by Electromagnetically Induced Transparency is extremely susceptible with respect to Doppler detuning. Consequently, slow-light gyroscopes should have ultrahigh sensitivity.
physics.optics:The second-harmonic interferometric spectroscopy (SHIS) which combines both amplitude (intensity) and phase spectra of the second-harmonic (SH) radiation is proposed as a new spectroscopic technique being sensitive to the type of critical points (CP's) of combined density of states at semiconductor surfaces. The increased sensitivity of SHIS technique is demonstrated for the buried Si(111)-SiO$_2$ interface for SH photon energies from 3.6 eV to 5 eV and allows to separate the resonant contributions from $E^\prime_0/E_1$, $E_2$ and $E^\prime_1$ CP's of silicon.
physics.optics:The frequency of a 700mW monolithic non-planar Nd:YAG ring laser (NPRO) depends with a large coupling coefficient (some MHz/mW) on the power of its laser-diode pump source. Using this effect we demonstrate the frequency stabilization of an NPRO to a frequency reference by feeding back to the current of its pump diodes. We achieved an error point frequency noise smaller than 1mHz/sqrt(Hz), and simultaneously a reduction of the power noise of the NPRO by 10dB without an additional power stabilization feed-back system.
physics.optics:We deduce the emissivity of radiation from a metallic surface as a function of angle and polarization. This effect has found application in the calibration of detectors for cosmic microwave background radiation.
physics.optics:We present the first experimental observation of modulation instability of partially spatially incoherent light beams in non-instantaneous nonlinear media. We show that even in such a nonlinear partially coherent system (of weakly-correlated particles) patterns can form spontaneously. Incoherent MI occurs above a specific threshold that depends on the beams' coherence properties (correlation distance), and leads to a periodic train of one-dimensional (1D) filaments. At a higher value of nonlinearity, incoherent MI displays a two-dimensional (2D) instability and leads to self-ordered arrays of light spots.
physics.optics:We study the polarization of light emitted by spatially correlated sources. We show that in general polarization acquires nontrivial spectral dependence due to spatial correlations. The spectral dependence is found to be absent only for a special class of sources where the correlation length scales as the wavelength of light. We further study the cross correlations between two spatially distinct points that are generated due to propagation. It is found that such cross correlation leads to sufficiently strong spectral dependence of polarization which can be measured experimentally.
physics.optics:We demonstrate experimentally that in a centrosymmetric paraelectric non-stationary boundary conditions can dynamically halt the intrinsic instability of quasi-steady-state photorefractive self-trapping, driving beam evolution into a stable oscillating two-soliton-state configuration.
physics.optics:Mugnai et al. have reported an experiment in which microwave packets appear to travel in air with a speed substantially greater than c. They calculate the group velocity of their packets and find that it agrees with their experimental result. That calculation is incorrect. A correct calculation gives a group velocity less than c. The reported experimental result cannot be reconciled with the Maxwell equations.
physics.optics:Scalar Bessel beams are derived both via the wave equation and via diffraction theory. While such beams have a group velocity that exceeds the speed of light, this is a manifestation of the "scissors paradox" of special relativty. The signal velocity of a modulated Bessel beam is less than the speed of light. Forms of Bessel beams that satisfy Maxwell's equations are also given.
physics.optics:Various algebraic structures of degenerate four-wave mixing equations of optical phase conjugation are analyzed. Two approaches (the spinorial and the Lax-pair based), complementary to each other, are utilized for a systematic derivation of conserved quantities. Symmetry groups of both the equations and the conserved quantities are determined, and the corresponding generators are written down explicitly. Relation between these two symmetry groups is found. Conserved quantities enable the introduction of new methods for integration of the equations in the cases when the coupling $\Gamma$ is either purely real or purely imaginary. These methods allow for both geometries of the process, namely the transmission and the reflection, to be treated on an equal basis. One approach to introduction of Hamiltonian and Lagrangian structures for the 4WM systems is explored, and the obstacles in successful implementation of that programe are identified. In case of real coupling these obstacles are removable, and full Hamiltonian and Lagrangian formulations of the initial system are possible.
physics.optics:Instead of using frequency dependent refractive index, we propose to use the extinction theorem to describe reflection and transmission of an ultrashort pulse passing through the boundary. When the duration of the pulse is comparable with the relaxation time, the results differ significantly from those given by the traditional method, especially if the carrier frequency is close to an absorbtion line. We compare the two approaches using the data of GaAs in the infrared domain.
physics.optics:The Classification of Polarization elements, the polarization affecting optical devices which have a Jones matrix representation, according to the types of eigenvectors they possess, is given a new visit through the Group-theoretical connection of polarization elements. The diattenuators and retarders are recognized as the elements corresponding to boosts and rotations respectively. The structure of homogeneous elements other than diattenuators and retarders are identified by giving the quaternion corresponding to these elements. The set of degenerate polarization elements is identified with the so called `null' elements of the Lorentz Group. Singular polarization elements are examined in their more illustrative Mueller matrix representation and finally the eigenstructure of a special class of singular Mueller matrices is studied.
physics.optics:Complex photonic band structures (CPBS) of transmission metallic gratings with rectangular slits are shown to exhibit strong discontinuities that are not evidenced in the usual energetic band structures. These discontinuities are located on Wood's anomalies and reveal unambiguously two different types of resonances, which are identified as horizontal and vertical surface-plasmon resonances. Spectral position and width of peaks in the transmission spectrum can be directly extracted from CPBS for both kinds of resonances.
physics.optics:We present a brief classical discussion of a process to reduce the group velocity of an electromagnetic pulse by many orders of magnitude.
physics.optics:The properties of pulse propagation in a nonlinear fiber including linear damped term added in the usual nonlinear Schr\"odinger equation is analyzed analytically. We apply variational modified approach based on the lagrangian that describe the dynamic of system and with a trial function we obtain a solution which is more accuracy when compared with a pertubative solution. As a result, the problem of pulse propagation in a fiber with loss can be described in good agreement with exact results.
physics.optics:The group velocity for pulses in an optical medium can be negative at frequencies between those of a pair of laser-pumped spectral lines. The gain medium then can amplify the leading edge of a pulse resulting in a time advance of the pulse when it exits the medium, as has been recently demonstrated in the laboratory. This effect has been called superluminal, but, as a classical analysis shows, it cannot result in signal propgation at speeds greater than that of light in vacuum.
physics.optics:In two models it is shown that a light pulse propagates from a vacuum into certain media with velocity greater than that of a light in a vacuum (c). By numerical calculation the propagating properties of such a light are given.
physics.optics:The results of the study of ultra-short pulse generation in continuous-wave Kerr-lens mode-locked (KLM) solid-state lasers with semiconductor saturable absorbers are presented. The issues of extremely short pulse generation are addressed in the frames of the theory that accounts for the coherent nature of the absorber-pulse interaction. We developed an analytical model that bases on the coupled generalized Landau-Ginzburg laser equation and Bloch equations for a coherent absorber. We showed, that in the absence of KLM semiconductor absorber produces 2pi - non-sech-pulses of self-induced transparency, while the KLM provides an extremely short sech-shaped pulse generation. 2pi- and pi-sech-shaped solutions and variable-area chirped pulses have been found. It was shown, that the presence of KLM removes the limitation on the minimal modulation depth in absorber. An automudulational stability and self-starting ability were analyzed, too.
physics.optics:Based on self - consistent field theory we study a soliton generation in cw solid-state lasers with semiconductor saturable absorber. Various soliton destabilizations, i.e. the switch from femtosecond to picosecond generation (''picosecond collapse''), an automodulation regime, breakdown of soliton generation and hysteresis behavior, are predicted.
physics.optics:The effect of transmission of x-ray beams through submicron capillaries was investigated with account for diffraction and roughness scattering. Possible explanation of anomalous energy dependence of transmission through thin Cr/C/Cr channeles was given due to effect of periodic deformations.
physics.optics:Nonstationary pulse regimes associated with self modulation of a Kerr-lens modelocked Ti:sapphire laser have been studied experimentally and theoretically. Such laser regimes occur at an intracavity group delay dispersion that is smaller or larger than what is required for stable modelocking and exhibit modulation in pulse amplitude and spectra at frequencies of several hundred kHz. Stabilization of such modulations, leading to an increase in the pulse peak power by a factor of ten, were accomplished by weakly modulating the pump laser with the self-modulation frequency. The main experimental observations can be explained with a round trip model of the fs laser taking into account gain saturation, Kerr lensing, and second- and third-order dispersion.
physics.optics:The theoretical calculation for nonlinear refractive index in Cr: ZnSe - active medium predicts the strong defocusing cascaded second-order nonlinearity within 2000 - 3000 nm spectral range. On the basis of this result the optimal cavity configuration for Kerr-lens mode locking is proposed that allows to achieve a sub-100 fs pulse duration. The numerical simulations testify about strong destabilizing processes in the laser resulting from a strong self-phase modulation. The stabilization of the ultrashort pulse generation is possible due to spectral filtering that increases the pulse duration up to 300 fs.
physics.optics:The influence of nonlinear properties of semiconductor saturable absorbers on ultrashort pulse generation was investigated. It was shown, that linewidth enhancement, quadratic and linear ac Stark effect contribute essentially to the mode locking in cw solid-state lasers, that can increase the pulse stability, decrease pulse duration and reduce the mode locking threshold
physics.optics:We demonstrate that the shift of the stop band position with increasing oblique angle in periodic structures results in a wide transverse exponential field distribution corresponding to strong angular confinement of the radiation. The beam expansion follows an effective diffusive equation depending only upon the spectral mode width. In the presence of gain, the beam cross section is limited only by the size of the gain area. As an example of an active periodic photonic medium, we calculate and measure laser emission from a dye-doped cholesteric liquid crystal film.
physics.optics:The frequency of the Calcium ^3P_1--^1S_0 intercombination line at 657 nm is phase-coherently measured in terms of the output of a primary cesium frequency standard using an optical frequency comb generator comprising a sub-10 fs Kerr-lens mode-locked Ti:Sapphire laser and an external microstructure fiber for self-phase-modulation. The measured frequency of \nu_Ca = 455 986 240 494 276 Hz agrees within its relative uncertainty of 4 10^-13 with the values previously measured with a conceptually different harmonic frequency chain and with the value recommended for the realization of the SI unit of length.
physics.optics:Accurate phase-locked 3:1 division of an optical frequency was achieved, by using a continuous-wave (cw) doubly resonant optical parametric oscillator. A fractional frequency stability of 2*10^(-17) of the division process has been achieved for 100s integration time. The technique developed in this work can be generalized to the accurate phase and frequency control of any cw optical parametric oscillator.
physics.optics:It was usually assumed that the resonator based on a waveguide has the eigen oscillations that are formed by interference of two waves which propagate in different directions and have equal amplitudes. These patterns are usually called standing waves. We have shown that the eigen oscillations of a resonator which is filled by a layered dielectric can be base on the evanescent (non-propagating) waves. In some cases we need only one eigen wave to compose the eigen oscillation of a closed cavity.
physics.optics:The plane-wave dynamics of 3*omega => (2*omega, omega) subharmonic optical parametric oscillators containing a second harmonic generator of the idler wave omega is analyzed analytically by using the meanfield approximation and numerically by taking into account the field propagation inside the media. The resonant Chi(2):Chi(2) cascaded second-order nonlinearities induce a mutual injection-locking of the signal and idler waves that leads to coherent self phase-locking of the pump and subharmonic waves, freezing the phase diffusion noise. In case of signal-and-idler resonant devices, largely detuned sub-threshold states occur due to a subcritical bifurcation, broadening out the self-locking frequency range to a few cavity linewidths.
physics.optics:For the first time an all optical flip-flop is demonstrated based on two coupled Mach-Zehnder interferometers which contain semiconductor optical amplifiers in their arms. The flip-flop operation is discussed and it is demonstrated using commercially available fiber pigtailed devices. Being based on Mach-Zehnder interferometers, the flip-flop has potential for very high speed operation.
physics.optics:The strong asymmetry in charge distribution supporting a single non-interacting spatial needle soliton in a paraelectric photorefractive is directly observed by means of electroholographic readout. Whereas in trapping conditions a quasi-circular wave is supported, the underlying double-dipolar structure can be made to support two distinct propagation modes.
physics.optics:I present a theoretical treatment of parametric scattering in strong coupling semiconductor microcavities to model experiments in which parametric oscillator behaviour has been observed. The model consists of a non-linear excitonic oscillator coupled to a cavity mode which is driven by the external fields, and predicts the output power, below threshold gain and spectral blue shifts of the parametric oscillator. The predictions are found to be in excellent agreement with the experimental data.
physics.optics:A number of factors that influence spectral position of the femtosecond pulse in a Kerr-lens modelocked Cr:LiSGaF laser have been identified: high-order dispersion, gain saturation, reabsorption from the ground state, and stimulated Raman scattering. Using the one-dimensional numerical model for the simulation of the laser cavity, the relative contributions of different factors have been compared. The Raman effect provides the largest self-frequency shift from the gain peak (up to 60 nm), followed by the gain saturation (25 nm), while the high-order dispersion contribution is insignificant (5 nm). Comparison with the experimental data confirm that the stimulated Raman scattering is a main cause of the ultrashort pulse self-frequency shift observed in Cr:LiSGaF and Cr:LiSAF lasers
physics.optics:Simultaneous measurements of the intensity and phase of a probe wave reflected from an interface between silica and elemental alpha-gallium reveal its very strong optical nonlinearity, affecting both these parameters of the reflected wave. The data corroborate with a non-thermal mechanism of optical response which assumes appearance of a homogeneous highly metallic layer, only a few nanometer thick, between the silica and bulk alpha-gallium.
physics.optics:We consider pulse propagation in a linear anomalously dispersive medium where the group velocity exceeds the speed of light in vacuum (c) or even becomes negative. A signal velocity is defined operationally based on the optical signal-to-noise ratio, and is computed for cases appropriate to the recent experiment where such a negative group velocity was observed. It is found that quantum fluctuations limit the signal velocity to values less than c.
physics.optics:Polarization dynamics of femtosecond light pulses propagating in air is studied by computer simulation. A rich variety of dynamics is found that depends on the initial polarization state and power of the pulse. Effects of polarization on the plasma and supercontinuum generation are also discussed.
physics.optics:We report measurements of thermal self-locking of a Fabry-Perot cavity containing a potassium niobate (KNbO3) crystal. We develop a method to determine linear and nonlinear optical absorption coefficients in intracavity crystals by detailed analysis of the transmission lineshapes. These lineshapes are typical of optical bistability in thermally loaded cavities. For our crystal, we determine the one-photon absorption coefficient at 846 nm to be (0.0034 \pm 0.0022) per m and the two-photon absorption coefficient at 846 nm to be (3.2 \pm 0.5) \times 10^{-11} m/W and the one-photon absorption coefficient at 423 nm to be (13 \pm 2) per m. We also address the issue of blue-light-induced-infrared-absorption (BLIIRA), and determine a coefficient for this excited state absorption process. Our method is particularly well suited to bulk absorption measurements where absorption is small compared to scattering. We also report new measurements of the temperature dependence of the index of refraction at 846 nm, and compare to values in the literature.
physics.optics:We review and extend the analogies between Gaussian pulse propagation and Gaussian beam diffraction. In addition to the well-known parallels between pulse dispersion in optical fiber and CW beam diffraction in free space, we review temporal lenses as a way to describe nonlinearities in the propagation equations, and then introduce further concepts that permit the description of pulse evolution in more complicated systems. These include the temporal equivalent of a spherical dielectric interface, which is used by way of example to derive design parameters used in a recent dispersion-mapped soliton transmission experiment. Our formalism offers a quick, concise and powerful approach to analyzing a variety of linear and nonlinear pulse propagation phenomena in optical fibers.
physics.optics:We have measured the frequency of the $6s^2S_{1/2} - 5d^2D_{3/2}$ electric-quadrupole transition of $^{171}$Yb$^+$ with a relative uncertainty of $1\times 10^{-14}$, $\nu_{Yb}$ = 688 358 979 309 312 Hz $\pm$ 6 Hz. A femtosecond frequency comb generator was used to phase-coherently link the optical frequency derived from a single trapped ion to a cesium fountain controlled hydrogen maser. This measurement is one of the most accurate measurements of optical frequencies ever reported, and it represents a contribution to the development of optical clocks based on an $^{171}$Yb$^+$ ion standard.
physics.optics:The time behaviour of microwaves undergoing partial reflection by photonic barriers was measured in the time and in the frequency domain. It was observed that unlike the duration of partial reflection by dielectric layers, the measured reflection duration of barriers is independent of their length. The experimental results point to a nonlocal behaviour of evanescent modes at least over a distance of some ten wavelengths. Evanescent modes correspond to photonic tunnelling in quantum mechanics.
physics.optics:We study the conditions for soliton-like wave propagation in the Photorefractive (PR) and electro-optic (i.e., Pockels) material, by using Nonlinear Schrodinger (NLS) equation. The complete NLS equation is solved analytically and numerically by transforming it into the phase space. Our results clearly show the existence of both the dark and bright solitary solutions for the PR medium. Interestingly, however, we find only one bright solitary solution in the Pockels case and there is no evidence of any dark solitary solution.
physics.optics:In the theory of optical gap solitons, slowly-moving finite-amplitude Lorentzian solutions are found to mediate the transition from bright to coexistent dark-antidark solitary wave pairs when the laser frequency is detuned out of the proper edge of a dynamical photonic bandgap. Catastrophe theory is applied to give a geometrical description of this strongly asymmetrical 'morphing' process.
physics.optics:Harmonic and Intermodulation distortions occur when a physical system is excited with a single or several frequencies and when the relationship between the input and output is non-linear. Working with non-linearities in the Frequency domain is not straightforward specially when the relationship between the input and output is not trivial. We outline the complete derivation of the Harmonic and Intermodulation distortions from basic principles to a general physical system. For illustration, the procedure is applied to the Single Mode laser diode where the relationship of input to output is non-trivial. The distortions terms are extracted directly from the Laser Diode rate equations and the method is tested by comparison to many results cited in the literature. This methodology is general enough to be applied to the extraction of distortion terms to any desired order in many physical systems in a general and systematic way.
physics.optics:We reelaborate on the basic properties of lossless multilayers. We show that the transfer matrices for these multilayers have essentially the same algebraic properties as the Lorentz group SO(2,1) in a (2+1)-dimensional spacetime, as well as the group SL(2,R) underlying the structure of the ABCD law in geometrical optics. By resorting to the Iwasawa decomposition, we represent the action of any multilayer as the product of three matrices of simple interpretation. This group-theoretical structure allows us to introduce bilinear transformations in the complex plane. The concept of multilayer transfer function naturally emerges and its corresponding properties in the unit disc are studied. We show that the Iwasawa decomposition reflects at this geometrical level in three simple actions that can be considered the basic pieces for a deeper undestanding of the multilayer behavior. We use the method to analyze in detail a simple practical example.
physics.optics:A method is presented to investigate diffraction of an electromagnetic plane wave by an infinitely thin infinitely conducting circular cylinder with longitudinal slots. It is based on the use of the combined boundary conditions method that consists on expressing the continuity of the tangential components of both the electric and the magnetic fields in a single equation. This method proves to be very efficient for this kind of problems and leads to fast numerical codes.
physics.optics:We present a reliable, narrow linewidth (100 kHz) continuous-wave optical parametric oscillator (OPO) suitable for high-resolution spectroscopy applications. The OPO is based on a periodically-poled lithium-niobate crystal and features a specially designed intracavity etalon which permits its continuous tuning and stable operation at any desired wavelength in a wide operation range. We demonstrate Doppler-free spectroscopy on a rovibrational transition of methane at 3.39 um.
physics.optics:We describe the action of a plane interface between two semi-infinite media in terms of a transfer matrix. We find a remarkably simple factorization of this matrix, which enables us to express the Fresnel coefficients as a hyperbolic rotation.
physics.optics:We report on a methodology for the evaluation of the DC characteristics, small-signal frequency response and large-signal dynamic response of carrier and photon density responses in semiconductor laser diodes. A single mode laser is considered and described with a pair of rate equations containing a novel non-linear gain compensation term depending on a single parameter that can be chosen arbitrarily. This approach can be applied to any type of solid-state laser as long as it is described by a set of rate equations.
physics.optics:We report the discovery of a "dark area theorem," a new quantum optical relation for propagation of unmatched pulses in thick three-level $\Lambda$-type media. We define dark area and derive the dark area theorem for a coherently prepared and inhomogeneously broadened lambda medium. We also obtain the first equation for the spatial evolution of the dark state amplitude prior to pulse-matching.
physics.optics:We propose experimentally simplified schemes of an optically dispersive interface region between two coupled free electron lasers (FELs), aimed at achieving a much broader gain bandwidth than in a conventional FEL or a conventional optical klystron composed of two separated FELs. The proposed schemes can {\it universally} enhance the gain of FELs, regardless of their design when operated in the short pulsed regime.
physics.optics:As a light beam is produced by an amplification of modes of the zero point field in its source, this field cannot be distinguished; consequently a nonlinear optical effect is a function of the total field. However, we generally prefer to use a conventional field which excludes the zero point field; for a low conventional field, the total field may be developed to the first order, so that the effect appears linear.   This nearly trivial remark allows a correct computation of the signal of a photocell used for photon counting and shows that the "impulsive stimulated Raman scattering" (ISRS), a nonlinear, without threshold effect, which shifts the frequencies, becomes linear at low light levels, so that the shifted spectra are not distorted.
physics.optics:The effect of thermal fluctuations in the resonance fluorescence of a three-level system is studied. The damped three-level system is driven by two strong incident classical fields near resonances frequencies. The simulation of a thermal bath is obtained with a large system of harmonic oscillators that represent the normal modes of the thermal radiation field. The time evolution of the fluorescent light intensities are obtained solving by a iterative method the Heisenberg equations of motion in the integral form. The results show that the time development of the intensity of the fluorescence light is strongly affected by the interaction of the system with the thermal bath.
physics.optics:The dynamical response of a relativistic bunch of electrons injected in a planar magnetic undulator and interacting with a counterpropagating electromagnetic wave is studied. We demonstrate a resonance condition for which the free electron laser (FEL) dynamics is strongly influenced by the presence of the external field. It opens up the possibility of control of short wavelength FEL emission characteristics by changing the parameters of the microwave field without requiring change in the undulator's geometry or configuration. Numerical examples, assuming realistic parameter values analogous to those of the TTF-FEL, currently under development at DESY, are given for possible control of the amplitude or the polarization of the emitted radiation.
physics.optics:We have operated a CW triply resonant OPO using a PPLN crystal pumped by a Nd:YAG laser at 1.06 micron and generating signal and idler modes in the 2-2.3 micron range. The OPO was operated stably in single mode operation over large periods of time with a pump threshold as low as 500 microwatts.
physics.optics:We extend a modal theory of diffraction by a set of parallel fibers to deal with the case of a hard boundary: that is a structure made for instance of air-holes inside a dielectric matrix. Numerical examples are given concerning some resonant phenomena.
physics.optics:We report observation of lasing in the scarred modes in an asymmetrically deformed microcavity made of liquid jet. The observed scarred modes correspond to morphology-dependent resonance of radial mode order 3 with their Q values in the range of 10^6. Emission directionality is also observed, corresponding to a hexagonal unstable periodic orbit.
physics.optics:We have demonstrated an ultrahigh-Q whispering-gallery-mode (WGM) microsphere laser based on the evanescent-wave-coupled gain. Dye molecules outside the sphere near the equator were excited, resulting in WGM lasing in the lowest radial mode order. The loaded quality factor of the lasing WGM was 8(2)\times 10^9, the highest ever achieved in the microlaser.
physics.optics:An extended cavity diode laser operating in the Littrow configuration emitting near 657 nm is stabilized via its injection current to a reference cavity with a finesse of more than 10^5 and a corresponding resonance linewidth of 14 kHz. The laser linewidth is reduced from a few MHz to a value below 30 Hz. The compact and robust setup appears ideal for a portable optical frequency standard using the Calcium intercombination line.
physics.optics:We introduce a novel concept for optical frequency measurement and division which employs a Kerr-lens mode-locked laser as a transfer oscillator whose noise properties do not enter the measurement process. We experimentally demonstrate, that this method opens up the route to phase-link signals with arbitrary frequencies in the optical or microwave range while their frequency stability is preserved.
physics.optics:We investigate the self-phase modulation of intense femtosecond laser pulses propagating in an ionizing gas and its effects on collective properties of high-order harmonics generated in the medium. Plasmas produced in the medium are shown to induce a positive frequency chirp on the leading edge of the propagating laser pulse, which subsequently drives high harmonics to become positively chirped. In certain parameter regimes, the plasma-induced positive chirp can help to generate sharply peaked high harmonics, by compensating for the dynamically-induced negative chirp that is caused by the steep intensity profile of intense short laser pulses.
physics.optics:We developed a novel technique for frequency measurement and synthesis, based on the operation of a femtosecond comb generator as transfer oscillator. The technique can be used to measure frequency ratios of any optical signals throughout the visible and near-infrared part of the spectrum. Relative uncertainties of $10^{-18}$ for averaging times of 100 s are possible. Using a Nd:YAG laser in combination with a nonlinear crystal we measured the frequency ratio of the second harmonic $\nu_{SH}$ at 532 nm to the fundamental $\nu_0$ at 1064 nm, $\nu_{SH}/\nu_0 = 2.000 000 000 000 000 001 \times (1 \pm 7 \times 10^{-19})$.
physics.optics:The mixed crystal of a para-dibromobenzene with a para-chloronitrobenzene is investigated at concentration of components from 0% up to 60% of a para-chloronitrobenzene by the method of Low-Frequency Raman spectroscopy. It is shown, that in range of concentrations from 25% up to 50% of a para-chloronitrobenzene the spectrum of the mixed crystal would consist of the sum of spectrums a and b phases which relation of intensities depends on concentration of components. It is also found, that the single crystal in this range has rod frame.
physics.optics:The stability of polarization, areas, and number of self-induced transparency (SIT)-solitons at the output from the LaF_3:Pr^{3+} crystal is theoretically studied versus the polarization direction and the area of the input linearly polarized laser pulse. For this purpose the Vector Area Theorem is rederived and two-dimensional Vector Area Theorem map is obtained. The map is governed by the crystal symmetry and takes into account directions of the dipole matrix element vectors of the different site subgroups of optically excited ions. The Vector Area Theorem mapping of the time evolution of the laser pulse allows one to highlight soliton polarization properties.
physics.optics:The dynamics of light in Fabry-Perot cavities with varying length and input laser frequency are analyzed and the exact condition for resonance is derived. This dynamic resonance depends on the light transit time in the cavity and the Doppler effect due to the mirror motions. The response of the cavity to length variations is very different from its response to laser frequency variations. If the frequency of these variations is equal to multiples of the cavity free spectral range, the response to length is maximized while the response to the laser frequency is zero. Implications of these results for the detection of gravitational waves using kilometer-scale Fabry-Perot cavities are discussed.
physics.optics:Numerical simulation of the National Ignition Facility (NIF) laser performance and automated control of the laser setup process are crucial to the project's success. These functions will be performed by two closely coupled computer code: the virtual beamline (VBL) and the laser performance operations model (LPOM).
physics.optics:Since its birth, the laser has been extraordinarily effective in the study and applications of laser-matter interaction at the atomic and molecular level and in the nonlinear optics of the bound electron. In its early life, the laser was associated with the physics of electron volts and of the chemical bond. Over the past fifteen years, however, we have seen a surge in our ability to produce high intensities, five to six orders of magnitude higher than was possible before. At these intensities, particles, electrons and protons, acquire kinetic energy in the mega-electron-volt range through interaction with intense laser fields. This opens a new age for the laser, the age of nonlinear relativistic optics coupling even with nuclear physics. We suggest a path to reach an extremely high-intensity level $10^{26-28} $W/cm$^2$ in the coming decade, much beyond the current and near future intensity regime $10^{23} $W/cm$^2$, taking advantage of the megajoule laser facilities. Such a laser at extreme high intensity could accelerate particles to frontiers of high energy, tera-electron-volt and peta-electron-volt, and would become a tool of fundamental physics encompassing particle physics, gravitational physics, nonlinear field theory, ultrahigh-pressure physics, astrophysics, and cosmology. We focus our attention on high-energy applications in particular and the possibility of merged reinforcement of high-energy physics and ultraintense laser.
physics.optics:A simple and intuitive geometical method to analyze Fresnel formulas is presented. It applies to transparent media and is valid for perpendicular and parallel polarizations. The approach gives a graphical characterization particularly simple of the critical and Brewster angles. It also provides an interpretation of the relation between the reflection coefficients for both basic polarizations as a symmetry in the plane.
physics.optics:The tunnel effect is considered here within the framework of electromagnetic propagation. The classical problem of a plane gap of dielectric, surrounded on both sides by a medium with larger refraction index, is studied in the case in which an electromagnetic plane wave impinges into the gap with an incidence angle larger than the critical angle. In this condition (total reflection), the gap acts as a classically forbidden region and behaves like a tunnel. The field inside the forbidden gap consists of two evanescent waves, each one having its wavefronts normal to the interface. In the present paper we study the total field derived as a superposition of two such evanescent waves, its wavefronts, and the directions of propagation of both phase and energy.
physics.optics:The motion of an electromagnetic wave, through a classically-forbidden region, has recently attracted renewed interest because of its implication with regard to the theoretical and experimental problems of superluminality. From an experimental point of view, many papers provide an evidence of superluminality in different physical systems. Theoretically, the problem of a passage through a forbidden gap has been treated by considering plane waves at oblique incidence into a plane parallel layer of a medium with a refractive index smaller than the index of the surrounding medium, and also confined (Gaussian) beams, still at oblique incidence. In the present paper the case of a Bessel beam is examined, at normal incidence into the layer (Secs. II and III), in the scalar approximation (Sec. IV) and by developing also a vectorial treatment (Sec. V). Conclusions are reported in Sic. VI.
physics.optics:The tunneling time is here investigated by means of an electromagnetic model, for a system where a gap, between two parallel planes, acts as a classically-forbidden region for an impinging pulse with incidence angle larger than the critical angle. In all cases of frustrated total reflection we obtain a superluminal behavior both for phase and group delays.
physics.optics:We report an injection-locked cw titanium:sapphire ring laser at 846 nm. It produces 1.00 W in a single frequency when pumped with 5.5 W. Single frequency operation requires only a few milliwatts of injected power.
physics.optics:Accurate knowledge of absorption coefficient of a sample is a prerequisite for measuring the third order optical nonlinearity of materials, which could become a serious limitation for unknown samples. We introduce a new method, which measures both the absorption coefficient and the third order optical nonlinearity of materials with high sensitivity in a single experimental setup. We use a dual-beam pump-probe experiment under different conditions to achieve this goal. We also demonstrate a counterintuitive coupling of the non-interacting probe-beam with the pump-beam in pump-probe z-scan experiment.
physics.optics:We obtain gain of the probe field at multiple frequencies in a closed three-level V-type system using frequency modulated pump field. There is no associated population inversion among the atomic states of the probe transition. We describe both the steady-state and transient dynamics of this system. Under suitable conditions, the system exhibits large gain simultaneously at series of frequencies far removed from resonance. Moreover, the system can be tailored to exhibit multiple frequency regimes where the probe experiences anomalous dispersion accompanied by negligible gain-absorption over a large bandwidth, a desirable feature for obtaining superluminal propagation of pulses with negligible distortion.
physics.optics:An undoped double quantum well (DQW) was driven with a terahertz (THz) electric field of frequency \omega_{THz} polarized in the growth direction, while simultaneously illuminated with a near-infrared (NIR) laser at frequency \omega_{NIR}. The intensity of NIR upconverted sidebands \omega_{sideband}=\omega_{NIR} + \omega_{THz} was maximized when a dc voltage applied in the growth direction tuned the excitonic states into resonance with both the THz and NIR fields. There was no detectable upconversion far from resonance. The results demonstrate the possibility of using gated DQW devices for all-optical wavelength shifting between optical communication channels separated by up to a few THz.
physics.optics:Driving a double-quantum-well excitonic intersubband resonance with a terahertz (THz) electric field of frequency \omega_{THz} generated terahertz optical sidebands \omega=\omega_{THz}+\omega_{NIR} on a weak NIR probe. At high THz intensities, the intersubband dipole energy which coupled two excitons was comparable to the THz photon energy. In this strong-field regime the sideband intensity displayed a non-monotonic dependence on the THz field strength. The oscillating refractive index which gives rise to the sidebands may be understood by the formation of Floquet states, which oscillate with the same periodicity as the driving THz field.
physics.optics:We elaborate on the consequences of the factorization of the transfer matrix of any lossless multilayer in terms of three basic matrices of simple interpretation. By considering the bilinear transformation that this transfer matrix induces in the complex plane, we introduce the concept of multilayer transfer function and study its properties in the unit disk. In this geometrical setting, our factorization translates into three actions that can be viewed as the basic pieces for understanding the multilayer behavior. Additionally, we introduce a simple trace criterion that allows us to classify multilayers in three types with properties closely related to one (and only one) of these three basic matrices. We apply this approach to analyze some practical examples that are representative of these types of matrices.
physics.optics:We consider the problem of radiation into free space from the end-facet of a single-mode photonic crystal fiber (PCF). We calculate the numerical aperture NA=sin theta from the half-divergence angle theta ~ tan^{-1}(lambda/pi w) with pi w^2 being the effective area of the mode in the PCF. For the fiber first presented by Knight et al. we find a numerical aperture NA ~ 0.07 which compares to standard fiber technology. We also study the effect of different hole sizes and demonstrate that the PCF technology provides a large freedom for NA-engineering. Comparing to experiments we find good agreement.
physics.optics:Polarized and azimuthal dependencies of optical second harmonics generation (SHG) at the surface of noncentrosymmetric semiconductor crystals have been measured on polished surfaces of ZnSe(100), using a fundamental wavelength of 1.06$\mu m$. The SHG intensity patterns were analyzed for all four combination of p- and s-polarized incidence and output, considering both the bulk and surface optical nonlinearities in the electric dipole approximation. We found that the measurement using $S_{in}-S_{out}$ is particularly useful in determining the symmetry of the oxdized layer interface, which would lower the effective symmetry of the surface from $C_{4v}$ to $C_{2v}.$ We also have shown that the [011] and [0$\bar{1}$1] directions can be distinguished through the analysis of p-incident and p-output confugration.
physics.optics:Fundamental rules and definitions of Fractional Differintegrals are outlined. Factorizing 1-D and 2-D Helmholtz equations four fractional eigenfunctions are determined. The functions exhibit incident and reflected plane waves as well as diffracted incident and reflected waves of the half-plane edge. They allow to construct the Sommerfeld half-plane diffraction solutions. Parabolic-Wave Equation (PWE, Leontovich-Fock) for paraxial propagation is factorized and differetial fractional solutions of Fresnel-integral type are derived. We arrived at two solutions, which are the mothers of known and new solutions.
physics.optics:In the classical theory, an electromagnetic field obeying Maxwell's equations cannot be absorbed quickly by matter, so that it remains a zero point field. Splitting the total, genuine electromagnetic field into the sum of a conventional field and a zero point field is physically meaningless until a receiver attenuates the genuine field down to the zero point field, or studying the amplification of the zero point field by a source.   In classical optics all optical effects must be written using the genuine field, so that at low light levels the nonlinear effects become linear in relation to the conventional field. The result of the interpretation of all observations, even at low light levels, is exactly the same in quantum electrodynamics and in the semi- classical theory.   The zero point field is stochastic only far from the sources and the receivers; elsewhere, it is shaped by matter, it may be studied through fields visible before an absorption or after an amplification.   A classical study of the reduction of the wave packet extends the domain of equivalence of the classical and quantum zero point field; using both interpretations of this field makes the results more reliable, because the traps are different.
physics.optics:The smaller the size of a light-emitting microcavity, the more important it becomes to understand the effects of the cavity boundary on the optical mode profile. Conventional methods of laser physics, such as the paraxial approximation, become inapplicable in many of the more exotic cavity designs to be discussed here. Cavities in the shape of microdisks, pillars and rings can yield low lasing thresholds in a wide variety of gain media: quantum wells, wires and even dots, as well as quantum cascade superlattices and GaN. An overview of the experimental and theoretical status is provided, with special emphasis on the light extraction problem.
physics.optics:The stationary states of a microlaser are related to the decaying quasibound states of the corresponding passive cavity. These are interpreted classically as originating from sequential escape attempts of an ensemble of rays obeying a curvature-corrected Fresnel formula. Polarization-dependent predictions of this model, and its limitations for stable orbits in partially chaotic systems are discussed.
physics.optics:We measured and calculated transmission spectra of two-dimensional quasiperiodic photonic crystals (PCs) based on a 5-fold (Penrose) or 8-fold (octagonal) symmetric quasiperiodic pattern. The photonic crystal consisted of dielectric cylindrical rods in air placed normal to the basal plane on vertices of tiles composing the quasiperiodic pattern. An isotropic photonic band gap (PBG) appeared in the TM mode, where electric fields were parallel to the rods, even when the real part of a dielectric constant of the rod was as small as 2.4. An isotropic PBG-like dip was seen in tiny Penrose and octagonal PCs with only 6 and 9 rods, respectively. These results indicate that local multiple light scattering within the tiny PC plays an important role in the PBG formation. Besides the isotropic PBG, we found dips depending on the incident angle of the light. This is the first report of anisotropic structures clearly observed in transmission spectra of quasiperiodic PCs. Based on rod-number and rod-arrangement dependence, it is thought that the shapes and positions of the anisotropic dips are determined by global multiple light scattering covering the whole system. In contrast to the isotropic PBG due to local light scattering, we could not find any PBGs due to global light scattering even though we studied transmission spectra of a huge Penrose PC with 466 rods.
physics.optics:A perfect focus telescope is one in which all rays parallel to the axis meet at a point and give equal magnification there. It is shown that these two conditions define the shapes of both primary and secondary mirrors. Apart from scale, the solution depends upon two parameters, $s$, which gives the mirror separation in terms of the effective focal length, and $K$, which gives the relative position of the final focus in that unit. The two conditions ensure that the optical systems have neither spherical aberration nor coma, no matter how fast the $f$ ratio. All known coma--free systems emerge as approximate special cases. In his classical paper, K. Schwarzschild studied all two mirror systems whose profiles were conic sections. We make no such a priori shape conditions but demand a perfect focus and solve for the mirrors' shapes.
physics.optics:The results of experimental testing the existence of intense Lorentzian--like wings with FWHM $\sim 4.5 cm^{-1}$ in the absorption spectra of polyatomic molecules in a gas phase are presented. Two independent experimental methods were used for evaluating the integral intensity of the line wings for a number of substances. In the first case, the cross--section of the far wings of absorption bands in a gas phase spectrum were measured. Then, these band wings were extrapolated inside the contour of absorption band. In the second case, the saturation degree of the linear spectrum of molecules was determined. Radiation of a pulsed $CO_2$--laser was used at low gas pressure ($\sim 16$ mtorr) and averaged excitation level of molecules ${<n>}\sim 0.1$ quanta/molecule. The values obtained by these two independent methods coincide for a variety of molecules. The average relative integral intensity of the line wings varied from $\sim 0.6%$ for $SF_6$ and $SiF_4$ to $\sim 90%$ for $(CF_3)_2O$ and $(CF_3)_2CO$.
physics.optics:It is shown that the direct Fourier synthesization of light beams allows one to create polarity-asymmetric waves, which are able, in the process of nonlinear interaction with a medium, to break its inversion symmetry. As a result, these "polar" waves may show the effect of optical rectification in nonlinear centrosymmetric media by generating light-induced dc electric polarization. At the same time, the waves of this type, due to their unusual symmetry properties, can be used for detecting the direction and sign of a dc electric field applied to the medium. The prospects of application of polar waves to data recording and processing are discussed.
physics.optics:Presented is an analysis of general scaling perturbations in a transmitting fiber. For elliptical perturbations, under some conditions an intermode dispersion parameter characterizing modal PMD is shown to be directly proportional to the mode dispersion.
physics.optics:Extensive Bose-Einstein condensation research activities have recently led to studies of fermionic atoms and optical confinements. Here we present a case of micro-optical fermionic electron phase transition. Optically confined ordering and phase transitions of a fermionic cloud in dynamic steady state are associated with Rayleigh emissions from photonic quantum ring manifold which are generated by nature without any ring lithography. The whispering gallery modes, produced in a semiconductor Rayleigh-Fabry-Perot toroidal cavity at room temperature, exhibit novel properties of ultralow thresholds open to nano-ampere regime, thermal stabilities from square-root-T-dependent spectral shift, and angularly varying intermode spacings. The photonic quantum ring phenomena are associated with a photonic field-driven phase transition of quantum-well-to-quantum-wire and hence the photonic (non-de Broglie) quantum corral effect on the Rayleigh cavity-confined carriers in dynamic steady state. Based upon the intra-cavity fermionic condensation we also offer a prospect for an electrically driven few-quantum dot single photon source from the photonic quantum ring laser for quantum information processors.
physics.optics:Nonlinear optical media that are normally dispersive, support a new type of localized (nondiffractive and nondispersive) wavepackets that are X-shaped in space and time and have slower than exponential decay. High-intensity X-waves, unlike linear ones, can be formed spontaneously through a trigger mechanism of conical emission, thus playing an important role in experiments.
physics.optics:We report three-dimensional laser microfabrication, which enables microstructuring of materials on the scale of 0.2-1 micrometers. The two different types of microfabrication demonstrated and discussed in this work are based on holographic recording, and light-induced damage in transparent dielectric materials. Both techniques use nonlinear optical excitation of materials by ultrashort laser pulses (duration < 1 ps).
physics.optics:We propose a concept for production of high power coherent attosecond pulses in X-ray range. An approach is based on generation of 8th harmonic of radiation in a multistage HGHG FEL (high gain high harmonic free electron laser) configuration starting from shot noise. Single-spike phenomena occurs when electron bunch is passed through the sequence of four relatively short undulators. The first stage is a conventional "long" wavelength (0.8 nm) SASE FEL which operates in the high-gain linear regime. The 0.1 nm wavelength range is reached by successive multiplication (0.8 nm $\to$ 0.4 nm $\to$ 0.2 nm $\to$ 0.1 nm) in a stage sequence. Our study shows that the statistical properties of the high-harmonic radiation from the SASE FEL, operating in linear regime, can be used for selection of radiation pulses with a single spike in time domain. The duration of the spikes is in attosecond range. Selection of single-spike high-harmonic pulses is achieved by using a special trigger in data acquisition system. The potential of X-ray SASE FEL at TESLA at DESY for generating attosecond pulses is demonstrated. Since the design of XFEL laboratory at TESLA is based on the use of long SASE undulators with tunable gap, no special place nor additional FEL undulators are required for attophysics experiments. The use of a 10 GW-level attosecond X-ray pulses at X-ray SASE FEL facility will enable us to track processes inside atoms.
physics.optics:We present an algorithm for the maximization of photonic bandgaps in two-dimensional crystals. Once the translational symmetries of the underlying structure have been imposed, our algorithm finds a global maximal (and complete, if one exists) bandgap. Additionally, we prove two remarkable results related to maximal bandgaps: the so-called `maximum contrast' rule, and about the location in the Brillouin zone of band edges.
physics.optics:We investigate the propagation of electromagnetic waves in finite photonic band gap structures. We analyze the phenomenon of conduction and forbidden bands and we show that two regimes are to be distinguished with respect to the existence of a strong field near the interfaces. We precise the domain for which an effective medium theory is sounded.
physics.optics:The maximum bit-rate of a slab waveguide is ultimately determined by the waveguide dispersion. We show that while the maximum bit rate in a waveguide is inversely proportional to the waveguide's width, bit rate per unit width (i.e., spatial capacity) decreases, and in the limit of a zero-width waveguide it converges to a value, which is independent of the waveguide's refractive indices. This value is qualitatively equivalent to the transmission rate per unit of width in free space. We also show that in a 3D waveguide (e.g., fibers), unlike free space, the spatial capacity vanishes in the same limit.
physics.optics:The photonic band dispersion and density of states (DOS) are calculated for the three-dimensional (3D) hexagonal structure corresponding to a distributed Bragg reflector patterned with a 2D triangular lattice of circular holes. Results for the Si/SiO$_2$ and GaAs/AlGaAs systems determine the optimal parameters for which a gap in the 2D plane occurs and overlaps the 1D gap of the multilayer. The DOS is considerably reduced in correspondence with the overlap of 2D and 1D gaps. Also, the local density of states (i.e., the DOS weighted with the squared electric field at a given point) has strong variations depending on the position. Both results imply substantial changes of spontaneous emission rates and patterns for a local emitter embedded in the structure and make this system attractive for the fabrication of a 3D photonic crystal with controlled radiative properties.
physics.optics:We present a Fourier transform methodology for all-order polarization mode dispersion (PMD) analysis, based on the first Born approximation to the coupled-mode equation solution. Our method predicts wavelength-dependent PMD effects and allows design of filters for their mitigation.
physics.optics:We demonstrate the combination of a hemispherical solid immersion lens with a micro-photoluminescence setup. Two advantages introduced by the SIL, an improved resolution of 0.4 times the wavelength in vacuum and a 5 times enhancement of the collection efficiency, make it an ideal system for spatially resolved spectroscopy applications. The influence of the air gap between the SIL and the sample surface is investigated in detail. We confirm the tolerance of the set--up to an air gap of several micrometers. Such a system is proven to be ideal system in the studies of exciton transport and polarization dependent single quantum dot spectroscopy.
physics.optics:It is assumed, that the clumps of lines do not connected with states mixing and IVR, but they are the result of breaking (destruction) of the process of averaging of momentum of inertia of molecules during the vibration motion of atoms. Rough estimates of the widths of clumps of lines in absorption spectra of some acetylenic derivatives were made with this model. Obtained results are in a satisfactory agreement with the available experimental data. This idea allows also in principle to explain the origin of intensive wings of lines, the existence of which was discussed earlier.
physics.optics:The origin of the Kerr type nonlinearity of the medium as a result of the interaction between photons via the Dirac delta-potential is presented in the formalism adopted from the photon wave function approach. In the view of the result the optical soliton may be treated as a bound state (cluster) of many photons.
physics.optics:The propagation of photon in a dielectric may be described with the help of the scalar and vector potentials of the medium. The main novelty of the paper is that the concept of the vector potential (which is connected with the velocity of the medium) can be extended to relativistic velocities of the medium. The position-dependent photon wave function was used to describe the propagation of the photon. The new concepts of the velocity of photon as particle and the photon mass in the dielectric medium were proposed.
physics.optics:Mathematical aspects of the SU(1,1) group parameter x dynamics governed by Hamiltonians exhibiting some special types of time dependence has been presented on an elementary level from the point of view of Moebius transformation of complex plane. The trajectories of x in continuous and mappings in discrete dynamics are considered. Some simple examples have been examined. Analytical considerations and numerical results have been given.
physics.optics:Propagation of the TE electromagnetic waves in self-focusing medium is governed by the nonlinear Schroedinger equation. In this paper the stationary solutions of this equation have been systematically presented. The phase-plane method, qualitative analysis, and mechanical interpretation of the differential equations are widely used. It is well known that TE waves can be guided by the single interface between two semi-infinite media, providing that one of the media has a self-focusing (Kerr type) nonlinearity. This special solution is called a spatial soliton. In this paper our interests are not restricted to the soliton solutions. In the context of the nonlinear substrate and cladding we have found solutions which could be useful to describe also the incident light in nonlinear medium. This result is the main point of the paper. Some of the presented stationary solutions were already used in similar optical context in literature but we show a little wider class of solutions. In the last section we review and illustrate some results concerning the spatial soliton solution.
physics.optics:We present angle- and polarization-resolved measurements of the optical transmission of a subwavelength hole array. These results give a (far-field) visualization of the corresponding (near-field) propagation of the excited surface plasmons and allow for a simple analysis of their polarization properties.
physics.optics:A monochromatic linear source of light is rotated with certain angular frequency and when such light is analysed after reflection then a change of frequency or wavelength may be observed depending on the location of the observer. This change of frequency or wavelength is different from the classical Doppler effect [1] or relativistic Doppler effect [2]. The reason behind this shift in wavelength is that a certain time interval observed by an observer in the rotating frame is different from that of a stationary observer.
physics.optics:We investigate the spectral response of a Brillouin amplifier in the frequency regime within the SBS bandwidth. This is done by amplitude modulating the pump with a low frequency, and therefore, unlike previous studies, the spectrum of the modulated pump is, in all cases, smaller than the SBS bandwidth. We show both theoretically and experimentally that unlike phase modulation, which was reported in the literature, the amplitude modulation increases the Brillouin amplifier gain, and that this effect has a very narrow bandwidth. Only modulation frequencies that are lower than a certain cut-off frequency increase the gain. This cut-off frequency is inversely proportional to the fiber's length, and can therefore be arbitrarily small.
physics.optics:The Phase Diverse Speckle (PDS) problem is formulated mathematically as Multi Frame Blind Deconvolution (MFBD) together with a set of Linear Equality Constraints (LECs) on the wavefront expansion parameters. This MFBD-LEC formulation is quite general and, in addition to PDS, it allows the same code to handle a variety of different data collection schemes specified as data, the LECs, rather than in the code. It also relieves us from having to derive new expressions for the gradient of the wavefront parameter vector for each type of data set. The idea is first presented with a simple formulation that accommodates Phase Diversity, Phase Diverse Speckle, and Shack-Hartmann wavefront sensing. Then various generalizations are discussed, that allows many other types of data sets to be handled.
physics.optics:A Monte Carlo simulation has been performed to track light rays in cylindrical fibres by ray optics. The trapping efficiencies for skew and meridional rays in active fibres and distributions of characteristic quantities for all trapped light rays have been calculated. The simulation provides new results for curved fibres, where the analytical expressions are too complex to be solved. The light losses due to sharp bending of fibres are presented as a function of the ratio of curvature to fibre radius and bending angle. It is shown that a radius of curvature to fibre radius ratio of greater than 65 results in a loss of less than 10% with the loss occuring in the initial stage of the bend (at bending angles Phi circa pi/8 rad).
physics.optics:We have measured the photonic bandgap in the transmission of microwaves through a two-dimensional photonic crystal slab. The structure was constructed by cementing acrylic rods in a hexagonal closed-packed array to form rectangular stacks. We find a bandgap centered at approximately 11 GHz, whose depth, width and center frequency vary with the number of layers in the slab, angle of incidence and microwave polarization.
physics.optics:We study forward stimulated Raman emission from weakly fluorescent dye 4'-diethylamino-N-methyl-4-stilbazolium tosylate (DEST) in 1,2,dichloroethane solution excited by a 28 ps, 532 nm Nd: YAG laser. Neat 1, 2, dichloroethane emits the first Stokes line at 631 nm with a spectral width of 1.6 nm corresponding to a Raman shift of 2956 per cm. We observe reduction of spectral width with the addition of DEST in 1, 2, dichloroethane solution. The single pass conversion efficiency for forward Raman emission is as high as 20 percent in a 1 cm path length sample. The pulse duration of forward stimulated Raman emission measured by a third order autocorrelation technique is 10 ps in neat 1, 2, dichloroethane, whereas it is nearly 3 ps for 0.04 mM of DEST solution.
physics.optics:Distribution of centrosymmetrical molecules of an impurity (p-diclorobenzene) in monocrystals of solid solutions in two different matrixes with centrosymmetrical (p-dibrombenzene) and noncentrosymmetrical (p-bromchlorbenzene) molecules by the method of a Raman Effect is determined.
physics.optics:We demonstrate that twisting one part of a chiral photonic structure about its helical axis produces a single circularly polarized localized mode that gives rise to an anomalous crossover in propagation. Up to a crossover thickness, this defect results in a peak in transmission and exponential scaling of the linewidth for a circularly polarized wave with the same handedness as structure. Above the crossover, however, the linewidth saturates and the defect mode can be excited only by the oppositely polarized wave, resulting in a peak in reflection instead of transmission.
physics.optics:We study experimentally and theoretically the polarization alternation during the switch-on transient of a quasi-isotropic CO$_2$ laser emitting on the fundamental mode. The observed transient dynamics is well reproduced by means of a model which provides a quantitative discrimination between the intrinsic asymmetry due to the kinetic coupling of molecules with different angular momenta, and the extrinsic anisotropies, due to a tilted intracavity window. Furthermore, the experiment provides a numerical assignment for the decay rate of the coherence term for a CO$_2$ laser.
physics.optics:In this paper we present an analysis of information transfer time based on holomorphism, causality and the classical principle of stationary phase. We also make a preliminary study of the effect of noise on information transfer time, and find that noise tends to increase transfer times. Noise and information signals are both essentially acausal, such that analytic continuation (i.e. prediction) is impossible, which also implies that their frequency spectra cannot be holomorphic. This leads to the paradox of a non-holomorphic information-bearing light signal, yet whose underlying Maxwell equations governing the propagation of the EM wave describe a holomorphic function in spacetime. We find that application of stationary phase and entropy arguments circumvents this difficulty, with stationary phase only suggesting the most likely transfer times of an information signal in the presence of noise. Faster transit times are not excluded, but are highly improbable. Stationary phase solutions, by definition, do not include signal forerunners, whose detection in the presence of noise is also unreliable. Hence a finite information capacity ensues, as expected from Shannon's law, and information cannot be transferred faster than c. We also find that the method of stationary phase implies complex transfer times. However, by considering spacetime to be isomorphic with the complex temporal plane, we find that an imaginary time is equivalent to a real distance, and can be interpreted as the uncertainty in the spatial position of the information pulse. Finally, we apply our theory to a photonic band gap crystal, and find that information transfer speed and tunneling is always subluminal.
physics.optics:Enhancement of optical Kerr nonlinearity for self-action by electro-magnetically induced transparency in a four-level atomic system including dephasing between the ground states is studied in detail by solving the density matrix equations for the atomic levels. We discern three major contributions, from energy shifts of the ground states induced by the probe light, to the third-order susceptibility in the four-level system. In this four-level system with the frequency-degenerate probes, quantum interference amongst the three contributions can, not only enhance the third-order susceptibility more effectively than in the three-level system with the same characteristic parameters, but also make the ratio between its real and imaginary part controllable. Due to dephasing between the two ground states and constructive quantum interference, the most effective enhancement generally occurs at an offset that is determined by the atomic transition frequency difference and the coupling Rabi frequency.
physics.optics:We numerically study supercontinuum (SC) generation in photonic crystal fibers pumped with low-power 30-ps pulses close to the zero dispersion wavelength 647nm. We show how the efficiency is significantly improved by designing the dispersion to allow widely separated spectral lines generated by degenerate four-wave-mixing (FWM) directly from the pump to broaden and merge. By proper modification of the dispersion profile the generation of additional FWM Stokes and anti-Stokes lines results in efficient generation of an 800nm wide SC. Simulations show that the predicted efficient SC generation is more robust and can survive fiber imperfections modelled as random fluctuations of the dispersion coefficients along the fiber length.
physics.optics:We numerically study the possibilities for improved large-mode area endlessly single mode photonic crystal fibers for use in high-power delivery applications. By carefully choosing the optimal hole diameter we find that a triangular core formed by three missing neighboring air holes considerably improves the mode area and loss properties compared to the case with a core formed by one missing air hole. In a realized fiber we demonstrate an enhancement of the mode area by ~30 % without a corresponding increase in the attenuation.
physics.optics:A new type of perturbative expansion is built in order to give a rigorous derivation and to clarify the range of validity of some commonly used model equations.   This model describes the evolution of the modulation of two short and localized pulses, fundamental and second harmonic, propagating together in a bulk uniaxial crystal with non-vanishing second order susceptibility $\chi^(2)$ and interacting through the nonlinear effect known as ``cascading'' in nonlinear optics.   The perturbative method mixes a multi-scale expansion with a power series expansion of the susceptibility, and must be carefully adapted to the physical situation. It allows the determination of the physical conditions under which the model is valid: the order of magnitude of the walk-off, phase-mismatch,and anisotropy must have determined values.
physics.optics:Nonlinear phase noise, often called the Gordon-Mollenauer effect, can be compensated electronically by subtracting from the received phase a correction proportional to the received intensity. The optimal scaling factor is derived analytically and found to be approximately equal to half of the ratio of mean nonlinear phase noise and the mean received intensity. Using optimal compensation, the standard deviation of residual phase noise is halved, doubling the transmission distance in systems limited by nonlinear phase noise.
physics.optics:On the basis of the data given in the works of different authors a criterion of phase-photometric method of measurement of energy angle of divergence has been formulated. Validity of application of the obtained relations for a ray beam with an arbitrary diameter and an arbitrary shape of the wave front has been proved. Advantages of the proposed phase-photometric method in comparison with the focal-spot method have been confirmed. Necessity and possibility of building a standard solid angle has been proved.
physics.optics:This document contains my detailed calculation of the Generalised Few-cycle Envelope Approximation (GFEA) propagation equation reported and used in Phys. Rev. A (submitted) and its associated longer version at arXiv.org. This GFEA propagation equation is intended to be applicable to optical pulses only a few cycles long, a regime where the standard Slowly Varying Envelope Approximation (SVEA) fails.
physics.optics:We present a comprehensive framework for treating the nonlinear interaction of few-cycle pulses using an envelope description that goes beyond the traditional SVEA method. This is applied to a range of simulations that demonstrate how the effect of a $\chi^{(2)}$ nonlinearity differs between the many-cycle and few-cycle cases. Our approach, which includes diffraction, dispersion, multiple fields, and a wide range of nonlinearities, builds upon the work of Brabec and Krausz[1] and Porras[2]. No approximations are made until the final stage when a particular problem is considered.   The original version (v1) of this arXiv paper is close to the published Phys.Rev.A. version, and much smaller in size.
physics.optics:Broadband noise on supercontinuum spectra generated in microstructure fiber is shown to lead to amplitude fluctuations as large as 50 % for certain input laser pulse parameters. We study this noise using both experimental measurements and numerical simulations with a generalized stochastic nonlinear Schroedinger equation, finding good quantitative agreement over a range of input pulse energies and chirp values. This noise is shown to arise from nonlinear amplification of two quantum noise inputs: the input pulse shot noise and the spontaneous Raman scattering down the fiber.
physics.optics:The probability density function of Kerr effect phase noise, often called the Gordon-Mollenauer effect, is derived analytically. The Kerr effect phase noise can be accurately modeled as the summation of a Gaussian random variable and a noncentral chi-square random variable with two degrees of freedom. Using the received intensity to correct for the phase noise, the residual Kerr effect phase noise can be modeled as the summation of a Gaussian random variable and the difference of two noncentral chi-square random variables with two degrees of freedom. The residual phase noise can be approximated by Gaussian distribution better than the Kerr effect phase noise without correction.
physics.optics:Steady-state and dynamics of the self-phase-locked (3\omega ==> 2\omega, \omega) subharmonic optical parametric oscillator are analyzed in the pump-and-signal resonant configuration, using an approximate analytical model and a full propagation model. The upper branch solutions are found always stable, regardless of the degree of pump enhancement. The domain of existence of stationary states is found to critically depend on the phase-mismatch of the competing second-harmonic process.
physics.optics:In the present paper we investigate the transmission and reflection band behavior for a plane electromagnetic wave falling obliquely on an ideal layered structure. The dependence of this behavior on the problem parameters and wave incident angle is considered. It is shown, that in general case the band width is a non-monotonous function of the problem parameters. A condition is found, which defines the possibility of the contact of the transmission bands. This condition has the same form for s and p waves. It is also shown that irrespective of the wave polarization, the transmission coefficient equals to the unit at the contact points.
physics.optics:The problem of determination of the maximum of second harmonic generation in the potential well containing a rectangular barrier is considered. It is shown that, in general, the problem of finding the ensemble of structures with equidistant first three levels has two types of solutions.   For the first type the second and third energy levels are located above a rectangular barrier, and for the second type the third level is located above the barrier only. It is also shown, that generation corresponding to the second type of solution always is less than generation for the first one. Taking into account the effective mass changes the problem of finding the generation maximum for a finite depth well is exactly solved.
physics.optics:We investigate numerically optical properties of novel two-dimensional photonic materials where parallel dielectric rods are randomly placed with the restriction that the distance between rods is larger than a certain value. A large complete photonic gap (PG) is found when rods have sufficient density and dielectric contrast. Our result shows that neither long-range nor short-range order is an essential prerequisite to the formation of PGs. A universal principle is proposed for designing arbitrarily shaped waveguides, where waveguides are fenced with side walls of periodic rods and surrounded by the novel photonic materials. We observe highly efficient transmission of light for various waveguides. Due to structural uniformity, the novel photonic materials are best suited for filling up the outer region of waveguides of arbitrary shape and dimension comparable with the wavelength.
physics.optics:We suggest an effective method for controlling nonlinear switching in arrays of weakly coupled optical waveguides. We demonstrate the digitized switching of a narrow input beam for up to eleven waveguides in the engineered waveguide arrays.
physics.optics:Plane waves in Kerr media spontaneously generate paraxial X-waves (i.e. non-dispersive and non-diffractive pulsed beams) that get amplified along propagation. This effect can be considered a form of conical emission (i.e. spatio-temporal modulational instability), and can be used as a key for the interpretation of the out of axis energy emission in the splitting process of focused pulses in normally dispersive materials. A new class of spatio-temporal localized wave patterns is identified. X-waves instability, and nonlinear X-waves, are also expected in periodical Bose condensed gases.
physics.optics:The Dicke superradiance on vibronic transitions of impurity crystals is considered. It is shown that parameters of the superradiance (duration and intensity of the superradiance pulse and delay times) on each vibronic transition depend on the strength of coupling of electronic states with the intramolecular impurity vibration (responsible for the vibronic structure of the optical spectrum in the form of vibrational replicas of the pure electronic line) and on the crystal temperature through the Debye-Waller factor of the lattice vibrations. Theoretical estimates of the ratios of the time delays, as well as of the superradiance pulse intensities for different vibronic transitions well agree with the results of experimental observations of two-color superradiance in the polar dielectric KCl:O2-. In addition, the theory describes qualitatively correctly the critical temperature dependence of the superradiance effect.
physics.optics:This work is concerned with the propagation of electromagnetic waves in isotropic chiral media and with the effects produced by a plane boundary between two such media. In analogy with the phenomena of reflection and refraction of plane electromagnetic waves in ordinary dielectrics, the kinematical and dynamical aspects of these phenomena are studied, such as the intensity of the various wave components and the change in the polarization of the wave as it crosses the boundary. As a prerequisite of this, we show that the plane wave solution must be written as a suitable superposition of the circularly amplitudes on both sides of the interface, we elucidate which is the appropriate set of conditions that the solution must satisfy at the boundary, and we set down the minimal, and complete, set of equations that must be solved for the coefficient amplitudes in order to satisfy the boundary conditions. The equations are solved explicitly for some particular cases and configurations (e.g., normal incidence), the salient features of those solutions are analyzed in some detail, and the general solution to the equations is given as well.
physics.optics:We study the class of endlessly single-mode all-silica photonic crystal fibers with a triangular air-hole cladding. We consider the sensibility to longitudinal nonuniformities and the consequences and limitations for realizing low-loss large-mode area photonic crystal fibers. We also discuss the dominating scattering mechanism and experimentally we confirm that both macro and micro-bending can be the limiting factor.
physics.optics:Some aspects of lasing at vibronic transitions in impurity crystals are theoretically studied. The threshold conditions for a vibronic laser are shown to be dependent on the strength of interaction of optical centers with a local vibration, which forms the vibronic spectrum, and the crystal lattice temperature. The theory can be easily generalized to the spectrum containing a structureless phonon sideband and well agrees with the experimental temperature dependence of the output power of a Mg2SiO4:Cr4+ forsterite laser.
physics.optics:We investigate the characteristics of guided wave modes in planar coupled waveguides. In particular, we calculate the dispersion relations for TM modes in which one or both of the guiding layers consists of negative index media (NIM)-where the permittivity and permeability are both negative. We find that the Poynting vector within the NIM waveguide axis can change sign and magnitude, a feature that is reflected in the dispersion curves.
physics.optics:It has recently been shown that periodic layered media can reflect strongly for all incident angles and polarizations in a given frequency range. The standard treatment gets these band gaps from an eigenvalue equation for the Bloch factor in an infinite periodic structure. We argue that such a procedure may become meaningless when dealing with structures with not very many periods. We propose an alternative approach based on a factorization of the multilayer transfer matrix in terms of three fundamental matrices of simple interpretation. We show that the trace of the transfer matrix sorts the periodic structures into three types with properties closely related to one (and only one) of the three fundamental matrices. We present the reflectance associated to each one of these types, which can be considered as universal features of the reflection in these media.
physics.optics:We study effects of finite height and surrounding material on photonic crystal slabs of one- and two-dimensional photonic crystals with a pseudo-spectral method and finite difference time domain simulation methods. The band gap is shown to be strongly modified by the boundary material. As an application we suggest reflection and guiding of light by patterning the material on top/below the slab.
physics.optics:The characteristics of an imaging system formed by a slab of a lossy left-handed material (LHM) are studied. The transfer function of the LHM imaging system is written in an appropriate product form with each term having a clear physical interpretation. A tiny loss of the LHM may suppress the transmission of evanescent waves through the LHM slab and this is explained physically. An analytical expression for the resolution of the imaging system is derived. It is shown that it is impossible to make a subwavelength imaging by using a realistic LHM imaging system unless the LHM slab is much thinner than the wavelength.
physics.optics:We observe the formation of an intense optical wavepacket fully localized in all dimensions, i.e. both longitudinally (in time) and in the transverse plane, with an extension of a few tens of fsec and microns, respectively. Our measurements show that the self-trapped wave is a X-shaped light bullet spontaneously generated from a standard laser wavepacket via the nonlinear material response (i.e., second-harmonic generation), which extend the soliton concept to a new realm, where the main hump coexists with conical tails which reflect the symmetry of linear dispersion relationship.
physics.optics:The statistical properties of nonlinear phase noise, often called the Gordon-Mollenauer effect, is studied analytically when the number of fiber spans is very large. The joint characteristic functions of the nonlinear phase noise with electric field, received intensity, and the phase of amplifier noise are all derived analytically. Based on the joint characteristic function of nonlinear phase noise with the phase of amplifier noise, the error probability of signal having nonlinear phase noise is calculated using the Fourier series expansion of the probability density function. The error probability is increased due to the dependence between nonlinear phase noise and the phase of amplifier noise. When the received intensity is used to compensate the nonlinear phase noise, the optimal linear and nonlinear minimum mean-square error compensators are derived analytically using the joint characteristic function of nonlinear phase noise and received intensity. Using the joint probability density of received amplitude and phase, the optimal maximum a posteriori probability detector is derived analytically. The nonlinear compensator always performs better than linear compensator.
physics.optics:The exact Green function for the scalar wave equation in a plane with any set of perfectly reflecting straight mirrors, which may be joined to form corners, is given as a diffraction scattering series. Instances would be slit diffraction in optics, or the Schrodinger equation inside (or outside) a general polygonal enclosure ('quantum polygon billiards'). The method is based on the seminal 1896 Riemann helicoid surface solution by Sommerfeld for optical diffraction by a single corner. It is generalised to account for multiple scatter by adapting the analysis of Stovicek for a closely related problem: a collection of magnetic flux lines (points) in a plane, the multi-flux Aharonov-Bohm effect. The short wavelength limit is shown to yield the 'geometrical theory of diffraction'. For slit diffraction the exact series is shown to coincide with that of Schwarzschild in 1902.
physics.optics:We experimentally demonstrate for the first time that a linearly polarized beam is focussed to an asymmetric spot when using a high-numerical aperture focussing system. This asymmetry was predicted by Richards and Wolf [Proc.R.Soc.London A, 253, 358 (1959)] and can only be measured when a polarization insensitive sensor is placed in the focal region. We used a specially modified photodiode in a knife edge type set up to obtain highly resolved images of the total electric energy density distribution at the focus. The results are in good agreement with the predictions of a vectorial focussing theory.
physics.optics:A simple model is used to estimate the Q factor in numerical simulations of differential phase shift keying (DPSK) with optical delay demodulation and balanced detection. It is found that an alternative definition of Q is needed for DPSK in order to have a more accurate prediction of the bit error ratio (BER).
physics.optics:With using of point-dipole model the theoretical calculations of main refractive indices and orientation of indicatrix of 18 minerals are performed. The feature of studied minerals is the statistically disordered arrangement of CO3, SO4, SO2, PO4 groups and also separate ions. The optical characters of uniaxial minerals and orientation of indicatrix of orthorhombic and monoclinic minerals, obtained by results of calculations, agree with experimental definitions.
physics.optics:The features of a compact, single pass, multi-pixel optical parametric generator are discussed. Several hundreds of independent high spatial-quality tunable ultrashort pulses were produced by pumping a bulk lithium triborate crystal with an array of tightly focussed intense beams. The array of beams was produced by shining a microlenses array with a large pump beam. Overall conversion efficiency to signal and idler up to 30% of the pump beam has been reported. Shot-to-shot energy fluctuation down to 3% was achieved for the generated radiation.
physics.optics:We use a spatially resolved cavity ring-down technique to show that the 2D eigenmode of an unstable optical cavity has a fractal pattern, i.e. it looks the same at different length scales. In agreement with theory, we find that this pattern has the maximum conceivable roughness, i.e., its fractal dimension is 3.01 plus\minus 0.04. This insight in the nature of unstable cavity eigenmodes may lead to better understanding of wave dynamics in open systems, for both light and matter waves.
physics.optics:Numerical Calculations are employed to study the modulation of light by surface acoustic waves (SAWs) in photonic band gap (PBG) structures. The on/off contrast ratio in PBG switch based on optical cavity is determined as a function of the SAW induced dielectric modulation. We show that these structures exhibit high contrast ratios even for moderate acousto-optic coupling
physics.optics:By combining the definition of the Wigner distribution function (WDF) and the matrix method of optical system modeling, we can evaluate the transformation of the former in centered systems with great complexity. The effect of stops and lens diameter are also considered and are shown to be responsible for non-linear clipping of the resulting WDF in the case of coherent illumination and non-linear modulation of the WDF when the illumination is incoherent. As an example, the study of a single lens imaging systems illustrates the applicability of the method.
physics.optics:Free-space propagation can be described as a shearing of the Wigner distribution function in the spatial coordinate; this shearing is linear in paraxial approximation but assumes a more complex shape for wide-angle propagation. Integration in the frequency domain allows the determination of near-field diffraction, leading to the well known Fresnel diffraction when small angles are considered and allowing exact prediction of wide-angle diffraction. The authors use this technique to demonstrate evanescent wave formation and diffraction elimination for very small apertures.
physics.optics:We reelaborate on the basic properties of lossless multilayers by using bilinear transformations. We study some interesting properties of the multilayer transfer function in the unit disk, showing that hyperbolic geometry turns out to be an essential tool for understanding multilayer action. We use a simple trace criterion to classify multilayers into three classes that represent rotations, translations, or parallel displacements. Moreover, we show that these three actions can be decomposed as a product of two reflections in hyperbolic lines. Therefore, we conclude that hyperbolic reflections can be considered as the basic pieces for a deeper understanding of multilayer optics.
physics.optics:In a coherent monoenergetic beam of non-interacting particles, the phase velocity and the particle transport velocity are functions of position, with the strongest variation being in the focal region. These velocities are everywhere parallel to each other, and their product is constant in space. For a coherent monochromatic electromagnetic beam, the energy transport velocity is never greater than the speed of light, and can even be zero. The phase velocities (one each for the non-zero components of the electric and magnetic fields, in general) can be different from each other and from the energy transport velocity, both in direction and in magnitude. The phase velocities at a given point are independent of time, for both particle and electromagnetic beams. The energy velocity is independent of time for the particle beam, but in general oscillates (with angular frequency 2w) in magnitude and direction about its mean value at a given point in the electromagnetic beam. However, there exist electromagnetic steady beams, within which the energy flux, energy density and energy velocity are all independent of time.
physics.optics:The polarization properties of monochromatic light beams are studied. In contrast to the idealization of an electromagnetic plane wave, finite beams which are everywhere linearly polarized in the same direction do not exist. Neither do beams which are everywhere circularly polarized in a fixed plane. It is also shown that transversely finite beams cannot be purely transverse in both their electric and magnetic vectors, and that their electromagnetic energy travels at less than c. The electric and magnetic fields in an electromagnetic beam have different polarization properties in general, but there exists a class of steady beams in which the electric and magnetic polarizations are the same (and in which energy density and energy flux are independent of time). Examples are given of exactly and approximately linearly polarized beams, and of approximately circularly polarized beams.
physics.optics:We suggest a geometrical framework to discuss periodic layered structures in the unit disk. The band gaps appear when the point representing the system approaches the unit circle. We show that the trace of the matrix describing the basic period allows for a classification in three families of orbits with quite different properties. The laws of convergence of the iterates to the unit circle can be then considered as universal features of the reflection.
physics.optics:The effect of the Kerr nonlinearity on linear non-diffractive Bessel beams is investigated analytically and numerically using the nonlinear Schr\"odinger equation. The nonlinearity is shown to primarily affect the central parts of the Bessel beam, giving rise to radial compression or decompression depending on whether the nonlinearity is focusing or defocusing, respectively. The dynamical properties of Gaussian-truncated Bessel beams are also analysed in the presence of a Kerr nonlinearity. It is found that although a condition for width balance in the root-mean-square sense exists, the beam profile becomes strongly deformed during propagation and may exhibit the phenomena of global and partial collapse.
physics.optics:We describe an optical technique based on the statistical analysis of the random intensity distribution due to the interference of the near-field scattered light with the strong transmitted beam. It is shown that, from the study of the two-dimensional power spectrum of the intensity, one derives the scattered intensity as a function of the scattering wave vector. Near-field conditions are specified and discussed. The substantial advantages over traditional scattering technique are pointed out, and is indicated that the technique could be of interest for wave lengths other than visible light.
physics.optics:The usual computation of the spontaneous emission uses a mixture of classical and quantum postulates. A purely classical computation shows that a source of electromagnetic field absorbs light in the eigenmode it is able to emit. Thus in an excitation by an other mode, the component of this mode on the eigenmode is absorbed, while the remainder is scattered. This loss of energy does not apply to the zero point field which has its regular energy in the eigenmode, so that the zero point field seems more effective than the other fields for the stimulation of light emission.
physics.optics:We perform numerical studies of the effect of sidewall imperfections on the resonant state broadening of the optical microdisk cavities for lasing applications. We demonstrate that even small edge roughness causes a drastic degradation of high-Q whispering gallery (WG) mode resonances reducing their Q-values by many orders of magnitude. At the same time, low-Q WG resonances are rather insensitive to the surface roughness. The results of numerical simulation obtained using the scattering matrix technique, are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surface of sections in the classical ray picture.
physics.optics:We consider plane waves propagating in quadratic nonlinear slab waveguides with nonlinear quasi-phase-matching gratings. We predict analytically and verify numerically the complete gain spectrum for transverse modulational instability, including hitherto undescribed higher order gain bands.
physics.optics:Optical near field has been generated by Laguarre-Gaussian doughnut beam on inner surface of "atom funnel". The resulting optical near field has been measured with the help of fiber probe and a consequent effect on cold atoms- released from MOT, has been estimated. Atoms with temperature less than 10 micro_kelvin can be reflected by the optical near field.
physics.optics:We report on an attempt to generate highly stable continuous terahertz (THz) wave by using optical frequency comb (OFC). About 10-nm wide OFC has been generated through a deep phase modulation of a 852 nm laser line in lithium niobate crystal cavity. The multiple optical modes (side bands) of the OFC, which are equally separated from each other by the modulation frequency (=6 GHz) are taken as the frequency reference. When another semiconductor laser is frequency locked, the stability of the difference frequency between the master laser and the second laser is improved on the same order of the RF modulator. An ultra-narrow line and tunable THz radiation source can be achieved by photomixing of this stable difference-frequency optical beat in a photoconductive antenna.
physics.optics:Doughnut shaped light beam has been generated from Gaussian mode ($TEM_{00}$) cw-Ti sapphire laser. After splitting the pump beam into two equal intensity components and introducing unequal convergence and phase delay while they are recombined it results in doughnut mode. Such a beam is tunable and have long propagation length. The evanescent field generated by 360 mW (at 780 nm wavelength) of such a beam creates optical field of 600 nm decay length with a 5.75 neV repulsive dipole potential. Thus cold Rb atoms (at 10{$\mu$}K or less temperature) released from MOT can be reflected by the surface so that the atoms are collected ultimately at the bottom of the prism. By focussing such doughnut beam with 8 cm focal length converging lens, the dark radius reduces to 22{$\mu$}. We also observe such beam to contain azimuthal phase as well as radial phase distribution.
physics.optics:Ferroelectric ordering, the electroclinic effect and chiral smectic C (SmC*) - smectic A (SmA*) phase transitions in thin planar ferroelectric liquid crystal (FLC) cells are studied by means of linear electrooptic and second harmonic generation techniques. The ferroelectric switching is detected in biased FLC cells by measuring azimuthal dependences of linear and nonlinear responses. The applied DC-electric field rotates the FLC symmetry axis with initial and final orientations in the cell plane. Comparative studies of the switching behavior in reflection and transmission allows to distinguish the contributions from the bulk and the sub-surface layers of the cell. The analysis of temperature dependence shows the existence of a strong surface coupling. The temperature dependent nonlinear polarization shows a critical behavior corresponding to the superfluid model.
physics.optics:We have generated high power doughnut beam suitable for atom funnel experiment with the conversion efficiency of about 50 %.
physics.optics:We consider large-mode area photonic crystal fibers for visible applications where micro-deformation induced attenuation becomes a potential problem when the effective area A_eff is sufficiently large compared to lambda^2. We argue how a slight increase in fiber diameter D can be used in screening the high-frequency components of the micro-deformation spectrum mechanically and we confirm this experimentally for both 15 and 20 micron core fibers. For typical bending-radii (R~16 cm) the operating band-width increases by ~3-400 nm to the low-wavelength side.
physics.optics:We investigate general properties of spatial 1-dimensional bright photorefractive solitons and suggest various analytical approximations for the soliton profile and the half width, both depending on an intensity parameter r.
physics.optics:A model for a non-Kerr cylindrical nematic fiber is presented. We use the multiple scales method to show the possibility of constructing different kinds of wavepackets of transverse magnetic (TM) modes propagating through the fiber. This procedure allows us to generate different hierarchies of nonlinear partial differential equations (PDEs) which describe the propagation of optical pulses along the fiber. We go beyond the usual weakly nonlinear limit of a Kerr medium and derive an extended Nonlinear Schrodinger equation (eNLS) with a third order derivative nonlinearity, governing the dynamics for the amplitude of the wavepacket. In this derivation the dispersion, self-focussing and diffraction in the nematic are taken into account. Although the resulting nonlinear $PDE$ may be reduced to the modified Korteweg de Vries equation (mKdV), it also has additional complex solutions which include two-parameter families of bright and dark complex solitons. We show analytically that under certain conditions, the bright solitons are actually double embedded solitons. We explain why these solitons do not radiate at all, even though their wavenumbers are contained in the linear spectrum of the system. Finally, we close the paper by making comments on the advantages as well as the limitations of our approach, and on further generalizations of the model and method presented.
physics.optics:The modal cut-off is investigated experimentally in a series of high quality non-linear photonic crystal fibers. We demonstrate a suitable measurement technique to determine the cut-off wavelength and verify it by inspecting the near field of the modes that may be excited below and above the cut-off. We observe a double peak structure in the cut-off spectra, which is attributed to a splitting of the higher order modes. The cut-off is measured for seven different fiber geometries with different pitches and relative hole size, and a very good agreement with recent theoretical work is found.
physics.optics:We address the long-standing unresolved problem concerning the V-parameter in a photonic crystal fiber (PCF). Formulate the parameter appropriate for a core-defect in a periodic structure we argue that the multi-mode cut-off occurs at a wavelength lambda* which satisfies V_PCF(lambda*)=pi. Comparing to numerics and recent cut-off calculations we confirm this result.
physics.optics:We propose in this article an unambiguous definition of the local density of electromagnetic states (LDOS) in a vacuum near an interface in an equilibrium situation at temperature $T$. We show that the LDOS depends only on the electric field Green function of the system but does not reduce in general to the trace of its imaginary part as often used in the literature. We illustrate this result by a study of the LDOS variations with the distance to an interface and point out deviations from the standard definition. We show nevertheless that this definition remains correct at frequencies close to the material resonances such as surface polaritons. We also study the feasability of detecting such a LDOS with apetureless SNOM techniques. We first show that a thermal near-field emission spectrum above a sample should be detectable and that this measurement could give access to the electromagnetic LDOS. It is further shown that the apertureless SNOM is the optical analog of the scanning tunneling microscope which is known to detect the electronic LDOS. We also discuss some recent SNOM experiments aimed at detecting the electromagnetic LDOS.
physics.optics:The nonlinearity of a transmission fiber may be compensated by a specialty fiber and an optical phase conjugator. Such combination may be used to pre-distort signals before each fiber span so to linearize an entire transmission line.
physics.optics:Two fiber lines may compensate each other for nonlinearity with the help of optical phase conjugation. The pair of fiber lines and the optical signals in them may be either mirror-symmetric or translationally symmetric about the conjugator.
physics.optics:We have developed a scattering-matrix approach for numerical calculation of resonant states and Q-values of a nonideal optical disk cavity of an arbitrary shape and of an arbitrary varying refraction index. The developed method has been applied to study the effect of surface roughness and inhomogeneity of the refraction index on Q-values of microdisk cavities for lasing applications. We demonstrate that even small surface roughness can lead to a drastic degradation of high-Q cavity modes by many orders of magnitude. The results of numerical simulation are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surfaces of section and Husimi distributions.
physics.optics:We analyse theoretically for the first time to our knowledge the perfect phase matching of guided TE and TM modes with a multilayer waveguide composed of linear isotropic dielectric materials. Alongside strict investigation into dispersion relations for multilayer systems, we give an explicit qualitative explanation for the phenomenon of mode matching on the basis of the standard one-dimensional homogenization technique, and discuss the minimum number of layers and the refractive index profile for the proposed device scheme. Direct applications of the scheme include polarization-insensitive, intermodal dispersion-free planar propagation, efficient fibre-to-planar waveguide coupling and, potentially, mode filtering. As a self-sufficient result, we present compact analytical expressions for the mode dispersion in a finite, N-period, three-layer dielectric superlattice.
physics.optics:The characteristic function of soliton phase jitter is found analytically when the soliton is perturbed by amplifier noise. In additional to that from amplitude jitter, the nonlinear phase noise due to frequency and timing jitter is also analyzed. Because the nonlinear phase noise is not Gaussian distributed, the overall phase jitter is also non-Gaussian. For a fixed mean nonlinear phase shift, the contribution of nonlinear phase noise from frequency and timing jitter decreases with distance and signal-to-noise ratio.
physics.optics:The field energy distributions and effective mode areas of silica-based photonic bandgap fibers with a honeycomb airhole structure in the cladding and an extra airhole defining the core are investigated. We present a generalization of the common effective area definition, suitable for the problem at hand, and compare the results for the photonic bandgap fibers with those of index-guiding microstructured fibers. While the majority of the field energy in the honeycomb photonic bandgap fibers is found to reside in the silica, a substantial fraction (up to ~30%) can be located in the airholes. This property may show such fibers particularly interesting for sensor applications, especially those based on nonlinear effects or interaction with other structures (e.g. Bragg gratings) in the glass.
physics.optics:The waveguiding properties of two silica-based airguiding photonic bandgap fiber designs are investigated with special emphasis on material effects. The nonlinear coefficients are found to be 1-2 orders of magnitude smaller than those obtained in index-guiding microstructured fibers with large mode areas. The material dispersion of silica makes a significant contribution to the total chromatic dispersion although less than 10% of the field energy is located in the silica regions of the fibers. These findings suggest that dispersion engineering through the choice of base material may be a possibility in this type of fibers.
physics.optics:Lithium thioindate (LiInS$_{2}$) is a new nonlinear chalcogenide biaxial material transparent from 0.4 to 12 $\mu$m, that has been successfully grown in large sizes and good optical quality. We report on new physical properties that are relevant for laser and nonlinear optics applications. With respect to AgGaS(e)$_2$ ternary chalcopyrite materials, LiInS$_{2}$ displays a nearly-isotropic thermal expansion behavior, a 5-times larger thermal conductivity associated with high optical damage thresholds, and an extremely low intensity-dependent absorption allowing direct high-power downconversion from the near-IR to the deep mid-IR. Continuous-wave difference-frequency generation (5-11$ \mu$m) of Ti:sapphire laser sources is reported for the first time.
physics.optics:We find that the function that describes the surface of spherical aberration free lenses can be used for both positive and negative refractive index media. With the inclusion of negative index, this function assumes the form of all the conic sections and expands the theory of aplanatic optical surfaces. There are two different symmetry centers with respect to the index that create an asymmetric relationship between positive and negative index lens profiles. In the thin lens limit the familiar formulas for image position and magnification hold for any index.
physics.optics:We report the surprising observation of directional tunneling escape from nearly spherical fused-silica optical resonators, in which most of the phase space is filled with nonchaotic regular trajectories. Experimental and theoretical studies of the dependence of the far-field emission pattern on both the degree of deformation and the excitation condition show that nonperturbative phase-space structures in the internal ray dynamics profoundly affect tunneling leakage of the whispering-gallery modes.
physics.optics:Intermodal interactions displayed through the phenomena of mode coupling and conversion in optical systems are treated by means of the Lindstedt-Poincare perturbation method of strained parameters more widely known in classical quantum mechanics and quantum chemistry as the stationary perturbation technique. The focus here is on the mode conversion at the points of virtual phase matching (otherwise called anticrossings or avoided crossings) associated with the maximum conversion efficiency. The method is shown to provide a convenient tool to deal with intermodal interactions at anticrossings -- interactions induced by any kind of perturbation in dielectric index profile of the waveguide, embracing optical inhomogeneity, magnetization of arbitrary orientation, and nonlinearity. Closed-form analytic expressions are derived for the minimum value of mode mismatch and for the length of complete mode conversion (the coupling length, or the beat length) in generic waveguiding systems exhibiting anticrossings. Demonstrating the effectiveness of the method, these general expressions are further applied to the case of TE -- TM mode conversion in (i) a multilayer gyrotropic waveguide under piecewise-constant, arbitrarily oriented magnetization, and (ii) an optically-inhomogeneous planar dielectric waveguide -- an example which the standard coupled-mode theory fails to describe.
physics.optics:We exploit a slightly noncollinear second-harmonic cross-correlation scheme to map the 3D space-time intensity distribution of an unknown complex-shaped ultrashort optical pulse. We show the capability of the technique to reconstruct both the amplitude and the phase of the field through the coherence of the nonlinear interaction down to a resolution of 10 $\mu$m in space and 200 fs in time. This implies that the concept of second-harmonic holography can be employed down to the sub-ps time scale, and used to discuss the features of the technique in terms of the reconstructed fields.
physics.optics:Nonlinear mode coupling in a coaxial waveguide filled with Faraday material has been considered. The picture of mode interaction is shown to resemble Coulomb interaction of charges: higher modes with nonzero angular momentum interact like effective charges via exchange of zero angular momentum quanta of the fundamental mode. Thus, at large distances this interaction becomes the dominant mechanism of mode coupling. The developed model may be used in designing coaxial photonic crystal fibers with strong tailored mode interaction.
physics.optics:We develop a general theory of spatial solitons in a liquid crystalline medium exhibiting a nonlinearity with an arbitrary degree of effective nonlocality. The model accounts the observability of "accessible solitons" and establishes an important link with parametric solitons.
physics.optics:Theoretical analysis is presented on quantum state evolution of polarization light waves at frequencies $\omega_{o}$ and $\omega_{e}$ in a periodically poled nonlinear crystal (PPNC). It is shown that the variances of all the four Stokes parameters can be squeezed.
physics.optics:Counter-propagating light fields have the ability to create self-organized one-dimensional optically bound arrays of microscopic particles, where the light fields adapt to the particle locations and vice versa. We develop a theoretical model to describe this situation and show good agreement with recent experimental data (Phys. Rev. Lett. 89, 128301 (2002)) for two and three particles, if the scattering force is assumed to dominate the axial trapping of the particles. The extension of these ideas to two and three dimensional optically bound states is also discussed.
physics.optics:The inversion of a diffraction pattern offers aberration-free diffraction-limited 3D images without the resolution and depth-of-field limitations of lens-based tomographic systems, the only limitation being radiation damage. We review our experimental results, discuss the fundamental limits of this technique and future plans.
physics.optics:The stability of two-dimensional bright vortex solitons in a media with focusing cubic and defocusing quintic nonlinearities is investigated analytically and numerically. It is proved that above some critical beam powers not only one- and two-charged but also multiple-charged stable vortex solitons do exist. A vortex soliton occurs robust with respect to symmetry-breaking modulational instability in the self-defocusing regime provided that its radial profile becomes flattened, so that a self-trapped wave beam gets a pronounced surface. It is demonstrated that the dynamics of a slightly perturbed stable vortex soliton resembles an oscillation of a liquid stream having a surface tension. Using the idea of sustaining effective surface tension for spatial vortex soliton in a media with competing nonlinearities the explanation of a suppression of the modulational instability is proposed.
physics.optics:Quasi error-free 10 Gbit/s data transmission is demonstrated over a novel type of 50 micron core diameter photonic crystal fiber with as much as 100 m length. Combined with 850$ nm VCSEL sources, this fiber is an attractive alternative to graded-index multi-mode fibers for datacom applications. A comparison to numerical simulations suggests that the high bit-rate may be partly explained by inter-modal diffusion.
physics.optics:The Casimir force between metallic plates made of realistic materials is evaluated for distances in the nanometer range. A spectrum over real frequencies is introduced and shows narrow peaks due to surface resonances (plasmon polaritons or phonon polaritons) that are coupled across the vacuum gap. We demonstrate that the Casimir force originates from the attraction (repulsion) due to the corresponding symmetric (antisymmetric) eigenmodes, respectively. This picture is used to derive a simple analytical estimate of the Casimir force at short distances. We recover the result known for Drude metals without absorption and compute the correction for weakly absorbing materials.
physics.optics:In recent years there has been an explosive development of interest in the measurement of forces at the microscopic level, such as within living cells, as well as the properties of fluids and suspensions on this scale, using optically trapped particles as probes. The next step would be to measure torques and associated rotational motion. This would allow measurement on very small scales since no translational motion is needed. It could also provide an absolute measurement of the forces holding a stationary non-rotating particle in place. The laser-induced torque acting on an optically trapped microscopic birefringent particle can be used for these measurements. Here we present a new method for simple, robust, accurate, simultaneous measurement of the rotation speed of a laser trapped birefringent particle, and the optical torque acting on it, by measuring the change in angular momentum of the light from passing through the particle. This method does not depend on the size or shape of the particle or the laser beam geometry, nor does it depend on the properties of the surrounding medium. This could allow accurate measurement of viscosity on a microscopic scale.
physics.optics:Optical tweezers are widely used for the manipulation of cells and their internal structures. However, the degree of manipulation possible is limited by poor control over the orientation of trapped cells. We show that it is possible to controllably align or rotate disc shaped cells - chloroplasts of Spinacia oleracea - in a plane polarised Gaussian beam trap, using optical torques resulting predominantly from circular polarisation induced in the transmitted beam by the non-spherical shape of the cells.
physics.optics:Optical trapping is a widely used technique, with many important applications in biology and metrology. Complete modelling of trapping requires calculation of optical forces, primarily a scattering problem, and non-optical forces. The T-matrix method is used to calculate forces acting on spheroidal and cylindrical particles.
physics.optics:Optical trapping, where microscopic particles are trapped and manipulated by light is a powerful and widespread technique, with the single-beam gradient trap (also known as optical tweezers) in use for a large number of biological and other applications.   The forces and torques acting on a trapped particle result from the transfer of momentum and angular momentum from the trapping beam to the particle.   Despite the apparent simplicity of a laser trap, with a single particle in a single beam, exact calculation of the optical forces and torques acting on particles is difficult. Calculations can be performed using approximate methods, but are only applicable within their ranges of validity, such as for particles much larger than, or much smaller than, the trapping wavelength, and for spherical isotropic particles.   This leaves unfortunate gaps, since wavelength-scale particles are of great practical interest because they are readily and strongly trapped and are used to probe interesting microscopic and macroscopic phenomena, and non-spherical or anisotropic particles, biological, crystalline, or other, due to their frequent occurance in nature, and the possibility of rotating such objects or controlling or sensing their orientation.   The systematic application of electromagnetic scattering theory can provide a general theory of laser trapping, and render results missing from existing theory. We present here calculations of force and torque on a trapped particle obtained from this theory and discuss the possible applications, including the optical measurement of the force and torque.
physics.optics:Multipole expansion of an incident radiation field - that is, representation of the fields as sums of vector spherical wavefunctions - is essential for theoretical light scattering methods such as the T-matrix method and generalised Lorenz-Mie theory (GLMT). In general, it is theoretically straightforward to find a vector spherical wavefunction representation of an arbitrary radiation field. For example, a simple formula results in the useful case of an incident plane wave. Laser beams present some difficulties. These problems are not a result of any deficiency in the basic process of spherical wavefunction expansion, but are due to the fact that laser beams, in their standard representations, are not radiation fields, but only approximations of radiation fields. This results from the standard laser beam representations being solutions to the paraxial scalar wave equation. We present an efficient method for determining the multipole representation of an arbitrary focussed beam.
physics.optics:The T-matrix method is widely used for the calculation of scattering by particles of sizes on the order of the illuminating wavelength. Although the extended boundary condition method (EBCM) is the most commonly used technique for calculating the T-matrix, a variety of methods can be used.   We consider some general principles of calculating T-matrices, and apply the point-matching method to calculate the T-matrix for particles devoid of symmetry. This method avoids the time-consuming surface integrals required by the EBCM.
physics.optics:Light-induced rotation of absorbing microscopic particles by transfer of angular momentum from light to the material raises the possibility of optically driven micromachines. The phenomenon has been observed using elliptically polarized laser beams or beams with helical phase structure. But it is difficult to develop high power in such experiments because of overheating and unwanted axial forces, limiting the achievable rotation rates to a few hertz. This problem can in principle be overcome by using transparent particles, transferring angular momentum by a mechanism first observed by Beth in 1936, when he reported a tiny torque developed in a quartz waveplate due to the change in polarization of transmitted light. Here we show that an optical torque can be induced on microscopic birefringent particles of calcite held by optical tweezers. Depending on the polarization of the incident beam, the particles either become aligned with the plane of polarization (and thus can be rotated through specified angles) or spin with constant rotation frequency. Because these microscopic particles are transparent, they can be held in three-dimensional optical traps at very high power without heating. We have observed rotation rates in excess of 350 Hz.
physics.optics:This report contains a tutorial introduction to the method of importance sampling. The use of this method is illustrated for simulations of the noise-induced energy jitter of return-to-zero pulses in optical communication systems.
physics.optics:The 3-dimensional coherence matrix is interpreted by emphasising its invariance with respect to spatial rotations. Under these transformations, it naturally decomposes into a real symmetric positive definite matrix, interpreted as the moment of inertia of the ensemble (and the corresponding ellipsoid), and a real axial vector, corresponding to the mean angular momentum of the ensemble. This vector and tensor are related by several inequalities, and the interpretation is compared to those in which unitary invariants of the coherence matrix are studied.
physics.optics:We propose novel multi-phase-matched process that starts with generation of a pair of symmetric second-harmonic waves. Each of them interacts again with the fundamental wave to produce two constructively interfering third harmonic waves collinear to the fundamental input wave.
physics.optics:We numerically calculate the equivalent mode-field radius of the fundamental mode in a photonic crystal fiber (PCF) and show that this is a function of the V-parameter only and not the relative hole size. This dependency is similar to what is found for graded-index standard fibers and we furthermore show that the relation for the PCF can be excellently approximated with the same general mathematical expression. This is to our knowledge the first semi-analytical description of the mode-field radius of a PCF.
physics.optics:The output spectrum of both gas and semiconductor lasers usually contains more than one frequency. Multimode operation in gas versus semiconductor lasers arises from different physics. In gas lasers, slow equilibration of the electron populations at different energies makes each frequency an independent single-mode laser. The slow electron diffusion in semiconductor lasers, combined with the spatially varying optical intensity patterns of the modes, makes each region of space an independent single-mode laser. We develop a rate equation model for the photon number in each mode which captures all these effects. Plotting the photon number versus pumping rate for the competing modes, in both subthreshold and above threshold operation, illustrates the changes in the laser output spectrum due to either slow equilibration or slow diffusion of electrons.
physics.optics:A cascaded iterative Fourier transform (CIFT) algorithm is presented for optical security applications. Two phase-masks are designed and located in the input and the Fourier domains of a 4-f correlator respectively, in order to implement the optical encryption or authenticity verification. Compared with previous methods, the proposed algorithm employs an improved searching strategy: modifying the phase-distributions of both masks synchronously as well as enlarging the searching space. Computer simulations show that the algorithm results in much faster convergence and better image quality for the recovered image. Each of these masks is assigned to different person. Therefore, the decrypted image can be obtained only when all these masks are under authorization. This key-assignment strategy may reduce the risk of being intruded.
physics.optics:Generic wave dislocations (phase singularities, optical vortices) in three dimensions have anisotropic local structure, which is analysed, with emphasis on the twist of surfaces of equal phase along the singular line, and the rotation of the local anisotropy ellipse (twirl). Various measures of twist and twirl are compared in specific examples, and a theorem is found relating the (quantised) topological twist and twirl for a closed dislocation loop with the anisotropy C line index threading the loop.
physics.optics:Diffraction is a fundamental property of light propagation. Owing to this phenomenon,light diffracts out in all directions when it passes through a subwavelength slit.This imposes a fundamental limit on the transverse size of a light beam at a given distance from the aperture. We show that a subwavelength-sized beam propagating without diffractive broadening can be produced in free space by the constructive interference of multiple beams of a Fresnel source of the respective high-refraction-index waveguide. Moreover, it is shown that such a source can be constructed not only for continuous waves, but also for ultra-short (near single-cycle) pulses. The results theoretically demonstrate the feasibility of completely diffraction-free subwavelength-beam optics, for both continuous waves and ultra-short pulses. The approach extends operation of the near-field subwavelength-beam optics, such as near-field scanning optical microscopy and spectroscopy,to the "not-too-distant" field regime (0.5 to about 10 wavelengths).
physics.optics:In recent years the topic of localized wave solutions of the homogeneous scalar wave equation, i.e., the wave fields that propagate without any appreciable spread or drop in intensity, has been discussed in many aspects in numerous publications. In this review the main results of this rather disperse theoretical material are presented in a single mathematical representation - the Fourier decomposition by means of angular spectrum of plane waves. This unified description is shown to lead to a transparent physical understanding of the phenomenon as such and yield the means of optical generation of such wave fields.
physics.optics:In recent experiments, localized and stationary pulses have been generated in second-order nonlinear processes with femtosecond pulses, whose asymptotic features relate with those of nondiffracting and nondispersing polychromatic Bessel beams in linear dispersive media. We investigate on the nature of these linear waves, and show that they can be identified with the X-shaped (O-shaped) modes of the hyperbolic (elliptic) wave equation in media with normal (anomalous) dispersion. Depending on the relative strengths of mode phase mismatch, group velocity mismatch with respect to a plane pulse, and of the defeated group velocity dispersion, these modes can adopt the form of pulsed Bessel beams, focus wave modes, and X-waves (O-waves), respectively.
physics.optics:We explain the main concepts centered around Sharafutdinov's ray transform, its kernel, and the extent to which it can be inverted. It is shown how the ray transform emerges naturally in any attempt to reconstruct optical and stress tensors within a photoelastic medium from measurements on the state of polarization of light beams passing through the strained medium. The problem of reconstruction of stress tensors is crucially related to the fact that the ray transform has a nontrivial kernel; the latter is described by a theorem for which we provide a new proof which is simpler and shorter as in Sharafutdinov's original work, as we limit our scope to tensors which are relevant to Photoelasticity. We explain how the kernel of the ray transform is related to the decomposition of tensor fields into longitudinal and transverse components. The merits of the ray transform as a tool for tensor reconstruction are studied by walking through an explicit example of reconstructing the $\sigma_{33}$-component of the stress tensor in a cylindrical photoelastic specimen. In order to make the paper self-contained we provide a derivation of the basic equations of Integrated Photoelasticity which describe how the presence of stress within a photoelastic medium influences the passage of polarized light through the material.
physics.optics:The controversial term "nondiffracting beam" was introduced into optics by Durnin in 1987. Discussions related to that term revived interest in problems of the light diffraction and resulted in an appearance of the new research direction of the classical optics dealing with the localized transfer of electromagnetic energy. In the paper, the physical concept of the nondiffracting propagation is presented and the basic properties of the nondiffracting beams are reviewed. Attention is also focused to the experimental realization and to applications of the nondiffracting beams.
physics.optics:Theory of the optical parametric amplification at high-frequency pumping in crystals with a regular space modulation of the sign of nonlinear coupling coefficient of interacting waves is developed. By applying the matrix method, the theory is based on a step-by-step approach. It is shown that, in the case where the pumping intensity is less than some critical value, the spatial dynamics of the signal intensity inside a separate layer with the constant nonlinear coefficient has an oscillatory behavior and the change of the signal intensity from layer to layer is defined, in general, by the power function. The same law is valid for the change of variance of signal's quadrature components. At large number of layers, these dependences can be reduced to the well-known ones for homogeneous nonlinear optical crystals.
physics.optics:We show how it is possible to controllably rotate or align microscopic particles of isotropic nonabsorbing material in a TEM00 Gaussian beam trap, with simultaneous measurement of the applied torque using purely optical means. This is a simple and general method of rotation, requiring only that the particle is elongated along one direction. Thus, this method can be used to rotate or align a wide range of naturally occurring particles. The ability to measure the applied torque enables the use of this method as a quantitative tool--the rotational equivalent of optical tweezers based force measurement. As well as being of particular value for the rotation of biological specimens, this method is also suitable for the development of optically-driven micromachines.
physics.optics:The near-field diffraction of fs and sub-fs light pulses by nm-size slit-type apertures and its implication for near-field scanning optical microscopy (NSOM) is analyzed. The amplitude distributions of the diffracted wave-packets having the central wavelengths in the visible spectral region are found by using the Neerhoff and Mur coupled integral equations, which are solved numerically for each Fourier's component of the wave-packet. In the case of fs pulses, the duration and transverse dimensions of the diffracted pulse remain practically the same as that of the input pulse. This demonstrates feasibility of the NSOM in which a fs pulse is used to provide the fs temporal resolution together with nm-scale spatial resolution. In the sub-fs domain, the Fourier spectrum of the transmitted pulse experiences a considerable narrowing that leads to the increase of the pulse duration in a few times. This imposes a limit on the simultaneous resolutions in time and space.
physics.optics:We show theoretically and demonstrate experimentally that highly absorbing particles can be trapped and manipulated in a single highly focused Gaussian beam. Our studies of the effects of polarized light on such particles show that they can be set into rotation by elliptically polarized light and that both the sense and the speed of their rotation can be smoothly controlled.
physics.optics:Wolf discovered how the spatial coherence characteristics of the source affect the spectrum of the radiation in the far zone. In particular the spatial coherence of the source can result either in red or blue shifts in the measured spectrum.His predictions have been verified in a large number of different classes of systems. Wolf and coworkers usually assume a given form of source correlations and study its consequence. In this paper we consider microscopic origin of spatial coherence and radiation from a system of atoms. We discuss how the radiation is different from that produced from an independent system of atoms. We show that the process of radiation itself is responsible for the creation of spatial correlations within the source. We present different features of the spectrum and other statistical properties of the radiation, which show strong dependence on the spatial correlations. We show the existence of a new type of two-photon resonance that arises as a result of such spatial correlations. We further show how the spatial coherence of the field can be used in the context of radiation generated by nonlinear optical processes. We conclude by demonstrating the universality of Wolf shifts and its application in the context of pulse propagation in a dispersive medium.
physics.optics:We experimentally demonstrate for the first time that a radially polarized field can be focussed to a spot size significantly smaller (0.16(1) lambda^2) than for linear polarization (0.26 lambda^2). The effect of the vector properties of light is shown by a comparison of the focal intensity distribution for radially and azimuthally polarized input fields. For strong focusing a radially polarized field leads to a longitudinal electric field component at the focus which is sharp and centered at the optical axis. The relative contribution of this component is enhanced by using an annular aperture.
physics.optics:Two strong simultaneous resonances of scattering--double-resonant extremely asymmetrical scattering (DEAS)--are predicted in two parallel, oblique, periodic Bragg arrays separated by a gap, when the scattered wave propagates parallel to the arrays. One of these resonances is with respect to frequency (which is common to all types of Bragg scattering), and another is with respect to phase variation between the arrays. The diffractional divergence of the scattered wave is shown to be the main physical reason for DEAS in the considered structure. Although the arrays are separated, they are shown to interact by means of the diffractional divergence of the scattered wave across the gap from one array into the other. It is also shown that increasing separation between the two arrays results in a broader and weaker resonance with respect to phase shift. The analysis is based on a recently developed new approach allowing for the diffractional divergence of the scattered wave inside and outside the arrays. Physical interpretations of the predicted features of DEAS in separated arrays are also presented. Applicability conditions for the developed theory are derived.
physics.optics:Radiation pressure forces in a focussed laser beam can be used to trap microscopic absorbing particles against a substrate. Calculations based on momentum transfer considerations show that stable trapping occurs before the beam waist, and that trapping is more effective with doughnut beams. Such doughnut beams can transfer angular momentum leading to rotation of the trapped particles. Energy is also transferred, which can result in heating of the particles to temperatures above the boiling point of the surrounding medium.
physics.optics:Extremely asymmetrical scattering (EAS) is a highly resonant type of Bragg scattering with a strong resonant increase of the scattered wave amplitude inside and outside the grating. EAS is realized when the scattered wave propagates parallel to the grating boundaries. We present a rigorous algorithm for the analysis of non-steady-state EAS, and investigate the relaxation of the incident and scattered wave amplitudes to their steady-state values. Non-steady-state EAS of bulk TE electromagnetic waves is analyzed in narrow and wide, slanted, holographic gratings. Typical relaxation times are determined and compared with previous rough estimations. Physical explanation of the predicted effects is presented.
physics.optics:We demonstrate generation and frequency doubling of unit charge vortices in a linear astigmatic resonator. Topological instability of the double charge harmonic vortices leads to well separated vortex cores that are shown to rotate, and become anisotropic, as the resonator is tuned across resonance.
physics.optics:We have developed a new method based on two cavities containing $\chi^{(2)}$ media to reshape optical pulses by an all-optical technique. The system is entirely passive \emph{i.e.}, all the energy is brought by the incoming pulse and uses two successive optical cavities with independent thresholds. The output pulse is close to a rectangular shape. We show that this technique could be extended to high bit rates and telecommunication wavelength using very small cavities containing current nonlinear materials.
physics.optics:We show that useful non-instantaneous nonlinear phase shifts can be obtained from cascaded quadratic processes in the presence of group velocity mismatch. The two-field nature of the process permits responses that can be effectively advanced or retarded in time with respect to one of the fields. There is an analogy to a generalized Raman-scattering effect, permitting both red and blue shifts of short pulses. We expect this capability to have many applications in short-pulse generation and propagation, such as the compensation of Raman-induced effects and high-quality pulse compression, which we discuss.
physics.optics:A method of formation of the tightly confined distortion-free fs pulses with the step-like decreasing of intensity under the finite-length propagation in free space is described. Such pulses are formed by the Fresnel source of a high refraction-index waveguide. The source reproduces in free space a propagation-invariant (distortion-free) pulse confined by the waveguide. Converse to the case of material waveguides, when the pulse goes out from the Fresnel (virtual) waveguide its shape is not changed, but the intensity immediately drops down to the near-zero level.
physics.optics:Based on a recent formulation of the V-parameter of a photonic crystal fiber we provide numerically based empirical expressions for this quantity only dependent on the two structural parameters - the air hole diameter and the hole-to-hole center spacing. Based on the unique relation between the V-parameter and the equivalent mode field radius we identify how the parameter space for these fibers is restricted in order for the fibers to remain single mode while still having a guided mode confined to the core region.
physics.optics:The alternative to dynamic alignment explanation of experimental results on spatial-asymmetric dissociation of molecules in a laser field is proposed. The concept of geometrical alignment is sufficient for explanation of these results. In this case the spatial anisotropy of interaction of molecules with laser radiation is transferred from one field to another through the ordinary mechanism of nonlinear optical interactions. Thus laser radiation does not create alignment, but only registers it. A physical basis of such nonlinear processes is inequality of forward and reversed optical transitions that corresponds to a concept of time invariance violation in electromagnetic interactions. Directions of the further researches in the field of alignment spectroscopy are discussed.
physics.optics:Periodic layered media can reflect strongly for all incident angles and polarizations in a given frequency range. Quarter-wave stacks at normal incidence are commonplace in the design of such omnidirectional reflectors. We discuss alternative design criteria to optimize these systems.
physics.optics:We study propagation of a pair of oppositely charged and mutually incoherent vortices in anisotropic nonlinear optical media. Mutual interactions retard the delocalization of the vortex core observed for isolated vortices.
physics.optics:The efficiency of evanescent coupling between a silica optical fiber taper and a silicon photonic crystal waveguide is studied. A high reflectivity mirror on the end of the photonic crystal waveguide is used to recollect, in the backwards propagating fiber mode, the optical power that is initially coupled into the photonic crystal waveguide. An outcoupled power in the backward propagating fiber mode of 88% of the input power is measured, corresponding to a lower bound on the coupler efficiency of 94%.
physics.optics:The reversible phase transition induced by femtosecond laser excitation of Gallium has been studied by measuring the dielectric function at 775 nm with ~ 200 fs temporal resolution. The real and imaginary parts of the transient dielectric function were calculated from absolute reflectivity of Gallium layer measured at two different angles of incidence, using Fresnel formulas. The time-dependent electron-phonon effective collision frequency, the heat conduction coefficient and the volume fraction of a new phase were restored directly from the experimental data, and the time and space dependent electron and lattice temperatures in the layer undergoing phase transition were reconstructed without ad hoc assumptions. We converted the temporal dependence of the electron-phonon collision rate into the temperature dependence, and demonstrated, for the first time, that the electron-phonon collision rate has a non-linear character. This temperature dependence converges into the known equilibrium function during the cooling stage. The maximum fraction of a new phase in the laser-excited Gallium layer reached only 60% even when the deposited energy was two times the equilibrium enthalpy of melting. We have also demonstrated that the phase transition pace and a fraction of the transformed material depended strongly on the thickness of the laser-excited Gallium layer, which was of the order of several tens of nanometers for the whole range of the pump laser fluencies up to the damage threshold. The kinetics of the phase transformation after the laser excitation can be understood on the basis of the classical theory of the first-order phase transition while the duration of non-thermal stage appears to be comparable to the sub-picosecond pulse length.
physics.optics:The derivation of a new condition for characterizing isotropic dielectric-magnetic materials exhibiting negative phase velocity, and the equivalence of that condition with previously derived conditions, are presented.
physics.optics:Linear and nonlinear directional couplers are currently used in fiber optics communications. They may also play a role in multiphoton approaches to quantum information processing if accurate control is obtained over the phases and polarizations of the signals at the output of the coupler. With this motivation, the constants of motion of the coupler equation are used to obtain an explicit analytical solution for the nonlinear coupler.
physics.optics:A single-mode all-silica photonic crystal fiber with an effective area of 600 square-micron and low bending loss is demonstrated. The fiber is characterized in terms of attenuation, chromatic dispersion and modal properties.
physics.optics:Based on the Wigner distribution approach, an analysis of the effect of partial incoherence on the transverse instability of soliton structures in nonlinear Kerr media is presented. It is explicitly shown, that for a Lorentzian incoherence spectrum the partial incoherence gives rise to a damping which counteracts, and tends to suppress, the transverse instability growth. However, the general picture is more complicated and it is shown that the effect of the partial incoherence depends crucially on the form of the incoherence spectrum. In fact, for spectra with finite rms-width, the partial incoherence may even increase both the growth rate and the range of unstable, transverse wave numbers.
physics.optics:We study theoretically and experimentally the modulational instability of broad optical beams in photorefractive nonlinear media. We demonstrate the impact of the anisotropy of the nonlinearity on the growth rate of periodic perturbations. Our findings are confirmed by experimental measurements in a strontium barium niobate photorefractive crystal.
physics.optics:Coherent Anti-Stokes Raman Scattering (CARS) processes are ``coherent,'' but the phase of the anti-Stokes radiation is usually lost by most incoherent spectroscopic CARS measurements. We propose a novel Raman microscopy imaging method called Nonlinear Interferometric Vibrational Imaging, which measures Raman spectra by obtaining the temporal anti-Stokes signal through nonlinear interferometry. With a more complete knowledge of the anti-Stokes signal, we show through simulations that a high-resolution Raman spectrum can be obtained of a molecule in a single pulse using broadband radiation. This could be useful for identifying the three-dimensional spatial distribution of molecular species in tissue.
physics.optics:An examination of the propagation of intense 200 fs pulses in water reveals light filaments not sustained by the balance between Kerr-induced self-focusing and plasma-induced defocusing. Their appearance is interpreted as the consequence of a spontaneous reshaping of the wave packet form a gaussian into a conical wave, driven by the requirement of maximum localization, minimum losses and stationarity in the presence of non-linear absorption.
physics.optics:In positive phase-mismatched SHG and normal dispersion, a gaussian spatio-temporal pulse transforms spontaneously into a X-pulse, underlies spatio-temporal compression and eventually leads to stationary 3-D propagation. Experimental and numerical data are provided
physics.optics:It is shown that a system of two coupled planar material sheets possessing surface mode (polariton) resonances can be used for the purpose of evanescent field restoration and, thus, for the sub-wavelength near-field imaging. The sheets are placed in free space so that they are parallel and separated by a certain distance. Due to interaction of the resonating surface modes (polaritons) of the sheets an exponential growth in the amplitude of an evanescent plane wave coming through the system can be achieved. This effect was predicted earlier for backward-wave (double-negative or Veselago) slab lenses. The alternative system considered here is proved to be realizable at microwaves by grids or arrays of resonant particles. The necessary electromagnetic properties of the resonating grids and the particles are investigated and established. Theoretical results are supported by microwave experiments that demonstrate amplification of evanescent modes.
physics.optics:Motivated by recent experimental work by Folkenberg et al. we consider the effect of weak disorder in the air-hole lattice of small-core photonic crystal fibers. We find that the broken symmetry leads to higher-order modes which have generic intensity distributions resembling those found in standard fibers with elliptical cores. This explains why recently reported experimental higher-order mode profiles appear very different from those calculated numerically for ideal photonic crystal fibers with inversion and six-fold rotational symmetry. The splitting of the four higher-order modes into two groups fully correlates with the observation that these modes have different cut-offs.
physics.optics:We present a numerical investigation of the ray dynamics in a paraxial optical cavity when a ray splitting mechanism is present. The cavity is a conventional two-mirror stable resonator and the ray splitting is achieved by inserting an optical beam splitter perpendicular to the cavity axis. We show that depending on the position of the beam splitter the optical resonator can become unstable and the ray dynamics displays a positive Lyapunov exponent.
physics.optics:This paper analyses theoretically and numerically the effect of varying grating amplitude on the extremely asymmetrical scattering (EAS) of bulk and guided optical modes in non-uniform strip-like periodic Bragg arrays with stepwise and gradual variations in the grating amplitude across the array. A recently developed new approach based on allowance for the diffractional divergence of the scattered wave is used for this analysis. It is demonstrated that gradual variations in magnitude of the grating amplitude may change the pattern of EAS noticeably but not radically. On the other hand, phase variations in the grating may result in a radically new type of Bragg scattering - double-resonant EAS (DEAS). In this case, a combination of two strong simultaneous resonances (one with respect to frequency, and another with respect to the phase variation) is predicted to take place in non-uniform arrays with a step-like phase and gradual magnitude variations of the grating amplitude. The tolerances of EAS and DEAS to small gradual variations in the grating amplitude are determined. The main features of these types of scattering in non-uniform arrays are explained by the diffractional divergence of the scattered wave inside and outside the array.
physics.optics:We present a symmetry-based theory of the depolarization induced by subwavelength metal hole arrays. We derive the Mueller matrices of hole arrays with various symmetries (in particular square and hexagonal) when illuminated by a finite-diameter (e.g. gaussian) beam. The depolarization is due to a combination of two factors: (i) propagation of surface plasmons along the surface of the array, (ii) a spread of wave vectors in the incident beam.
physics.optics:We investigate the ray dynamics in an optical cavity when a ray splitting mechanism is present. The cavity is a conventional two-mirror stable resonator and the ray splitting is achieved by inserting an optical beam splitter perpendicular to the cavity axis. Using Hamiltonian optics, we show that such a simple device presents a surprisingly rich chaotic ray dynamics.
physics.optics:We concentrate on the forces and torques exerted on transparent and absorbing particles trapped in laser beams containing optical vortices. We review previous theoretical and experimental work and then present new calculations of the effect of vortex beams on absorbing particles.
physics.optics:We have monitored the space-time transformation of 150-fs pulse, undergoing self-focusing and filamentation in water, by means of the nonlinear gating tech- nique. We have observed that pulse splitting and subsequent recombination apply to axial temporal intensity only, whereas space-integrated pulse profile preserves its original shape.
physics.optics:It is shown that three-dimensional nonparaxial beams are described by the oblate spheroidal exact solutions of the Helmholtz equation. For the first time, their beam behaviour is investigated and their corresponding parameters are defined. Using the fact that the beam width of the family of paraxial Gaussian beams is described by an hyperbola, the connection between the physical parameters of nonparaxial spheroidal beam solutions and those of paraxial beams is formally stablished. These results are also helpful to investigate the exact vector nonparaxial beams.
physics.optics:The images of the silicon test object has been carried out. An image for in-line hard X-ray hologram is presented. The transmission of a hologram image for hard X-ray radiation using Fresnel phase zone plate has been investigated.
physics.optics:We develop a theory of light transmission through an aperture-type near-field optical probe with a dissipative matter in its semiconducting core described by a complex frequency-dependent dielectric function. We evaluate the near-field transmission coefficient of a metallized silicon probe with a large taper angle of in the visible and near-infrared wavelength range. It is shown that in this spectral range the use of a short silicon probe instead of a glass one allows to achieve a strong (up to 10$^2-10^{3}$) enhancement in the transmission efficiency.
physics.optics:A metal nanoparticle plasmon waveguide for electromagnetic energy transport utilizing dispersion engineering to dramatically increase lateral energy confinement via a two-dimensional pattern of Au dots on an optically thin Si membrane is described. Using finite-difference time-domain simulations and coupled-mode theory, we show that phase-matched evanescent excitation from conventional fiber tapers is possible with efficiencies > 90 % for realistic geometries. Energy loss in this waveguide is mainly due to material absorption, allowing for 1/e energy decay distances of about 2 mm for excitation at telecommunication frequencies. This concept can be extended to the visible regime and promises applications in optical energy guiding, optical sensing, and switching.
physics.optics:A two-dimensional photonic crystal microcavity design supporting a wavelength-scale volume resonant mode with a calculated quality factor (Q) insensitive to deviations in the cavity geometry at the level of Q~2x10^4 is presented. The robustness of the cavity design is confirmed by optical fiber-based measurements of passive cavities fabricated in silicon. For microcavities operating in the lambda = 1500 nm wavelength band, quality factors between 1.3-4.0x10^4 are measured for significant variations in cavity geometry and for resonant mode normalized frequencies shifted by as much as 10% of the nominal value.
physics.optics:e investigate both experimentally and theoretically the waveguiding properties of a novel double trench waveguide where a conventional single-mode strip waveguide is embedded in a two dimensional photonic crystal (PhC) slab formed in silicon on insulator (SOI) wafers. We demonstrate that the bandwidth for relatively low-loss (50dB/cm) waveguiding is significantly expanded to 250nm covering almost all the photonic band gap owing to nearly linear dispersion of the TE-like waveguiding mode. The flat transmission spectrum however is interrupted by numerous narrow stop bands. We found that these stop bands can be attributed to anti-crossing between TE-like (positive parity) and TM-like (negative parity) modes. This effect is a direct result of the strong asymmetry of the waveguides that have an upper cladding of air and lower cladding of oxide. To our knowledge this is the first demonstration of the effects of cladding asymmetry on the transmission characteristics of the PhC slab waveguides.
physics.optics:In this paper, we present an approximate expression for determining the effective permittivity describing the coherent propagation of an electromagnetic wave in random media. Under the Quasicrystalline Coherent Potential Approximation (QC-CPA), it is known that multiple scattering theory provided an expression for this effective permittivity. The numerical evaluation of this one is, however, a challenging problem. To find a tractable expression, we add some new approximations to the (QC-CPA) approach. As a result, we obtained an expression for the effective permittivity which contained at the same time the Maxwell-Garnett formula in the low frequency limit, and the Keller formula, which has been recently proved to be in good agreement for particles exceeding the wavelength.
physics.optics:We investigate the electromagnetic propagation in two-dimensional photonic crystals, formed by parallel dielectric cylinders embedded a uniform medium. The frequency band structure is computed using the standard plane-wave expansion method, while the propagation and scattering of the electromagnetic waves are calculated by the multiple scattering theory. It is shown that within partial bandgaps, the waves tend to bend away from the forbidden directions. Such a property may render novel applications in manipulating optical flows. In addition, the relevance with the imaging by flat photonic crystal slabs will also be discussed.
physics.optics:This paper is the result of setting up GRENOUILLE in the Nonlinear Dynamics Laboratory at the University of Maryland at College Park. With the experience acquired in the process of setting up GRENOUILLE, this manual was compiled from literature and from hand-on experience to serve as a quick guide, a step-by-step help to construct GRENOUILLE and to understand some of its basic principles.
physics.optics:Accurate knowledge of absorption coefficient of a sample is a prerequisite for measuring the third order optical nonlinearity of materials, which can be a serious limitation for unknown samples. We introduce a method, which measures both the absorption coefficient and the third order optical nonlinearity of materials with high sensitivity in a single experimental arrangement. We use a dual-beam pump-probe experiment and conventional single-beam z-scan under different conditions to achieve this goal. We also demonstrate a counterintuitive coupling of the non-interacting probe-beam with the pump-beam in pump-probe z-scan experiment.
physics.optics:The concept of a plane scatterer that was developed earlier for scalar waves is generalized so that polarization of light is included. Starting from a Lippmann-Schwinger formalism for vector waves, we show that the Green function has to be regularized before T-matrices can be defined in a consistent way. After the regularization, optical modes and Green functions are determined exactly for finite structures built up of an arbitrary number of parallel planes, at arbitrary positions, and where each plane can have different optical properties. The model is applied to the special case of finite crystals consisting of regularly spaced identical planes, where analytical methods can be taken further and only light numerical tasks remain. The formalism is used to calculate position- and orientation-dependent spontaneous-emission rates inside and near the finite photonic crystals. The results show that emission rates and reflection properties can differ strongly for scalar and for vector waves. The finite size of the crystal influences the emission rates. For parallel dipoles close to a plane, emission into guided modes gives rise to a peak in the frequency-dependent emission rate.
physics.optics:Single-shot ultrafast absorbance spectroscopy based on the frequency encoding of the kinetics is analyzed theoretically and implemented experimentally. The kinetics are sampled in the frequency domain using linearly chirped, amplified 33 fs FWHM pulses derived from a Ti:sapphire laser. A variable length grating pair compressor is used to achieve the time resolution of 500-1000 channels per a 2-to-160 ps window with sensitivity > 5x10-4. In terms of the acquisition time, FDSS has an advantage over the pump-probe spectroscopy in a situation when the "noise" is dominated by amplitude variations of the signal, due to the pump and flow instabilities. The possibilities of FDSS are illustrated with the kinetics obtained in multiphoton ionization of water and aqueous iodide and one-photon excitation of polycrystalline ZnSe and thin-film amorphous Si:H. Unlike other "single-shot" techniques, FDSS can be implemented for fluid samples flowing in a high-speed jet and for thin solid samples that exhibit interference fringes; no a priori knowledge of the excitation profile of the pump across the beam is needed. Another advantage is that due to the interference of quasimonochromatic components of the chirped probe pulse, an oscillation pattern near the origin of the FDSS kinetics emerges. This pattern is unique and can be used to determine the complex dielectric function of the photogenerated species.
physics.optics:Single-shot ultrafast absorbance spectroscopy based on the frequency encoding of the kinetics is analyzed theoretically and implemented experimentally. In Part II of the series, arbitrary thickness sample is analysed theroretically. The model is then used to simulate the results for a-Si:H films.
physics.optics:We present a unifying point of view which allows to understand spectral features reported in recent experiments with two-dimensional arrays of subwavelength holes in metal films. We develop a Fano analysis of the related scattering problem by distinguishing two interfering contributions to the transmission process, namely a non-resonant contribution (direct scattering) and a resonant contribution (surface plasmon excitation). The introduction of a coupling strength between these two contributions naturally induces resonance shifts and asymmetry of profiles which satisfy simple scaling relations. We also report an experiment to confirm this analysis.
physics.optics:We study the dispersion and leakage properties for the recently reported low-loss photonic band-gap fiber by Smith et al. [Nature 424, 657 (2003)]. We find that surface modes have a significant impact on both the dispersion and leakage properties of the fundamental mode. Our dispersion results are in qualitative agreement with the dispersion profile reported recently by Ouzounov et al. [Science 301, 1702 (2003)] though our results suggest that the observed long-wavelength anomalous dispersion is due to an avoided crossing (with surface modes) rather than band-bending caused by the photonic band-gap boundary of the cladding.
physics.optics:Micron-scale optical cavities are produced using a combination of template sphere self-assembly and electrochemical growth. Transmission measurements of the tunable microcavities show sharp resonant modes with a Q-factor>300, and 25-fold local enhancement of light intensity. The presence of transverse optical modes confirms the lateral confinement of photons. Calculations show sub-micron mode volumes are feasible. The small mode volume of these microcavities promises to lead to a wide range of applications in microlasers, atom optics, quantum information, biophotonics and single molecule detection.
physics.optics:We experimentally compare the optical bandwidth of a conventional single-mode fiber (SMF) with 3 different photonic crystal fibers (PCF) all optimized for visible applications. The spectral attenuation, single-turn bend loss, and mode-field diameters (MFD) are measured and the PCF is found to have a significantly larger bandwidth than the SMF for an identical MFD. It is shown how this advantage can be utilized for realizing a larger MFD for the PCF while maintaining a bending resistant fiber.
physics.optics:We consider an air-silica honeycomb lattice and demonstrate a new approach to the formation of a core defect. Typically, a high or low-index core is formed by adding a high-index region or an additional air-hole (or other low-index material) to the lattice, but here we discuss how a core defect can be formed by manipulating the cladding region rather than the core region itself. Germanium-doping of the honeycomb lattice has recently been suggested for the formation of a photonic band-gap guiding silica-core and here we experimentally demonstrate how an index-guiding silica-core can be formed by fluorine-doping of the honeycomb lattice.
physics.optics:Self-similar propagation of ultrashort, parabolic pulses in a laser resonator is observed theoretically and experimentally. This constitutes a new type of pulse-shaping in modelocked lasers: in contrast to the well-known static (soliton-like) and breathing (dispersion-managed soliton) pulse evolutions, asymptotic solutions to the nonlinear wave equation that governs pulse propagation in most of the laser cavity are observed. Stable self-similar pulses exist with energies much greater than can be tolerated in soliton-like pulse shaping, and this will have implications for practical lasers.
physics.optics:The existence of resonant enhanced transmission and collimation of light waves by subwavelength slits in metal films [for example, see T.W. Ebbesen et al., Nature (London) 391, 667 (1998) and H.J. Lezec et al., Science, 297, 820 (2002)] leads to the basic question: Can a light be enhanced and simultaneously localized in space and time by a subwavelength slit? To address this question, the spatial distribution of the energy flux of an ultrashort (femtosecond) wave-packet diffracted by a subwavelength (nanometer-size) slit was analyzed by using the conventional approach based on the Neerhoff and Mur solution of Maxwell's equations. The results show that a light can be enhanced by orders of magnitude and simultaneously localized in the near-field diffraction zone at the nm- and fs-scales. Possible applications in nanophotonics are discussed.
physics.optics:We demonstrate an optical system that can apply and accurately measure the torque exerted by the trapping beam on a rotating birefringent probe particle. This allows the viscosity and surface effects within liquid media to be measured quantitatively on a micron-size scale using a trapped rotating spherical probe particle. We use the system to measure the viscosity inside a prototype cellular structure.
physics.optics:The core theorem on which the above paper is centred - that a perfectly conducting body of revolution absorbs no angular momentum from an axisymmetric electromagnetic wave field - is in fact a special case of a more general result in electromagnetic scattering theory. In addition, the scaling of the efficiency of transfer of angular momentum to an object with the wavelength and object size merits further discussion. Finally, some comments are made on the choice of terminology and the erroneous statement that a circularly polarized plane wave does not carry angular momentum.
physics.optics:The propagation of plane waves in a Faraday chiral medium is investigated. Conditions for the phase velocity to be directed opposite to the direction of power flow are derived for propagation in an arbitrary direction; simplified conditions which apply to propagation parallel to the distinguished axis are also established. These negative phase-velocity conditions are explored numerically using a representative Faraday chiral medium, arising from the homogenization of an isotropic chiral medium and a magnetically biased ferrite. It is demonstrated that the phase velocity may be directed opposite to power flow, provided that the gyrotropic parameter of the ferrite component medium is sufficiently large compared with the corresponding nongyrotropic permeability parameters.
physics.optics:Second harmonic optical coherence tomography, which uses coherence gating of second-order nonlinear optical response of biological tissues for imaging, is described and demonstrated. Femtosecond laser pulses were used to excite second harmonic waves from collagen harvested from rat tail tendon and a reference nonlinear crystal. Second harmonic interference fringe signals were detected and used for image construction. Because of the strong dependence of second harmonic generation on molecular and tissue structures, this technique offers contrast and resolution enhancement to conventional optical coherence tomography.
physics.optics:Considering the diffraction of a plane wave by a periodically corrugated half-space, we show that the transformation of the refracting medium from positive/negative phase-velocity to negative/positive phase-velocity type has an influence on the diffraction efficiencies. This effect increases with increasing corrugation depth, owing to the presence of evanescent waves in the troughs of the corrugated interface.
physics.optics:We have developed a scheme to measure the optical torque, exerted by a laser beam on a phase object, by measuring the orbital angular momentum of the transmitted beam. The experiment is a macroscopic simulation of a situation in optical tweezers, as orbital angular momentum has been widely used to apply torque to microscopic objects. A hologram designed to generate LG02 modes and a CCD camera are used to detect the orbital component of the beam. Experimental results agree with theoretical numerical calculations, and the strength of the orbital component suggest its usefulness in optical tweezers for micromanipulation.
physics.optics:We investigate optical parametric oscillations through four-wave mixing in resonant cavities and photonic crystals. The theoretical analysis underlines the relevant features of the phenomenon and the role of the density of states. Using fully vectorial 3D time-domain simulations, including both dispersion and nonlinear polarization, for the first time we address this process in a face centered cubic lattice and in a photonic crystal slab. The results lead the way to the development of novel parametric sources in isotropic media.
physics.optics:We present a study of orbital angular momentum transfer from pump to down-converted beams in a type-II Optical Parametric Oscillator. Cavity and anisotropy effects are investigated and demostrated to play a central role in the transverse mode dynamics. While the idler beam can oscillate in a Laguerre-Gauss mode, the crystal birefringence induces an astigmatic effect in the signal beam that prevents the resonance of such mode.
physics.optics:We introduce a prototype model for globally-coupled oscillators in which each element is given an oscillation frequency and a preferential oscillation direction (polarization), both randomly distributed. We found two collective transitions: to phase synchronization and to polarization ordering. Introducing a global-phase and a polarization order parameters, we show that the transition to global-phase synchrony is found when the coupling overcomes a critical value and that polarization order enhancement can not take place before global-phase synchrony. We develop a self-consistent theory to determine both order parameters in good agreement with numerical results.
physics.optics:We report on a polarization maintaining large mode area photonic crystal fiber. Unlike, previous work on polarization maintaining photonic crystal fibers, birefringence is introduced using stress applying parts. This has allowed us to realize fibers, which are both single mode at any wavelength and have a practically constant birefringence for any wavelength. The fibers presented in this work have mode field diameters from about 4 to 6.5 micron, and exhibit a typical birefringence of 1.5e-4.
physics.optics:Speden is a computer program that reconstructs the electron density of single particles from their x-ray diffraction patterns, using a single-particle adaptation of the Holographic Method in crystallography. (Szoke, A., Szoke, H., and Somoza, J.R., 1997. Acta Cryst. A53, 291-313.) The method, like its parent, is unique that it does not rely on ``back'' transformation from the diffraction pattern into real space and on interpolation within measured data. It is designed to deal successfully with sparse, irregular, incomplete and noisy data. It is also designed to use prior information for ensuring sensible results and for reliable convergence. This article describes the theoretical basis for the reconstruction algorithm, its implementation and quantitative results of tests on synthetic and experimentally obtained data. The program could be used for determining the structure of radiation tolerant samples and, eventually, of large biological molecular structures without the need for crystallization.
physics.optics:We detect the vortex evolution from the increase of the fractional phase step by interfering two beams of opposite but equal fractional step increment.The interference pattern generated shows evidence of the birth of an additional single extra charge as the fractional phase step increase extends above a half-integer value
physics.optics:We report on a single-mode photonic crystal fiber with attenuation and effective area at 1550 nm of 0.48 dB/km and 130 square-micron, respectively. This is, to our knowledge, the lowest loss reported for a PCF not made from VAD prepared silica and at the same time the largest effective area for a low-loss (< 1 dB/km) PCF. We briefly discuss the future applications of PCFs for data transmission and show for the first time, both numerically and experimentally, how the group velocity dispersion is related to the mode field diameter
physics.optics:We resort to the concept of turns to provide a geometrical representation of the action of any lossless multilayer, which can be considered as the analogous in the unit disk to the sliding vectors in Euclidean geometry. This construction clearly shows the peculiar effects arising in the composition of multilayers. A simple optical experiment revealing the appearance of the Wigner angle is analyzed in this framework.
physics.optics:We examine the Seidel aberrations of thin spherical lenses composed of media with refractive index not restricted to be positive. We find that consideration of this expanded parameter space allows reduction or elimination of more aberrations than is possible with only positive index media. In particular we find that spherical lenses possessing real aplanatic focal points are possible only with negative index. We perform ray tracing, using custom code that relies only on Maxwell's equations and conservation of energy, that confirms the results of the aberration calculations.
physics.optics:We study coupling and decoupling of parallel waveguides in two-dimensional square-lattice photonic crystals. We show that the waveguide coupling is prohibited at some wavelengths when there is an odd number of rows between the waveguides. In contrast, decoupling does not take place when there is even number of rows between the waveguides. Decoupling can be used to avoid cross talk between adjacent waveguides.
physics.optics:We report on an easy-to-evaluate expression for the prediction of the bend-loss for a large mode area photonic crystal fiber (PCF) with a triangular air-hole lattice. The expression is based on a recently proposed formulation of the V-parameter for a PCF and contains no free parameters. The validity of the expression is verified experimentally for varying fiber parameters as well as bend radius. The typical deviation between the position of the measured and the predicted bend loss edge is within measurement uncertainty.
physics.optics:We proposed and realized a two-dimensional (2D) photonic bandedge laser surrounded by the photonic bandgap. The heterogeneous photonic crystal structure consists of two triangular lattices of the same lattice constant with different air hole radii. The photonic crystal laser was realized by room-temperature optical pumping of air-bridge slabs of InGaAsP quantum wells emitting at 1.55 micrometer. The lasing mode was identified from its spectral positions and polarization directions. A low threshold incident pump power of 0.24mW was achieved. The measured characteristics of the photonic crystal lasers closely agree with the results of real space and Fourier space calculations based on the finite-difference time-domain method.
physics.optics:The effect of lattice termination on the surface states in a two-dimensional truncated photonic crystal slab is experimentally studied in a high index contrast silicon-on-insulator system. A single-mode silicon strip waveguide that is separated from the photonic crystal by a trench of variable width is used to evanescently couple to surface states in the surrounding lattice. It is demonstrated that the dispersion of the surface states depends strongly on the specific termination of the lattice.
physics.optics:A new phasing algorithm has been used to determine the phases of diffuse elastic X-ray scattering from a non-periodic array of gold balls of 50 nm diameter. Two-dimensional real-space images, showing the charge-density distribution of the balls, have been reconstructed at 50 nm resolution from transmission diffraction patterns recorded at 550 eV energy. The reconstructed image fits well with scanning electron microscope (SEM) image of the same sample. The algorithm, which uses only the density modification portion of the SIR2002 program, is compared with the results obtained via the Gerchberg-Saxton-Fienup HIO algorithm. In this way the relationship between density modification in crystallography and the HiO algorithm used in signal and image processing is elucidated.
physics.optics:Iterative projection algorithms for phase retrieval are tested on two simple toy models. The result provides useful insights in the behavior of these algorithms.
physics.optics:We demonstrate a technique for shaping current inputs for the direct modulation of a semiconductor laser for digital communication. The introduction of shaped current inputs allows for the suppression of relaxation oscillations and the avoidance of dynamical memory in the physical laser device, i.e., the output will not be influenced by previously communicated information. On the example of time-optimized bits, the possible performance enhancement for high data rate communications is shown numerically.
physics.optics:Phase-stabilized 12-fs, 1-nJ pulses from a commercial Ti:sapphire oscillator are directly amplified in a chirped-pulse optical parametric amplifier and recompressed to yield near-transform-limited 17.3-fs pulses. The amplification process is demonstrated to be phase preserving and leads to 85-uJ, carrier-envelope-offset phase-locked pulses at 1 kHz for 0.9 mJ of pump, corresponding to a single-pass gain of 8.5 x 10^4.
physics.optics:We have observed for the first time a new photonic quantum ring emission of anti-whispering gallery modes from a negative mesa-type toroid cavity due to semiconductor photonic corrals.
physics.optics:We predict that nonlinear left-handed metamaterials can support both TE- and TM-polarized self-trapped localized beams, spatial electromagnetic solitons. Such solitons appear as single- and multi-hump beams, being either symmetric or antisymmetric, and they can exist due to the hysteresis-type magnetic nonlinearity and the effective domains of negative magnetic permeability.
physics.optics:The use of one or more gold nanoballs as reference objects for Fourier Transform holography (FTH) is analysed using experimental soft X-ray diffraction from objects consisting of separated clusters of these balls. The holograms are deconvoluted against ball reference objects to invert to images, in combination with a Wiener filter to control noise. A resolution of ~30nm, smaller than one ball, is obtained even if a large cluster of balls is used as the reference, giving the best resolution yet obtained by X-ray FTH. Methods of dealing with missing data due to a beamstop are discussed. Practical prepared objects which satisfy the FTH condition are suggested, and methods of forming them described.
physics.optics:We study the transmission properties of a nonlinear periodic structure containing alternating slabs of a nonlinear right handed material and a linear left handed material. We find that the transmission associated with the zero averaged- refractive- index gap exhibits a bistable characteristic that is relatively insensitive to incident angle. This is in contrast to the nonlinear behavior of the usual Bragg gap
physics.optics:By use of an imaging spectrometer we map the far-field ($\theta-\lambda$) spectra of 200 fs optical pulses that have undergone beam collapse and filamentation in a Kerr medium. By studying the evolution of the spectra with increasing input power and using a model based on stationary linear asymptotic wave modes, we are able to trace a consistent model of optical beam collapse high-lighting the interplay between conical emission, multiple pulse splitting and other effects such as spatial chirp.
physics.optics:Periodic structures consisting of alternating layers of positive index and negative index materials possess a novel band gap at the frequency at which the average refractive index is zero. We show that in the presence of a Kerr nonlinearity, this zero-n gap can switch from low transmission to a perfectly transmitting state, forming a nonlinear resonance or gap soliton in the process. This zero-n gap soliton is omnidirectional in contrast to the usual Bragg gap soliton of positive index periodic structures
physics.optics:Experimental evidence of mode-selective evanescent power coupling at telecommunication frequencies with efficiencies up to 75 % from a tapered optical fiber to a carefully designed metal nanoparticle plasmon waveguide is presented. The waveguide consists of a two-dimensional square lattice of lithographically defined Au nanoparticles on an optically thin silicon membrane. The dispersion and attenuation properties of the waveguide are analyzed using the fiber taper. The high efficiency of power transfer into these waveguides solves the coupling problem between conventional optics and plasmonic devices and could lead to the development of highly efficient plasmonic sensors and optical switches.
physics.optics:In optical second harmonic generation with normal dispersion, the virtually infinite bandwidth of the unbounded, hyperbolic, modulational instability leads to quenching of spatial multi-soliton formation and to the occurrence of a catastrophic spatio-temporal break-up when an extended beam is let to interact with an extremely weak external noise with coherence time much shorter than that of the pump.
physics.optics:Interfering liquid surface waves are generated by electrically driven vertical oscillations of two or more equispaced pins immersed in a liquid (water). The corresponding intensity distribution, resulting from diffraction of monochromatic light by the reflection phase grating formed on the liquid surface, is calculated theoretically and found to tally with experiments. The curious features of the diffraction pattern and its relation to the interference of waves on the liquid surface are used to measure the amplitude and wavelength of the resultant surface wave along the line joining the two sources of oscillation. Finally, a sample diffraction pattern obtained by optically probing surface regions where interference produces a lattice--like structure is demonstrated and qualitatively explained.
physics.optics:We report polarization tomography experiments on metallic nanohole arrays with square and hexagonal symmetry. As a main result, we find that a fully polarized input beam is partly depolarized after transmission through a nanohole array. This loss of polarization coherence is found to be anisotropic, i.e. it depends on the polarization state of the input beam. The depolarization is ascribed to a combination of two factors: i) the nonlocal response of the array due to surface plasmon propagation, ii) the non-plane wave nature of a practical input beam.
physics.optics:Using analytical modeling and detailed numerical simulations, we investigate properties of hybrid systems of Photonic Crystal micro-cavities which incorporate a highly non-linear Ultra Slow Light medium. We demonstrate that such systems, while being miniature in size (order wavelength), and integrable, could enable ultra-fast non-linear all-optical switching at single photon energy levels.
physics.optics:We fabricated two dimensional photonic crystal structures in zinc oxide films with focused ion beam etching. Lasing is realized in the near ultraviolet frequency at room temperature under optical pumping. From the measurement of lasing frequency and spatial profile of the lasing modes, as well as the photonic band structure calculation, we conclude that lasing occurs in the strongly localized defect modes near the edges of photonic band gap. These defect modes originate from the structure disorder unintentionally introduced during the fabrication process.
physics.optics:One dimensional rectangular metallic gratings enable enhanced transmission of light for specific resonance frequencies. Two kinds of modes participating to enhanced transmission have already been demonstrated : (i) waveguide modes and (ii) surface plasmon polaritons (SPP). Since the original paper of Hessel and Oliner \cite{hessel} pointing out the existence of (i), no progress was made in their understanding. We present here a carefull analysis, and show that the coupling between the light and such resonances can be tremendously improved using an {\it evanescent} wave. This leads to enhanced localisation of light in cavities, yielding, in particular, to a very selective light transmission through these gratings.
physics.optics:We describe an experiment showing a spontaneous symmetry breaking phenomenon between the intensities of the ordinary and extraordinary components of the fundamental field in intracavity type-II harmonic generation. It is based on a triply resonant dual cavity containing a type II phase matched $\chi^{(2)}$ crystal pumped at the fundamental frequency $\omega_0$. The pump beam generates in the cavity a second harmonic mode at frequency $2\omega_0$ which acts as a pump for frequency degenerate type II parametric down conversion. Under operating conditions which are precisely symmetric with respect to the ordinary and extraordinary components of the fundamental wave, we have observed a breaking of the symmetry on the intensities of these two waves in agreement with the theoretical predictions.
physics.optics:Unavoidable variations in size and position of the building blocks of photonic crystals cause light scattering and extinction of coherent beams. We present a new model for both 2 and 3-dimensional photonic crystals that relates the extinction length to the magnitude of the variations. The predicted lengths agree well with our new experiments on high-quality opals and inverse opals, and with literature data analyzed by us. As a result, control over photons is limited to distances up to 50 lattice parameters ($\sim 15 \mu$m) in state-of-the-art structures, thereby impeding large-scale applications such as integrated circuits. Conversely, scattering in photonic crystals may lead to novel physics such as Anderson localization and non-classical diffusion.
physics.optics:A semi-analytical method evaluates the error probability of DPSK signals with intrachannel four-wave-mixing (IFWM) in a highly dispersive fiber link with strong pulse overlap. Depending on initial pulse width, the mean nonlinear phase shift of the system can be from 1 to 2 rad for signal-to-noise ratio (SNR) penalty less than 1 dB. An approximated empirical formula, valid for penalty less than 2 dB, uses the variance of the differential phase of the ghost pulses to estimate the penalty.
physics.optics:The major sources causing deterioration of optical quality in extremely large optical telescopes are misadjustments of the mirrors, deformations of monolithic mirrors, and misalignments of segments in segmented mirrors. For active optics corrections, all three errors, which can partially compensate each other, are measured simultaneously. It is therefore of interest to understand the similarities and differences between the three corresponding types of modes which describe these errors. The first two types are best represented by Zernike polynomials and elastic modes respectively, both of them being continuous and smooth functions. The segment misaligment modes, which are derived by singular value decomposition, are by their nature not smooth and in general discontinuous. However, for mirrors with a large number of segments, the lowest modes become effectively both smooth and continuous. This paper derives analytical expressions for these modes, using differential operators and their adjoints, for the limit case of infinitesimally small segments. For segmented mirrors with approximately 1000 segments, it is shown that these modes agree well with the corresponding lowest singular value decomposition modes. Furthermore, the analytical expressions reveal the nature of the segment misalignment modes and allow for a detailed comparison with the elastic modes of monolithic mirrors. Some mathematical features emerge as identical in the two cases.
physics.optics:We report on remote delivery of 25 pJ broadband near-infrared femtosecond light pulses from a Ti:sapphire laser through 150 meters of single-mode optical fiber. Pulse distortion due to dispersion is overcome with pre-compensation using adaptive pulse shaping techniques, while nonlinearities are mitigated using an SF10 rod for the final stage of pulse compression. Near transform limited pulse duration of 130 fs is measured after the final compression.
physics.optics:High refractive index contrast optical microdisk resonators fabricated from silicon-on-insulator wafers are studied using an external silica fiber taper waveguide as a wafer-scale optical probe. Measurements performed in the 1500 nm wavelength band show that these silicon microdisks can support whispering-gallery modes with quality factors as high as 5.2 x 10^5, limited by Rayleigh scattering from fabrication induced surface roughness. Microdisks with radii as small as 2.5 microns are studied, with measured quality factors as high as 4.7 x 10^5 for an optical mode volume of 5.3 cubic wavelengths in the material.
physics.optics:We develop and demonstrate two numerical methods for solving the class of open cavity problems which involve a curved, cylindrically symmetric conducting mirror facing a planar dielectric stack. Such dome-shaped cavities are useful due to their tight focusing of light onto the flat surface. The first method uses the Bessel wave basis. From this method evolves a two-basis method, which ultimately uses a multipole basis. Each method is developed for both the scalar field and the electromagnetic vector field and explicit ``end user'' formulas are given. All of these methods characterize the arbitrary dielectric stack mirror entirely by its 2\times2 transfer matrices for s- and p-polarization. We explain both theoretical and practical limitations to our method. Non-trivial demonstrations are given, including one of a stack-induced effect (the mixing of near-degenerate Laguerre-Gaussian modes) that may persist arbitrarily far into the paraxial limit. Cavities as large as 50 \lambda are treated, far exceeding any vectorial solutions previously reported.
physics.optics:We recently proposed two-dimensional coupled photonic crystal microcavity arrays as a route to achieve a slow-group velocity of light (flat band) in all crystal directions. In this paper we present the first experimental demonstration of such structures with a measured group velocity below 0.008c and discuss the feasibility of applications such as low-threshold photonic crystal lasers with increased output powers, optical delay components and sensors.
physics.optics:We present the design and fabrication of photonic crystal structures exhibiting electromagnetic bands that are flattened in all crystal directions, i.e., whose frequency variation with wavevector is minimized. Such bands can be used to reduce group velocity of light propagating in arbitrary crystal direction, which is of importance for construction of miniaturized tunable optical delay components, low-threshold photonic crystal lasers, and study of nonlinear optics phenomena.
physics.optics:By direct numerical simulations of the kinoform refractive lens within the quazioptical approach the effects of shape misalinement were investigated. The quazioptical approach was based on numerical integration of parabolic equation for complex wave amplitude by spep-by-step method with "splitting" procedure. The effect of 2pi- shift compensation was calculated for different orders and imperfections. The performance of kinoform lens was compared with the compound refractive lens and thin lens approximation.
physics.optics:We review a novel method for characterizing both the spectral and spatial properties of resonant cavities within two-dimensional photonic crystals (PCs). An optical fiber taper serves as an external waveguide probe whose micron-scale field is used to source and couple light from the cavity modes, which appear as resonant features in the taper's wavelength-dependent transmission spectrum when it is placed within the cavity's near field. Studying the linewidth and depth of these resonances as a function of the taper's position with respect to the resonator produces quantitative measurements of the quality factor Q and modal volume Veff of the resonant cavity modes. Polarization information about the cavity modes can be obtained by studying their depths of coupling when the cavity is probed along different axes by the taper. This fiber-based technique has been used to measure Q ~ 40,000 and Veff ~ 0.9 cubic wavelengths in a graded square lattice PC microcavity fabricated in silicon. The speed and versatility of this fiber-based probe is highlighted, and a discussion of its applicability to other wavelength-scale resonant elements is given.
physics.optics:Micro-domes based on a combination of metallic and dielectric multilayer mirrors are studied using a fully vectorial numerical basis-expansion method that accurately accounts for the effects of an arbitrary Bragg stack and can efficiently cover a large range of dome shapes and sizes. Results are examined from three different viewpoints: (i) the ray-optics limit, (ii) the (semi-) confocal limit for which exact wave solutions are known, and (iii) the paraxial approximation using vectorial Gaussian beams.
physics.optics:An overview is provided over the physics of dielectric microcavities with non-paraxial mode structure; examples are microdroplets and edge-emitting semiconductor microlasers. Particular attention is given to cavities in which two spatial degrees of freedom are coupled via the boundary geometry. This generally necessitates numerical computations to obtain the electromagnetic cavity fields, and hence intuitive understanding becomes difficult. However, as in paraxial optics, the ray picture shows explanatory and predictive strength that can guide the design of microcavities. To understand the ray-wave connection in such asymmetric resonant cavities, methods from chaotic dynamics are required.
physics.optics:We analyze theoretically the propagation of surface plasmon polaritons about a metallic corner with a finite bend radius, using a one-dimensional model analogous to the scattering from a finite-depth potential well. We obtain expressions for the energy reflection and transmission coefficients in the short wavelength limit, as well as an upper bound for the transmittance. In certain cases we find that propagation on non-planar interfaces may result in lower losses than on flat surfaces, contrary to expectation. In addition, we also find that the maximum transmittance depends non-monotonously on the bend radius, allowing increased transmission with decreasing radius.
physics.optics:This book chapter is the first part of a review of nanoporous materials for optical applications. Whereas the second part [J.Sauer, F. Marlow and F.Schueth, pp. 153-172 of the same volume] discusses material properties, this part gives a self-contained discussion of fluorescence and lasing in dielectric microresonators, with special emphasis on the hexagonal morphology found in molecular-sieve-dye compounds.
physics.optics:We address the formation and propagation of multi-spot soliton packets in saturable Kerr nonlinear media with an imprinted harmonic transverse modulation of the refractive index. We show that, in sharp contrast to homogeneous media where stable multi-peaked solitons do not exist, the photonic lattices support stable higher-order structures in the form of soliton packets, or soliton trains. Intuitively, such trains can be viewed as made of several lowest order solitons bound together with appropriate relative phases and their existence as stable objects puts forward the concept of compact manipulation of several solitons as a single entity.
physics.optics:The formation, deformation, and break-up of liquid interfaces are ubiquitous phenomena in nature. In the present article we discuss the deformation of a liquid interface produced by optical radiation forces. Usually, the bending of such an interface by the radiation pressure of a c.w. laser beam is weak. However, the effect can be enhanced significantly if one works with a near-critical phase-separated liquid mixture, whereby the surface tension becomes weak. The bending may in this way become as large as several tenths of micrometers, even with the use of only moderate laser power. This near-criticality is a key element in our experimental investigations as reviewed in the article. The effect is achieved by working with a micellar phase of microemulsions, at room temperature. We give a brief survey of the theory of electromagnetic forces on continuous matter, and survey earlier experiments in this area, such as the Ashkin-Dziedzic optical radiation force experiment on a water/air surface (1973), the Zhang-Chang experiment on the laser-induced deformation of a micrometer-sized spherical water droplet (1988), and the experiment of Sakai et al. measuring surface tensions of interfaces in a non-contact manner (2001). Thereafter, we survey results we obtained in recent years by performing experiments on near-critical interfaces, such as interface bending in the linear regime, stationary large deformations of liquid interfaces, asymmetric pressure effects on interfaces under intense illumination, nonlinear deformations, and laser-sustained liquid columns.
physics.optics:We introduce solitons supported by Bessel photonic lattices in cubic nonlinear media. We show that the cylindrical geometry of the lattice, with several concentric rings, affords unique soliton properties and dynamics. In particular, besides the lowest-order solitons trapped in the center of the lattice, we find soliton families trapped at different lattice rings. Such solitons can be set into controlled rotation inside each ring, thus featuring novel types of in-ring and inter-ring soliton interactions.
physics.optics:We present a novel general framework to deal with forward and backward components of the electromagnetic field in axially-invariant nonlinear optical systems, which include those having any type of linear or nonlinear transverse inhomogeneities. With a minimum amount of approximations, we obtain a system of two first-order equations for forward and backward components explicitly showing the nonlinear couplings among them. The modal approach used allows for an effective reduction of the dimensionality of the original problem from 3+1 (three spatial dimensions plus one time dimension) to 1+1 (one spatial dimension plus one frequency dimension). The new equations can be written in a spinor Dirac-like form, out of which conserved quantities can be calculated in an elegant manner. Finally, these new equations inherently incorporate spatio-temporal couplings, so that they can be easily particularized to deal with purely temporal or purely spatial effects. Nonlinear forward pulse propagation and non-paraxial evolution of spatial structures are analyzed as examples.
physics.optics:Metamaterials--artificially structured materials with tailored electromagnetic response--can be designed to have properties difficult to achieve with existing materials. Here we present a structured metamaterial, based on conducting split ring resonators (SRRs), which has an effective index-of-refraction with a constant spatial gradient. We experimentally confirm the gradient by measuring the deflection of a microwave beam by a planar slab of the composite metamaterial over a broad range of frequencies. The gradient index metamaterial represents an alternative approach to the development of gradient index lenses and similar optics that may be advantageous, especially at higher frequencies. In particular, the gradient index material we propose may be suited for terahertz applications, where the magnetic resonant response of SRRs has recently been demonstrated.
physics.optics:A recent advance in optical coherence tomography (OCT), termed swept-source OCT, is generalized into a new technique, Fourier-domain OCT. It represents a realization of a full-field OCT system in place of the conventional serial image acquisition in transverse directions typically implemented in "flying-spot" mode. To realize the full-field image acquisition, a Fourier holography system illuminated with a swept-source is employed instead of a Michelson interferometer commonly used in OCT. Fourier-domain OCT offers a new leap in signal-to-noise ratio improvement, as compared to flying-spot OCT systems. This paper presents experimental evidence that the signal-to-noise ratio of this new technique is indeed improved.
physics.optics:We show that a slab of meta-material (with $\epsilon=\mu=-1+i\Delta$) possesses a vortex-like surface wave with no ability to transport energy, whose nature is completely different from a localized mode or a standing wave. Through computations based on a rigorous time-dependent Green's function approach, we demonstrate that such a mode inevitably generates characteristic image oscillations in two dimensional focusing with even a monochromatic source, which were observed in many numerical simulations, but such oscillations are weak in three dimensional focusing.
physics.optics:The holographic imaging of rigid objects with diode lasers emitting in many wavelengths in a sillenite Bi12TiO20 photorefractive crystal is both theoretically and experimentally investigated. It is shown that, due to the multi-wavelength emission and the typically large free spectral range of this light source, contour fringes appear on the holographic image corresponding to the surface relief, even in single-exposure recordings. The influence of the number of emitted modes on the fringe width is analysed, and the possible applications of the contour fringes in the field of optical metrology are pointed out.
physics.optics:We develop a point-scattering approach to the plane-wave optical transmission of subwavelength metal hole arrays. We present a real space description instead of the more conventional reciprocal space description; this naturally produces interfering resonant features in the transmission spectra and makes explicit the tensorial properties of the transmission matrix. We give transmission spectra simulations for both square and hexagonal arrays; these can be evaluated at arbitrary angles and polarizations.
physics.optics:We derive phase-matching conditions for four-wave mixing between solitons and linear waves in optical fibres with arbitrary dispersion and demonstrate resonant excitation of new spectral components via this process.
physics.optics:We have experimentally studied polarization properties of the two-dimensional coupled photonic crystal microcavity arrays, and observed a strong polarization dependence of the transmission and reflection of light from the structures - the effects that can be employed in building miniaturized polarizing optical components. Moreover, by combining these properties with a strong sensitivity of the coupled bands on the surrounding refractive index, we have demonstrated a detection of small refractive index changes in the environment, which is useful for construction of bio-chemical sensors.
physics.optics:We demonstrate a new class of hollow-core Bragg fibers that are composed of concentric cylindrical silica rings separated by nanoscale support bridges. We theoretically predict and experimentally observe hollow-core confinement over an octave frequency range. The bandwidth of bandgap guiding in this new class of Bragg fibers exceeds that of other hollow-core fibers reported in the literature. With only three rings of silica cladding layers, these Bragg fibers achieve propagation loss of the order of 1 dB/m.
physics.optics:We analyze, by the finite-difference time-domain numerical methods, several ways to enhance the directional emission from photonic crystal waveguides through the beaming effect recently predicted by Moreno et al. [Phys. Rev. E 69, 121402(R) (2004)], by engineering the surface modes and corrugation of the photonic crystal surface. We demonstrate that the substantial enhancement of the light emission can be achieved by increasing the refractive index of the surface layer. We also measure power of surface modes and reflected power and confirm that the enhancement of the directional emission is related to the manipulation of the photonic crystal surface modes.
physics.optics:It is shown that the monochromatic optical wave propagating through the medium with linear birefringence in presence of a signal electromagnetic wave (whose wavelength is equal to the polarization beats length), displays Faraday rotation having the frequency of the signal wave and unsuppressed by linear birefringence. The effect is resonant with respect to the frequency of a signal wave. The "sharpness" of the resonance is defined by length of the birefringent medium.
physics.optics:Second harmonic generation in a two dimensional nonlinear quasi-crystal is demonstrated for the first time. Temperature and wavelength tuning of the crystal reveal the uniformity of the pattern while angle tuning reveals the dense nature of the crystal's Fourier spectrum. These results compare well with theoretical predictions showing the excellent uniformity of the crystal and suggest that more complicated ``nonlinear holograms'' should be possible.
physics.optics:We show analytically, and numerically that highly-dispersive media can be used to drastically increase lifetimes of high-Q microresonators. In such a resonator, lifetime is limited either by undesired coupling to radiation, or by intrinsic absorption of the constituent materials. The presence of dispersion weakens coupling to the undesired radiation modes and also effectively reduces the material absorption.
physics.optics:The Lorentz transformations for the optical constants (electric permittivity, magnetic permeability and index of refraction) of moving media are considered.
physics.optics:The existence of resonant enhanced transmission and collimation of light waves by subwavelength slits in metal films [for example, see T.W. Ebbesen et al., Nature (London) 391, 667 (1998) and H.J. Lezec et al., Science, 297, 820 (2002)] leads to the basic question: Can a light pulse be enhanced and simultaneously localized in space and time by a subwavelength slit? To address this question, the spatial distribution of the energy flux of an ultrashort (femtosecond) wave-packet diffracted by a subwavelength (nanometer-size) slit was analyzed by using the conventional approach based on the Neerhoff and Mur solution of Maxwell's equations. The results show that a light pulse can be enhanced by orders of magnitude and simultaneously localized in the near-field diffraction zone at the nm- and fs-scales. Possible applications in nanophotonics are discussed.
physics.optics:We address the dynamics of higher-order solitons in optical lattices, and predict their self-splitting into the set of their single-soliton constituents. The splitting is induced by the potential introduced by the lattice, together with the imprinting of a phase tilt onto the initial multisoliton states. The phenomenon allows the controllable generation of several coherent solitons linked via their Zakharov-Shabat eigenvalues. Application of the scheme to the generation of correlated matter waves in Bose-Einstein condensates is discussed.
physics.optics:We report on a high-efficiency 461 nm blue light conversion from an external cavity-enhanced second-harmonic generation of a 922 nm diode laser with a quasi-phase-matched KTP crystal (PPKTP). By choosing a long crystal (LC=20 mm) and twice looser focusing (w0=43 $\mu$m) than the "optimal" one, thermal lensing effects due to the blue power absorption are minimized while still maintaining near-optimal conversion efficiency. A stable blue power of 234 mW with a net conversion efficiency of eta=75% at an input mode-matched power of 310 mW is obtained. The intra-cavity measurements of the conversion efficiency and temperature tuning bandwidth yield an accurate value d33(461 nm)=15 pm/V for KTP and provide a stringent validation of some recently published linear and thermo-optic dispersion data of KTP.
physics.optics:We consider an anisotropic homogenized composite medium (HCM) arising from isotropic particulate component phases based on ellipsoidal geometries. For cubically nonlinear component phases, the corresponding zeroth-order strong-permittivity-fluctuation theory (SPFT) (which is equivalent to the Bruggeman homogenization formalism) and second-order SPFT are established and used to estimate the constitutive properties of the HCM. The relationship between the component phase particulate geometry and the HCM constitutive properties is explored. Significant differences are highlighted between the estimates of the Bruggeman homogenization formalism and the second-order SPFT estimates. The prospects for nonlinearity enhancement are investigated.
physics.optics:Under certain circumstances, Voigt waves can propagate in a biaxial composite medium even though the component material phases individually do not support Voigt wave propagation. This phenomenon is considered within the context of the strong--permittivity--fluctuation theory. A generalized implementation of the theory is developed in order to explore the propagation of Voigt waves in any direction. It is shown that the correlation length--a parameter characterizing the distributional statistics of the component material phases--plays a crucial role in facilitating the propagation of Voigt waves in the homogenized composite medium.
physics.optics:In conventional approaches to the homogenization of random particulate composites, both the distribution and size of the component phase particles are often inadequately taken into account. Commonly, the spatial distributions are characterized by volume fraction alone, while the electromagnetic response of each component particle is represented as a vanishingly small depolarization volume. The strong-permittivity-fluctuation theory (SPFT) provides an alternative approach to homogenization wherein a comprehensive description of distributional statistics of the component phases is accommodated. The bilocally-approximated SPFT is presented here for the anisotropic homogenized composite which arises from component phases comprising ellipsoidal particles. The distribution of the component phases is characterized by a two-point correlation function and its associated correlation length. Each component phase particle is represented as an ellipsoidal depolarization region of nonzero volume. The effects of depolarization volume and correlation length are investigated through considering representative numerical examples. It is demonstrated that both the spatial extent of the component phase particles and their spatial distributions are important factors in estimating coherent scattering losses of the macroscopic field.
physics.optics:The polarization matrix ($2\times2$) obtained from two component eigen-spinors of spherical harmonics help us to evaluate the differential matrix $N$ of the anisotropic optical medium. The geometric phase is realized through {\it helicity} of photon, assuming the transmission of polarized light through the crystal which has been twisted about the normal to its surface over a closed path.
physics.optics:A circularly polarized rotationally symmetric paraxial laser beams carries hbar angular momentum per photon as spin. Focussing the beam with a rotationally symmetric lens cannot change this angular momentum flux, yet the focussed beam must have spin less than hbar per photon. The remainder of the original spin is converted to orbital angular momentum, manifesting itself as a longitudinal optical vortex at the focus. This demonstrates that optical orbital angular momentum can be generated by a rotationally symmetric optical system which preserves the total angular momentum of the beam.
physics.optics:The sign of the refractive index of any medium is soley determined by the requirement that the propagation of an electromagnetic wave obeys Einstein causality. Our analysis shows that this requirement predicts that the real part of the refractive index may be negative in an isotropic medium even if the electric permittivity and the magnetic permeability are both positive. Such a system may be a route to negative index media at optical frequencies. We also demonstrate that the refractive index may be positive in left-handed media that contain two molecular species where one is in its excited state.
physics.optics:This paper presents a novel approach to wave propagation inside the Fabry-Perot framework. It states that the time-averaged Poynting vector modulus could be nonequivalent with the squared-field amplitude modulus. This fact permits the introduction of a new kind of nonlinear medium whose nonlinearity is proportional to the time-averaged Poynting vector modulus. Its transmittance is calculated and found to differ with that obtained for the Kerr medium, whose nonlinearity is proportional to the squared-field amplitude modulus. The latter emphasizes the nonequivalence of these magnitudes. A space-time symmetry analysis shows that the Poynting nonlinearity should be only possible in noncentrosymmetric materials.
physics.optics:The resonant optical modes of a high permittivity dielectric prism with an equilateral triangular cross section are discussed. Eigenmode solutions of the scalar Helmholtz equation with Dirichlet boundary conditions, appropriate to a conducting boundary, are applied for this purpose. The particular plane wave components present in these modes are analyzed for their total internal reflection behavior and implied mode confinement when the conducting boundary is replaced by a sharp dielectric mismatch. Improvement in TIR confinement by adjusting the longitudinal wavevector $k_z$ is also discussed. For two-dimensional electromagnetic solutions ($k_z=0$), TE polarization leads to longer lifetime than TM polarization, assuming that escape of evanescent boundary waves at the corners is the primary decay process.
physics.optics:We present a simple polarizing Mach-Zehnder interferometer that can be used for optimal minimal ellipsometry: Only four intensities are measured to determine the three Stokes parameters, and an optimal choice for the four polarization projections can be achieved for any sufficiently small wavelength range of interest.
physics.optics:Gold micro-mirrors have been formed in silicon in an inverted pyramidal shape. The pyramidal structures are created in the (100) surface of a silicon wafer by anisotropic etching in potassium hydroxide. High quality micro-mirrors are then formed by sputtering gold onto the smooth silicon (111) faces of the pyramids. These mirrors show great promise as high quality optical devices suitable for integration into MOEMS systems.
physics.optics:We demonstrate low-threshold random lasing in random amplifying layered medium via photon localization. Lasing is facilitated by resonant excitation of localized modes at the pump laser wavelength, which are peaked deep within the sample with greatly enhanced intensity. Emission occurs into long-lived localized modes overlapping the localized gain region. This mechanism overcomes a fundamental barrier to reducing lasing thresholds in diffusive random lasers, in which multiple scattering restricts the excitation region to the proximity of the sample surface.
physics.optics:The rate of linear collisionless damping (Landau damping) in a classical electron gas confined to a heated ionized thin film is calculated. The general expression for the imaginary part of the dielectric tensor in terms of the parameters of the single-particle self-consistent electron potential is obtained. For the case of a deep rectangular well, it is explicitly calculated as a function of the electron temperature in the two limiting cases of specular and diffuse reflection of the electrons from the boundary of the self-consistent potential. For realistic experimental parameters, the contribution of Landau damping to the heating of the electron subsystem is estimated. It is shown that for films with a thickness below about 100 nm and for moderate laser intensities it may be comparable with or even dominate over electron-ion collisions and inner ionization.
physics.optics:We propose and analyze a new type of a resonator in an annular geometry which is based on a single defect surrounded by radial Bragg reflectors on both sides. We show that the conditions for efficient mode confinement are different from those of the conventional Bragg waveguiding in a rectangular geometry. A simple and intuitive approach to the design of optimal radial Bragg reflectors is proposed and employed, yielding chirped gratings. Small bending radii and strong control over the resonator dispersion are possible by the Bragg confinement. A design compromise between large Free Spectral Range (FSR) requirements and fabrication tolerances is suggested.
physics.optics:Lasing at telecommunication wavelengths from annular resonators employing radial Bragg reflectors is demonstrated at room temperature under pulsed optical pumping. Sub milliwatt pump threshold levels are observed for resonators with 0.5-1.5 wavelengths wide defects of radii 7-8 mm. The quality factors of the resonator modal fields are estimated to be on the order of a few thousands. The electromagnetic field is shown to be guided by the defect. Good agreement is found between the measured and calculated spectrum.
physics.optics:We address Bessel photonic lattices of radial symmetry imprinted in cubic Kerr-type nonlinear media and show that they support families of stable dipole-mode solitons featuring two out-of-phase light spots located in different lattice rings. We show that the radial symmetry of the Bessel lattices afford a variety of unique soliton dynamics including controlled radiation-free rotation of the dipole-mode solitons.
physics.optics:Criteria for experimental observation of multi-dimensional optical solitons in media with saturable refractive nonlinearities are developed. The criteria are applied to actual material parameters (characterizing the cubic self-focusing and quintic self-defocusing nonlinearities, two-photon loss, and optical-damage threshold) for various glasses. This way, we identify operation windows for soliton formation in these glasses. It is found that two-photon absorption sets stringent limits on the windows. We conclude that, while a well-defined window of parameters exists for two-dimensional solitons (spatial or spatiotemporal), for their three-dimensional spatiotemporal counterparts such a window \emph{does not} exist, due to the nonlinear loss in glasses.
physics.optics:We report the observation of the self-guided propagation of 120 fs, 0.56 mJ infrared radiation in air for distances greater than one meter. In contrast to the known case of filamentation, in the present experiment the laser power is lower than the collapse threshold. Therefore the counter balance between Kerr self focussing and ionization induced defocussing as the stabilizing mechanism is ruled out. Instead, we find evidences of a process in which the transversal beam distribution reshapes into a form similar to a Townes soliton. We include numerical support for this conclusion.
physics.optics:We consider a plane electromagnetic wave incident on a periodic stack of dielectric layers. One of the alternating layers has an anisotropic refractive index with an oblique orientation of the principal axis relative to the normal to the layers. It was shown recently (A. Figotin and I. Vitebskiy, Phys. Rev. E68, 036609 2003) that an obliquely incident light, upon entering such a periodic stack, can be converted into an abnormal axially frozen mode with drastically enhanced amplitude and zero normal component of the group velocity. The stack reflectivity at this point can be very low, implying nearly total conversion of the incident light into the frozen mode with huge energy density, compared to that of the incident light. Supposedly, the frozen mode regime requires strong birefringence in the anisotropic layers - by an order of magnitude stronger than that available in common anisotropic dielectric materials. In this paper we show how to overcome the above problem by exploiting higher frequency bands of the photonic spectrum. We prove that a robust frozen mode regime at optical wavelengths can be realized in stacks composed of common anisotropic materials, such as YVO&#8324;, LiNb, CaCO&#8323;, and the like.
physics.optics:The paper aims at presenting a didactic and self-contained overview of Gauss-Hermite and Gauss-Laguerre laser beam modes. The usual textbook approach for deriving these modes is to solve the Helmoltz electromagnetic wave equation within the paraxial approximation. Here, a different technique is presented: Using the plane wave representation of the fundamental Gaussian mode as seed function, all higher-order beam modes are derived by acting with differential operators on this fundamental solution. Even special beam modes as the recently introduced Bessel beams are easily described within that framework.
physics.optics:This letter presents a comparison of exact probability density function with the Gaussian noise approximation in optically pre-amplified DPSK receivers with optical Mach-Zehnder interferometer demodulation (MZI) and balanced detection, including the impact of phase noise. It is found that the Gaussian noise approximation significantly over-estimates ASE-ASE beat noise in DPSK receivers with balanced detection particularly when phase noise is negligible, compared to IM/DD receivers, ASE- amplified spontaneous emission. However, the Gaussian noise approximation is still applicable for DPSK receivers with balanced detection and the measured 3-dB advantage is predicted by the Gaussian noise distribution.
physics.optics:Spatial Kerr solitons, typically associated with the standard paraxial nonlinear Schroedinger equation, are shown to exist to all nonparaxial orders, as exact solutions of Maxwell's equations in the presence of vectorial Kerr effect. More precisely, we prove the existence of azimuthally polarized, spatial, dark soliton solutions of Maxwell's equations, while exact linearly polarized (2+1)-D solitons do not exist. Our ab initio approach predicts the existence of dark solitons up to an upper value of the maximum field amplitude, corresponding to a minimum soliton width of about one fourth of the wavelength.
physics.optics:In any polarimetric measurement technique, enhancing the laser polarization change of a laser beam before it reaches the analyzer can help in improving the sensitivity. This can be performed using an optical component having a large linear dichroism, the enhancement factor being equal to the square root of the ratio of the two transmission factors. A pile of parallel plates at Brewster incidence looks appropriate for realizing such a polarization magnifier. In this paper, we address the problem raised by the interference in the plates and between the plates, which affects the measurement by giving rise to birefringence. We demonstrate that wedged plates provide a convenient and efficient way to avoid this interference. We have implemented and characterized devices with 4 and 6 wedged plates at Brewster incidence which have led to a decisive improvement of the signal to noise ratio in our ongoing Parity Violation measurement.
physics.optics:A catadioptric sensor induces a projection between a given object surface and an image plane. The prescribed projection problem is the problem of finding a catadioptric sensor that realizes a given projection. Here we present a functional that describes the image error induced by a given mirror when compared with a given projection. This expression can be minimized to find solutions to the prescribed projection problem. We present an example of this approach, by finding the optimum spherical mirror that serves as a passenger side mirror on a motor vehicle.
physics.optics:The various non-linear transformations incurred by the rays in an optical system can be modelled by matrix products up to any desired order of approximation. Mathematica software has been used to find the appropriate matrix coefficients for the straight path transformation and for the transformations induced by conical surfaces, both direction change and position offset. The same software package was programmed to model optical systems in seventh-order. A Petzval lens was used to exemplify the modelling power of the program.
physics.optics:The non-linear transformations incurred by the rays in an optical system can be suitably described by matrices to any desired order of approximation. In systems composed of uniform refractive index elements, each individual ray refraction or translation has an associated matrix and a succession of transformations correspond to the product of the respective matrices. This paper describes a general method to find the matrix coefficients for translation and surface refraction irrespective of the surface shape or the order of approximation. The choice of coordinates is unusual as the orientation of the ray is characterised by the direction cosines, rather than slopes; this is shown to greatly simplify and generalise coefficient calculation. Two examples are shown in order to demonstrate the power of the method: The first is the determination of seventh order coefficients for spherical surfaces and the second is the determination of third order coefficients for a toroidal surface.
physics.optics:Current quantum cascade lasers based upon conduction band electron transitions are predominantly TM (electrical field normal to the epitaxial direction) polarized. Here we present a study of localized defect modes, with the requisite TM polarization, in connected square and hexagonal lattice two-dimensional (2D) photonic crystals for application as quantum cascade laser resonators. A simple group-theory based analysis is used to produce an approximate description of the resonant modes supported by defects situated at different high symmetry points within the host photonic lattice. The results of this analysis are compared with 2D finite-difference time-domain (FDTD) simulations, showing a close correspondence between the two analyses, and potential applications of the analysis in quantum cascade laser design are considered.
physics.optics:We present an analysis of quantum-limited DPSK receivers with optical Mach-Zehnder interferometer (MZI) demodulation. It is shown for the first time that the quantum limits for DPSK/MZI receivers with single-port and balanced detections exactly differ from 3-dB in receiver sensitivity, obtained by both Poisson and Gaussian noise statistics.
physics.optics:This paper discusses free carrier generation by pulsed laser fields as a mechanism to switch the optical properties of semiconductor photonic crystals and bulk semiconductors on an ultrafast time scale. Requirements are set for the switching magnitude, the time-scale, the induced absorption as well as the spatial homogeneity, in particular for silicon at lambda= 1550 nm. Using a nonlinear absorption model, we calculate carrier depth profiles and define a homogeneity length l_hom. Homogeneity length contours are visualized in a plane spanned by the linear and two-photon absorption coefficients. Such a generalized homogeneity plot allows us to find optimum switching conditions at pump frequencies near v/c= 5000 cm^{-1} (lambda= 2000 nm). We discuss the effect of scattering in photonic crystals on the homogeneity. We experimentally demonstrate a 10% refractive index switch in bulk silicon within 230 fs with a lateral homogeneity of more than 30 micrometers. Our results are relevant for switching of modulators in absence of photonic crystals.
physics.optics:A theoretical investigation on the quantum control of optical coherent four-wave mixing interactions in two-level systems driven by two intense synchronized femtosecond laser pulses of central angular frequencies $\omega$ and $3\omega$ is reported. By numerically solving the full Maxwell-Bloch equations beyond the slowly-varying envelope and rotating-wave approximations in the time domain, the nonlinear coupling to the optical field at frequency $5\omega$ is found to depend critically on the initial relative phase $\phi$ of the two propagating pulses; the coupling is enhanced when the pulses interfere constructively in the center ($\phi=0$), while it is nearly suppressed when they are out of phase ($\phi=\pi$). The tuning of the initial absolute phase of the different frequency components of synchronously propapagating $\omega$-$3\omega$ femtosecond pulses can serve as a means to control coherent anti-Stokes Raman scattering (CARS) processes.
physics.optics:We address azimuthally modulated Bessel optical lattices imprinted in focusing cubic Kerr-type nonlinear media, and reveal that such lattices support different types of stable solitons, whose complexity increases with growth of lattice order. We reveal that the azimuthally modulated lattices cause single solitons launched tangentially to the guiding rings to jump along consecutive sites of the optical lattice. The position of the output channel can be varied by small changes of the launching angle.
physics.optics:This paper presents for the first time a comprehensive study of noise statistics by use pdf.
physics.optics:We prove that spatial Kerr solitons, usually obtained in the frame of nonlinear Schroedinger equation valid in the paraxial approximation, can be found in a generalized form as exact solutions of Maxwell's equations. In particular, they are shown to exist, both in the bright and dark version, as linearly polarized exactly integrable one-dimensional solitons, and to reduce to the standard paraxial form in the limit of small intensities. In the two-dimensional case, they are shown to exist as azimuthally polarized circularly symmetric dark solitons. Both one and two-dimensional dark solitons exhibit a characteristic signature in that their asymptotic intensity cannot exceed a threshold value in correspondence of which their width reaches a minimum subwavelength value.
physics.optics:The idea of working with a near-critical phase-separated liquid mixture whereby the surface tension becomes weak, has recently made the field of laser manipulation of liquid interfaces a much more convenient tool in practice. The deformation of interfaces may become as large as several tenths of micrometers, even with the use of conventional laser power. This circumstance necessitates the use of nonlinear geometrical theory for the description of surface deformations. The present paper works out such a theory, for the surface deformation under conditions of axial symmetry and stationarity. Good agreement is found with the experimental results of Casner and Delville [A. Casner and J. P. Delville, Phys. Rev. Lett. {\bf 87}, 054503 (2001); Opt. Lett. {\bf 26}, 1418 (2001); Phys. Rev. Lett. {\bf 90}, 144503 (2003)], in the case of moderate power or a broad laser beam. In the case of large power and a narrow beam, corresponding to surface deformations of about 50 micrometers or higher, the theory is found to over-predict the deformation. Possible explanations of this discrepancy are discussed.
physics.optics:Maxwell equations are solved in a layer comprising a finite number of homogeneous isotropic dielectric regions ended by anisotropic perfectly matched layers (PMLs). The boundary-value problem is solved and the dispersion relation inside the PML is derived. The general expression of the eigenvalues equation for an arbitrary number of regions in each layer is obtained, and both polarization modes are considered. The modal functions of a single layer ended by PMLs are found, and their orthogonality relation is derived. The present method is useful to simulate scattering problems from dielectric objects as well as propagation in planar slab waveguides. Its potential to deal with more complex problems such as the scattering from an object with arbitrary cross section in open space using the multilayer modal method is briefly discussed.
physics.optics:An analytical expression for the self coherence function of a microcavity and partially coherent source is derived from first principles in terms of the component self coherence functions. Excellent agreement between the model and experimental measurements of two Resonant Cavity LEDs (RCLEDs) is evident. The variation of coherence length as a function of numerical aperture is also described by the model. This is explained by a microcavity's angular sensitivity in filtering out statistical fluctuations of the underlying light source. It is further demonstrated that the variable coherence properties of planar microcavities can be designed by controlling the underlying coherences of microcavity and emitter whereby coherence lengths ranging over nearly an order of magnitude could be achieved.
physics.optics:Stimulated Raman scattering (SRS) in ultra-high-Q surface-tension-induced spherical and chip-based toroid microcavities is considered both theoretically and experimentally. These microcavities are fabricated from silica, exhibit small mode volume (typically 1000 $\mu m^{3}$) and possess whispering-gallery type modes with long photon storage times (in the range of 100 ns), significantly reducing the threshold for stimulated nonlinear optical phenomena. Oscillation threshold levels of less than 100 $\mu $% -Watts of launched fiber pump power, in microcavities with quality factors of 100 million are observed. Using a steady state analysis of the coupled-mode equations for the pump and Raman whispering-gallery modes, the threshold, efficiencies and cascading properties of SRS in UHQ devices are derived. The results are experimentally confirmed in the telecommunication band (1550nm) using tapered optical fibers as highly efficient waveguide coupling elements for both pumping and signal extraction. The device performance dependence on coupling, quality factor and modal volume are measured and found to be in good agreement with theory. This includes analysis of the threshold and efficiency for cascaded Raman scattering. The side-by-side study of nonlinear oscillation in both spherical microcavities and toroid microcavities on-a-chip also allows for comparison of their properties. In addition to the benefits of a wafer-scale geometry, including integration with optical, electrical or mechanical functionality, microtoroids on-a-chip exhibit single mode Raman oscillation over a wide range of pump powers.
physics.optics:We address soliton spiraling in optical lattices induced by multiple coherent Bessel beams and show that the dynamical nature of such lattices make them able to drag different soliton structures, setting them into rotation. The rotation rate can be controlled by varying the topological charges of lattice-inducing Bessel beams.
physics.optics:We investigate theoretically a fiber laser passively mode-locked with nonlinear polarization rotation. A unidirectional ring cavity is considered with a polarizer placed between two sets of a halfwave plate and a quarterwave plate. A master equation is derived and the stability of the continuous and mode-locked solutions is studied. In particular, the effect of the orientation of the four phase plates and of the polarizer on the mode-locking regime is investigated.
physics.optics:This paper presents an investigation of organic LED extraction efficiency enhancement using a low refractive index scattering layer. A scattering model is developed based on rigorous electromagnetic modelling techniques. The model accounts for proportions of scattered guided and radiation modes as well as the efficiencies with which emitted and scattered light are extracted. Constrained optimisation techniques are implemeneted for a single operation wavelength to maximise the extraction efficiency of a generic OLED device. Calculations show that a 2 fold efficiency enhancement is achievable with a correctly engineered scattering layer. The detailed analysis of the enhancement mechanism highlights ways in which this scheme could be improved.
physics.optics:It is reported that when a light beam travels through a slab of left-handed medium in the air, the lateral shift of the transmitted beam can be negative as well as positive. The necessary condition for the lateral shift to be positive is given. The validity of the stationary-phase approach is demonstrated by numerical simulations for a Gaussian-shaped beam. A restriction to the slab's thickness is provided that is necessary for the beam to retain its profile in the traveling. It is shown that the lateral shift of the reflected beam is equal to that of the transmitted beam in the symmetric configuration.
physics.optics:We present an experimental technique enabling mechanical-noise free, cavity-enhanced frequency measurements of an atomic transition and its hyperfine structure. We employ the 532nm frequency doubled output from a Nd:YAG laser and an iodine vapour cell. The cell is placed in a traveling-wave Fabry-Perot interferometer (FPI) with counter-propagating pump and probe beams. The FPI is locked using the Pound-Drever-Hall (PDH) technique. Mechanical noise is rejected by differencing pump and probe signals. In addition, this differenced error signal gives a sensitive measure of differential non-linearity within the FPI.
physics.optics:Soliton propagation is usually described in the ``slowly varying envelope approximation'' (SVEA) regime, which is not applicable for ultrashort pulses. We present theoretical results and numerical simulations for both NLS and parametric ($\chi^{(2)}$) ultrashort solitons in the ``generalised few-cycle envelope approximation'' (GFEA) regime, demonstrating their altered propagation.
physics.optics:Optical vortices generically arise when optical beams are combined. Recently, we reported how several laser beams containing optical vortices could be combined to form optical vortex loops, links and knots embedded in a light beam (Leach et al 2004). Here, we describe in detail the experiments in which vortex loops form these structures. The experimental construction follows a theoretical model originally proposed by Berry and Dennis, and the beams are synthesised using a programmable spatial light modulator and imaged using a CCD camera.
physics.optics:Absorption of light by a nanoparticle in the presence of resonant atom and fluorescence of the latter are theoretically investigated. It is shown, that absorption of light by a nanoparticle can be increased by several orders because of presence of atom. It is established, that optical bistability in such system is possible.
physics.optics:A time-domain approach is proposed for the propagation of ultrashort electro- magnetic wave packets beyond the paraxial and the slowly-varying-envelope approximations. An analytical method based on perturbation theory is used to solve the wave equation in free space without resorting to Fourier trans- forms. An exact solution is obtained in terms of successive temporal and spatial derivatives of a monochromatic paraxial beam. The special case of a radially polarized transverse magnetic wave packet is discussed.
physics.optics:We present in this paper a method to recover the refractive index profile of graded waveguide from the effective indices by cubic spline interpolation function. It is proved by numerical analysis of several typical index distributions that the refractive index profile can be reconstructed closely to its exact profile with the presented interpolation model. This method can reliably retrieve index profile of both more-mode (more than 4 guiding mode) and fewer-mode (2-4) waveguides.
physics.optics:Using femtosecond laser machining, we have fabricated photonic bandgap materials that influence propagation of phonon-polaritons in ferroelectric crystals. Broadband polaritons were generated with impulsive stimulated Raman scattering (ISRS) using an ultrashort laser pulse, and the spatial and temporal evolution of the polaritons were imaged as they propagated through the fabricated structures with polariton real-space imaging. These techniques offer a new approach to optical materials design.
physics.optics:We study the effect of disorder on the effective magnetic response of composite left-handed metamaterials and their specific properties such as negative refraction. We show that relatively weak disorder in the split-ring resonators can reduce and even completely eliminate the frequency domain where the composite material demonstrates the left-handed properties. We introduce the concept of the order parameter to describe novel physics of this effect.
physics.optics:We studied both theoretically and experimentally the intensity distribution of a Gaussian laser beam when it was focussed by an objective lens with its numerical-aperture (NA) up to 0.95. Approximate formulae for full widths at half maximum (FWHM) of the intensity distribution at focus were derived for very large and very small initial beam waists with respect to the entrance pupil radius of the objective lens. In experiments the energy flux through a 0.5 micron pinhole was measured for various pinhole positions. We found that the FWHM's at focus in the transverse and the longitudinal directions do not increase much from the ultimate FWHM's until the input beam waist is reduced below the half of the entrance pupil radius. In addition, we observed significance of the spatial distribution of the input beam against a true Gaussian beam profile in the case of small initial beam waist. For high NA with resulting focal beam waists comparable to or smaller than the wavelength of the laser, the interaction between the electric field and the conducting surface of the pinhole caused the transverse FWHM to be measured slightly smaller than FWHM of the unperturbed intensity distribution convoluted with the pinhole opening.
physics.optics:The diffraction problem of a plane wave impinging on a grating formed by nested cavities is solved by means of the modal method, for $s$ and $p$ polarization modes. The cavities are formed by perfectly conducting sheets that describe rectangular profiles. The electromagnetic response of the grating is analyzed, paying particular attention to the generation of resonances within the structure. The dependence of the resonances on the geometrical parameters of the grating is studied, and results of far and near field are shown. The results are checked and compared with those available in the literature for certain limit cases.
physics.optics:We investigated the Onsager relations in the context of electromagnetic constitutive relations of linear, homogeneous materials. We determined that application of the Onsager relations to the constitutive equations relating $#P$ and $#M$ to both $#E$ and $#B$ is in accord with Lorentz reciprocity as well as the Post constraint.
physics.optics:We demonstrate femtosecond performance of an ultra-broadband high-index-contrast saturable Bragg reflector consisting of a silicon/silicon-dioxide/germanium structure that is fully compatible with CMOS processing. This device offers a reflectivity bandwidth of over 700 nm and sub-picosecond recovery time of the saturable loss. It is used to achieve mode-locking of an Er-Yb:glass laser centered at 1540 nm, generating 220 fs pulses, with the broadest output spectrum to date.
physics.optics:We study the electromagnetic beam reflection from layered structures that include the so-called double-negative materials, also called left-handed metamaterials. We predict that such structures can demonstrate a giant lateral Goos-Hanchen shift of the scattered beam accompanied by splitting of the reflected and transmitted beams due to the resonant excitation of surface waves at the interfaces between the conventional and double-negative materials as well as due to excitation of leaky modes in the layered structures. The beam shift can be either positive or negative, depending on the type of the guided waves excited by the incoming beam. We also perform finite-difference time-domain simulations and confirm the major effects predicted analytically.
physics.optics:We derive coupled nonlinear Schr\"{o}dinger equation (CNLSE) for arbitrary polarized light propagation in a single-mode fiber. We introduce a basis of transverse eigen modes with the appropriate projecting hence solutions depend on the waveguide geometry. Considering a weak nonlinearity which is connected with Kerr effect, we give explicit expressions for nonlinear constants via integrals of Bessel functions. We compare numerical results for the nonlinear constant extracted from experimental observations of a soliton for the nonlinear Schr\"{o}dinger equation (NLSE) (single-mode one). The method of projecting we use allows a direct generalization to multi-mode fiber case.
physics.optics:We study nonlinear transmission of an asymmetric multilayer structure created by alternating slabs of two materials with positive and negative refractive index. We demonstrate that such a structure exhibits passive spatially nonreciprocal transmission of electromagnetic waves, the analogue of the electronic diode. We study the properties of this left-handed diode and confirm its highly nonreciprocal and bistable transmittance by employing direct simulations.
physics.optics:We investigate resonant nonlinear optical interactions and demonstrate induced transparency in acetylene molecules in a hollow-core photonic band-gap fiber at 1.5$\mu$m. The induced spectral transmission window is used to demonstrate slow-light effects, and we show that the observed broadening of the spectral features is due to collisions of the molecules with the inner walls of the fiber core. Our results illustrate that such fibers can be used to facilitate strong coherent light-matter interactions even when the optical response of the individual molecules is weak.
physics.optics:We reveal the existence of dynamically stable composite topological solitons in Bessel photonic lattices imprinted in focusing Kerr-type nonlinear media. The new stable composite solitons are made of a vortex-ring with unit topological charge incoherently coupled to a fundamental soliton. The stabilization of the otherwise highly azimuthally unstable vortex-rings is provided under suitable conditions, which we study in detail, by cross-phase-modulation coupling with the fundamental soliton companion.
physics.optics:Stable ring vortex solitons, featuring a bright-shape, appear to be very rare in nature. However, here we show that they exist and can be made dynamically stable in defocusing cubic nonlinear media with an imprinted Bessel optical lattice. We find the families of vortex lattice solitons and reveal their salient properties, including the conditions required for their stability. We show that the higher the soliton topological charge, the deeper the lattice modulation necessary for stabilization.
physics.optics:Optical microcavities confine light spatially and temporally and find application in a wide range of fundamental and applied studies. In many areas, the microcavity figure of merit is not only determined by photon lifetime (or the equivalent quality-factor, Q), but also by simultaneous achievement of small mode volume V . Here we demonstrate ultra-high Q-factor small mode volume toroid microcavities on-a-chip, which exhibit a Q/V factor of more than $10^{6}(\lambda/n)^{-3}$. These values are the highest reported to date for any chip-based microcavity. A corresponding Purcell factor in excess of 200 000 and a cavity finesse of $2.8\times10^{6}$ is achieved, demonstrating that toroid microcavities are promising candidates for studies of the Purcell effect, cavity QED or biochemical sensing
physics.optics:Evolution of the main signal in a Lorentz dispersive medium is considered. The signal propagating in the medium is excited by a sine-modulated pulse signal, with its envelope described by a hyperbolic tangent function. Both uniform and non-uniform asymptotic representations for the signal are found. It is shown when the uniform representation can be reduced to the non-uniform one. The results obtained are illustrated with a numerical example.
physics.optics:Propagation of a Brillouin precursor in a Lorentz dispersive medium is considered. The precursor is excited by a sine modulated initial signal, with its envelope described by a hyperbolic tangent function. The purpose of the paper is to show how the rate of growth of the initial signal affects the form of the Brillouin precursor. Uniform asymptotic approach, pertinent to coalescing saddle points, is applied in the analysis. The results are illustrated with numerical examples.
physics.optics:A one-dimensional electromagnetic problem of Sommerfeld precursor evolution, resulting from a finite rise-time signal excitation in a dispersive Lorentz medium is considered. The effect of the initial signal rate of growth as well as of the medium dumping on the precursor shape and its magnitude is discussed. The analysis applied is based on an approach employing uniform asymptotic expansions. In addition, new approximate formulas are given for the location of the distant saddle points which affect local frequency and dumping of the precursor. The results obtained are illustrated numerically and compared with the results known from the literature.
physics.optics:We propose a new class of one-dimensional (1D) photonic waveguides: the fractal photonic crystal waveguides (FPCWs). These structures are photonic crystal waveguides (PCWs) etched with fratal distribution of grooves such as Cantor bars. The transmission properties of the FPCWs are investigated and compared with those of the conventional 1D PCWs. It is shown that the FPCW transmission spectrum has self-similarity properties associated with the fractal distribution of grooves. Furthermore, FPCWs exhibit sharp localized transmissions peaks that are approximately equidistant inside the photonic band gap.
physics.optics:We experimentally investigate the depolarizing power and the polarization entropy of a broad class of scattering optical media. By means of polarization tomography, these quantities are derived from an effective Mueller matrix, which is introduced through a formal description of the multi-mode detection scheme we use, as recently proposed by Aiello and Woerdman (arXiv:quant-ph/0407234). This proposal emphasized an intriguing universality in the polarization aspects of classical as well as quantum light scattering; in this contribution we demonstrate experimentally that this universality is obeyed by a surprisingly wide class of depolarizing media. This, in turn, provides the experimentalist with a useful characterization of the polarization properties of any scattering media, as well as a universal criterion for the validity of the measured data.
physics.optics:We present methods and results of the testing of an inexpensive home-made diffraction limited lens system, the design of which was proposed in a recent paper and which has since been used (with slight alterations) by several research groups. Our system will be used for both: focussing a collimated laser beam at a wavelength of lambda=830 nm down to a narrow spot and for collimating fluorescence light (lambda=780 nm) emitted from rubidium atoms captured in this spot. Useful tests for lens systems include the use of ray tracing software, shear-wave interferometers, the imaging of test charts and of polystyrene beads of a very small size. We present these methods and show how conclusions can be drawn for the design under test.
physics.optics:The propagation of plane waves in an isotropic chiral medium (ICM) is investigated. Simple conditions are derived--in terms of the constitutive parameters of the ICM--for the phase velocity to be directed opposite to the direction of power flow. It is demonstrated that phase velocity and power flow may be oppositely directed provided that the magnetoelectric coupling is sufficiently strong.
physics.optics:The paper examines superradiance in impurity crystals in the field of a coherent phonon wave excited by two ultrashort laser pulses via Raman scattering processes at the moment of preparation of the initial state of an ensemble of emitters. It is shown that by varying the power of the excitation pulses and their mutual direction of propagation, one can control the superradiance parameters and extract data on the electron-phonon coupling constant and its anisotropy.
physics.optics:The collective and single-electron amplification regimes of a non-collinear free electron laser are analyzed within the framework of dispersion equations. The small-signal gain and the conditions for self-amplified excitations are found. The collective excitations in a free electron laser are shown to be favored by the non-collinear arrangement of the relativistic electrons and the laser wave. Implications for free-electron lasing without inversion are discussed.
physics.optics:Using a 3D fully-vectorial nonlinear time-domain analysis we numerically investigate the generation of terahertz radiation by pumping a photonic crystal microcavity out of resonance. High quality factors and a quadratic susceptibility lead to few-cycle terahertz pulses via optical rectification. Material dispersion as well as linear and nonlinear anisotropy is fully accounted for.
physics.optics:We study the bandgap properties of two-dimensional photonic crystals created by a lattice of rods or holes conformed in a symmetric or asymmetric triangular structure. Using the plane-wave analysis, we calculate a minimum value of the refractive index contrast for opening both partial and full two-dimensional spectral gaps for both TM and TE polarized waves. We also analyze the effect of ellipticity of rods and holes and their orientation on the threshold value and the relative size of the bandgap.
physics.optics:We review the latest progress and properties of the families of bright and dark one-dimensional periodic waves propagating in saturable Kerr-type and quadratic nonlinear media. We show how saturation of the nonlinear response results in appearance of stability (instability) bands in focusing (defocusing) medium, which is in sharp contrast with the properties of periodic waves in Kerr media. One of the key results discovered is the stabilization of multicolor periodic waves in quadratic media. In particular, dark-type waves are shown to be metastable, while bright-type waves are completely stable in a broad range of energy flows and material parameters. This yields the first known example of completely stable periodic wave patterns propagating in conservative uniform media supporting bright solitons. Such results open the way to the experimental observation of the corresponding self-sustained periodic wave patterns.
physics.optics:We study the existence and stability of cnoidal periodic wave arrays propagating in uniform quadratic nonlinear media and discover that they become completely stable above a threshold light intensity. To the best of our knowledge, this is the first example in physics of completely stable periodic wave patterns propagating in conservative uniform media supporting bright solitons.
physics.optics:We consider photonic crystal fibers made from arbitrary base materials and derive a unified semi-analytical approach for the dispersion and modal properties which applies to the short-wavelength regime. In particular we calculate the dispersion and the effective index and comparing to fully-vectorial plane wave simulations we find excellent agreement. We also calculate asymptotic results for the mode-field diameter and the V-parameter and from the latter we predict that the fibers are endlessly single mode for a normalized air-hole diameter smaller than 0.42, independently of the base material.
physics.optics:We analyze an experimental setup in which a quasi-monochromatic spatially coherent beam of light is used to probe a paraxial optical scatterer. We discuss the effect of the spatial coherence of the probe beam on the Mueller matrix representing the scatterer. We show that according to the degree of spatial coherence of the beam, the \emph{same} scattering system can be represented by \emph{different} Mueller matrices. This result should serve as a warning for experimentalists.
physics.optics:In a recently published paper (J. of Modern Optics 50 (9) (2003) 1477-1486) a qualitative analysis of the moire effect observed by superposing two grids containing Cantor fractal structures was presented. It was shown that the moire effect is sensible to variations in the order of growth, dimension and lacunarity of the Cantor fractal. It was also verified that self-similarity of the original fractal is inherited by the moire pattern. In this work it is shown that these Cantor moire structures are also fractals and the fractal dimension associated with them is theoretically determined and experimentally measured attending the size of rhombuses in the different orders of growth.
physics.optics:We show that diffraction of electromagnetic radiation (in particular of a visible light) can disappear in propagation through materials with periodically in space modulated refraction index, i.e. photonic crystals. In this way the light beams of arbitrary width can propagate without diffractive broadening and, equivalently, arbitrary light patterns can propagate without diffractive smearing.
physics.optics:We consider a nonlinear optical system in general, and a broad aperture laser in particular in a resonator where the diffraction coefficients are of opposite signs along two transverse directions. The system is described by the hyperbolic Maxwell-Bloch equations, where the spatial coupling is provided by the DAlambert operator rather than by the Laplace operator. We show that this system supports hyperbolic transverse patterns residing on hyperbolas in far field domain, and consisting of stretched vortices in near field domain. These structures are described analytically by normal form analysis, and investigated numerically.
physics.optics:We present a novel, real-time, experimental technique for linear and nonlinear Brillouin zone spectroscopy of photonic lattices. The method relies on excitation with random-phase waves and far-field visualization of the spatial spectrum of the light exiting the lattice. Our technique facilitates mapping the borders of the extended Brillouin zones and the areas of normal and anomalous dispersion within each zone. For photonic lattices with defects (e.g., photonic crystal fibers), our technique enables far-field visualization of the defect mode overlaid on the extended Brillouin zone structure of the lattice. The technique is general and can be used for photonic crystal fibers as well as for periodic structures in areas beyond optics.
physics.optics:A novel class of circular resonators, based on a radial defect surrounded by Bragg reflectors, is studied in detail. Simple rules for the design and analysis of such structures are derived using a transfer matrix formalism. Unlike conventional ring resonators, annular Bragg resonators (ABR) are not limited by the total internal reflection condition, and can exhibit both large free spectral ranges and low bend losses. The Bragg reflection mechanism enables the confinement of light within a defect consisting of a low refractive index medium (such as air). Strong atom-photon interaction can be achieved in such a structure, making it a promising candidate for sensing and cavity QED applications. For sensing applications, we show that the ABR structure can possess significantly higher sensitivity when compared to a conventional ring resonator sensor. Lasing action and low threshold levels are demonstrated in ABR lasers at telecommunication wavelengths under pulsed optical pumping at room temperatures. The impact of the intensity and dimensions of the pump-spot on the emitted spectrum is studied in detail.
physics.optics:This paper has been withdrawn by the authors
physics.optics:A Fourier-space based design approach for the systematic control of single and multiple photon localization states in a 1D lattice is presented. Resultant lattices are aperiodic in nature, such that lattice periodicity is not a useful optimization parameter to achieve novel field localization characteristics. Instead, direct control of field localization comes via control of the Parseval strength competition between the different Fourier components characterizing a lattice. This is achieved via an inverse optimization algorithm, tailoring the aperiodic lattice Fourier components to match that of a target Fourier distribution appropriate for the desired photonic localization properties. We present simulation results indicating the performance of a novel aperiodic lattice exhibiting a doubly-resonant high-Q characteristic.
physics.optics:We consider the resonant optical force acting on a pair of transparent microspheres by the excitation of the Morphology Dependent Resonance (MDR). The bonding and anti-bonding modes of the MDR correspond to strong attractions and repulsions respectively. The dependence of the force on separation and the role of absorption are discussed. At resonance, the force can be enhanced by orders of magnitude so that it will dominate over other relevant forces. We find that a stable binding configuration can be induced by the resonant optical force.
physics.optics:This paper has been withdrawn by the authors
physics.optics:We generalize the projection to orthogonal function basis (including polarization modes) method for nonlinear (Kerr medium) fiber and use this method in a case of two-mode waveguide. We consider orthogonal Bessel functions basis that fit the choice of cylindrical geometry of a fiber. The coupled nonlinear Schr\"{o}dinger equations (CNLS) are derived. Analytic expressions and numerical results for coupling coefficients are given; the fiber parameters dependence is illustrated by plots.
physics.optics:We report a detailed experimental study of vector modulation instability in highly birefringent optical fibers in the anomalous dispersion regime. We prove that the observed instability is mainly induced by vacuum fluctuations. The detuning of the spectral peaks agrees with linear perturbation analysis. The exact shape of the spectrum is well reproduced by numerical integration of stochastic nonlinear Schrodinger equations describing quantum propagation.
physics.optics:We study the transmission of a monochromatic "image" through a paraxial cavity. Using the formalism of self-transform functions, we show that a transverse degenerate cavity transmits the self-transform part of the image, with respect to the field transformation over one round-trip of the cavity. This formalism gives a new insight on the understanding of the behavior of a transverse degenerate cavity, complementary to the transverse mode picture. An experiment of image transmission through a hemiconfocal cavity show the interest of this approach.
physics.optics:The thermal radiation of small conducting particles was investigated in the region where the Stephan-Boltzmann law is not valid and strongly overestimates radiation losses. The new criterion for the particle size, at which black body radiation law fails, was formulated. The approach is based on the magnetic particle polarization, which is valid until very small sizes (cluster size) where due to drop of particle conductivity the electric polarization prevails over the magnetic one. It was also shown that the radiation power of clusters, estimated on the basis of the experimental data, is lower than that given by the Stephan-Boltzmann law.
physics.optics:We present an experimental and theoretical study of cascaded high order Stimulated Brillouin Scatterings (SBS) in single mode fibers. It is shown that because of the back-scattering nature of the process, feedback in the input port is needed for obtaining a significant cascaded effect in nonresonant systems. We also discuss similarities to nonlinear photorefractive processes.
physics.optics:We report experiments in which two photoluminescent samples of Strontium Aluminate pigments and Zinc Sulfide pebbles were quantum entangled via photoexcitation with entangled photons from a mercury lamp and a CRT screen. After photo-excitation, remote triggering of one of the sample with infrared (IR) photons yielded stimulated light variation from the quantum entangled other sample located 4 m away. The initial half-life of Strontium Aluminate is approximately one minute. However, molecules with a longer half-life may be found in the future. These experiments demonstrate that useful quantum information could be transferred through quantum channels via de-excitation of one sample of photoluminescent material quantum entangled with another.
physics.optics:We demonstrate a possibility to stabilize three-dimensional spatiotemporal solitons ("light bullets") in self--focusing Kerr media by means of a combination of dispersion management in the longitudinal direction (with the group-velocity dispersion alternating between positive and negative values) and periodic modulation of the refractive index in one transverse direction, out of the two. The analysis is based on the variational approximation (results of direct three-dimensional simulations will be reported in a follow-up work). A predicted stability area is identified in the model's parameter space. It features a minimum of the necessary strength of the transverse modulation of the refractive index, and finite minimum and maximum values of the soliton's energy. The former feature is also explained analytically.
physics.optics:We show that Coherent Population Oscillations effect allows to burn a narrow spectral hole (26Hz) within the homogeneous absorption line of the optical transition of an Erbium ion-doped crystal. The large dispersion of the index of refraction associated with this hole permits to achieve a group velocity as low as 2.7m/s with a ransmission of 40%. We especially benefit from the inhomogeneous absorption broadening of the ions to tune both the transmission coefficient, from 40% to 90%, and the light group velocity from 2.7m/s to 100m/s.
physics.optics:A novel open cavity formed by three 60-degree wedges of a photonic crystal with negative effective index is designed and studied. The quality factor of the open cavity can be larger than 2000. The influence of the interface termination on the resonant frequency and the quality factor is studied.
physics.optics:We report electro-absorption modulation of light at around 1550 nm in a unipolar InGaAlAs optical waveguide containing a InGaAs/AlAs double-barrier resonant tunneling diode (DB-RTD). The RTD peak-to-valley transition increases the electric field across the waveguide, which shifts the core material absorption band-edge to longer wavelengths via the Franz-Keldysh effect, thus changing the light-guiding characteristics of the waveguide. Low-frequency characterisation of a device shows modulation up to 28 dB at 1565 nm. When dc biased close to the negative differential conductance (NDC) region, the RTD optical waveguide behaves as an electro-absorption modulator integrated with a wide bandwidth electrical amplifier, offering a potential advantage over conventional pn modulators.
physics.optics:We propose a new scheme to achieve sub-Rayleigh resolution of interference pattern with independent laser beams. We perform an experimental observation of a double-slit interference with two orthogonally polarized laser beams. The resolution of the interference pattern measured by a two-photon detection is doubled provided the two beams illuminate the double-slit with certain incident angles. The scheme is simple and can be in favor of both high intensity and perfect visibility.
physics.optics:We recently reported on the first realization of ultraviolet photonic crystal laser based on zinc oxide [Appl. Phys. Lett. {\bf 85}, 3657 (2004)]. Here we present the details of structural design and its optimization. We develop a computational super-cell technique, that allows a straightforward calculation of the photonic band structure of ZnO photonic crystal slab on sapphire substrate. We find that despite of small index contrast between the substrate and the photonic layer, the low order eigenmodes have predominantly transverse-electric (TE) or transverse-magnetic (TM) polarization. Because emission from ZnO thin film shows strong TE preference, we are able to limit our consideration to TE bands, spectrum of which can possess a complete photonic band gap with an appropriate choice of structure parameters. We demonstrate that the geometry of the system may be optimized so that a sizable band gap is achieved.
physics.optics:We have performed near-field spectroscopy and microscopy of the InAs/AlGaAs quantum-dot-based nanoclusters. It is observed that the photoluminescence spectra of spatially confined excitons in the nanoclusters is blue-shifted up to 20 meV as the power density is increased. In particular, the near-field photoluminescence images have shown that excitons became spatially confined gradually from lower energy state (1.4150 eV) to higher energy state (1.4392 eV) as the excitation power is increased, which is indicative of the band-filling effect of semiconductor nanostructures.
physics.optics:We propose a novel method enabling to create a high-contrast dark resonance in the 87Rb vapor D2-line. The method is based on an optical pumping of atoms into the working states by a two-frequency, linearly-polarized laser radiation propagating perpendicularly to the probe field. This new scheme is compared to the traditional scheme involving the circularly-polarized probe beam only, and significant improvement of the dark resonance parameters is found. Qualitative considerations are confirmed by numerical calculations.
physics.optics:We demonstrate a dramatic change in the interaction forces between dark solitons in nonlocal nonlinear media. We present what we believe is the first experimental evidence of attraction of dark solitons. Our results indicate that attraction should be observable in other nonlocal systems, such as Bose-Einstein condensates with repulsive long-range interparticle interaction.
physics.optics:We consider waveguides formed by single or multiple two-dimensional chaotic cavities connected to leads. The cavities are chaotic in the sense that the ray (or equivalently, classical particle) dynamics within them is chaotic. Geometrical parameters are chosen to produce a mixed phase space (chaotic regions surrounding islands of stability where motion is regular). Incoming rays (or particles) cannot penetrate into these islands but incoming plane waves dynamically tunnel into them at a certain discrete set of frequencies (energies). The support of the corresponding quasi-bound states is along the trajectories of periodic orbits trapped within the cavity. We take advantage of this difference in the ray/wave behavior to demonstrate how chaotic waveguides can be used to design beam splitters and microlasers. We also present some preliminary experimental results in a microwave realization of such chaotic waveguide.
physics.optics:We propose the construction of electromagnetic (or electronic) switches and beam splitters using chaotic two-dimensional multi-port waveguides. A prototype two-port waveguide is locally deformed in order to produce a ternary incomplete horseshoe proper of mixed phase space (chaotic regions surrounding islands of stability where motion is regular). Due to tunneling to the phase space stability islands the appearance of quasi-bound states (QBS) is induced. Then, we attach transversal ports to the waveguide on the deformation region in positions where the phase space structure is only slightly perturbed. We show how QBS can be guided out of the waveguide through the attached transversal ports giving rise to frequency selective switches and beam splitters.
physics.optics:We report on high-resolution photoluminescence (PL) spectroscopy of spatial structure of InAs/AlGaAs quantum dots (QDs) by using a near-field scanning optical microscope (NSOM). The double-peaked distribution of PL spectra is clearly observed, which is associated with the bimodal size distribution of single QDs. In particular, the size difference of single QDs, represented by the doublet spectral distribution, can be directly observed by the NSOM images of PL.
physics.optics:We show that nonlinear phase shifts and third-order dispersion can compensate each other in short-pulse fiber amplifiers. In particular, we consider chirped-pulse fiber amplifiers at wavelengths for which the fiber dispersion is normal. The nonlinear phase shift accumulated in the amplifier can be compensated by the third-order dispersion of the combination of a fiber stretcher and grating compressor. A numerical model is used to predict the compensation, and initial experimental results that exhibit the main features of the calculations are presented. In the presence of third-order dispersion, an optimal nonlinear phase shift reduces the pulse duration, and enhances the peak power and pulse contrast compared to the pulse produced in linear propagation. Contrary to common belief, fiber stretchers can perform as well or better than grating stretchers in fiber amplifiers, while offering the major practical advantages of a waveguide medium. The relative benefits of a fiber stretcher increase with increasing pulse energy, so the results presented here will be relevant to fiber amplifiers designed for the highest pulse energies.
physics.optics:This paper is devoted to the theoretical and experimental demonstration of the possibility to perform time-resolved diffusing wave spectroscopy: we successfully registered field fluctuations for selected photon path lengths that can overpass 300 transport mean free paths. Such a performance opens new possibilities for biomedical optics applications.
physics.optics:An analytically expression for the temperature dependence of the signal gain of an erbium-doped fiber amplifier (EDFA) pumped at 1480 \textit{nm} are theoretically obtained by solving the propagation equations with the amplified spontaneous emission (ASE). It is seen that the temperature dependence of the gain strongly depends on the distribution of population of Er$^{3+}$-ions in the second level. In addition, the output pump power and the intrinsic saturation power of the signal beam are obtained as a function of the temperature. Numerical calculations are carried out for the temperature range from $-$ 20 to $+$ 60 \textit {$^{o}$C} and the various fiber lengths. But the other gain parameters, such as the pump and signal wavelengths and their powers, are taken as constants. It is shown that the gain decreases with increasing temperature within the range of $L\leq 27 m$.
physics.optics:We derive a coupled mode theory for the interaction of an optical cavity with a waveguide that includes waveguide dispersion. The theory can be applied to photonic crystal cavity waveguide structures. We derive an analytical solution to the add and drop spectra arising from such interactions in the limit of linear dispersion. In this limit, the spectra can accurately predict the cold cavity quality factor (Q) when the interaction is weak. We numerically solve the coupled mode equations for the case of a cavity interacting with the band edge of a periodic waveguide, where linear dispersion is no longer a good approximation. In this regime, the density of states can distort the add and drop spectra. This distortion can lead to more than an order of magnitude overestimation of the cavity Q.
physics.optics:We report a new computational method based on the recursive Green's function technique for calculation of light propagation in photonic crystal structures. The advantage of this method in comparison to the conventional finite-difference time domain (FDTD) technique is that it computes Green's function of the photonic structure recursively by adding slice by slice on the basis of Dyson's equation. This eliminates the need for storage of the wave function in the whole structure, which obviously strongly relaxes the memory requirements and enhances the computational speed. The second advantage of this method is that it can easily account for the infinite extension of the structure both into the air and into the space occupied by the photonic crystal by making use of the so-called "surface Green's functions". This eliminates the spurious solutions (often present in the conventional FDTD methods) related to e.g. waves reflected from the boundaries defining the computational domain. The developed method has been applied to study scattering and propagation of the electromagnetic waves in the photonic band-gap structures including cavities and waveguides. A particular attention has been paid to surface modes residing on a termination of a semi-infinite photonic crystal. We demonstrate that coupling of the surface states with incoming radiation may result in enhanced intensity of an electromagnetic field on the surface and very high Q factor of the surface state. This effect can be employed as an operational principle for surface-mode lasers and sensors.
physics.optics:An array of pairs of parallel gold nanorods is shown to have a negative refractive index n'=-0.3 at the optical communication wavelength of 1.5 micron. This effect results from the plasmon resonance in the pairs of nanorods for both the electric and magnetic components of light. The refractive index is retrieved from the direct phase and amplitude measurements for transmission and reflection, which are all in excellent agreement with our finite difference time domain simulations. The refraction critically depends on the phase of the transmitted wave, which emphasizes the importance of phase measurements in finding n'.
physics.optics:Coupling of external light signals into a photonic crystal waveguide becomes increasingly inefficient as the group velocity of the waveguiding mode slows down. We have systematically studied the efficiency of coupling in the slow light regime for samples with different truncations of the photonic lattice at the coupling interface. Inverse power law dependence is found to best fit the experimental scaling of the coupling loss on the group index. Coupling efficiency is found to be significantly improved up to group indices of 100 for a truncation of the lattice that favors the appearance of the photonic surface states at the coupling interface in resonance with the slow light mode.
physics.optics:We report on the first observation of hole whispering gallery lasers from semiconductor microcavities with three dimensional optical confinement, with thresholds potentially reducible to micro-to-nano ampere regimes according to a quadratic size-dependent reduction, due to ideal quantum wire properties of the naturally formed photonic quantum rings before imminent recombination in a dynamic steady state fashion. If the device size grows over a critical diameter, the quantum ring whispering gallery then begins to disappear. However, cooperative small hole arrays like 256x256 quantum ring emitters avoid the criticality and open a possibility of constructing practical dense electro-pumped micro-to-nano watt emitter arrays, amenable to mega-to-giga ring emitter chip development via present fabrication techniques.
physics.optics:We report on systematic experimental mapping of the transmission properties of two-dimensional silicon-on-insulator photonic crystal waveguides for a broad range of hole radii, slab thicknesses and waveguide lengths for both TE and TM polarizations. Detailed analysis of numerous spectral features allows a direct comparison of experimental data with 3D plane wave and finite-difference time-domain calculations. We find, counter-intuitively, that the bandwidth for low-loss propagation completely vanishes for structural parameters where the photonic band gap is maximized. Our results demonstrate that, in order to maximize the bandwidth of low-loss waveguiding, the hole radius must be significantly reduced. While the photonic band gap considerably narrows, the bandwidth of low-loss propagation in PhC waveguides is increased up to 125nm with losses as low as 8$\pm$2dB/cm.
physics.optics:Much experimental evidence of superluminal phenomena has been available by electromagnetic wave propagation experiments, with the results showing that the phase time do describe the barrier traversal time. Based on the extrapolated phase time approach and numerical methods, we show that, in contrary to the ordinary Bessel waves of real argument, the group velocities of modified Bessel waves are superluminal, and obtain the following results: 1) the group velocities increase with the increase of propagation distance, which is similar to the evanescent plane-wave cases; 2) for large wave numbers, the group velocities fall off as the wave numbers increase, which is similar to the evanescent plane-wave cases; 3) for small wave numbers, the group velocities increase with the increase of wave numbers, this is different from the evanescent plane-wave cases.
physics.optics:Peculiar light-matter interactions can break the rule that a single beam polarization can address only two states in an optical memory device. Multistate storage of a single beam polarization is achieved using self-induced surface diffraction gratings in a photo-active polymer material. The grating orientation follows the incident light beam polarization direction. The permanent self-induced surface relief grating can be readout in real time using the same laser beam.
physics.optics:We report a nanofabricated medium made of electromagnetically coupled pairs of gold dots with geometry carefully designed at a 10-nm level. The medium exhibits strong magnetic response at visible-light frequencies, including bands with negative \mu. The magnetism arises due to the excitation of quadrupole plasmon resonances. Our approach shows for the first time the feasibility of magnetism at optical frequencies and paves a way towards magnetic and left-handed components for visible optics.
physics.optics:Metal-based negative refractive index materials have been extensively studied in the microwave region. However, negative-index metamaterials have not been realized at near-IR or visible fre-quencies due to difficulties of fabrication and to the generally poorer optical properties of metals at these wavelengths. In this paper, we report the first fabrication and experimental verification of a transversely structured metal-dielectric-metal multilayer exhibiting a negative refractive in-dex around 2 mm. Both the amplitude and the phase of the transmission and reflection were measured experimentally, and are in good agreement with a rigorous coupled wave analysis.
physics.optics:We report on a strategy for achieving negative phase velocity (NPV) in a homogenized composite medium (HCM) conceptualized using the Bruggeman formalism. The constituent material phases of the HCM do not support NPV propagation. The HCM and its constituent phases are isotropic dielectric-magnetic mediums; the real parts of their permittivities/permeabilities are negative-valued whereas the real parts of their permeabilities/permittivities are positive-valued.
physics.optics:Photonic crystal fibers are used in fiber amplifiers and lasers because of the flexibility in the design of mode area and dispersion. However, these quantities depend strongly on the wavelength. The wavelength dependence of gain, nonlinearity and dispersion are investigated here by including the wavelength dependence explicitly in the nonlinear Schr\"odinger equation for photonic crystal fibers with varying periods and hole sizes. The effect of the wavelength dependence of each parameter is studied separately as well as combined. The wavelength dependence of the parameters is shown to create asymmetry to the spectrum and chirp, but to have a moderating effect on pulse broadening. The effect of including the wavelength dependence of nonlinearity in the simulations is demonstrated to be the most significant compared that of dispersion or gain.
physics.optics:The report deals with classical and quantum descriptions of particles that interact with smooth random potentials, for example ultracold atoms in the dipole potential of an optical speckle pattern. In addition, a discussion of the link between Radiative Transfer theory and the underlying coherence theory of wave optics is presented. The Radiative Transfer Equation is shown to emerge as a limiting case of the Bethe-Salpeter Equation, and some next-order corrections are discussed.
physics.optics:Observation of coherent population trapping (CPT) at ground-state Zeeman sublevels of $Cr^{3+}$-ion in ruby is reported. The experiments are performed at room temperature by using both nanosecond optical pulses and nanosecond trains of ultrashort pulses. In both cases sharp drops in the resonantly induced fluorescence are detected as the external magnetic field is varied. Theoretical analysis of CPT in a transient regime due to pulsed action of optical pulses is presented.
physics.optics:Nanostructured metal surfaces comprised of periodically arranged spherical voids are grown by electrochemical deposition through a self-assembled template. Detailed measurements of the angle- and orientation-dependent reflectivity reveal the spectral dispersion, from which we identify the presence of both delocalized Bragg- and localized Mie-plasmons. These couple strongly producing bonding and anti-bonding mixed plasmons with anomalous dispersion properties. Appropriate plasmon engineering of the void morphology selects the plasmon spatial and spectral positions, allowing these plasmonic crystal films to be optimised for a wide range of sensing applications.
physics.optics:We study the free-propagation features of an optical field endowed with a non-uniform polarization pattern with elliptical symmetry. The fields derived in this way are called Elliptically Symmetric Polarized Beams (ESPB for short). Some properties of these fields are analysed. Moreover, it is shown how it is possible to obtain such light beams by applying the results to Bessel-Gauss beams.
physics.optics:The treatment found on most general optics textbooks related to the phase contrast technique imposes limitations on the filter phase and object phase variations in order to mathematically explain it in a simple manner. We consider that this simplified treatment can be misleading in terms of the concept the student may develop and also about the potential applications of the phase contrast technique. In this paper we describe a broader and yet simple explanation of the phase contrast process, creating a parallelism between optical image processing and interferometry.
physics.optics:We demonstrate single-mode lasing at telecommunication wavelengths from a circular nanocavity employing a radial Bragg reflector. Ultra-small modal volume and Sub milliwatt pump threshold level are observed for lasers with InGaAsP quantum well active membrane. The electromagnetic field is shown to be tightly confined within the 300nm central pillar of the cavity. The quality factors of the resonator modal fields are estimated to be on the order of a few thousands.
physics.optics:Optical levitation of a liquid droplet in gas phase was investigated under timedependent change of the gravitational acceleration with specific flight pattern of an airplane. Through multiple trials under linear increase of effective gravitational acceleration, we performed the experiment of ptical trapping of a droplet from 0.3g_0 to 0.9g_0, where g_0 = 9.8 m/s^2. During such change of the effective gravitational acceleration, the trapping position on a droplet with the radius of 14 &#956;m was found to be lowered by ca. 100 &#956;m. The essential feature of the change of the trapping position is reproduced by a theoretical calculation under the framework of ray optics. As far as we know, the present study is the first report on optical levitation under time-dependent gravitational change.
physics.optics:We calculate the static dielectric tensor of a periodic system of aligned anisotropic dielectric cylinders. Exact analytical formulas for the effective dielectric constants for the E- and H- eigenmodes are obtained for arbitrary 2D Bravais lattice and arbitrary cross-section of anisotropic cylinders. It is shown that depending on the symmetry of the unit cell photonic crystal of anisotropic cylinders behaves in the low-frequency limit like uniaxial or biaxial natural crystal. The developed theory of homogenization of anisotropic cylinders is applied for calculations of the dielectric properties of photonic crystals of carbon nanotubes.
physics.optics:Scattering processes in an optical microcavity are investigated for the case of silicon nanocrystals embedded in an ultra-high Q toroid microcavity. Using a novel measurement technique based on the observable mode-splitting, we demonstrate that light scattering is highly preferential: more than 99.8% of the scattered photon flux is scattered into the original doubly-degenerate cavity modes. The large capture efficiency is attributed to an increased scattering rate into the cavity mode, due to the enhancement of the optical density of states over the free space value and has the same origin as the Purcell effect in spontaneous emission. The experimentally determined Purcell factor amounts to 883.
physics.optics:The six-fold rotational symmetry of photonic crystal fibers has important manifestations in the radiated fields in terms of i) a focusing phenomena at a finite distance from the end-facet and ii) the formation of low-intensity satellite peaks in the asymptotic far field. For our study, we employ a surface equivalence principle which allows us to rigorously calculate radiated fields starting from fully-vectorial simulations of the near field. Our simulations show that the focusing is maximal at a characteristic distance from the end-facet. For large-mode area fibers the typical distance is of the order 10 Lambda with Lambda being the pitch of the triangular air-hole lattice of the photonic crystal fiber.
physics.optics:Time Shaping of ultrashort visible pulses has been performed using a specially designed Acousto-Optic Programmable Dispersive Filter of 50% efficiency at the output of a two-stage noncollinear optical parametric amplifier. The set-up is compact and reliable. It provides a tunable shaped source in the visible with unique features: 4 ps shaping window with preserved tunability over 500-650 nm, and pulses as short as 30 fs. Several $\mu$J output energy is easily obtained.
physics.optics:We investigate propagation of light pulses in photonic crystals in the vicinity of the zero-diffraction point. We show that Gaussian pulses due to nonzero width of their spectrum spread weakly in space and time during the propagation. We also find the family of nonspreading pulses, propagating invariantly in the vicinity of the zero diffraction point of photonic crystals.
physics.optics:We measured the overall motion of Brownian particles suspended in water by a self-mixing thin-slice solid-state laser with extreme optical sensitivity. From the demodulated signal of laser intensity fluctuations through self-mixing modulations by the interference between the lasing field and Doppler-shifted scattered fields from Brownian particles, it was found that the changes over time in random walks of small particles suspended in water result in different overall dynamics when the field of vision for particles seen by the laser beam (scale of the observation) is changed. At a small focal volume of the laser beam, in which the relevant diffusion broadening is observed, the overall motion which can be represented by the motion of a ''virtual'' single particle whose velocity obeys a double-peaked non-Gaussian distribution function. The fast motion of a virtual particle, featuring fractional Brownian motion, which leads to a pure random walk with increasing the observation time, has been characterized in terms of mean-square displacements of a virtual particle.
physics.optics:Ordinary metallic photonic crystals (PCs) have photonic band gaps in which the density of states (DOS) is strongly modified. Thermal emission of photons can be suppressed and enhanced accordingly. We consider the thermal emission characteristics of a metallic photonic crystal slab with a tunable thickness which in the thick limit approaches that of a photonic crystal and in the thin limit approaches that of a textured surface. We find that a thick photonic crystal suppresses emission in a specific range while a thin slab suppresses low frequency emission.
physics.optics:For emitters in a medium, different macroscopic or microscopic theoretical models predict substantially different dependencies of the spontaneous emission lifetime on refractive index. Various measurements have been carried out on Eu$^{3+}$, Ce$^{3+}$ and Nd$^{3+}$ ions, and quantum dots in different surrounding medial. The dependence of the spontaneous emission rates on refractive index has been interpreted with different models. By closely examining some of the experimental results, we notice that some interpretations are based on implicit assumptions which are hard to justify, and some measurements contain too big uncertainties to discriminate among different models. In this work we reanalyse the avilable measured results and give a consistent interpretation.
physics.optics:The oblique incidence of a Bessel beam on a dielectric slab with refractive index n1 surrounded by a medium of a refractive index n>n1 may be studied simply by expanding the Bessel beam into a set of plane waves forming the same angle with the axis of the beam. In the present paper we examine a Bessel beam that impinges at oblique incidence onto a layer in such a way that each plane-wave component impinges with an angle larger than the critical angle.
physics.optics:Photoemission electron microscopy was used to image the electrons photoemitted from specially tailored Ag nanoparticles deposited on a Si substrate (with its native oxide SiO$_{x}$). Photoemission was induced by illumination with a Hg UV-lamp (photon energy cutoff $\hbar\omega_{UV}=5.0$ eV, wavelength $\lambda_{UV}=250$ nm) and with a Ti:Sapphire femtosecond laser ($\hbar\omega_{l}=3.1$ eV, $\lambda_{l}=400$ nm, pulse width below 200 fs), respectively. While homogeneous photoelectron emission from the metal is observed upon illumination at energies above the silver plasmon frequency, at lower photon energies the emission is localized at tips of the structure. This is interpreted as a signature of the local electrical field therefore providing a tool to map the optical near field with the resolution of emission electron microscopy.
physics.optics:In this Comment I argue that the method of images used by Huang, Yu, Gu and co-authors [Phys Rev. E, 65, 21401 (2002); Phys Rev. E., 69, 51402 (2004)] to calculate electromagnetic properties of interacting spheres at finite frequencies is inapplicable and does not provide a physically meaningful approximation.
physics.optics:Origin and properties of non-Lorentzian spectral lines in linear chains of nanospheres are discussed. The lines are shown to be super-exponentially narrow with the characteristic width proportional to exp[-C(h/a)^3] where C is a numerical constant, h the spacing between the nanospheres in the chain and a the sphere radius. The fine structure of these spectral lines is also investigated.
physics.optics:Optical microcavities based on zero-group-velocity surface modes in photonic crystal slabs are studied. It is shown that high quality factors can be easily obtained for such microcavities in photonic crystal slabs. With increasing of the cavity length, the quality factor is gradually enhanced and the resonant frequency converges to that of the zero-group-velocity surface mode in the photonic crystal. The number of the resonant modes with high quality factors is mainly determined by the number of surface modes with zero-group velocity.
physics.optics:The backward scattering of TM-polarized light by a two-side-open subwavelength slit in a metal film is analyzed. We show that the reflection coefficient versus wavelength possesses a Fabry-Perot-like dependence that is similar to the anomalous behavior of transmission reported in the study [Y. Takakura, Phys. Rev. Lett. \textbf{86}, 5601 (2001)]. The open slit totally reflects the light at the near-to-resonance wavelengths. In addition, we show that the interference of incident and resonantly backward-scattered light produces in the near-field diffraction zone a spatially localized wave whose intensity is 10-10$^3$ times greater than the incident wave, but one order of magnitude smaller than the intra-cavity intensity. The amplitude and phase of the resonant wave at the slit entrance and exit are different from that of a Fabry-Perot cavity.
physics.optics:The supercell method is used to study the variation of the photonic bandgaps in one-dimensional photonic crystals under random perturbations to thicknesses of the layers. The results of both plane wave and analytical band structure and density of states calculations are presented along with the transmission cofficient as the level of randomness and the supercell size is increased. It is found that higher bandgaps disappear first as the randomness is gradually increased. The lowest bandgap is found to persist up to a randomness level of 55 percent.
physics.optics:We study the effect of rotation on the propagation of electromagnetic waves in slow-light waveguide structures consisting of coupled micro-ring resonators. We show that such configurations exhibit new a type of the Sagnac effect which can be used for the realization of highly-compact integrated rotation sensors and gyroscopes.
physics.optics:The dispersion relations for conventional uniaxial dielectric mediums may be characterized as elliptical or elliptical-like, according to whether the medium is nondissipative or dissipative, respectively. However, under certain constitutive parameter regimes, the dispersion relations may be hyperbolic or hyperbolic-like. We investigate planewave propagation in a hyperbolic/hyperbolic-like uniaxial dielectric medium. For both evanescent and nonevanescent propagation, the phase velocity is found to be positive with respect to the time-averaged Poynting vector. A conceptualization of a hyperbolic--like uniaxial medium as a homogenized composite medium is presented.
physics.optics:We show how to synthesize a CW, single-frequency optical field from the frequency-dispersed, pulsed field of a mode-locked laser. This process, which relies on difference frequency generation in an optical cavity, is efficient and can be considered as an optical rectification. Quantitative estimates for the output power and amplitude noise properties of a realistic system are given. Possible applications to optical frequency synthesis and optical metrology are envisaged.
physics.optics:We study nonlinear coupling of mutually incoherent beams associated with different Floquet-Bloch waves in a one-dimensional optically-induced photonic lattice. We demonstrate experimentally how such interactions lead to asymmetric mutual focusing and, for waves with opposite diffraction properties, to simultaneous focusing and defocusing as well as discreteness-induced beam localization and reshaping effects.
physics.optics:We study the second-harmonic generation in left-handed metamaterials with a quadratic nonlinear response. We demonstrate a novel type of the exact phase matching between the backward propagating wave of the fundamental frequency and the forward propagating wave of the second harmonics. We show that this novel parametric process can convert a surface of the left-handed metamaterial into an effective mirror totally reflecting the second harmonics generated by an incident wave. We derive and analyze theoretically the coupled-mode equations for a semi-infinite nonlinear metamaterial. We also study numerically the second-harmonic generation by a metamaterial slab of a finite thickness, and reveal the existence of multistable nonlinear effects.
physics.optics:Expressions corresponding to the transmission of a uniaxial optically active crystal platelet are provided for an optical axis parallel and perpendicular to the plane of interface. The optical activity is taken into account by a consistent multipolar expansion of the crystal medium response due to the path of an electromagnetic wave. Numerical examples of the effect of the optical activity are given for quartz platelets of chosen thicknesses. The optical activity's effects on the variations of the transmission of quartz platelets as a function of the angle of incidence is also investigated.
physics.optics:We use a phenomenological Hamiltonian approach to derive a set of coupled mode equations that describe light propagation in waveguides that are periodically side-coupled to microcavities. The structure exhibits both Bragg gap and (polariton like) resonator gap in the dispersion relation. The origin and physical significance of the two types of gaps are discussed. The coupled-mode equations derived from the effective field formalism are valid deep within the Bragg gaps and resonator gaps.
physics.optics:We reconsider the basic properties of ray-transfer matrices for first-order optical systems from a geometrical viewpoint. In the paraxial regime of scalar wave optics, there is a wide family of beams for which the action of a ray-transfer matrix can be fully represented as a bilinear transformation on the upper complex half-plane, which is the hyperbolic plane. Alternatively, this action can be also viewed in the unit disc. In both cases, we use a simple trace criterion that arranges all first-order systems in three classes with a clear geometrical meaning: they represent rotations, translations, or parallel displacements. We analyze in detail the relevant example of an optical resonator.
physics.optics:Using a two-dipole model of an optical near-field of Scanning Near-field Optical Microscope tip, i. e. taking into account contributions of magnetic and electric dipoles, we propose and analyze a new type of 3D optical nanotrap found for certain relations between electric and magnetic dipoles. Electric field attains a minimum value in vacuum in the vicinity of the tip and hence such a trap is quite suitable for manipulations with cold atoms.
physics.optics:The propagation of a Bessel beam (or Bessel-X wave) is analyzed on the basis of a vectorial treatment. The electric and magnetic fields are obtained by considering a realistic situation able to generate that kind of scalar field. Specifically, we analyze the field due to a ring-shaped aperture over a metallic screen on which a linearly polarized plane wave impinges. On this basis, and in the far field approximation, we can obtain information about the propagation of energy flux and the velocity of the energy.
physics.optics:Under certain circumstances, the group velocity in a homogenized composite medium (HCM) can exceed the group velocity in its component material phases. We explore this phenomenon for a uniaxial dielectric HCM comprising isotropic component material phases distributed as oriented spheroidal particles. The theoretical approach is based upon the Bruggeman homogenization formalism. Enhancement in group velocity in the HCM with respect to the component material phases is shown to be sensitively dependent upon the shape of the component spheroids and their alignment relative to the direction of propagation.
physics.optics:It is shown that the copropagating three-wave-mixing parametric process, with appropriate type-II extended phase matching and pumped with a short second-harmonic pulse, can perform spectral phase conjugation and parametric amplification, which shows a threshold behavior analogous to backward wave oscillation. The process is also analyzed in the Heisenberg picture, which predicts a spontaneous parametric down conversion rate in agreement with the experimental result reported by Kuzucu et al. [Phys. Rev. Lett. 94, 083601 (2005)]. Applications in optical communications, signal processing, and quantum information processing can be envisaged.
physics.optics:Radiation loss and resonant frequency shift due to sidewall surface roughness of circular and square high-contrast microcavities are estimated and compared by using a boundary integral equations method. An effect of various harmonic components of the contour perturbation on the Whispering-Gallery (WG) modes in the circular microdisk and WG-like modes in the square microcavity is demonstrated. In both cases, contour deformations that are matched to the mode field pattern cause the most significant frequency detuning and Q-factor change. Favorably mode-matched deformations have been found, enabling one to manipulate the Q-factors of the microcavity modes.
physics.optics:A diffractive arrangement that allows imaging of an object without any intermediate or complementary element is presented. This optical system with only two diffraction gratings forms color images with white light.
physics.optics:Linear defect modes in one-dimensional photonic lattices are studied theoretically. For negative (repulsive) defects, various localized defect modes are found. The strongest confinement of the defect modes appear when the lattice intensity at the defect site is {\em non-zero} rather than zero. When launched at small angles into such a defect site of the lattice, a Gaussian beam can be trapped and undergo snake oscillations under appropriate conditions.
physics.optics:We report on phenomenon of Anderson-type localization of walking solitons in optical lattices with random frequency modulation, manifested as dramatic enhancement of soliton trapping probability on lattice inhomogeneities with growth of the frequency fluctuation level. The localization process is strongly sensitive to the lattice depth since in shallow lattices walking solitons experience random refraction and/or multiple scattering in contrast to relatively deep lattices, where solitons are typically immobilized in the vicinity of local minimums on modulation frequency.
physics.optics:We consider Fabry-Perot cavity resonance in periodic stacks of anisotropic layers with misaligned in-plane anisotropy at the frequency close to a photonic band edge. We show that in-plane dielectric anisotropy can result in a dramatic increase in field intensity and group delay associated with the transmission resonance. The field enhancement appears to be proportional to forth degree of the number N of layers in the stack. By contrast, in common periodic stacks of isotropic layers, those effects are much weaker and proportional to N^2. Thus, the anisotropy allows to drastically reduce the size of the resonance cavity with similar performance. The key characteristic of the periodic arrays with the gigantic transmission resonance is that the dispersion curve omega(k)at the photonic band edge has the degenerate form Delta(omega) ~ Delta(k)^4, rather than the regular form Delta(omega) ~ Delta(k)^2. This can be realized in specially arranged stacks of misaligned anisotropic layers. The degenerate band edge cavity resonance with similar outstanding properties can also be realized in a waveguide environment, as well as in a linear array of coupled multimode resonators, provided that certain symmetry conditions are in place.
physics.optics:Propagation and tunneling of light through photonic barriers formed by thin dielectric films with continuous curvilinear distributions of dielectric susceptibility across the film, are considered. Giant heterogeneity-induced dispersion of these films, both convex and concave, and its influence on their reflectivity and transmittivity are visualized by means of exact analytical solutions of Maxwell equations. Depending on the cut-off frequency of the film, governed by the spatial profile of its refractive index, propagation or tunneling of light through such barriers are examined. Subject to the shape of refractive index profile the group velocities of EM waves in these films are shown to be either increased or deccreased as compared with the homogeneous layers; however, these velocities for both propagation and tunneling regimes remain subluminal. The decisive influence of gradient and curvature of photonic barriers on the efficiency of tunneling is examined by means of generalized Fresnel formulae. Saturation of the phase of the wave tunneling through a stack of such films (Hartman effect), is demonstrated. The evanescent modes in lossy barriers and violation of Hartman effect in this case is discussed.
physics.optics:The theoretical work of V.B. Braginsky predicted that radiation pressure can couple the mechanical, mirror-eigenmodes of a Fabry-Perot resonator to it's optical modes, leading to a parametric oscillation instability. This regime is characterized by regenerative mechanical oscillation of the mechanical mirror eigenmodes. We have recently observed the excitation of mechanical modes in an ultra-high-Q optical microcavity. Here, we present a detailed experimental analysis of this effect and demonstrate that radiation pressure is the excitation mechanism of the observed mechanical oscillations.
physics.optics:We propose and analyze a new type of a resonant high-Q cavity for lasing, sensing or filtering applications, which is based on a surface states of a finite photonic crystal. We demonstrate that such the cavity can have a Q factor comparable with that one of conventional photonic band-gap defect mode cavities. At the same time, the distinguished feature of the surface mode cavity is that it is situated directly at the surface of the photonic crystal. This might open up new possibilities for design of novel photonic devices and integration of photonic circuits.
physics.optics:Resonant electromagnetic modes are analyzed inside a dielectric cavity of equilateral triangular cross section and refractive index n, surrounded by a uniform medium of refractive index n'. The field confinement is determined only under the requirements needed to maintain total internal reflection of the internal electromagnetic fields, matched to exponentially decaying evanescent waves outside the cavity. Two-dimensional electromagnetics is considered, with no dependence on the coordinate perpendicular to the cross section; hence, independent transverse electric (TE) and transverse magnetic (TM) polarizations are described separately. A linear combination of six plane waves is sufficient within the cavity, whose wavevectors are related by 120-degree rotations and whose phases are related by Fresnel reflection coefficients. Generally, the mode spectrum becomes sparse and the minimum mode frequency increases rapidly as the index ratio N=n/n' approaches 2. For specified quantum numbers and N, the TM modes are lower in frequency than the TE modes. Assuming the evanescent boundary waves escape at the triangle vertices, TE modes generally are found to have greater confinement of the fields inside the cavity and much higher quality factors than TM modes.
physics.optics:We study the optical properties of metamaterials made from cut-wire pairs or plate pairs. We obtain a more pronounced optical response for arrays of plate pairs -- a geometry which also eliminates the undesired polarization anisotropy of the cut-wire pairs. The measured optical spectra agree with simulations, revealing negative magnetic permeability in the range of telecommunications wavelengths. Thus, nanoscopic plate pairs might serve as an alternative to the established split-ring resonator design.
physics.optics:The four-wave mixing produced with two ultrashort phase-locked w-3w laser pulses propagating coherently in a two-level system in the infrared spectral region is shown to depend on the pulses relative phase. The Maxwell-Bloch equations are solved beyond the rotating-wave approximation to account for field frequencies which are largely detuned from the atomic resonance. The relative phase dominating the efficiency of the coupling to the 5w anti-Stokes Raman component is determined by sign of the total ac Stark shift induced in the system, in such a way that the phase influence disappears precisely where the ac Stark effect due to both pulses is compensated. This fundamental quantum interference effect can be the basis for nonlinear ultrafast optical spectroscopy techniques.
physics.optics:We study the beaming effect of light for the case of increased-index photonic crystal (PhC) waveguides, formed through the omission of low-dielectric media in the waveguide region. We employ the finite-difference time-domain numerical method for characterizing the beaming effect and determining the mechanisms of loss and the overall efficiency of the directional emission. We find that, while this type of PhC waveguides is capable of producing a highly collimated emission as was demonstrated experimentally, the inherent characteristics of the structure result in a restrictively low efficiency in the coupling of light into the collimated beam of light.
physics.optics:The existence of phase resonances in obliquely illuminated, perfectly conducting compound gratings is investigated. The diffraction problem of a p-polarized plane wave impinging on the structure is solved using the modal approach. The results show that even under oblique illumination, where no symmetry is imposed by the incident field, there are resonant wavelengths that are clearly associated with a certain degree of symmetry in the phase distribution of the magnetic field inside the cavities. New configurations of this phase distribution take place, that were not allowed under normal incidence conditions. It was found that the interior field is intensified in the resonances, and the specularly reflected efficiency is maximized. In particular, this efficiency is optimized for Littrow mount.
physics.optics:We report the measurement of the r23 and r33 electro-optic coefficients and of the z-axis electrical conductivity of the nonlinear optical crystals Na:KTP and RTA. We observed a marked decrease of the electro-optic effect in the Na:KTP crystals for frequencies below 100 Hz. This effect is absent in RTA.
physics.optics:Under certain conditions, a transparent photonic band can be designed into a one-dimensional metallodielectric nanofilm structure. Unlike conventional pass bands in photonic crystals, where the finite thickness of the structure affects the transmission of electromagnetic fields having frequency within the pass band, the properties of the transparent band are almost unaffected by the finite thickness of the structure. In other words, an incident field at a frequency within the transparent band exhibits 100% transmission independent of the number of periods of the structure. The transparent photonic band corresponds to excitation of pure eigenstate modes across the entire Bloch band in structures possessing mirror symmetry. The conditions to create these modes and thereby to lead to a totally transparent band phenomenon are discussed.
physics.optics:We study the higher order harmonics of scalar modulational instability in the regime where it arises spontaneously through amplification of vacuum fluctuations. We obtain detailed predictions concerning the detunings, intensities, growth rates and spectral widths of the harmonics. These predictions are well verified by experimental results obtained by propagating high intensity light pulses through optical fibers.
physics.optics:We have shown within the quasistatic approximation that the giant fluctuations of local electromagnetic field in random fractal aggregates of silver nanospheres are strongly correlated with a local anisotropy factor S which is defined in this paper. The latter is a purely geometrical parameter which characterizes the deviation of local environment of a given nanosphere in an aggregate from spherical symmetry. Therefore, it is possible to predict the sites with anomalously large local fields in an aggregate without explicitly solving the electromagnetic problem. We have also demonstrated that the average (over nanospheres) value of S does not depend noticeably on the fractal dimension D, except when D approaches the trivial limit D=3. In this case, as one can expect, the average local environment becomes spherically symmetrical and S approaches zero. This corresponds to the well-known fact that in trivial aggregates fluctuations of local electromagnetic fields are much weaker than in fractal aggregates. Thus, we find that, within the quasistatics, the large-scale geometry does not have a significant impact on local electromagnetic responses in nanoaggregates in a wide range of fractal dimensions. However, this prediction is expected to be not correct in aggregates which are sufficiently large for the intermediate- and radiation-zone interaction of individual nanospheres to become important.
physics.optics:A possibility of tuning the phase of the third-order Kerr-type nonlinear susceptibility in a system consisting of two interacting metal nanospheres and a nonlinearly polarizable molecule is investigated theoretically and numerically. It is shown that by varying the relative inter-sphere separation, it is possible to tune the phase of the effective nonlinear susceptibility \chi^{(3)}(\omega;\omega,\omega,-\omega)$ in the whole range from 0 to $2\pi$.
physics.optics:We present an optical interference model to create chiral microstructures (spirals) and its realization in photoresist using holographic lithography. The model is based on the interference of six equally-spaced circumpolar linear polarized side beams and a circular polarized central beam. The pitch and separation of the spirals can be varied by changing the angle between the side beams and the central beam. The realization of the model is carried out using the 325 nm line of a He-Cd laser and spirals of sub-micron size are fabricated in photoresist.
physics.optics:We studied the optical properties of a dielectric photonic crystal structure with spirals arranged in a hexagonal lattice. The dielectric constant of the material is 9 and the filling ratio is 15.2%. We found that this kind of structure exhibits a significant polarization gap for light incident along the axis of the spirals. The eigenmodes inside the polarization gap are right-hand (left-hand) circularly polarized depending on the whether the spirals are left-handed (right-handed). The transmission spectrum of a slab of such a structure has been calculated and matches well with the analysis of the eigenmodes.
physics.optics:The evanescent coupling from a photonic crystal resonator to a micron-thick optical fiber is investigated in detail by using a 3D-FDTD method. Properly designed photonic crystal cavity and taper structures are proposed, and optimal operating conditions are found to enhance the coupling strength while suppressing other cavity losses including the coupling to the slab propagating mode and to the higher-order fiber mode. In simulation, the coupling into the fundamental fiber mode is discriminated from other cavity losses by spatial and parity filtering of the FDTD results. The coupling efficiency of more than 80% into the fundamental fiber mode together with a total Q factor of 5,200 is achieved for the fiber diameter of 1.0 um and the air gap of 200 nm between the fiber and the cavity.
physics.optics:The backscattering of circularly polarized light at normal incidence to a half-space of scattering particles is studied using the Electric Field Monte Carlo (EMC) method. The spatial distribution of the backscattered light intensity is examined for both the time-resolved and continuous-wave cases for large particles with anisotropy factor, g, in the range 0.8 to 0.97. For the time-resolved case, the backscattered light with the same helicity as that of the incident beam (co-polarized) is found to form a ring centered on the point of incidence. The ring expands and simultaneously grows weak as time increases. The intensity of backscattered light with helicity opposite to that of the incident beam (cross-polarized) is found to exhibit a ring behavior for g>=0.85, with significant backscattering at the point of incidence. For the continuous-wave case no such ring pattern is observed in backscattered light for either helicity. The present EMC study suggests that the ring behavior can only be observed in the time domain, in contrast to previous studies of light backscattered from forward scattering media based on the scalar time-independent Fokker-Planck approximation to the radiative transfer equation. The time-dependent ring structure of backscattered light may have potential use in subsurface imaging applications.
physics.optics:Three dimensional images having continuous horizontal parallax were developed by wavelength enconding of view followed by a natural decoding process of projection onto a diffractive screen. It allows for the direct criation of "holoimages" by using projected images of a real object substituting holographic images for some applications in the visual aarts. It also allows for the enlargement of holograms performing simultaneously its conversion to the white light observation. White light is employed in this process, since laser light is only necessary for constructing the diffractive components.
physics.optics:This article recounts definition, classification, history, and applications of microstructured optical fibers.
physics.optics:The temperature-dependent photoluminescence (PL) spectra of zinc oxide (ZnO) nanocrystals deposited inside the ultraviolet (UV) opal were studied. ZnO was grown in the voids between FCC packed silicon dioxide spheres using spray pyrolysis under ultrasonic vibration in the solution containing a zinc nitrate precursor. The ZnO nanoparticles inside opal matrix with UV photonic band-gap exhibit suppression of the excitonic emission and enhancement of the deep level emission. Suppression of the excitonic lines is due to the inhibition of spontaneous emission, while enhancement and broadening of the DL emission in the green spectral region is due to Purcell effect. The infiltration of ZnO inside the photonic crystal may be a useful technique to increase its emission efficiency in the selected spectral region.
physics.optics:A novel, 3-dimensional, convex, multi-pass optical cavity with partially-chaotic ray dynamics is presented. The light is localized near stable, long-path length trajectories supported by the cavity, and beam diffraction is suppressed by the phase space barriers between the regions of regular and chaotic ray dynamics that are generally present in partially-chaotic systems. For a centimeter-size cavity, the design supports meter-scale optical path lengths, suggesting future applications in trace gas detection. An exemplary cavity has been fabricated from a hollow, gold-coated, acrylic shell. Our measurements using a HeNe laser and a pulsed red diode laser for characterization of the cavity beam pattern and optical path length, respectively, confirm the theoretically predicted optical dynamics and the ability of the cavity to support meter-scale path lengths.
physics.optics:We report on measurements of visible extinction spectra of semicontinuous silver nanoshells grown on colloidal silica spheres. We find that thin, fractal shells below the percolation threshold exhibit geometrically tunable plasmon resonances. A modified scaling theory approach is used to model the dielectric response of such shells, which is then utilized to obtain the extinction cross section in a retarded Mie scattering formalism. We show that such spherical resonators support unique plasmon dynamics: in the visible there is a new regime of coherently driven cluster-localized plasmons, while crossover to homogeneous response in the infrared predicts a delocalized shell plasmon.
physics.optics:We describe a semi-analytical approach for three-dimensional analysis of photonic crystal fibre devices. The approach relies on modal transmission-line theory. We offer two examples illustrating the utilization of this approach in photonic crystal fibres: the verification of the coupling action in a photonic crystal fibre coupler and the modal reflectivity in a photonic crystal fibre distributed Bragg reflector.
physics.optics:We demonstrate ultrafast all-optical deflection of spatial solitons in an AlGaAs slab waveguide using 190 fs, 1550 nm pulses which are used to generate and deflect the spatial soliton. The steering beam is focused onto the top of the waveguide near the soliton pathway and the soliton is steered due to refractive index changes induced by optical Kerr, or free carrier (Drude) effects. Angular deflections up to 8 mR are observed.
physics.optics:A highly unusual pattern of strong multiple resonances for bulk electromagnetic waves is predicted and analysed numerically in thick periodic holographic gratings in a slab with the mean permittivity that is larger than that of the surrounding media. This pattern is shown to exist in the geometry of grazing-angle scattering (GAS), that is when the scattered wave (+1 diffracted order) in the slab propagates almost parallel to the slab (grating) boundaries. The predicted resonances are demonstrated to be unrelated to resonant generation of the conventional guided modes of the slab. Their physical explanation is associated with resonant generation of a completely new type of eigenmodes in a thick slab with a periodic grating. These new slab eigenmodes are generically related to the grating; they do not exist if the grating amplitude is zero. The field structure of these eigenmodes and their dependence on structural and wave parameters is analysed. The results are extended to the case of GAS of guided modes in a slab with a periodic groove array of small corrugation amplitude and small variations in the mean thickness of the slab at the array boundaries.
physics.optics:Grazing-angle scattering (GAS) is a type of Bragg scattering of waves in slanted non-uniform periodic gratings, when the diffracted order satisfying the Bragg condition propagates at a grazing angle with respect to the boundaries of a slab-like grating. Rigorous analysis of GAS of bulk TE electromagnetic waves is undertaken in holographic gratings by means of the enhanced T-matrix algorithm. A comparison of the rigorous and the previously developed approximate theories of GAS is carried out. A complex pattern of numerous previously unknown resonances is discovered and analysed in detail for gratings with large amplitude, for which the approximate theory fails. These resonances are associated not only with the geometry of GAS, but are also typical for wide transmitting gratings. Their dependence on grating amplitude, angles of incidence and scattering, and grating width is investigated numerically. Physical interpretation of the predicted resonances is linked to the existence and the resonant generation of special new eigenmodes of slanted gratings. Main properties of these modes and their field structure are discussed.
physics.optics:Double-resonant extremely asymmetrical scattering (DEAS) is a strongly resonant type of Bragg scattering in two joint or separated uniform gratings with different phases. It is characterised by a very strong increase of the scattered and incident wave amplitudes inside and between the gratings at a resonant phase shift between the gratings. DEAS is realised when the first diffracted order satisfying the Bragg condition propagates parallel to the grating boundaries, and the joint or separated gratings interact by means of the diffractional divergence of the scattered waves from one grating into another. This Letter develops a theory of non-steady-state DEAS of bulk TE electromagnetic waves in holographic gratings, and investigates the process of relaxation of the incident and scattered wave amplitudes to their steady-state values inside and outside the gratings. Typical relaxation times are determined. Physical explanation of the predicted effects is presented.
physics.optics:Extremely asymmetrical scattering (EAS) is a new type of Bragg scattering in thick, slanted, periodic gratings. It is realised when the scattered wave propagates parallel to the front boundary of the grating. Its most important feature is the strong resonant increase in the scattered wave amplitude compared to the amplitude of the incident wave: the smaller the grating amplitude, the larger the amplitude of the scattered wave. In this paper, rigorous numerical analysis of EAS is carried out by means of the enhanced T-matrix algorithm. This includes investigation of harmonic generation inside and outside the grating, unusually strong edge effects, fast oscillations of the incident wave amplitude in the grating, etc. Comparison with the previously developed approximate theory is carried out. In particular, it is demonstrated that the applicability conditions for the two-wave approximation in the case of EAS are noticeably more restrictive than those for the conventional Bragg scattering. At the same time, it is shown that the approximate theory is usually highly accurate in terms of description of EAS in the most interesting cases of scattering with strong resonant increase of the scattered wave amplitude. Physical explanation of the predicted effects is presented.
physics.optics:Although coherent light is usually required for the self-organization of regular spatial patterns from optical beams, we show that peculiar light matter interaction can break this evidence. In the traditional method to record laser-induced periodic surface structures, a light intensity distribution is produced at the surface of a polymer film by an interference between two coherent optical beams. We report on the self-organization followed by propagation of a surface relief pattern. It is induced in a polymer film by using a low-power and small-size coherent beam assisted by a high-power and large-size incoherent and unpolarized beam. We demonstrate that we can obtain large size and well organized patterns starting from a dissipative interaction. Our experiments open new directions to improve optical processing systems.
physics.optics:Extremely asymmetrical scattering (EAS) is an unusual type of Bragg scattering in slanted periodic gratings with the scattered wave (the +1 diffracted order) propagating parallel to the grating boundaries. Here, a unique and strong sensitivity of EAS to small stepwise variations of mean structural parameters at the grating boundaries is predicted theoretically (by means of approximate and rigorous analyses) for bulk TE electromagnetic waves and slab optical modes of arbitrary polarization in holographic (for bulk waves) and corrugation (for slab modes) gratings. The predicted effects are explained using one of the main physical reasons for EAS--the diffractional divergence of the scattered wave (similar to divergence of a laser beam). The approximate method of analysis is based on this understanding of the role of the divergence of the scattered wave, while the rigorous analysis uses the enhanced T-matrix algorithm. The effect of small and large stepwise variations of the mean permittivity at the grating boundaries is analysed. Two distinctly different and unusual patterns of EAS are predicted in the cases of wide and narrow (compared to a critical width) gratings. Comparison between the approximate and rigorous theories is carried out.
physics.optics:Low crosstalk intersections formed by two crossing coupled cavity waveguides (CCWs), which are formed by series of \textit{mono-mode} cavities in a two dimensional photonic crystal structure, are investigated. Although the individual cavities are \textit{mono-mode}, the modes of the properly designed CCWs (which are called \textit{supermodes} in this paper) may be dipole-like, and antisymmetry along the axis of the other perpendicular CCW, which ensures the low crosstalk of the intersections. According to the results of coupled mode analysis, we divide the intersections into two kinds based on the number of cavities of each CCW, and the transmission and crosstalk spectra of both kinds are numerically obtained using finite difference time domain (FDTD) method. The results show that crosstalk power is lower than -30dB. We also discuss the factors that affect the performance of the intersections.
physics.optics:An extended interference pattern close to surface may result in both a transmissive or evanescent surface fields for large area manipulation of trapped particles. The affinity of differing particle sizes to a moving standing wave light pattern allows us to hold and deliver them in a bi-directional manner and importantly demonstrate experimentally particle sorting in the sub-micron region. This is performed without the need of fluid flow (static sorting). Theoretical calculations experimentally confirm that certain sizes of colloidal particles thermally hop more easily between neighboring traps. A new generic method is also presented for particle position detection in an extended periodic light pattern and applied to characterization of optical traps and particle behavior
physics.optics:We combine a tunable continuous-wave optical parametric oscillator and a femtosecond Ti:Sapphire laser frequency comb to provide a phase-coherent bridge between the visible and mid-infrared spectral ranges. As a first demonstration of this new technique we perform a direct frequency comparison between an iodine stabilized Nd:YAG laser at 1064 nm and an infrared methane optical frequency standard at $3.39 \mu$m.
physics.optics:The reorientational nonlinearity of nematic liquid crystals enables a self-localized spatial soliton and its waveguide to be deflected or destroyed by a control beam propagating across the cell. We demonstrate a simple all-optical readdressing scheme by exploiting the lens-like perturbation induced by an external beam on both a nematicon and a co-polarized guided signal of different wavelength. Angular steering as large as 2.2 degrees was obtained for control powers as low as 32mW in the near infrared.
physics.optics:Coherent X-ray diffraction microscopy is a method of imaging non-periodic isolated objects at resolutions only limited, in principle, by the largest scattering angles recorded. We demonstrate X-ray diffraction imaging with high resolution in all three dimensions, as determined by a quantitative analysis of the reconstructed volume images. These images are retrieved from the 3D diffraction data using no a priori knowledge about the shape or composition of the object, which has never before been demonstrated on a non-periodic object. We also construct 2D images of thick objects with infinite depth of focus (without loss of transverse spatial resolution). These methods can be used to image biological and materials science samples at high resolution using X-ray undulator radiation, and establishes the techniques to be used in atomic-resolution ultrafast imaging at X-ray free-electron laser sources.
physics.optics:Forces arising from overlap between the guided waves of parallel, microphotonic waveguides are calculated. Both attractive and repulsive forces, determined by the choice of relative input phase, are found. Using realistic parameters for a silicon-on-insulator material system, we estimate that the forces are large enough to cause observable displacements. Our results illustrate the potential for a broader class of optically-tunable microphotonic devices and microstructured artificial materials.
physics.optics:There exists a class of realizable, active media for which the refractive index cannot be defined as an analytic function in the upper half-plane of complex frequency. The conventional definition of the refractive index based on analyticity is modified such that it is valid for active media in general, and associated Fresnel equations are proved. In certain active media, the presence of a ``backward'' wave, for which both phase velocity and Poynting's vector point towards the excitation source, is demonstrated.
physics.optics:We study the transmission properties for the waveguide bends composed by a circular photonic crystal. Two types (Y and U type) of the waveguide bends utilizing the circular photonic crystal are studied. It has been shown, compared with the conventional photonic crystal waveguide bends, transmission properties for these bends can be significantly improved. Over a 6.4% bandwidth, less than 1-dB loss/bend are observed. U bent waveguide, i.e., $180^o$ bend, can be easily realized with low loss using the circular photonic crystal.
physics.optics:We demonstrate optical trapping and manipulation of defects and transparent microspheres in nematic liquid crystals (LCs). The three-dimensional director fields and positions of the particles are visualized using the Fluorescence Confocal Polarizing Microscopy. We show that the disclinations can be manipulated by either using optically trapped colloidal particles or directly by tightly-focused laser beams. We employ this effect to measure the line tensions of disclinations.
physics.optics:Line-by-line calculations are becoming the standard procedure for carrying spectral simulations. However, it is important to insure the accuracy of such spectral simulations through the choice of adapted models for the simulation of key parameters such as line position, intensity, and shape. Moreover, it is necessary to rely on accurate spectral data to guaranty the accuracy of the simulated spectra. A discussion on the most accurate models available for such calculations is presented for diatomic and linear polyatomic discrete radiation, and possible reductions on the number of calculated lines are discussed in order to reduce memory and computational overheads. Examples of different approaches for the simulation of experimentally determined low-pressure molecular spectra are presented. The accuracy of different simulation approaches is discussed and it is verified that a careful choice of applied computational models and spectroscopic datasets yields precise approximations of the measured spectra.
physics.optics:We present the design, fabrication and preliminary experiments of two-dimensional photonic crystal cavities made in nanoporous silicon luminescent at 700-800 nm. Enhancement in photoluminescence extraction efficiency at the resonant wavelength is expected due to Purcell effect and directed radiation pattern defined by the cavity. Such cavities should also enhance nonlinearities exhibited by porous Si beyond what is observed in one-dimensional distributed Bragg reflection cavities due to their small mode volumes and modest quality factors. This design aligns itself well to integration with conventional silicon based electronics on a single chip.
physics.optics:Spatial effects of interference and interaction of light modes in the subwavelength part of the near-field optical microscopy probe have been theoretically studied. It was found that the mode interference can lead to higher spatial compression of light (lambda = 500 nm in free space) within the transverse size of 25 nm inside the probe output aperture of 100 nm in diameter. The results principally demonstrate the possibility of increasing spatial resolution of the near-field optical microscopy technique.
physics.optics:Self-consistent simulations of the ultrafast electron dynamics in thin metal films are performed. A regime of nonlinear oscillations is observed, which corresponds to ballistic electrons bouncing back and forth against the film surfaces. When an oscillatory laser field is applied to the film, the field energy is partially absorbed by the electron gas. Maximum absorption occurs when the period of the external field matches the period of the nonlinear oscillations, which, for sodium films, lies in the infrared range. Possible experimental implementations are discussed.
physics.optics:Two-dimensional (2-D) boundary integral equation analysis of a notched circular microdisk resonator is presented. Results obtained provide accurate description of optical modes, free from the staircasing and discretization errors of other numerical techniques. Splitting of the double degenerate Whispering-Gallery (WG) modes and directional light output is demonstrated. The effect of the notch depth and width on the resonance wavelengths, Q-factors, and emission patterns is studied. Further improvement of the directionality is demonstrated in an elliptical notched microdisk. Applications of the notched resonators to the design of microdisk lasers, oscillators, and biosensors are discussed.
physics.optics:Fast and highly accurate method based on the Muller contour integral equations and a trigonometric-trigonometric Galerkin discretization technique is presented for spectral design and fine-tuning of clusters of whispering-gallery (WG) mode microdisk resonators and photonic crystal (PC) defect cavities. It is shown that degeneracy and near-degeneracy of natural modes as well as modes optical confinement in coupled resonator clusters are significantly influenced by the symmetry of the structure. Photonic molecules with greatly enhanced quality factors are designed by exploiting the symmetry of the molecular geometry and tuning the inter-cavity coupling distances. Potential applications of the spectrally engineered coupled-resonator structures to low-threshold microdisk lasers, optical biosensors, and random powder lasers are discussed.
physics.optics:A theoretical approach to determine the optimal form of the near-field optical microscope probe is proposed. An analytical expression of the optimal probe form with subwavelength aperture has been obtained. The advantages of the probe with the optimal form are illustrated using numerical calculations. The conducted calculations show 10 times greater light throughput and the reception possibility of the more compactly localized light at the output probe aperture which could indicate better spatial resolution of the optical images in near-field optical technique using optimal probe.
physics.optics:The existing optical microscopes form an image by collecting photons emitted from an object. Here we report on the experimental realization of microscopy without the need for direct optical communication with the sample. To achieve this, we have scanned a single gold nanoparticle acting as a nano-antenna in the near field of a sample and have studied the modification of its intrinsic radiative properties by monitoring its plasmon spectrum.
physics.optics:Using quasiclassical approach rather precise analytical approximations for the eigenfrequencies of whispering gallery modes in convex axisymmetric bodies may be found. We use the eikonal method to analyze the limits of precision of quasiclassical approximation using as a practical example spheroidal dielectric cavity. The series obtained for the calculation of eigenfrequencies is compared with the known series for dielectric sphere and with numerical calculations. We show how geometrical interpretation allows expansion of the method on arbitrary shaped axisymmetric bodies.
physics.optics:In this paper we present a systematic study of photonic bandgap engineering using aperiodic lattices (ALs). Up to now ALs have tended to be defined by specific formulae (e.g. Fibonacci, Cantor), and theories have neglected other useful ALs along with the vast majority of non-useful (random) ALs. Here, we present a practical and efficient Fourier space-based general theory to identify all those ALs having useful band properties, which are characterized by well-defined Fourier (i.e. lattice momentum) components. Direct control of field localization comes via control of the Parseval strength competition between the different Fourier components characterizing a lattice. Real-space optimization of ALs tends to be computationally demanding. However, via our Fourier space-based simulated annealing inverse optimization algorithm, we efficiently tailor the relative strength of the AL Fourier components for precise control of photonic band and localization properties.
physics.optics:Quantum optical techniques may yield immersion fluids with high indices of refraction without absorption. We describe one such technique in which a probe field experiences a large index of refraction with amplification rather than absorption, and examine its practicality for an immersion lithography application. Enhanced index can be observed in a three-level system with a tunable, near-resonant, coherent probe and incoherent pump field that inverts population of the probe transition. This observation contradicts the common belief that large indices of refraction are impossible without absorption, however it is well in accord with existing electromagnetic theory and practice. Calculations show that a refractive index >> 2 is possible with practical experimental parameters. A scheme with an incoherent mixture of pumped and unpumped atoms is also examined, and is seen to have a lower refractive index (~2) accompanied by neither gain nor loss.
physics.optics:We experimentally demonstrate a prototype of a cylindrical enlarging lens capable of enhancing and restoring evanescent fields. The enabling phenomenon is the resonant excitation of coupled surface modes in a system of two cylindrical arrays of small resonant particles. As was shown in [J. Appl. Phys. 96, 1293 (2004)], this phenomenon in planar arrays can be used in electromagnetic near-field imaging. Here, we use a similar structure in a cylindrically symmetric configuration, which gives us a possibility to obtain an enlarged near-field image.
physics.optics:We have demonstrated several inexpensive methods which can be used to measure the deflection angles of prisms with microradian precision. The methods are self-referenced, using various reversals to achieve absolute measurements without the need of a reference prism or any expensive precision components other than the prisms under test. These techniques are based on laser interferometry and have been used in our lab to characterize parallel-plate beamsplitters, penta prisms, right angle prisms, and corner cube reflectors using only components typically available in an optics lab.
physics.optics:Specially designed metal-dielectric composites can have a negative refractive index in the optical range. Specifically, it is shown that arrays of single and paired nanorods can provide such negative refraction. For pairs of metal rods, a negative refractive index has been observed at 1.5 micrometer. The inverted structure of paired voids in metal films may also exhibit a negative refractive index. A similar effect can be accomplished with metal strips in which the refractive index can reach -2. The refractive index retrieval procedure and the critical role of light phases in determining the refractive index is discussed.
physics.optics:Truncated sum rules have been used to calculate the fundamental limits of the nonlinear susceptibilities; and, the results have been consistent with all measured molecules. However, given that finite-state models result in inconsistencies in the sum rules, it is not clear why the method works. In this paper, the assumptions inherent in the truncation process are discussed and arguments based on physical grounds are presented in support of using truncated sum rules in calculating fundamental limits. The clipped harmonic oscillator is used as an illustration of how the validity of truncation can be tested; and, several limiting cases are discussed as examples of the nuances inherent in the method.
physics.optics:A short note describing the development of a database providing factual and numerical data on the spectral properties of diatomic molecules. This database is available online for the overall scientific community at the following adress: http://cfp.ist.utl.pt/radiation/
physics.optics:The Fourier inversion of phased coherent diffraction patterns offers images without the resolution and depth-of-focus limitations of lens-based tomographic systems. We report on our recent experimental images inverted using recent developments in phase retrieval algorithms, and summarize efforts that led to these accomplishments. These include ab-initio reconstruction of a two-dimensional test pattern, infinite depth of focus image of a thick object, and its high-resolution (~10 nm resolution) three-dimensional image. Developments on the structural imaging of low density aerogel samples are discussed.
physics.optics:In coherent X-ray diffraction microscopy the diffraction pattern generated by a sample illuminated with coherent x-rays is recorded, and a computer algorithm recovers the unmeasured phases to synthesize an image. By avoiding the use of a lens the resolution is limited, in principle, only by the largest scattering angles recorded. However, the imaging task is shifted from the experiment to the computer, and the algorithm's ability to recover meaningful images in the presence of noise and limited prior knowledge may produce aberrations in the reconstructed image. We analyze the low order aberrations produced by our phase retrieval algorithms. We present two methods to improve the accuracy and stability of reconstructions.
physics.optics:We shortly recall the mathematical and physical aspects of Talbot's self-imaging effect occurring in near-field diffraction. In the rational paraxial approximation, the Talbot images are formed at distances z=p/q, where p and q are coprimes, and are superpositions of q equally spaced images of the original binary transmission (Ronchi) grating. This interpretation offers the possibility to express the Talbot effect through Gauss sums. Here, we pay attention to the Talbot effect in the case of dispersion in optical fibers presenting our considerations based on the close relationships of the mathematical representations of diffraction and dispersion. Although dispersion deals with continuous functions, such as gaussian and supergaussian pulses, whereas in diffraction one frequently deals with discontinuous functions, the mathematical correspondence enables one to characterize the Talbot effect in the two cases with minor differences. In addition, we apply, for the first time to our knowledge, the wavelet transform to the fractal Talbot effect in both diffraction and fiber dispersion. In the first case, the self similar character of the transverse paraxial field at irrational multiples of the Talbot distance is confirmed, whereas in the second case it is shown that the field is not self similar for supergaussian pulses. Finally, a high-precision measurement of irrational distances employing the fractal index determined with the wavelet transform is pointed out
physics.optics:We have implemented a new approach for measuring the time-dependent intensity and phase of ultrashort optical pulses. It is based on the interaction between shaped pulses and atoms, leading to coherent transients.
physics.optics:We suggest double-resonant (binary) metamaterials composed of two types of magnetic resonant elements, and demonstrate that in the nonlinear regime such metamaterials provide unique possibilities for phase-matched parametric interaction and enhanced second-harmonic generation.
physics.optics:A new kind of tridimensional scalar optical beams is introduced. These beams are called Lorentz beams because the form of their transverse pattern in the source plane is the product of two independent Lorentz functions. Closed-form expression of free-space propagation under paraxial limit is derived and pseudo non-diffracting features pointed out. Moreover, as the slowly varying part of these fields fulfils the scalar paraxial wave equation, it follows that there exist also Lorentz-Gauss beams, i.e. beams obtained by multipying the original Lorentz beam to a Gaussian apodization function. Although the existence of Lorentz-Gauss beams can be shown by using two different and independent ways obtained recently from Kiselev [Opt. Spectr. 96, 4 (2004)] and Gutierrez-Vega et al. [JOSA A 22, 289-298, (2005)], here we have followed a third different approach, which makes use of Lie's group theory, and which possesses the merit to put into evidence the symmetries present in paraxial Optics.
physics.optics:We report on the first experimental observation of photonic Bloch oscillations and Zener tunneling in two-dimensional periodic systems. We study the propagation of an optical beam in a square photonic lattice superimposed on a refractive index ramp, and demonstrate the tunneling of light from the first to the higher-order transmission bands of the lattice bandgap spectrum, associated with the spectral dynamics inside the first Brillouin zone and corresponding oscillations of the primary beam.
physics.optics:The spectral dependence of a bending loss of cascaded 60-degree bends in photonic crystal (PhC) waveguides is explored in a slab-type silicon-on-insulator system. Ultra-low bending loss of (0.05+/-0.03)dB/bend is measured at wavelengths corresponding to the nearly dispersionless transmission regime. In contrast, the PhC bend is found to become completely opaque for wavelengths range corresponding to the slow light regime. A general strategy is presented and experimentally verified to optimize the bend design for improved slow light transmission.
physics.optics:Surface waves on liquids act as a dynamical phase grating for incident light. In this article, we revisit the classical method of probing such waves (wavelengths of the order of mm) as well as inherent properties of liquids and liquid films on liquids, using optical diffraction. A combination of simulation and experiment is proposed to trace out the surface wave profiles in various situations (\emph{eg.} for one or more vertical, slightly immersed, electrically driven exciters). Subsequently, the surface tension and the spatial damping coefficient (related to viscosity) of a variety of liquids are measured carefully in order to gauge the efficiency of measuring liquid properties using this optical probe. The final set of results deal with liquid films where dispersion relations, surface and interface modes, interfacial tension and related issues are investigated in some detail, both theoretically and experimentally. On the whole, our observations and analyses seem to support the claim that this simple, low--cost apparatus is capable of providing a wealth of information on liquids and liquid surface waves in a non--destructive way.
physics.optics:It is experimentally shown, that doping an organic molecular crystal reduces diffusion rate of impurity. Also reduces its influence on a modification of temperature. Examinations are lead on a crystal paradibromobenzene doping by paradiclorobenzene. Diffusing impurity was parabromochlorobenzene.
physics.optics:The precise observation of the angle-frequency spectrum of light filaments in water reveals a scenario incompatible with current models of conical emission (CE). Its description in terms of linear X-wave modes leads us to understand filamentation dynamics requiring a phase- and group-matched, Kerr-driven four-wave-mixing process that involves two highly localized pumps and two X-waves. CE and temporal splitting arise naturally as two manifestations of this process.
physics.optics:We consider a recently reported micro-fluidic dye ring laser and study the full wave nature of TE modes in the cavity by means of finite-element simulations. The resonance wave-patterns of the cavity modes support a ray-tracing view and we are also able to explain the spectrum in terms of standing waves with a mode spacing dk = 2pi/L_eff where L_eff is the effective optical path length in the cavity.
physics.optics:We introduce a small-core air-guiding photonic crystal fiber whose cladding is made of improved air-silica photonic crystal with non-circular air holes placed in triangular lattice. The fiber achieves un-disturbed bandgap guidance over 350nm wavelength range.
physics.optics:Low-dimensional ordered arrays of optical elements can possess bound modes having an extremely high quality factor. Typically, these arrays consist of metal elements which have significantly high light absorption thus restricting performance. In this paper we address the following question: can bound modes be formed in dielectric systems where the absorption of light is negligible? Our investigation of circular arrays of spherical particles shows that (1) high quality modes in an array of 10 or more particles can be attained at least for a refractive index $n_{r}>2$, so optical materials like TiO$_{2}$ or GaAs can be used; (2) the most bound modes have nearly transverse polarization perpendicular to the circular plane; (3) in a particularly interesting case of TiO$_{2}$ particles (rutile phase, $n_{r}=2.7$), the quality factor of the most bound mode increases almost by an order of magnitude with the addition of 10 extra particles, while for particles made of GaAs the quality factor increases by almost two orders of magnitude with the addition of ten extra particles. We hope that this preliminary study will stimulate experimental investigations of bound modes in low-dimensional arrays of dielectric particles.
physics.optics:We derive envelope equations which generate the Maxwell-Lorentz model and describe the interaction of optical pulses with plasmonic oscillations in metal nanoparticle composites. A family of solitary wave solutions is found which is analogous to self-induced transparency in Maxwell-Bloch. The evolution of incident optical pulses is studied numerically as are the collision dynamics of the solitary waves. These simulations reveal that the collision dynamics vary from near perfectly elastic to highly radiative depending on the relative phase of the initial pulses.
physics.optics:We propose and analyze novel surface-state-based waveguides in bandgap photonic crystals. We discuss surface mode band structure, field localization and effect of imperfections on the waveguiding properties of the surface modes. We demonstrate that surface-state-based waveguides can be used to achieve directional emission out of the waveguide. We also discuss the application of the surface-state waveguides as efficient light couplers for conventional photonic crystal waveguides.
physics.optics:We calculate the characteristics of ultraslow light in an inhomogeneously broadened medium. We present analytical and numerical results for the group delay as a function of power of the propagating pulse. We apply these results to explain the recently reported saturation behavior [Baldit {\it et al.}, \prl {\bf 95}, 143601 (2005)] of ultraslow light in rare earth ion doped crystal.
physics.optics:We have experimentally realized Brewster's effect for transverse-electric waves with metamaterials. In dielectric media, Brewster's no-reflection effect arises only for transverse-magnetic waves. However, it has been theoretically predicted that Brewster's effect arises for TE waves under the condition that the relative permeability r is not equal to unity. We have designed an array of split-ring resonators as a metamaterial with mu_r 1 using a finite-difference time-domain method. The reflection measurements were carried out in a 3-GHz region and the disappearance of reflected waves at a particular incident angle was confirmed.
physics.optics:We used two-dimensional Finte-Difference-Time-Domain (FDTD) software to study the transition behavior of nano-particles from scatterers to an optically uniform medium. We measured the transmission efficiency of the dipole source, which is located in the high refractive index medium(index=2.00) and encapsulated by low index resin(index=1.41). In an effort to compose index-matched resin and to reduce internal reflection, high-index nano-particles are added to low-index resin in simulations of various sizes and densities. As the size of the nano-particles and the average spacing between particles are reduced to 0.02 lambda and 0.07 lambda respectively, the transmission efficiency improves two-fold compared to that without nanoparticles. The numerical results can be used to understand the optical behavior of nano-particles and to improve the extraction efficiency of high brightness light-emitting-diodes(LEDs), through the use of nano-composite encapsulant.
physics.optics:A scheme of left-handed metamaterial (LHM) composed of superconducting quantum interference devices (SQUIDs) and conducting wires is proposed. The permeability of a probe field can be smoothly tuned over a wide range with another electromagnetic (coupling) field due to quantum interference effect. Similar to electromagnetically induced transparency (EIT) of atomic systems, the absorption of the probe field can be strongly suppressed even in the case of negative permeability. There are two passbands of negative refractive index with low loss, which can be tuned with the coupling field.
physics.optics:We presents the results of study of focusing and imaging properties of double-lens system for hard x-ray radiation consisting of two Fresnel zone plates (ZP) made from silicon. We demonstrate for the first time the phenomenon of focusing by two crystal ZP located at significant distance from each other. We investigate by both theoretically and experimentally the peculiarities of intensity distribution at the focal plane during a scan by second ZP normally to the optical axis of the system. We investigate as well the intensity distribution along the optical axis for our double-lens system from crystal ZP. We realize experimentally a registration of the focused image of the object by means of double-lens system based on ZP. Measurements are performed on the beam line BM-5 of the European Synchrotron Radiation Facility (ESRF) at the x-ray energy 9.4 keV. We elaborate a computer program for theoretical simulation of the optical properties of x-ray double-lens system based on ZPs. A calculation is made by convolution of transmission function and Kirchhoff propagator in paraxial approximation by means of Fast Fourier Transformation.
physics.optics:We investigate for the first time the enhancement of the stimulated Raman scattering in slow-light Silicon-on-Insulator (SOI) photonic crystal line defect waveguides. By applying the Bloch-Floquet formalism to the guided modes in a planar photonic crystal, we develop a formalism that relates the intensity of the down-shifted Stokes signal to the pump intensity and the modal group velocities. The formalism is then applied to two prospective schemes for enhanced stimulated Raman generation in slow-light photonic crystal waveguides. The results demonstrate a maximum factor of 10^4 (66,000) enhancement with respect to SOI channel waveguides. Effects of two photon absorption, intrinsic scattering, and disorder with respect to slow-light Raman generation towards optically-pumped silicon amplifiers and lasers are also discussed.
physics.optics:The paper deals with optical excitations arising in a one-dimensional chain of identical spheres due optical coupling of whispering gallery modes (WGM). The band structure of these excitations depends significantly on the inter-mixing between WGMs characterized by different values of angular quantum number, $l$. We develop a general theory of the photonic band structure of these excitations taking these effects into account and applied it to several cases of recent experimental interest. In the case of bands originating from WQMs with the angular quantum number of the same parity, the calculated dispersion laws are in good qualitative agreement with recent experiment results. Bands resulting from hybridization of excitations resulting from whispering gallery modes with different parity of $l$ exhibits anomalous dispersion properties characterized by a gap in the allowed values of \emph{wave numbers} and divergence of group velocity.
physics.optics:We present what is to our knowledge the most complete 1-D numerical analysis of the evolution and the propagation dynamics of an ultrashort laser pulse in a Ti:Sapphire laser oscillator. This study confirms the dispersion managed model of mode-locking, and emphasizes the role of the Kerr nonlinearity in generating mode-locked spectra with a smooth and well-behaved spectral phase. A very good agreement with preliminary experimental measurements is found.
physics.optics:We observe experimentally that ensembles of quantum dots in three-dimensional (3D) photonic crystals reveal strongly nonexponential time-resolved emission. These complex emission decay curves are analyzed with a continuous distribution of decay rates. The log-normal distribution describes the decays well for all studied lattice parameters. The distribution width is identified with variations of the radiative emission rates of quantum dots with various positions and dipole orientations in the unit cell. We find a striking sixfold change of the width of the distribution by varying the lattice parameter. This interpretation qualitatively agrees with the calculations of the 3D projected local density of states. We therefore conclude that fluorescence decay of ensembles of quantum dots is highly nonexponential to an extent that is controlled by photonic crystals.
physics.optics:The restrictions on the resolution of transmission devices formed by wire media (arrays of conductive cylinders) recently proposed in [Phys. Rev. B, 71, 193105 (2005)] and experimentally tested in [Phys. Rev. B, 73, 033108 (2006)] are studied in this paper using both analytical and numerical modeling. It is demonstrated that such transmission devices have sub-wavelength resolution that can in principle be made as fine as required by a specific application by controlling the lattice constant of the wire medium. This confirms that slabs of the wire medium are unique imaging devices at the microwave frequency range, and are capable of transmitting distributions of TM-polarized electric fields with nearly unlimited sub-wavelength resolution to practically arbitrary distances.
physics.optics:With the recent advancement in visualization devices over the last years, we are seeing a growing market for stereoscopic content. In order to convey 3D content by means of stereoscopic displays, one needs to transmit and display at least 2 points of view of the video content. This has profound implications on the resources required to transmit the content, as well as demands on the complexity of the visualization system. It is known that stereoscopic images are redundant, which may prove useful for compression and may have positive effect on the construction of the visualization device. In this paper we describe an experimental evaluation of data redundancy in color stereoscopic images. In the experiments with computer generated and real life and test stereo images, several observers visually tested the stereopsis threshold and accuracy of parallax measuring in anaglyphs and stereograms as functions of the blur degree of one of two stereo images and color saturation threshold in one of two stereo images for which full color 3D perception with no visible color degradations is maintained. The experiments support a theoretical estimate that one has to add, to data required to reproduce one of two stereoscopic images, only several percents of that amount of data in order to achieve stereoscopic perception.
physics.optics:The emission from a radiating source embedded in a photonic lattice is calculated. The analysis considers the photonic lattice and free space as a combined system. Furthermore, the radiating source and electromagnetic field are quantized. Results show the deviation of the photonic lattice spectrum from the blackbody distribution, with intracavity emission suppressed at certain frequencies and enhanced at others. In the presence of rapid population relaxation, where the photonic lattice and blackbody populations are described by the same equilibrium distribution, it is found that the enhancement does not result in output intensity exceeding that of the blackbody at the same frequency. However, for slow population relaxation, the photonic lattice population has a greater tendency to deviate from thermal equilibrium, resulting in output intensities exceeding those of the blackbody, even for identically pumped structures.
physics.optics:We present a method that formally calculates \emph{exact} frequency shifts of an electromagnetic field for arbitrary changes in the refractive index. The possible refractive index changes include both anisotropic changes and boundary shifts. Degenerate eigenmode frequencies pose no problems in the presented method. The approach relies on operator algebra to derive an equation for the frequency shifts, which eventually turn out in a simple and physically sound form. Numerically the equations are well-behaved, easy implementable, and can be solved very fast. Like in perturbation theory a reference system is first considered, which then subsequently is used to solve another related, but different system. For our method precision is only limited by the reference system basis functions and the error induced in frequency is of second order for first-order basis set error. As an example we apply our method to the problem of variations in the air-hole diameter in a photonic crystal fiber.
physics.optics:We demonstrate experimentally all-optical beam steering in modulated photonic lattices induced optically by three beam interference in a biased photorefractive crystal. We identify and characterize the key physical parameters governing the beam steering, and show that the spatial resolution can be enhanced by the additional effect of nonlinear beam self-localization.
physics.optics:We consider an index-guiding silica photonic crystal fiber with a triangular air-hole structure and a poled quadratic nonlinearity. By tuning the pitch and the relative hole size, second-harmonic generation with zero group-velocity mismatch is found for any fundamental wavelength above 780 nm. The phase-velocity mismatch has a lower limit with coherence lengths in the micron range. The dimensionless nonlinear parameter is inversely proportional to the pitch and proportional to the relative hole size. Selected cases show bandwidths suitable for 20 fs pulse-conversion with conversion efficiencies as high as 25%/mW.
physics.optics:We present a theoretical study of the dispersion relation of surface plasmon resonances of mesoscopic metal-dielectric-metal microspheres. By analyzing the solutions to Maxwell's equations, we obtain a simple geometric condition for which the system exhibits a band of surface plasmon modes whose resonant frequencies are weakly dependent on the multipole number. Using a modified Mie calculation, we find that a large number of modes belonging to this flat-dispersion band can be excited simultaneously by a plane wave, thus enhancing the absorption cross-section. We demonstrate that the enhanced absorption peak of the sphere is geometrically tunable over the entire visible range.
physics.optics:The Bergman-Milton bounds provide limits on the effective permittivity of a composite material comprising two isotropic dielectric materials. These provide tight bounds for composites arising from many conventional materials. We reconsider the Bergman-Milton bounds in light of the recent emergence of metamaterials, in which unconventional parameter ranges for relative permittivities are encountered. Specifically, it is demonstrated that: (a) for nondissipative materials the bounds may be unlimited if the constituent materials have relative permittivities of opposite signs; (b) for weakly dissipative materials characterized by relative permittivities with real parts of opposite signs, the bounds may be exceedingly large.
physics.optics:The silicon/silicon dioxide (Si/SiO2) interface plays a crucial role in the performance, cost, and reliability of most modern microelectronic devices, from the basic transistor to flash memory, digital cameras, and solar cells. Today the gate oxide thickness of modern transistors is roughly 5 atomic layers, with 8 metal wire layers required to transport all the signals within a microprocessor. In addition to the increasing latency of such reduced-dimension metal wires, further "Moore's Law" scaling of transistor cost and density is predicted to saturate in the next decade. As a result, silicon-based microphotonics is being explored for the routing and generation of high-bandwidth signals. In comparison to the extensive knowledge of the electronic properties of the Si/SiO2 interface, little is known about the optical properties of Si surfaces used in microphotonics. In this Letter, we explore the optical properties of the Si surface in the telecommunication-relevant wavelength band of 1400-1600 nm. Utilizing a high quality factor (Q ~ 1.5x10^6) optical microresonator to provide sensitivity down to a fractional surface optical loss of 10^-7, we show that optical loss within Si microphotonic components can be dramatically altered by Si surface preparation, with fraction loss of 2 x 10^-5 measured for chemical oxide surfaces as compared to <2 x 10^-6 for hydrogen-terminated Si surfaces. These results indicate that the optical properties of Si surfaces can be significantly and reversibly altered by standard microelectronics treatments, and that stable, high optical quality surface passivation layers will be critical in future Si micro- and nano-photonic systems.
physics.optics:Experimental evidence of bouncing localized structures in a nonlinear optical system is reported.
physics.optics:By numerically integrating the three-dimensional Maxwell equations in the time domain with reference to a dispersive quadratically nonlinear material, we study second harmonic generation in planar photonic crystal microresonators. The proposed scheme allows efficient coupling of the pump radiation to the defect resonant mode. The out-coupled generated second harmonic is maximized by impedance matching the photonic crystal cavity to the output waveguide.
physics.optics:We study the optical interaction in a coupled dielectric microdisks by investigating the splitting of resonance positions of interacting whispering gallery modes (WGMs) and their pattern change, depending on the distance between the microdisks. It is shown that the interaction between the WGMs with odd parity about y-axis becomes appreciable at a distance less than a wavelength and causes directional emissions of the resulting interacting WGMs. The directionality of the interacting WGMs can be understood in terms of an effective boundary deformation in ray dynamical analysis. We also discuss about the oscillation of the splitting when the distance is greater than a wavelength.
physics.optics:We studied numerically and experimentally the effects of structural disorder on the performance of ultraviolet photonic crystal slab lasers. Optical gain selectively amplifies the high-quality modes of the passive system. For these modes, the in-plane and out-of-plane leakage rates may be automatically balanced in the presence of disorder. The spontaneous optimization of in-plane and out-of-plane confinement of light in a photonic crystal slab may lead to a reduction of the lasing threshold.
physics.optics:In this paper we proposed a novel overlay alignment method using two sets of identical photonic crystals (PhCs). In this method the reflection or transmission spectrum of the two overlaid photonic crystals is measured to help wafer tilt, yaw rotation, and translation aligning. The initial testing results with two 1D photonic crystals and analysis of the alignment accuracy are presented. This method is particularly useful in building photonic crystal stacks with nanoimprint lithography (NIL).
physics.optics:We consider the lateral shift of a light beam reflecting from a dielectric slab backed by a metal. It is found that the lateral shift of the reflected beam can be negative while the intensity of reflected beam is almost equal to the incident one under a certain condition. The explanation for the negativity of the lateral shift is given in terms of the interference of the reflected waves from the two interfaces. It is also shown that the lateral shift can be enhanced or suppressed under some other conditions. The numerical calculation on the lateral shift for a realistic Gaussian-shaped beam confirms our theoretical prediction.
physics.optics:In this paper we evaluate the cut-off properties of holey fibers (HFs) with a triangular lattice of air holes and the core formed by the removal of a single (HF1) or more air holes (HF3 and HF7). With the aid of finite-element simulations we determine the single-mode and multi-mode phases and also find the air hole diameters limiting the endlessly single-mode regime. From calculations of V and W parameters we find that in general HF1 is less susceptible to longitudinal non-uniformities compared to the other designs for equivalent effective areas. As an example we illustrate this general property for the particular case of a macro-bending induced loss.
physics.optics:We analyse the optical (or microwave) tunnelling properties of electromagnetic waves passing through thin films presenting a specific index profile providing a cut-off frequency, when they are used below this frequency. We show that contrary to the usual case of a square index profile, where tunnelling is accompanied with a strong attenuation of the wave due to reflection, such films present the possibility of a reflectionless tunnelling, where the incoming intensity is totally transmitted.
physics.optics:We present an exact treatment of wave propagation in some inhomogeneous thin films with highly space-dependent dielectric constant. It is based on a space transformation which replaces the physical space by the optical path. In the new space, the dispersion equation is that of a normal progressive wave. We will show that the dispersion properties of such films are plasma- or waveguide-like, the characteristic frequency being determined by the spatial characteristics of the dielectric constant's variations only. The theory is scalable, so that it can be applied in any wavelength range : optical, IR, radiofrequency, etc. depending only on the characteristic space scales. Several applications will be presented, concerning the reflection properties of such films (broadband anti-reflection, or dichroic coatings) or to the propagation and transmission through the film. We will show that depending on the type of space dependence, an incident wave can either propagate or tunnel through such films. We will investigate the behaviour of the light group-velocity and tunneling time inside or through such films. Though we can reproduce the phase-time saturation corresponding to the Hartman effect, analysis of the group velocity in the tunneling case shows no sign of superluminal propagation. A strong frequency dependence can be obtained in some situations, which allows to anticipate a strong reshaping of brodband laser pulses.
physics.optics:The use of photonic crystal and negative refractive index materials is known to improve resolution of optical microscopy and lithography devices down to 80 nm level. Here we demonstrate that utilization of well-known digital image recovery techniques allows us to further improve resolution of optical microscope down to 30 nm level. Our microscope is based on a flat dielectric mirror deposited onto an array of nanoholes in thin gold film. This two-dimensional photonic crystal mirror may have either positive or negative effective refractive index as perceived by surface plasmon polartions in the visible frequency range. The optical images formed by the mirror are enhanced using simple digital filters.
physics.optics:We present waveguides with photonic crystal cores, supporting energy propagation in subwavelength regions with a mode structure similar to that in telecom fibers. We design meta-materials for near-, mid-, and far-IR frequencies, and demonstrate efficient energy transfer to and from regions smaller than 1/25-th of the wavelength. Both positive- and negative-refractive index light transmissions are shown. Our approach, although demonstrated here in circular waveguides for some specific frequencies, is easily scalable from optical to IR to THz frequency ranges, and can be realized in a variety of waveguide geometries. Our design may be used for ultra high-density energy focusing, nm-resolution sensing, near-field microscopy, and high-speed all-optical computing.
physics.optics:The generation of watt-level cw narrow-linewidth sources at specific deep UV wavelengths corresponding to atomic cooling transitions usually employs external cavity-enhanced second-harmonic generation (SHG) of moderate-power visible lasers in birefringent materials. In this work, we investigate a novel approach to cw deep-UV generation by employing the low-loss BBO in a monolithic walkoff-compensating structure [Zondy {\it{et al}}, J. Opt. Soc. Am. B {\bf{20}} (2003) 1675] to simultaneously enhance the effective nonlinear coefficient while minimizing the UV beam ellipticity under tight focusing. As a preliminary step to cavity-enhanced operation, and in order to apprehend the design difficulties stemming from the extremely low acceptance angle of BBO, we investigate and analyze the single-pass performance of a $L_c=8 $mm monolithic walk-off compensating structure made of 2 optically-contacted BBO plates cut for type-I critically phase-matched SHG of a cw $\lambda=570.4$nm dye laser. As compared with a bulk crystal of identical length, a sharp UV efficiency enhancement factor of 1.65 has been evidenced with the tandem structure, but at $\sim-1$nm from the targeted fundamental wavelength, highlighting the sensitivity of this technique when applied to a highly birefringent material such as BBO. Solutions to angle cut residual errors are identified so as to match accurately more complex periodic-tandem structure performance to any target UV wavelength, opening the prospect for high-power, good beam quality deep UV cw laser sources for atom cooling and trapping.
physics.optics:The double diffraction of white light can produce a thin-prism-like image in certain conditions by using ordinary diffraction gratings. The diffractive deviation of rays happens mainly in one direction because the diffracting elements are straight and parallel. We show similar images using elements where diffraction happens in both directions: spiral grooves discs. Some differences between the straight-groove and the almost circular curved-groove cases are described.
physics.optics:In this letter, we study the scattering of light by a single subwavelength slit in a metal screen. In contrast to previous theoretical works, we provide a microscopic description of the scattering process by emphasizing the generation of surface plasmons at the slit apertures. The analysis is supported by a rigorous formalism based on a normal-mode-decomposition technique and by a semi-analytical model which provides accurate formulae for the plasmonic generation strengths. The generation is shown to be fairly efficient for metals with a low conductivity, like gold in the visible regime. Verification of the theory is also shown by comparison with recent experimental data [H.F. Schouten et al., Phys. Rev. Lett. 94, 053901 (2005)].
physics.optics:Cerenkov radiation from cavities have been analyzed by quantum electrodynamic theory. Analytical expressions of some basic radiation properties including Einstein's $A$ and $B$ coefficients are derived and shown to be directly modified by the cavities. Coherent and incoherent radiations are analyzed with the aim of generating THz radiation from the devices.
physics.optics:Accurate analytic approximations are developed for the band gap boundaries and surface waves of a 1D photonic crystal, making use of the semiclassical theory recently developed by the authors: [Phys. Rev. E {69} (2004) 016612 and {70} (2004) 016606]. These analytic results provide useful insight on systematics of surface states.
physics.optics:We have observed the compensation of loss in metal by gain in interfacing dielectric in the mixture of aggregated silver nanoparticles and rhodamine 6G dye. The demonstrated six-fold enhancement of the Rayleigh scattering is the evidence of the increase of the quality factor of the surface plasmon (SP) resonance. The reported experimental observation paves the road to many practical applications of nanoplasmonics. We have also predicted and experimentally observed a suppression of the surface SP resonance in metallic nanoparticles embedded in a dielectric host with absorption.
physics.optics:Based on boundary conditions and dispersion relations, the anomalous propagation of waves incident from regular isotropic media into quasiisotropic media is investigated. It is found that the anomalous negative refraction, anomalous total reflection and oblique total transmission can occur in the interface associated with quasiisotropic media. The Brewster angles of E- and H-polarized waves in quasiisotropic media are also discussed. It is shown that the propagation properties of waves in quasiisotropic media are significantly different from those in isotropic and anisotropic media.
physics.optics:This paper presents an analysis of how to calculate bit error ratio (BER) with physical explanation for optically pre-amplified DPSK receivers using optical Mach-Zehnder interferometer (MZI) demodulation and balanced detection. It is shown that BER calculation method for this kind of receivers is different from the conventional calculation method used widely for IM/DD receivers. An analytical relationship in receiver sensitivity between DPSK receivers using MZI demodulation with balanced detection and IM/DD receivers (or DPSK receivers using MZI demodulation and single-port detection) is given based on the Gaussian noise approximation. Our calculation method correctly predicts the 3-dB improvement of receiver sensitivity by using balanced detection over single-port detection or IM/DD receivers. Furthermore, quantum-limited DPSK receivers with MZI demodulation are also analyzed.
physics.optics:Polarization-resolved second-harmonic spectra are obtained from the resonant modes of a two-dimensional planar photonic crystal microcavity patterned in a free-standing InP slab. The photonic crystal microcavity is comprised of a single missing-hole defect in a hexagonal photonic crystal host formed with elliptically-shaped holes. The cavity supports two orthogonally-polarized resonant modes split by 60 wavenumbers. Sum-frequency data are reported from the nonlinear interaction of the two coherently excited modes, and the polarization dependence is explained in terms of the nonlinear susceptibility tensor of the host InP.
physics.optics:We characterize the reflectance peak near the Brewster angle for both an interface between two dielectric media and a single slab. To approach this problem analytically, we approximate the reflectance by a first-order diagonal Pad\'e. In this way, we calculate the width and the skewness of the peak and we show that, although they present a well-resolved maximum, they are otherwise not so markedly dependent on the refractive index.
physics.optics:Based on boundary condition and dispersion relation, the superluminal group velocity in an anisotropic metamaterial (AMM) is investigated. The superluminal propagation is induced by the hyperbolic dispersion relation associated with the AMM. It is shown that a modulated Gaussian beam exhibits a superluminal group velocity which depends on the choice of incident angles and optical axis angles. The superluminal propagation does not violate the theory of special relativity because the group velocity is the velocity of the peak of the localized wave packet which does not carry information. It is proposed that a triglycine sulfate (TGS) crystal can be designed and the superluminal group velocity can be measured experimentally.
physics.optics:We generate experimentally different types of two-dimensional Bloch waves of a square photonic lattice by employing the phase imprinting technique. We probe the local dispersion of the Bloch modes in the photonic lattice by analyzing the linear diffraction of beams associated with the high-symmetry points of the Brillouin zone, and also distinguish the regimes of normal, anomalous, and anisotropic diffraction through observations of nonlinear self-action effects.
physics.optics:Channel drop filters in a plasmon-polaritons metal are studied. It shows that light can be efficiently dropped. Results obtained by the FDTD method are consistent with those from coupled mode theory. It also shows, without considering the loss of the metal, that the quality factor for the channel drop system reaches 4000. The quality factor decreases significantly if we take into account the loss, which also leads to a weak drop efficiency.
physics.optics:We analytically study the linear propagation of arbitrarily shaped light-pulses through an absorbing medium with a narrow transparency-window or through a resonant amplifying medium. We point out that, under certain general conditions, the pulse acquires a nearly Gaussian shape, irrespective of its initial shape and of the spectral profile of the line. We explicitly derive in this case the pulse parameters, including its skewness responsible for a deviation of the delay of the pulse maximum from the group delay. We illustrate our general results by analysing the slow-light experiments having demonstrated the largest fractional pulse-delays
physics.optics:We present a fully relativistic analysis of Bessel beams revealing some noteworthy features that are not explicit in the standard description. It is shown that there is a reference frame in which the field takes a particularly simple form, the wave appearing to rotate in circles. The concepts of polarization and angular momentum for Bessel beams is also reanalyzed.
physics.optics:We show that the finite-difference frequency-domain method is well-suited to study subwavelength lensing effects in left-handed materials (LHM's) and related problems. The method is efficient and works in the frequency domain, eliminating the need for specifying dispersion models for the permeability and permittivity as required by the popular finite-difference time-domain method. We show that "superlensing" in a LHM slab with refractive index n = -1 can be approached by introducing an arbitrarily small loss term. We also study a thin silver slab, which can exhibit subwavelength imaging in the electrostatic limit.
physics.optics:We compare the asymmetry-induced exchange splitting delta_1 of the bright-exciton ground-state doublet in self-assembled (In,Ga)As/GaAs quantum dots, determined by Faraday rotation, with its homogeneous linewidth gamma, obtained from the radiative decay in time-resolved photoluminescence. Post-growth thermal annealing of the dot structures leads to a considerable increase of the homogeneous linewidth, while a strong reduction of the exchange splitting is simultaneously observed. The annealing can be tailored such that delta_1 and gamma become comparable, whereupon the carriers are still well confined. This opens the possibility to observe polarization entangled photon pairs through the biexciton decay cascade.
physics.optics:Nonlocal communication between two laser light beams is experimented in a photochromic polymer thin films. Information exchange between the beams is mediated by the self-induction of a surface relief pattern. The exchanged information is related to the pitch and orientation of the grating. Both are determined by the incident beam. The process can be applied to experiment on a new kind of logic gates.
physics.optics:Recent theoretical and experimental studies have shown that imaging with resolution well beyond the diffraction limit can be obtained with so-called superlenses. Images formed by such superlenses are, however, in the near field only, or a fraction of wavelength away from the lens. In this paper, we propose a far-field superlens (FSL) device which is composed of a planar superlens with periodical corrugation. We show in theory that when an object is placed in close proximity of such a FSL, a unique image can be formed in far-field. As an example, we demonstrate numerically that images of 40 nm lines with a 30 nm gap can be obtained from far-field data with properly designed FSL working at 376nm wavelength.
physics.optics:The variance of nonlinear phase noise is analyzed by including the effect of intrachannel cross-phase modulation (IXPM)-induced nonlinear phase noise. Consistent with Ho and Wang [1] but in contrary to the conclusion of both Kumar [2] and Green et al. [3], the variance of nonlinear phase noise does not decrease much with the increase of chromatic dispersion. The results are consistent with a careful reexamination of both Kumar [2] and Green et al. [3].
physics.optics:An unambiguous proof of lasing in an active nanocavity with ultrahigh spontaneous emission coupling factor (beta = 0.65) is presented. To distinguish the subtle lasing threshold features from possible material-related phenomena, such as saturable absorption in the gain medium, a series of active nanocavities with different values of beta have been designed to systematically approach the high-beta device. The demonstration of the lasing threshold is obtained through the observation of the transition from thermal to coherent light photon statistics that is well understood and identified in the beta << 1 lasing regime. The systematic investigation allows a more definitive validation of the onset of lasing in these active nanocavities.
physics.optics:Very recently we present a theory to discuss the nature of light and show that the quantization of light energy in vacuum can be derived directly from classical electromagnetic theory. In the theory a key concept of stability of statistical distribution and a variational procedure searching for those of relatively stable statistical distributions were introduced. This is a classical interpretation to the concept and method concerned. It would be helpful for avoiding some of possible obsession from the variational procedure and its final quantum result.
physics.optics:Theoretical and experimental evidence of light driven structuring of glasses is presented. We show that light overcomes Coulomb repulsion and effective electron-electron interaction in glasses under strong light pumping becomes attractive. As the result homogenious distribution of trapped electrons gets unstable and macroscopic electron bunches are grown. At different conditions ordered structures with period about 2 microns determined by internal properties of the material are formed These structures were observed in ablation: surface profile after laser treatment reveals ordered pattern corresponding to the light induced electron domains.
physics.optics:We consider chromatic dispersion of capillary tubes and photonic crystal fibers infiltrated with liquid crystals. A perturbative scheme for inclusion of material dispersion of both liquid crystal and the surrounding waveguide material is derived. The method is used to calculate the chromatic dispersion at different temperatures.
physics.optics:An optical vortex (phase singularity) with a high topological strength resides on the axis of a high-order light beam. The breakup of this vortex under elliptic perturbation into a straight row of unit strength vortices is described. This behavior is studied in helical Ince-Gauss beams and astigmatic, generalized Hermite-Laguerre-Gauss beams, which are perturbations of Laguerre-Gauss beams. Approximations of these beams are derived for small perturbation, in which a neighborhood of the axis can be approximated by a polynomial in the complex plane: a Chebyshev polynomial for Ince-Gauss beams, and a Hermite polynomial for astigmatic beams.
physics.optics:An optical vortex, produced at one point in an optical beam, would propagate through an optical system to another point where the vortex can be used for some purpose. However, asymmetrical optical elements in such a system can cause astigmatism or at least distroy the rotational symmetry of the beam, which may affect the propagation of the vortex in an undesirable way. While an optical vortex in a rotationally symmetric, stigmatic Gaussian beam retains its initial morphology for as far as it propagates, the morphology of an optical vortex in an asymmetric or astigmatic Gaussian beam changes. The vortex can even be replaced by another with the opposite topological charge. We consider the behavior of single noncanonical vortices propagating in Gaussian beams that are asymmetric and/or astigmatic. General expressions for the vortex trajectories are provided. The locations of the flip planes and the evolution of the anisotropy of the vortex are considered for different non-ideal situations.
physics.optics:Transmission spectra of metallic films or membranes perforated by arrays of subwavelength slits or holes have been widely interpreted as resonance absorption by surface plasmon polaritons (SPPs). Alternative interpretations involving evanescent waves diffracted on the surface have also been proposed. These two approaches lead to divergent predictions for some surface wave properties. Using far-field interferometry, we have carried out a series of measurements on elementary one-dimensional (1-D) subwavelength structures with the aim of testing key properties of the surface waves and comparing them to predictions of these two points of view.
physics.optics:The resonant buildup of light within optical microcavities elevates the radiation pressure which mediates coupling of optical modes to the mechanical modes of a microcavity. Above a certain threshold pump power, regenerative mechanical oscillation occurs causing oscillation of certain mechanical eigenmodes. Here, we present a methodology to spatially image the micro-mechanical resonances of a toroid microcavity using a scanning probe technique. The method relies on recording the induced frequency shift of the mechanical eigenmode when in contact with a scanning probe tip. The method is passive in nature and achieves a sensitivity sufficient to spatially resolve the vibrational mode pattern associated with the thermally agitated displacement at room temperature. The recorded mechanical mode patterns are in good qualitative agreement with the theoretical strain fields as obtained by finite element simulations.
physics.optics:The Method a Raman of spectroscopy studies allocation of molecules in ternary mix-crystals of a p-dibromobenzene of p-dichlorobenzene and p-bromochlorobenzene. It is shown, that the mutual concentration of builders depends on requirements of growing. Was possibly as a uniform modification of concentration of all builders along a specimen, and a wavy modification of concentration of two substances.
physics.optics:We investigate the optical response of two sub-wavelength grooves on a metallic screen, separated by a sub-wavelength distance. We show that the Fabry-Perot-like mode, already observed in one-dimensional periodic gratings and known for a single slit, splits into two resonances in our system : a symmetrical mode with a small Q-factor, and an antisymmetric one which leads to a much stronger light enhancement. This behavior results from the near-field coupling of the grooves. Moreover, the use of a second incident wave allows to control the localization of the photons in the groove of our choice, depending on the phase difference between the two incident waves. The system exactly acts as a sub-wavelength optical switch operated from far-field.
physics.optics:While the alignment and rotation of microparticles in optical traps have received increased attention recently, one of the earliest examples has been almost totally neglected the alignment of particles relative to the beam axis, as opposed to about the beam axis. However, since the alignment torques determine how particles align in a trap, they are directly relevant to practical applications. Lysozyme crystals are an ideal model system to study factors determining the orientation of nonspherical birefringent particles in a trap. Both their size and their aspect ratio can be controlled by the growth parameters, and their regular shape makes computational modeling feasible. We show that both external shape and internal birefringence anisotropy contribute to the alignment torque. Three-dimensionally trapped elongated objects either align with their long axis parallel or perpendicular to the beam axis depending on their size. The shape-dependent torque can exceed the torque due to birefringence, and can align negative uniaxial particles with their optic axis parallel to the electric field, allowing an application of optical torque about the beam axis.
physics.optics:We demonstrate experimentally that the interactions between a pair of nonlocal spatial optical solitons in a nematic liquid crystal (NLC) can be controlled by the degree of nonlocality. For a given beam width, the degree of nonlocality can be modulated by varying the pretilt angle of NLC molecules via the change of the bias. When the pretilt angle is smaller than pi/4, the nonlocality is strong enough to guarantee the independence of the interactions on the phase difference of the solitons. As the pretilt angle increases, the degree of nonlocality decreases. When the degree is below its critical value, the two solitons behavior in the way like their local counterpart: the two in-phase solitons attract and the two out-of-phase solitons repulse.
physics.optics:Raman spectroscopies are carried out by the Method polarizable examinations of the lattice oscillations of p-chloronitrobenzene at temperature 293 K. The led matching of spectrums of p-chloronitrobenzene with p-bromochlorobenzene and p-dibromobenzene has shown that the significant line broadening in p-chloronitrobenzene is caused not only clutter in allocation of molecules concerning parasubstitution benzol. It is necessary to consider also and clutter in rotational displacements of the nitro of group concerning a plane of a molecule. Calculations of frequency spectra were led on a method the Dyne.
physics.optics:We present an analytical model of the resonantly enhanced transmission of light through a subwavelength nm-size slit in a thick metal film. The simple formulae for the transmitted electromagnetic fields and the transmission coefficient are derived by using the narrow-slit approximation and the Green's function formalism for the solution of Maxwell's equations. The resonance wavelengths are in agreement with the semi-analytical model [Y. Takakura, Phys. Rev. Lett. 86, 5601 (2001)], which solves the wave equations by using the Rayleigh field expansion. Our formulae, however, show great resonant enhancement of a transmitted wave, while the Rayleigh expansion model predicts attenuation. The difference is attributed to the near-field subwavelength diffraction, which is not considered by the Rayleigh-like expansion models.
physics.optics:A critical review of experimental studies of the so-called 'slow light' arising due to anomalously high steepness of the refractive index dispersion under conditions of electromagnetically induced transparency or coherent population oscillations is presented. It is shown that a considerable amount of experimental evidence for observation of the 'slow light' is not related to the low group velocity of light and can be easily interpreted in terms of a standard model of interaction of light with a saturable absorber.
physics.optics:The dynamics of two-dimensional electromagnetic (EM) pulses through one-dimensional photonic crystals (1DPC) has been theoretically studied. Employing the time expectation integral over the Poynting vector as the arrival time [Phys. Rev. Lett. 84, 2370, (2000)], we show that the superluminal tunneling process of EM pulses is the propagation of the net forward-going Poynting vector through the 1DPC, and the Hartman effect is due to the saturation effect of the arrival time (smaller and smaller time accumulated) of the net forward energy flow caused by the interference effect of the forward and the backward field (from the interfaces of each layer) happened in the region before the 1DPC and in the front part of the 1DPC.
physics.optics:The noise figure and photon probability distribution are calculated for coherent anti-Stokes Raman scattering (CARS) where an anti-Stokes signal is converted to Stokes. We find that the minimum noise figure is ~ 3dB.
physics.optics:The application of the copper bromide (CuBr) laser as an attractive tool in the micro-machining of different materials has been demonstrated. High-quality drilling by trepanning and precision cutting was established on several materials with a negligible heat-affected zone (HAZ). That good performance was a result of the combination of high power visible radiation, short pulses, and close to the diffraction-limited laser beam quality with high-speed galvo scanner beam steering.
physics.optics:Excerpt: We apply the wavelet transform to the fractal Talbot effect in both diffraction and fiber dispersion. In the first case, the self similar character of the transverse paraxial field at irrational multiples of the Talbot distance is confirmed, whereas in the second case it is shown that the field is not self similar for supergaussian pulses. Finally, a high-precision measurement of irrational distances employing the fractal index determined with the wavelet transform is pointed out
physics.optics:Electron spin coherence has been generated optically in n-type modulation doped (In,Ga)As/GaAs quantum dots (QDs) which contain on average a single electron per dot. The coherence arises from resonant excitation of the QDs by circularly-polarized laser pulses, creating a coherent superposition of an electron and a trion state. Time dependent Faraday rotation is used to probe the spin precession of the optically oriented electrons about a transverse magnetic field. Spin coherence generation can be controlled by pulse intensity, being most efficient for (2n+1)pi-pulses.
physics.optics:We present ultrafast all-optical switching measurements of Si woodpile photonic band gap crystals. The crystals are spatially homogeneously excited, and probed by measuring reflectivity over an octave in frequency (including the telecom range) as a function of time. After 300 fs, the complete stop band has shifted to higher frequencies as a result of optically excited free carriers. The switched state relaxes quickly with a time constant of 18 ps. We present a quantitative analysis of switched spectra with theory for finite photonic crystals. The induced changes in refractive index are well described by a Drude model with a carrier relaxation time of 10 fs. We briefly discuss possible applications of high-repetition rate switching of photonic crystal cavities.
physics.optics:Quasiclassical approach and geometric optics allow to describe rather accurately whispering gallery modes in convex axisymmetric bodies. Using this approach we obtain practical formulas for the calculation of eigenfrequencies and radiative Q-factors in dielectrical spheroid and compare them with the known solutions for the particular cases and with numerical calculations. We show how geometrical interpretation allows expansion of the method on arbitrary shaped axisymmetric bodies.
physics.optics:Pairing together material slabs with opposite signs for the real parts of their constitutive parameters has been shown to lead to interesting and unconventional properties that are not otherwise observable for single slabs. One such case was demonstrated analytically for the conjugate (i.e., complementary) pairing of infinite planar slabs of epsilon-negative (ENG) and mu-negative (MNG) media [A. Alu, and N. Engheta, IEEE Trans. Antennas Prop., 51, 2558 (2003)]. There it was shown that when these two slabs are juxtaposed and excited by an incident plane wave, resonance, complete tunneling, total transparency and reconstruction of evanescent waves may occur in the steady-state regime under a monochromatic excitation, even though each of the two slabs by itself is essentially opaque to the incoming radiation. This may lead to virtual imagers with sub-wavelength resolution and other anomalous phenomena overcoming the physical limit of diffraction. Here we explore how a transient sinusoidal signal that starts at t = 0 interacts with such an ENG-MNG pair of finite size using an FDTD technique. Multiple reflections and transmissions at each interface are shown to build up to the eventual steady state response of the pair, and during this process one can observe how the growing exponential phenomenon may actually occur inside this bilayer.
physics.optics:Following our recent theoretical development of the concept of nano-inductors, nano-capacitors and nano-resistors at optical frequencies and the possibility of synthesizing more complex nano-scale circuits, here we theoretically investigate in detail the problem of optical nano-transmission-lines (NTL) that can be envisioned by properly joining together arrays of these basic nano-scale circuit elements. We show how, in the limit in which these basic circuit elements are closely packed together, the NTLs can be regarded as stacks of plasmonic and non-plasmonic planar slabs, which may be designed to effectively exhibit the properties of planar metamaterials with forward (right-handed) or backward (left-handed) operation. With the proper design, negative refraction and left-handed propagation are shown to be possible in these planar plasmonic guided-wave structures, providing possibilities for sub-wavelength focusing and imaging in planar optics, and laterally-confined waveguiding at IR and visible frequencies. The effective material parameters for such NTLs are derived, and the connection and analogy between these optical NTLs and the double-negative and double-positive metamaterials are also explored. Physical insights and justification for the results are also presented.
physics.optics:Using an extended plane-wave-based transfer-matrix method, the photonic band structures and the corresponding transmission spectrum of a two-dimensional left-handed photonic crystal are calculated. Comparisons between the periodic structure with a single left-handed cylindric rod are made, and many interesting similarities are found. It is shown that, due to the localized surface polaritons presented by an isolated left-handed rod, there exist many exciting physical phenomena in high-dimensional left-handed photonic crystals. As direct results of coupling of the localized surface polaritons of neighboring left-handed rod, a lot of almost dispersionless bands, anti-crossing behavior, and a zero $\bar{n}$ gap are exhibited in the left-handed periodic structure. Moreover, in a certain frequency region, except distorted by a lot of anti-crossing behavior, there exists a continual dispersion relation, which can be explained by the long-wavelength approximation. It is also pointed out that high-dimensional left-handed photonic crystals can be used to design narrow-band filter.
physics.optics:Utilizing the underlying physics of evanescent wave amplification by a negative-refractive-index slab, it is shown that evanescent waves with specific spatial frequencies can also be amplified without any reflection simply by two dielectric planar waveguides. The simple configuration allows one to take advantage of the high resolution limit of a high-refractive-index material without contact with the object.
physics.optics:The analysis of optical pulse generation by phase modulation of narrowband continuous-wave light, and subsequent propagation through a group-delay-dispersion circuit, is usually performed in terms of the so-called bunching parameter. This heuristic approach does not provide theoretical support for the electrooptic flat-top-pulse generation reported recently. Here, we perform a waveform synthesis in terms of the Fresnel images of the periodically phase-modulated input light. In particular, we demonstrate flat-top-pulse generation with a duty ratio of 50% at a quarter of the Talbot condition for the sinusoidal phase modulation. Finally, we propose a binary modulation format to generate a well-defined square-wave-type optical bit pattern.
physics.optics:Using an analytical expression for an integral involving Bessel and Legendre functions we succeeded to obtain the partial wave decomposition of a general optical beam at an arbitrary location from the origin. We also showed that the solid angle integration will eliminate the radial dependence of the expansion coefficients. The beam shape coefficients obtained are given by an exact expression in terms of single or double integrals. These integrals can be evaluated numerically in a short time scale. We presented the results for the case of linear polarized Gaussian beam.
physics.optics:What we believe to be a new type of resonant coupling of an incident bulk wave into guided modes of a slab with a thick holographic grating is shown to occur in the presence of strong frequency detunings of the Bragg condition. This happens through the reflection of the strongly noneigen +1 diffracted order with the slab-grating boundaries, the resultant reflected waves forming a guided slab mode. Rigorous coupled-wave analysis is used for the numerical analysis of the predicted resonant effects. Possible applications include enhanced options for the design of multiplexing and demultiplexing systems, optical signal-processing devices, optical sensors, and measurement techniques.
physics.optics:A theoretical and experimental investigation of the effects of mode coupling in a resonant macro- scopic quantum device is achieved in the case of a ring laser. In particular, we show both analytically and experimentally that such a device can be used as a rotation sensor provided the effects of mode coupling are controlled, for example through the use of an additional coupling. A possible general- ization of this example to the case of another resonant macroscopic quantum device is discussed.
physics.optics:Superluminal behavior has been extensively studied in recent years, especially with regard to the topic of superluminality in the propagation of a signal. Particular interest has been devoted to Bessel-X waves propagation, since some experimental results showed that these waves have both phase and group velocities greater that light velocity c. However, because of the lack of an exact definition of signal velocity, no definite answer about the signal propagation (or velocity of information) has been found. The present paper is a short note that deals in a general way with this vexed question. By analyzing the field of existence of the Bessel X-wave in pseudo-Euclidean space-time, it is possible to give a general description of the propagation, and to overcome the specific question related to a definition of signal velocity.
physics.optics:We present in this paper results on a new dissemination system of ultra-stable reference signal at 100 MHz on a standard fibre network. The 100 MHz signal is simply transferred by amplitude modulation of an optical carrier. Two different approaches for compensating the noise introduced by the link have been implemented. The limits of the two systems are analyzed and several solution suggested in order to improve the frequency stability and to further extend the distribution distance. Nevertheless, our system is a good tool for the best cold atom fountains comparison between laboratories, up to 100 km, with a relative frequency resolution of 10-14 at one second integration time and 10-17 for one day of measurement. The distribution system may be upgraded to fulfill the stringent distribution requirements for the future optical clocks.
physics.optics:Polymer filling of the air holes of Indium Phosphide based two-dimensional photonic crystals is reported. After infiltration of the holes with a liquid monomer and solidification of the infill in situ by thermal polymerization, complete filling is proven using scanning electron microscopy. Optical transmission measurements of a filled photonic crystal structure exhibit a redshift of the air band, confirming the complete filling.
physics.optics:We observe pulse delays of up to twenty times the input pulse duration when 200-ps laser pulses pass through a hot Rb 85 vapor cell. The pulse peak travels with a velocity equal to c/20, and the energy transmission is 5%. For pulses with linewidth greater than typical features in the atomic dispersion, pulse delay is predicted and observed for all center frequencies near resonance. Pulse advance is never observed. The measurements are in good agreement with a three-level linear-dispersion calculation. We are able to control the amount of delay by using a steady-state laser beam for optical pumping of the ground states prior to sending in the test pulse.
physics.optics:Efficient nonresonant optical pumping of a high-Q scar mode in a two-dimensional quadrupole-deformed microlaser has been demonstrated based on ray and wave chaos. Three-fold enhancement in the lasing power was achieved at a properly chosen pumping angle. The experimental result is consistent with ray tracing and wave overlap integral calculations.
physics.optics:We analyze theoretically and experimentally the influence of current noise on the longitudinal mode hopping dynamics of a bulk semiconductor laser. It is shown that the mean residence times on each mode have different sensitivity to external noise added to the bias current. In particular, an increase of the noise level enhances the residence time on the longitudinal mode that dominates at low current, evidencing the multiplicative nature of the stochastic process. A two-mode rate equation model for semiconductor laser is able to reproduce the experimental findings. Under a suitable separation of the involved time scales, the model can be reduced to a 1D bistable potential system with a multiplicative stochastic term related to the current noise strength. The reduced model clarifies the influence of the different noise sources on the hopping dynamics.
physics.optics:The filamentational instability of spatially broadband femtosecond optical pulses in air is investigated by means of a kinetic wave equation for spatially incoherent photons. An explicit expression for the spatial amplification rate is derived and analyzed. It is found that the spatial spectral broadening of the pulse can lead to stabilization of the filamentation instability. Thus, optical smoothing techniques could optimize current applications of ultra-short laser pulses, such as atmospheric remote sensing.
physics.optics:We report in this paper what is to our knowledge the first observation of a time-resolved diffusing wave spectroscopy signal recorded by transillumination through a thick turbid medium: the DWS signal is measured for a fixed photon transit time, which opens the possibility of improving the spatial resolution. This technique could find biomedical applications, especially in mammography.
physics.optics:We theoretically investigate the existence and properties of hybrid surface waves forming at interfaces between left-handed materials and dielectric birefringent media. The existence conditions of such waves are found to be highly relaxed in comparison to the original hybrid surface waves, discovered by Dyakonov, in configurations involving birefringent materials and right-handed media. Hybrid surface waves in left-handed materials feature remarkable properties: (i) a high degree of localization and (ii) coexistence of several guided solutions. The existence of several hybrid surface waves for the same parameter set is linked to the birefringent nature of the medium whereas the strong localization is related to the presence of the left-handed material. The hybrid surface modes appear for large areas in the parameter space.
physics.optics:The coherence length of the thermal electromagnetic field near a planar surface has a minimum value related to the nonlocal dielectric response of the material. We perform two model calculations of the electric energy density and the field's degree of spatial coherence. Above a polar crystal, the lattice constant gives the minimum coherence length. It also gives the upper limit to the near field energy density, cutting off its $1/z^3$ divergence. Near an electron plasma described by the semiclassical Lindhard dielectric function, the corresponding length scale is fixed by plasma screening to the Thomas-Fermi length. The electron mean free path, however, sets a larger scale where significant deviations from the local description are visible.
physics.optics:The basic assumption of the authors of manuscript arXiv:physics/0603175v1 (21 Mar 2006) is that a laser wave differs from a plane wave because of photon coherence. This is not true and therefore their conclusion is also wrong.
physics.optics:We discuss optical constants in artificial metamaterials showing negative magnetic permeability and electric permittivity. Using effective field theory, we calculate effective permeability of nanofabricated media composed of pairs of identical gold nano-pillars with magnetic response in the visible spectrum.
physics.optics:Laser trapping near the surface of a nanostructured substrate is demonstrated. Stable microbubbles with radii of 1-20micrometers have been created and manipulated with sub-micron precision by a focused laser beam in an immersion oil covering arrays of pairs of gold nanopillars deposited on a glass substrate. The threshold for bubble creation and trapping characteristics depended on near-field coupling of nanopillars. The nanometric laser tweezers showed giant trapping efficiency of Q~50 for the trapped microbubbles.
physics.optics:We have investigated quasi-eigenmodes of a quadrupolar deformed microcavity by extensive numerical calculations. The spectral structure is found to be quite regular, which can be explained on the basis of the fact that the microcavity is an open system. The far-field emission directions of the modes show unexpected similarity irrespective of their distinct shapes in phase space. This universal directionality is ascribed to the influence from the geometry of the unstable manifolds in the corresponding ray dynamics.
physics.optics:We report a theoretical study of clusters of evanescently-coupled 2D whispering-gallery (WG) mode optical micro-cavities (termed "photonic molecules") as chemosensing and biosensing platforms. Photonic molecules (PMs) supporting modes with narrow linewidths, wide mode spacing, and greatly enhanced sensitivity to the changes in the dielectric constant of their environment and to the presence of individual sub-wavelength sized nanoparticles in the PM evanescent-field region are numerically designed. This type of optical biosensor can be fabricated in a variety of material platforms and integrated on a single chip that makes it a promising candidate for a small and robust lab-on-a-chip device. Possible applications of the developed methodology and the designed PM structures to the near-field microscopy, single nano-emitter microcavity lasing, and cavity-controlled single-molecule fluorescence enhancement are also discussed.
physics.optics:We have developed a technique for realizing a two-dimensional quadrupolar microcavity with its deformation variable from 0% to 20% continuously. We employed a microjet ejected from a noncircular orifice in order to generate a stationary column with modulated quadrupolar deformation in its cross section. Wavelength red shifts of low-order cavity modes due to shape deformation were measured and were found to be in good agreement with the wave calculation for the same deformation, indicating the observed deformation is quadrupolar in nature.
physics.optics:Experimental investigation of the characteristics of quasi-bound states of a quadrupole deformed microcavity has revealed five distinct mode groups in cavity emission spectra with cavity quality factors different by orders of magnitude and consistently with much different intracavity mode distributions but with almost universal far-field emission patterns. These universal directionality of high Q modes are explained by a subtle manifestation of unstable manifolds of classical chaos in the formation of quasi-bound states.
physics.optics:We analyze theoretically the effect of technical fluctuations on laser lineshape in terms of statistics of amplitude and phase noise and their respective bandwidths. While the phase noise tends to broaden the linewidth as the magnitude of the noise increases, the amplitude noise brings out an additional structure with its spectral density reflecting the magnitude of the noise. The effect of possible coupling between those two noises is also discussed.
physics.optics:We demonstrate lineshape measurement of an extreme-weak amplitude fluctuating light source by using the photon-counting-based second-order correlation spectroscopy combined with the heterodyne technique. The amplitude fluctuation of a finite bandwidth introduces a low-lying spectral structure in the lineshape and thus its effect can be isolated from that of the phase fluctuation. Our technique provides extreme sensitivity suited for single-atom-level applications.
physics.optics:A nanofabricated regular array of coupled gold nano-pillars is employed to detect local indices of refraction of different liquids using a shift of an antisymmetric plasmon resonance peak observed in the reflection spectra. The peak spectral position is found to be a unique function of the local refractive index for a wide range of indices. We discuss possible applications of the fabricated nanostructured arrays in bio and chemical sensors.
physics.optics:We report that $> 80%$ of the photons generated inside a photonic crystal slab resonator can be funneled within a small divergence angle of $\pm 30^\circ$. The far-field radiation properties of a photonic crystal slab resonant mode are modified by tuning the cavity geometry and by placing a reflector below the cavity. The former method directly shapes the near-field distribution so as to achieve directional and linearly-polarized far-field patterns. The latter modification takes advantage of the interference effect between the original waves and the reflected waves to enhance the energy-directionality. We find that, regardless of the slab thickness, the optimum distance between the slab and the reflector closely equals one wavelength of the resonance under consideration. We have also discussed an efficient far-field simulation algorithm based on the finite-difference time-domain method and the near- to far-field transformation.
physics.optics:We present an investigation of the modulational instability of partially coherent signals in electrical transmission lines. Starting from the modified Ginzburg-Landau equations and the Wigner-Moyal representation, we derive a nonlinear dispersion relation for the modulational instability. It is found that the effect of signal broadbandness reduces the growth rate of the modulational instability.
physics.optics:Bright blue organic light-emitting diodes (OLEDs) based on 1,4,5,8,N-pentamethylcarbazole (PMC) and on dimer of N-ethylcarbazole (N,N'-diethyl-3,3'-bicarbazyl) (DEC) as emitting layers or as dopants in a 4,4'-bis(2,2'-diphenylvinyl)-1,1'-biphenyl (DPVBi) matrix are described. Pure blue-light with the C.I.E. coordinates x = 0.153 y = 0.100, electroluminescence efficiency \eta_{EL} of 0.4 cd/A, external quantum efficiency \eta_{ext.} of 0.6% and luminance L of 236 cd/m2 (at 60 mA/cm2) were obtained with PMC as an emitter and the 2,9-dimethyl-4,7-diphenyl-1,10-phenantroline (BCP) as a hole-blocking material in five-layer emitting devices. The highest efficiencies \eta_{EL.} of 4.7 cd/A, and \eta_{ext} = 3.3% were obtained with a four-layer structure and a DPVBi DEC-doped active layer (CIE coordinates x = 0.158, y=0.169, \lambda_{peak} = 456 nm). The \eta_{ext.} value is one the highest reported at this wavelength for blue OLEDs and is related to an internal quantum efficiency up to 20%.
physics.optics:A novel approach is used to enhance by nearly two orders of magnitude the conversion efficiency of a 125 nm-coherent source, based on four-wave mixing in room-temperature mercury vapor. Saturation issues are observed and discussed.
physics.optics:Phase steps are an important type of wavefront aberrations generated by large telescopes with segmented mirrors. In a closed-loop correction cycle these phase steps have to be measured with the highest possible precision using natural reference stars, that is with a small number of photons. In this paper the classical Fisher information of statistics is used for calculating the Cramer-Rao bound, which determines the limit to the precision with which the height of the steps can be estimated in an unbiased fashion with a given number of photons and a given measuring device. Four types of measurement devices are discussed: a Shack-Hartmann sensor with one small cylindrical lenslet covering a sub-aperture centred over a border, a modified Mach-Zehnder interferometer, a Foucault test, and a curvature sensor. The Cramer-Rao bound is calculated for all sensors under ideal conditions, that is narrowband measurements without additional noise or disturbances apart from the photon shot noise. This limit is compared with the ultimate quantum statistical limit for the estimate of such a step which is independent of the measuring device. For the Shack-Hartmann sensor, the effects on the Cramer-Rao bound of broadband measurements, finite sampling, and disturbances such as atmospheric seeing and detector readout noise are also investigated. The methods presented here can be used to compare the precision limits of various devices for measuring phase steps and for optimising the parameters of the devices. Under ideal conditions the Shack-Hartmann and the Foucault devices nearly attain the ultimate quantum statistical limits, whereas the Mach-Zehnder and the curvature devices each require approximately twenty times as many photons in order to reach the same precision.
physics.optics:Group delay for a reflected light pulse from a weakly absorbing dielectric slab is theoretically investigated, and large negative group delay is found for weak absorption near a resonance of the slab ($Re(kd)=m\pi$). The group delays for both the reflected and transmitted pulses will be saturated with the increase of the absorption.
physics.optics:An analytic solution for Bragg grating with linear chirp in the form of confluent hypergeometric functions is analyzed in the asymptotic limit of long grating. Simple formulas for reflection coefficient and group delay are derived. The simplification makes it possible to analyze irregularities of the curves and suggest the ways of their suppression. It is shown that the increase in chirp at fixed other parameters decreases the oscillations in the group delay, but gains the oscillations in the reflection spectrum. The conclusions are in agreement with numerical calculations.
physics.optics:We report the first experimental evidence that electromagnetic coupling between physically separated planar metal patterns located in parallel planes provides for exceptionally strong polarization rotatory power if one pattern is twisted in respect to the other, creating a 3D chiral object. In terms of optical rotary power per sample of thickness equal to one wavelength, the bi-layered structure rotates five orders of magnitude stronger than a gyrotropic crystal of quartz in the visible spectrum. We also saw a signature of negative refraction for circularly polarized waves propagating through the chiral slab.
physics.optics:The light propagation through one-dimensional photonic crystal using Four-wave mixing (FWM) nonlinear process is modeled. The linear and nonlinear indexes of refraction are approximated with the first Fourier harmonic term. Based on this approximation, a complete set of coupled wave equations, including pump fields depletion, for description of FWM process and conversion efficiency from pump to signal and idler waves for periodic structures is presented. The derived coupled wave equations are evaluated numerically. Some of important system parameters effects on FWM performance are investigated. The obtained relations are suitable and can easily be applied for description of Wavelength Division Multiplexing (WDM) optical signals transmitted through (parametric process) nonlinear fiber Bragg Gratings compatible to optical fiber communications.
physics.optics:We demonstrate the association of two-photon nonlinear microscopy with balanced homodyne detection for investigating second harmonic radiation properties at nanoscale dimensions. Variation of the relative phase between second-harmonic and fundamental beams is retrieved, as a function of the absolute orientation of the nonlinear emitters. Sensitivity down to approximately 3.2 photon/s in the spatio-temporal mode of the local oscillator is obtained. This value is high enough to efficiently detect the coherent second-harmonic emission from a single KTiOPO4 crystal of sub-wavelength size.
physics.optics:The total scattering and the extinction efficiencies of a nihility cylinder of infinite length and circular cross--section are identical and independent of the polarization state of a normally incident plane wave.
physics.optics:On interrogation by a plane wave, the back-scattering efficiency of a nihility sphere is identically zero, and its extinction and forward-scattering efficiencies are higher than those of a perfectly conducting sphere.
physics.optics:We study light transmission through a homeotropically oriented nematic liquid crystal cell and solve self-consistently a nonlinear equation for the nematic director coupled to Maxwell's equations. We demonstrate that above a certain threshold of the input light intensity, the liquid-crystal cell changes abruptly its optical properties due to the light-induced Freedericksz transition, demonstrating multistable hysteresis-like dependencies in the transmission. We suggest that these properties can be employed for tunable all-optical switching photonic devices based on liquid crystals.
physics.optics:We derive coupled propagation equations for ultrashort pulses in a degenerate three-wave mixing process in quadratic media, using approximations consistent with the slowly evolving wave approximation [T. Brabec and F. Krausz, Phys. Rev. Lett. 78, 3282 (1997)]. From these we derive an approximate single-field equation for the fundamental field. This document expands upon mathematics used for work submitted by the same authors to Physical Review Letters.
physics.optics:We describe a method to probe the spectral fluctuations of a transition over broad ranges of frequencies and timescales with the high spectral resolution of Fourier spectroscopy, and a temporal resolution as high as the excited state lifetime, even in the limit of very low photocounting rates. The method derives from a simple relation between the fluorescence spectral dynamics of a single radiating dipole and its fluorescence intensity correlations at the outputs of a continuously scanning Michelson interferometer. These findings define an approach to investigate the fast fluorescence spectral dynamics of single molecules and other faint light sources beyond the time-resolution capabilities of standard spectroscopy experiments.
physics.optics:A method to reduce the transit time of majority of carriers in photomixers and photo detectors to $< 1$ ps is proposed. Enhanced optical fields associated with surface plasmon polaritons, coupled with velocity overshoot phenomenon results in net decrease of transit time of carriers. As an example, model calculations demonstrating $> 280\times$ (or $\sim$2800 and 31.8 $\mu$W at 1 and 5 THz respectively) improvement in THz power generation efficiency of a photomixer based on Low Temperature grown GaAs are presented. Due to minimal dependence on the carrier recombination time, it is anticipated that the proposed method paves the way for enhancing the speed and efficiency of photomixers and detectors covering UV to far infrared communications wavelengths (300 to 1600 nm).
physics.optics:The injection of a beam of free 50 keV electrons into an unstructured gold surface creates a highly localized source of traveling surface plasmons with spectrum centered around 1.8 eV. The plasmons were detected by a controlled decoupling into light with a grating at a distance from the excitation point. The dominant contribution to the plasmon generation appears to come from the recombination of d-band holes created by the electron beam excitation.
physics.optics:The standard description of material media in electromagnetism is based on multipoles. It is well known that these moments depend on the point of reference chosen, except for the lowest order. It is shown that this "origin dependence" is not unphysical as has been claimed in the literature but forms only part of the effect of moving the point of reference. When also the complementary part is taken into account then different points of reference lead to different but equivalent descriptions of the same physical reality. This is shown at the microscopic as well as at the macroscopic level. A similar interpretation is valid regarding the "origin dependence" of the reflection coefficients for reflection on a semi infinite medium. We show that the "transformation theory" which has been proposed to remedy this situation (and which is thus not needed) is unphysical since the transformation considered does not leave the boundary conditions invariant.
physics.optics:We study dynamical reshaping of polychromatic beams due to collective nonlinear self-action of multiple-frequency components in periodic photonic lattices and predict the formation of polychromatic discrete solitons facilitated by localization of light in spectral gaps. We show that the self-trapping efficiency and structure of emerging polychromatic gap solitons depends on the spectrum of input beams due to the lattice-enhanced dispersion, including the effect of crossover from localization to diffraction in media with defocusing nonlinearity.
physics.optics:We report the first experiential observation and theoretical analysis of the new phenomenon of planar chiral circular conversion dichroism, which in some aspects resembles the Faraday effect in magnetized media, but does not require the presence of a magnetic field for its observation. It results from the interaction of an electromagnetic wave with a planar chiral structure patterned on the sub-wavelength scale, and manifests itself in asymmetric transmission of circularly polarized waves in the opposite directions through the structure and elliptically polarized eigenstates. The new effect is radically different from conventional gyrotropy of three-dimensional chiral media.
physics.optics:A widespread, intuitive and computationally inexpensive method to analyze light guidance through waveguide bends is by introducing an equivalent straight waveguide with refractive index profile modified to account for actual waveguide bend. Here we revise the commonly used equivalent-index formula, ending up with its simple extension that enables rigorous treatment of one- and two-dimensionally confined, uniformly bent waveguides, including tightly coiled microstructure fibers, curved ridge waveguides and ring microresonators. We also show that such technique is applicable only to waveguides composed of isotropic or uniaxially anisotropic materials, with anisotropy axis directed perpendicular to the curvature plane.
physics.optics:We have observed enhanced transmission of light through a gold film due to excitation of standing surface plasmon Bloch waves in a surface Fabry-Perot resonator. Our experimental results strongly contradict the recently suggested model of light transmission via excitation of a composite diffractive evanescent wave.
physics.optics:In this paper, a spatially dispersive finite-difference time-domain (FDTD) method to model wire media is developed and validated. Sub-wavelength imaging properties of the finite wire medium slabs are examined. It is demonstrated that the slab with its thickness equal to an integer number of half-wavelengths is capable of transporting images with sub-wavelength resolution from one interface of the slab to another. It is also shown that the operation of such transmission devices is not sensitive to their transverse dimensions, which can be made even comparable to the wavelength. In this case, the edge diffractions are negligible and do not disturb the image formation.
physics.optics:We show that the optical transients recently read as "Sommerfeld-Brillouin precursors" by Jeong, Dawes and Gauthier are not identifiable with optical precursors (unless one considers that any coherent transient propagating in a dilute medium at the velocity c is an optical precursor) and that they can be interpreted in very simple physical terms.
physics.optics:This letter describes the design and operation of a polymer-based third order distributed feed-back (DFB) microfluidic dye laser. The device relies on light-confinement in a nano-structured polymer film where an array of nanofluidic channels is filled by capillary action with a liquid dye solution which has a refractive index lower than that of the polymer. In combination with a third order DFB grating, formed by the array of nanofluidic channels, this yields a low threshold for lasing. The laser is straight-forward to integrate on Lab-on-a-Chip micro-systems where coherent, tunable light in the visible range is desired.
physics.optics:A class of multiwavelength Fabry-Perot lasers is introduced where the spectrum is tailored through a non-periodic patterning of the cavity effective index. The cavity geometry is obtained using an inverse scattering approach and can be designed such that the spacing of discrete Fabry-Perot lasing modes is limited only by the bandwidth of the inverted gain medium. A specific two-color semiconductor laser with a mode spacing in the THz regime is designed, and measurements are presented demonstrating the simultaneous oscillation of the two wavelengths. The extension of the Fabry-Perot laser concept described presents significant new possibilities in laser cavity design.
physics.optics:Using a generalized Gaussian beam decomposition method we determine the propagation of a Laguerre-Gaussian beam that has passed through a thin nonlinear optical Kerr medium. The orbital angular momentum per photon of the beam is found to be conserved while the component beams change. As an illustration of applications, we propose and demonstrate a z-scan experiment using an $LG_0^1$ beam and a dye-doped polymer film.
physics.optics:We present a comparative overview of existing laser beam profiling methods. We compare the the knife-edge, scanning slit, and pin-hole methods. Data is presented in a comparative fashion. We also elaborate on the use of CCD profiling methods and present appropriate imagery. These methods allow for a quantitative determination of transverse laser beam-profiles using inexpensive and accessible methods. The profiling techniques presented are inexpensive and easily applicable to a variety of experiments.
physics.optics:Continuous-wave Raman amplification in silicon waveguides with negative electrical power dissipation is reported. It is shown that a p-n junction can simultaneously achieve carrier sweep-out leading to net continuous-wave gain, and electrical power generation. The approach is also applicable to silicon Raman lasers and other third-order nonlinear optical devices.
physics.optics:Formation of a novel hybrid-vector spatial plasmon-soliton in a Kerr slab embedded in-between metal plates is predicted and analyzed with a modified NLSE, encompassing hybrid vector field characteristics. Assisted by the transverse plasmonic effect, the self trapping dimension of the plasmon-soliton was substantially compressed (compared to the dielectrically cladded slab case) when reducing the slab width. The practical limitation of the Plasmon-soliton size reduction is determined by available nonlinear materials and metal loss. For the extreme reported values of nonlinear index change, we predict soliton with a cross section of 300nm x 30nm (average dimension of 100nm).
physics.optics:Modal volumes at the nano-scale, much smaller than the "diffraction-limit", with appreciable quality factors, are calculated for a dielectric cavity embedded in a space between metal plates. The modal field is bounded between the metal interfaces in one dimension and can be reduced in size almost indefinitely in this dimension. But more surprisingly, due to the "plasmonic" slow wave effect, this reduction is accompanied by a similar in-plane modal size reduction. Another interesting result is that higher order cavity modes exhibit lower radiation loss. The scheme is studied with effective index analysis, and validated by FDTD simulations.
physics.optics:We analyze the resonant linear and nonlinear transmission through a photonic crystal waveguide side-coupled to a Kerr-nonlinear photonic crystal resonator. Firstly, we extend the standard coupled-mode theory analysis to photonic crystal structures and obtain explicit analytical expressions for the bistability thresholds and transmission coefficients which provide the basis for a detailed understanding of the possibilities associated with these structures. Next, we discuss limitations of standard coupled-mode theory and present an alternative analytical approach based on the effective discrete equations derived using a Green's function method. We find that the discrete nature of the photonic crystal waveguides allows a novel, geometry-driven enhancement of nonlinear effects by shifting the resonator location relative to the waveguide, thus providing an additional control of resonant waveguide transmission and Fano resonances. We further demonstrate that this enhancement may result in the lowering of the bistability threshold and switching power of nonlinear devices by several orders of magnitude. Finally, we show that employing such enhancements is of paramount importance for the design of all-optical devices based on slow-light photonic crystal waveguides.
physics.optics:We present the first observation, to our knowledge, of lasing from a levitated, dye droplet. The levitated droplets are created by computer controlled pico-liter dispensing into one of the nodes of a standing ultrasonic wave (100 kHz), where the droplet is trapped. The free hanging droplet forms a high quality optical resonator. Our 750 nL lasing droplets consist of Rhodamine 6G dissolved in ethylene glycol, at a concentration of 0.02 M. The droplets are optically pumped at 532 nm light from a pulsed, frequency doubled Nd:YAG laser, and the dye laser emission is analyzed by a fixed grating spectrometer. With this setup we have achieved reproducible lasing spectra in the visible wavelength range from 610 nm to 650 nm. The levitated droplet technique has previously successfully been applied for a variety of bio-analytical applications at single cell level. In combination with the lasing droplets, the capability of this high precision setup has potential applications within highly sensitive intra-cavity absorbance detection.
physics.optics:The propagation of electromagnetic waves in the anisotropic medium with a single-sheeted hyperboloid dispersion relation is investigated. It is found that in such an anisotropic medium E- and H-polarized waves have the same dispersion relation, while E- and H-polarized waves exhibit opposite amphoteric refraction characteristics. E- (or H-) polarized waves are positively refracted whereas H- (or E-) polarized waves are negatively refracted at the interface associated with the anisotropic medium. By suitably using the properties of anomalous refraction in the anisotropic medium it is possible to realize a very simple and very efficient beam splitter to route the light. It is shown that the splitting angle and the splitting distance between E- and H- polarized beam is the function of anisotropic parameters, incident angle and slab thickness.
physics.optics:A composite material comprising randomly distributed spherical particles of two different isotropic dielectric-magnetic materials is homogenized using the second-order strong-property-fluctuation theory in the long-wavelength approximation. Whereas neither of the two constituent materials by itself supports planewave propagation with negative phase velocity (NPV), the homogenized composite material (HCM) can. The propensity of the HCM to support NPV propagation is sensitive to the distributional statistics of the constituent material particles, as characterized by a two--point covariance function and its associated correlation length. The scope for NPV propagation diminishes as the correlation length increases.
physics.optics:We study the electromagnetic surface waves localized at an interface separating a one-dimensional photonic crystal and left-handed metamaterial, the so-called surface Tamm states. We demonstrate that the metamaterial allows for a flexible control of the dispersion properties of surface states, and can support the Tamm states with a backward energy flow and a vortex-like structure.
physics.optics:We investigate the wave propagation in the anisotropic metamaterial with single-sheeted hyperboloid dispersion relation. Based on boundary conditions and dispersion relations, we find that the opposite amphoteric refraction, such that E- (or H-) polarized waves are positively refracted whereas H- (or E-) polarized waves are negatively refracted, can occur at the interface associated with the anisotropic metamaterial. Under a certain condition, both E- and H-polarized waves can exhibit the same single-sheeted hyperboloid or straight dispersion relation, while the two polarized waves exhibit different propagation characteristics. We expect some potential device applications can be derived based on based on the unique amphoteric refraction properties.
physics.optics:In this paper, we numerically demonstrate a near-infrared negative-index metamaterial (NIM) slab consisting of multiple layers of perforated metal-dielectric stacks and exhibiting low imaginary part of index over the wavelength of negative refraction. The effective index is obtained using two different numerical methods and found to be consistent. Backward phase propagation is verified by calculation of fields inside the metamaterial. These results point to a new design of low loss thick metamaterial at optical frequencies.
physics.optics:The seamless transition between micro-scale photonics and nano-scale plasmonics requires the mitigation between different waveguiding mechanisms as well as between few orders of magnitude in the field lateral size, down to a small fraction of a wavelength. By exploiting gap plasmon polariton waves both at the micro and nano scale, very high power transfer efficiency (>60%) can be achieved using an ultrashort (few microns) non adiabatic tapered gap plasmon waveguide. Same mechanism may be used to harvest impinging light waves and direct them into a nano hole or slit, to exhibit an anomalous transmission - without the conventional periodic structures. The special interplay of plasmonic and oscillating modes is analyzed.
physics.optics:We demonstrate how to control independently both spatial and temporal dynamics of slow light. We reveal that specially designed nonlinear waveguide arrays with phase-shifted Bragg gratings demonstrate the frequency-independent spatial diffraction near the edge of the photonic bandgap, where the group velocity of light can be strongly reduced. We show in numerical simulations that such structures allow a great flexibility in designing and controlling dispersion characteristics, and open a way for efficient spatiotemporal self-trapping and the formation of slow-light optical bullets.
physics.optics:Theoretical investigations of dynamical behavior in optical parametric oscillators (OPO) have generally assumed that the cavity detunings of the interacting fields are controllable parameters. However, OPOs are known to experience mode hops, where the system jumps to the mode of lowest cavity detuning. We note that this phenomenon significantly limits the range of accessible detunings and thus may prevent instabilities predicted to occur above a minimum detuning from being evidenced experimentally. As a simple example among a number of instability mechanisms possibly affected by this limitation, we discuss the Hopf bifurcation leading to periodic behavior in the monomode mean-field model of a triply resonant OPO and show that it probably can be observed only in very specific setups.
physics.optics:The race to engineering metamaterials comprising of a negative refractive index in the optical range has been fueled by the realization of negative index materials for GHz frequencies six years ago. Sheer miniaturization of the GHz resonant structures is one approach. Alternative designs make use of localized plasmon resonant metal nanoparticles or nanoholes in metal films. Following this approach, a negative refractive index has been realized in the optical range very recently. We review these recent results and summarize how to unambiguously retrieve the effective refractive index of thin layers from data accessible to measurements. Numerical simulations show that a composite material comprising of silver strips and a gain providing material can have a negative refractive index of -1.3 and 100% transmission, simultaneously.
physics.optics:We present a theoretical formulation of competition among electromagnetically induced transparency (EIT) and Raman processes. The latter become important when the medium can no longer be considered to be dilute. Unlike the standard formulation of EIT, we consider all fields applied and generated as interacting with both the transitions of the $\Lambda$ scheme. We solve Maxwell equations for the net generated field using a fast-Fourier-transform technique and obtain predictions for the probe, control and Raman fields. We show how the intensity of the probe field is depleted at higher atomic number densities due to the build up of multiple Raman fields.
physics.optics:We develop an amended ray optics description for reflection at the curved dielectric interfaces of optical microresonators which improves the agreement with wave optics by about one order of magnitude. The corrections are separated into two contributions of similar magnitude, corresponding to ray displacement in independent quantum phase space directions, which can be identified with Fresnel filtering and the Goos-Haenchen shift, respectively. Hence we unify two effects which only have been studied separately in the past.
physics.optics:An analytical description of arbitrary strongly aberrated axially symmetric focusing is developed. This is done by matching the solution of geometrical optics with a wave pattern which is universal for the underlying ray structure. The corresponding canonical integral is the Bessoid integral, which is a three dimensional generalization of the Pearcey integral that approximates the field near an arbitrary two-dimensional cusp. We first develop the description for scalar fields and then generalize it to the vector case. As a practical example the formalism is applied to the focusing of light by transparent dielectric spheres with a few wavelengths in diameter. The results demonstrate good agreement with the Mie theory down to Mie parameters of about 30. Compact analytical expressions are derived for the intensity on the axis and the position of the diffraction focus both for the general case and for the focusing by microspheres. The high intensity region is narrower than for an ideal lens of the same aperture at the expense of longitudinal localization and has a polarization dependent fine structure, which can be explained quantitatively. The results are relevant for aerosol and colloid science where natural light focusing occurs and can be used in laser micro- and nano-processing of materials.
physics.optics:We demonstrate a novel convenient nondestructive method based on optical parametric amplification that allows retrieval of the zero-dispersion wavelength map along a short optical fiber span with a high-spatial resolution. The improved resolution relies on the high sensitivity to the local longitudinal dispersion fluctuations of the parametric high-gain spectrum.
physics.optics:When a circularly polarized plane wave is normally incident on a slab of a structurally chiral material with local $\bar{4}2m$ point group symmetry and a central twist defect, the slab can function as either a narrowband reflection hole filter for co-handed plane waves or an ultranarrowband transmission hole filter for cross-handed plane waves, depending on its thickness and the magnitude of the applied dc electric field. Exploitation of the Pockels effect significantly reduces the thickness of the slab.
physics.optics:Horizontal resonances of slit arrays are studied. They can lead to an enhanced transmission that cannot be explained using the single-mode approximation. A new type of cavity resonance is found when the slits are narrow for a wavelength very close to the period. It can be excited for very low thicknesses. Optimization shows these structures could constitute interesting monochromatic filters.
physics.optics:It is found that the amphoteric refraction, i.e. the refraction can be either positive or negative depending on the incident angles, could occur at a planar interface associated with a uniaxially anisotropic positive index media (PIM) or an anisotropic negative index media (NIM). Particularly, the anomalous negative refraction can occur at a planar interface from an isotropic PIM to an anisotropic PIM, whereas the anomalous positive refraction occurs at the interface from an isotropic PIM to an anisotropic NIM. The optimal conditions to yield the two unusual refractions are obtained. The difference of the two types of amphoteric refraction is discussed.
physics.optics:We show that a Rubidium vapor can be produced within the core of a photonic band-gap fiber yielding an optical depth in excess of 2000. Our technique for producing the vapor is based on coating the inner walls of the fiber core with an organosilane and using light-induced atomic desorption to release Rb atoms into the core. We develop a model to describe the dynamics of the atomic density, and as an initial demonstration of the potential of this system for supporting ultra-low-level nonlinear optical interactions, we perform electromagnetically-induced transparency with control-field powers in the nanowatt regime, which represents more than a 1000-fold reduction from the power required for bulk, focused geometries.
physics.optics:We consider layered heterostructures combining ordinary positive index materials and dispersive metamaterials. We show that these structures can exhibit a new type of photonic gap around frequencies where either the magnetic permeability \mu or the electric permittivity \epsilon of the metamaterial is zero. Although the interface of a semi-infinite medium with zero refractive index (a condition attained either when \mu= 0 or when \epsilon= 0) is known to give full reflectivity for all incident polarizations, here we show that a gap corresponding to \mu = 0 occurs only for TE polarized waves, whereas a gap corresponding to \epsilon = 0 occurs only for TM polarized waves. These band gaps are scale-length invariant and very robust against disorder, although they may disappear for the particular case of propagation along the stratification direction.
physics.optics:Here we demonstrate that photons launched into a specially designed metamaterial waveguide act as massive quasi-particles, which experience strong acceleration. Laser light propagating through such a waveguide may be used as a thermometer which would measure the Unruh temperature. Moreover, the metamaterial waveguide design may be approximated by a tapered optical waveguide.
physics.optics:We investigate the effects of delayed Raman response on pulse dynamics in massive multichannel optical fiber communication systems. Taking into account the stochastic nature of pulse sequences in different frequency channels and the Raman induced energy exchange in pulse collisions we show that the pulse parameters exhibit intermittent dynamic behavior, and that the pulse amplitudes exhibit relatively strong and long-range correlations. Moreover, we find that the Raman-induced cross frequency shift is the main intermittency-related mechanism leading to bit pattern deterioration and evaluate the bit-error-rate of the system.
physics.optics:Model of partially coherent pulses, based on the concept of "hidden coherence", introduced recently by Picozzi and Haelterman in framework of parametric wave mixing, is presented. The nonuiform and nonstationary phase shift, while completely deterministic, results in the beam properties, which are typical for partially coherent light - low contrast of interference effects, increase of spectral width and so on - i. e. light becomes effectively non-coherent. The proposed model is studied in framework of coherent mode decomposition, its main properties and limitations of the model are discussed.
physics.optics:Negative refraction is known to occur in materials that simultaneously possess a negative electric permittivity and magnetic permeability; hence they are termed negative index materials. However, there are no known natural materials that exhibit a negative index of refraction. In large part, interest in these materials is due to speculation that they could be used as perfect lenses with superresolution. We propose a new way of achieving negative refraction with currently available technology, based on transparent, metallo-dielectric multilayer structures. The advantage of these structures is that both tunability and transmission (well above 50%) can be achieved in the visible wavelength regime. We demonstrate both negative refraction and superresolution in these structures. Our findings point to a simpler way to fabricate a material that exhibits negative refraction. This opens up an entirely new path not only for negative refraction, but also to expand the exploration of wave propagation effects in metals.
physics.optics:We present detailed experimental and numerical studies of random lasing in weakly scattering systems. The interference of scattered light, which is weak in the passive systems, is greatly enhanced in the presence of high gain, providing coherent and resonant feedback for lasing. The lasing modes are confined in the vicinity of the pumped volume due to absorption of emitted light outside it. In the ballistic regime where the size of gain volume is less than the scattering mean free path, lasing oscillation occurs along the direction in which the gain volume is most extended, producing directional laser output. The feedback for lasing originates mainly from backscattering of particles near the boundaries of pumped region. It results in nearly constant frequency spacing of lasing modes, which scales inversely with the maximum dimension of the gain volume.
physics.optics:We model the process of ultra broadband light generation in which a pair of laser pulses separated by the Raman frequency drive a Raman transition. In contrast to the usual approach using separate field envelopes for the different frequency components, we treat the field as a single entity. This requires the inclusion of few-cycle corrections to the pulse propagation. Our single-field model makes fewer approximations and is mathematically (and hence computationally) simpler, although it does require greater computational resources to implement. The single-field theory reduces to the traditional multi-field one using appropriate approximations.
physics.optics:I present a detailed derivation of wideband optical pulses interacting with a Raman transition in the kind of scheme currently used to generate the ultra broadband light fields needed to create ultrashort pulses. In contrast to the usual approach using separate field envelopes for the pump, Stokes, and anti-Stokes spectral lines, I use a single field envelope. This requires the inclusion of few-cycle corrections to the pulse propagation. The single-field model makes fewer approximations and is mathematically (and hence computationally) simpler, although it does require greater computational resources to implement. The single-field theory reduces to the traditional multi-field one using appropriate approximations.
physics.optics:The absorption and extinction spectra of sub-wavelength cylinder arrays are shown to present two different kind of resonances. Close to the Rayleigh anomalies, the diffractive coupling with the lattice periodicity leads to sharp peaks in the extinction spectra with characteristic Fano line shapes for both s and p-polarizations. When the material exhibits an absorption line or in the presence of localized surface plasmon/polaritons, the system is shown to present resonant absorption with wider and symmetric line shapes. For s-polarization our analysis predicts a theoretical limit of 50 % of absorption. Interestingly, for p-polarized light and an appropriate choice of parameters, subwavelength cylinder arrays can present perfect (100 %) absorption.
physics.optics:We present simulation results of a design for negative index materials that uses magnetic resonators to provide negative permeability and metal film for negative permittivity. We also discuss the possibility of using semicontinuous metal films to achieve better manufacturability and enhanced impedance matching.
physics.optics:A medium which is an isotropic chiral medium from the perspective of a co-moving observer is a Faraday chiral medium (FCM) from the perspective of a non-co-moving observer. The Tellegen constitutive relations for this FCM are established. By an extension of the Beltrami field concept, these constitutive relations are exploited to show that planewave propagation is characterized by four generally independent wavenumbers. This FCM can support negative phase velocity at certain translational velocities and with certain wavevectors, even though the corresponding isotropic chiral medium does not. The constitutive relations and Beltrami--like fields are also used to develop a convenient spectral representation of the dyadic Green functions for the FCM.
physics.optics:Silicon and germanium are perhaps the two most well-understood semiconductor materials in the context of solid state device technologies and more recently micromachining and nanotechnology. Meanwhile, these two materials are also important in the field of infrared lens design. Optical instruments designed for the wavelength range where these two materials are transmissive achieve best performance when cooled to cryogenic temperatures to enhance signal from the scene over instrument background radiation. In order to enable high quality lens designs using silicon and germanium at cryogenic temperatures, we have measured the absolute refractive index of multiple prisms of these two materials using the Cryogenic, High-Accuracy Refraction Measuring System (CHARMS) at NASA Goddard Space Flight Center, as a function of both wavelength and temperature. For silicon, we report absolute refractive index and thermo-optic coefficient (dn/dT) at temperatures ranging from 20 to 300 K at wavelengths from 1.1 to 5.6 microns, while for germanium, we cover temperatures ranging from 20 to 300 K and wavelengths from 1.9 to 5.5 microns. We compare our measurements with others in the literature and provide temperature-dependent Sellmeier coefficients based on our data to allow accurate interpolation of index to other wavelengths and temperatures. Citing the wide variety of values for the refractive indices of these two materials found in the literature, we reiterate the importance of measuring the refractive index of a sample from the same batch of raw material from which final optical components are cut when absolute accuracy greater than +/-5 x 10^-3 is desired.
physics.optics:Optical plasmon-polariton modes confined in both transverse dimensions to significantly less than a wavelength are exhibited in open waveguides structured as sharp metal wedges. The analysis reveals two distinctive modes corresponding to a localized mode on the wedge point and surface mode propagation on the abruptly bent interface. These predictions are accompanied by unique field distributions and dispersion characteristics.
physics.optics:We study propagation of two lowest order Gaussian laser beams with different wavelengths in weak atmospheric turbulence. Using the Rytov approximation and assuming a slow detector we calculate the longitudinal and radial components of the scintillation index for a typical free space laser communication setup. We find the optimal configuration of the two laser beams with respect to the longitudinal scintillation index. We show that the value of the longitudinal scintillation for the optimal two-beam configuration is smaller by more than 50% compared with the value for a single lowest order Gaussian beam with the same total power. Furthermore, the radial scintillation for the optimal two-beam system is smaller by 35%-40% compared with the radial scintillation in the single beam case. Further insight into the reduction of intensity fluctuations is gained by analyzing the self- and cross-intensity contributions to the scintillation index.
physics.optics:A study of band edge phenomena in ridge waveguide magnetophotonic crystals is presented. Normal mode analysis is used to examine the near-band edge and Bragg center-wavelength group velocities and their relation to the polarization. Large polarization departures from the input are observed in Bragg gratings patterned by focused-ion-beam milling on liquid-phase-epitaxial (LPE) BiLuIG films on (111) gadolinium gallium garnet (GGG) substrates. Transfer matrix analysis of the spectral response in the photonic crystals allows the determination of the phase of the elliptically polarized normal modes from measured transmittances and polarization rotation. Unlike non-birefringent Faraday rotators a non-linear-relation exists between polarization rotation and the normal mode phase difference. Large phase differences between right- and left-elliptically polarized normal modes and optical slow-down are found near the band edges and at the resonant cavity center-wavelength. Standing-wave photon trapping and optical slow-down are analyzed and compared for these systems.
physics.optics:Inter-modal coupling in photonic bandgap optical channels in magnetic films is used to leverage the transverse-electric (TE) to transverse-magnetic (TM) mode conversion due to the Faraday Effect. The underlying mechanism is traced to the dependence of the grating coupling-constant on the modal refractive index and the mode profile of the propagating beam, a feature that arises in waveguide propagation and is not present in normal-incidence stack magnetophotonic crystals. Large changes in polarization near the band edges are observed in first and second orders as a result of the photonic bandgap structure. Extreme sensitivity to linear birefringence exists in second order.
physics.optics:This work reports single-frequency laser oscillation at 1003.4 nm of an optically pumped external cavity semiconductor laser. By using a gain structure bonded onto a high conductivity substrate, we demonstrate both theoretically and experimentally the strong reduction of the thermal resistance of the active semiconductor medium, resulting in a high power laser emission. The spectro-temporal dynamics of the laser is also explained. Furthermore, an intracavity frequency-doubling crystal was used to obtain a stable single-mode generation of blue (501.5 nm) with an output power around 60 mW.
physics.optics:We explore the role of laser induced anti-alignment in enhancing molecular orientation. A field-free enhanced orientation via anti-alignment scheme is presented, which combines a linearly polarized femtosecond laser pulse with a half-cycle pulse. The laser pulse induces transient anti-alignment in the plane orthogonal to the field polarization, while the half-cycle pulse leads to the orientation. We identify two qualitatively different enhancement mechanisms depending on the pulse order, and optimize their effects using classical and quantum models both at zero and non-zero temperature.
physics.optics:We study surface states of 1D photonic crystals using a semiclassical coupled wave theory. Both TE and TM modes are treated. We derive analytic approximations that clarify the systematics of the dispersion relations, and the roles of the various parameters defining the crystal.
physics.optics:A 2D transparency may be projected on a diffractive screen by just illuminating it with a filament lamp of the same height. Sharpness of the filament width is naturally related to sharpness on the image, but some peculiar properties makes this experience to be different from a shadow projection case.
physics.optics:Although Gigahertz-scale free-carrier modulators have been previously demonstrated in silicon, intensity modulators operating at Terahertz speeds have not been reported because of silicon's weak ultrafast optical nonlinearity. We have demonstrated intensity modulation of light with light in a silicon-polymer integrated waveguide device, based on the all-optical Kerr effect - the same ultrafast effect used in four-wave mixing. Direct measurements of time-domain intensity modulation are made at speeds of 10 GHz. We showed experimentally that the ultrafast mechanism of this modulation functions at the optical frequency through spectral measurements, and that intensity modulation at frequencies in excess of 1 THz can be obtained in this device. By integrating optical polymers through evanescent coupling to high-mode-confinement silicon waveguides, we greatly increase the effective nonlinearity of the waveguide for cross-phase modulation. The combination of high mode confinement, multiple integrated optical components, and high nonlinearities produces all-optical ultrafast devices operating at continuous-wave power levels compatible with telecommunication systems. Although far from commercial radio frequency optical modulator standards in terms of extinction, these devices are a first step in development of large-scale integrated ultrafast optical logic in silicon, and are two orders of magnitude faster than previously reported silicon devices.
physics.optics:The Sagnac effect in two dimensional (2D) resonant microcavities is studied theoretically and numerically. The frequency shift due to the Sagnac effect occurs as a threshold phenomenon for the angular velocity in a rotating microcavity. Above the threshold, the eigenfunctions of a rotating microcavity become rotating waves while they are standing waves below the threshold.
physics.optics:The interaction between two parallel beams in one-dimensional discrete saturable systems has been investigated using lithium niobate nonlinear waveguide arrays. When the beams are separated by one channel and in-phase it is possible to observe soliton fusion at low power levels. This new result is confirmed numerically. By increasing the power, soliton-like propagation of weakly-coupled beams occurs. When the beams are out-of-phase the most interesting result is the existence of oscillations which resemble the recently discovered Tamm oscillations.
physics.optics:We study propagation of TM- and TE-polarized light in two-dimensional arrays of silver nanorods of various diameters in a gelatin background. We calculate the transmittance, reflectance and absorption of arranged and disordered nanorod arrays and compare the exact numerical results with the predictions of the Maxwell-Garnett effective-medium theory. We show that interactions between nanorods, multipole contributions and formations of photonic gaps affect strongly the transmittance spectra that cannot be accounted for in terms of the conventional effective-medium theory. We also demonstrate and explain the degradation of the transmittance in arrays with randomly located rods as well as weak influence of their fluctuating diameter. For TM modes we outline the importance of skin-effect, which causes the full reflection of the incoming light. We then illustrate the possibility of using periodic arrays of nanorods as high-quality polarizers.
physics.optics:Expressions are obtained for the Wigner function moments of a paraxial light beam represented by arbitrary coherent superposition of Hermite-Gaussian beams with plane wave fronts. Possibilities are discussed for application of the obtained results to modeling real laser beams and to design of optical systems implementing prescribed transformations of a beam transverse structure.
physics.optics:We demonstrate the dynamics of stimulated Raman scattering in designed high-Q/Vm silicon photonic band gap nanocavities through the coupled-mode theory framework towards optically-pumped silicon lasing. The interplay of other chi(3) effects such as two-photon absorption and optical Kerr, related free-carrier dynamics, thermal effects, as well as linear losses such as cavity radiation and linear material absorption are included and investigated numerically. Our results clarify the relative contributions and evolution of the mechanisms, and demonstrate the lasing and shutdown thresholds. Our studies illustrate the conditions for continuous-wave and pulsed highly-efficient Raman frequency conversion to be practically realized in monolithic silicon high-Q/Vm photonic band gap defect cavities.
physics.optics:We consider group delay and broadening using two strongly absorbing and widely spaced resonances. We derive relations which show that very large pulse bandwidths coupled with large group delays and small broadening can be achieved. Unlike single resonance systems, the dispersive broadening dominates the absorptive broadening which leads to a dramatic increase in the possible group delay. We show that the double resonance systems are excellent candidates for realizing all-optical delay lines. We report on an experiment which achieved up to 50 pulse delays with 40% broadening.
physics.optics:Pseudoscopic (inverted depth) images that keep a continuous parallax were shown to be possible by use of a double diffraction process intermediated by a slit. One diffraction grating directing light to the slit acts as a wavelength encoder of views, while a second diffraction grating decodes the projected image. The process results in the enlargement of the image under common white light illumination up to infinite magnification at a critical point. We show that this point corresponds to another simple-symmetry object-observer system. Our treatment allows us to explain the experience by just dealing with main ray directions.
physics.optics:Pseudoscopic images that keep the continuous parallax are shown to be possible due to a double diffraction process intermediated by a slit. One diffraction grating acts as a wavelength encoder of views while a second diffraction grating decodes the projected image. The process results in the enlargement of the image under common white light illumination.
physics.optics:We propose to employ the quasiisotropic metamaterial (QIMM) slab to construct a polarization insensitive lens, in which both E- and H-polarized waves exhibit the same refocusing effect. For shallow incident angles, the QIMM slab will provide some degree of refocusing in the same manner as an isotropic negative index material. The refocusing effect allows us to introduce the ideas of paraxial beam focusing and phase compensation by the QIMM slab. On the basis of angular spectrum representation, a formalism describing paraxial beams propagating through a QIMM slab is presented. Because of the negative phase velocity in the QIMM slab, the inverse Gouy phase shift and the negative Rayleigh length of paraxial Gaussian beam are proposed. We find that the phase difference caused by the Gouy phase shift in vacuum can be compensated by that caused by the inverse Gouy phase shift in the QIMM slab. If certain matching conditions are satisfied, the intensity and phase distributions at object plane can be completely reconstructed at image plane. Our simulation results show that the superlensing effect with subwavelength image resolution could be achieved in the form of a QIMM slab.
physics.optics:We report the use of an optical interference holographic setup with a five-beam configuration, consisting of four side beams and one central beam from the same half space, to fabricate woodpile and diamond structures for the use as photonic bandgap materials in which electromagnetic waves are forbidden in the bandgap. By exploiting the advantage of the binarization of the interference pattern, using intensity cut-off, either linear or circular central beam can be used. More importantly, the beam configurations can be easily implemented experimentally as compared to other configurations in which the interfering beams are counter-propagating from both half spaces.
physics.optics:The identification of the refractive index and wave vector for general (possibly active) linear, isotropic, homogeneous, and non-spatially dispersive media is discussed. Correct conditions for negative refraction necessarily include the global properties of the permittivity and permeability functions $\epsilon=\epsilon(\omega)$ and $\mu=\mu(\omega)$. On the other hand, a necessary and sufficient condition for left-handedness can be identified at a single frequency ($\re\epsilon/|\epsilon|+\re\mu/|\mu|<0$). At oblique incidence to semi-infinite, active media it is explained that the wave vector generally loses its usual interpretation for real frequencies.
physics.optics:We further miniaturize a recently established silver-based negative-index metamaterial design. By comparing transmittance, reflectance and phase-sensitive time-of-flight experiments to theory, we infer a real part of the refractive index of -0.6 at 780 nm wavelength -- which is visible in the laboratory.
physics.optics:Recently it was suggested that Hawking radiation may be observed in a nonlinear electromagnetic waveguide upon propagation of an optical pulse. We show that the spectral characteristics of the Hawking effect in such a waveguide are indistinguishable from the well-known effect of frequency broadening of an optical pulse due to self-phase modulation. Furthermore, we derive an estimate on the critical optical power at which Hawking effect is dominated by the self-phase modulation. It appears that optical experiments reported so far are clearly dominated by self-phase modulation.
physics.optics:We report the use of a (4+1)-beam optical interference holography technique to fabricate woodpile structures in photo-resists. The configuration consists of 4 linear polarized side beams arranged symmetrically around a circular polarized central beam with all the beams from the same half space, making it easily accessible experimentally. The fabricated woodpile structures are in good agreement with model simulations. Furthermore, woodpiles with the diamond symmetry are also obtained by exploiting the deformations of the photo-resists. Directional bandgaps in the visible range are also observed for the samples with and without the correct stacking of the woodpile structures.
physics.optics:We investigate slow-light via stimulated Brillouin scattering in a room temperature optical fiber that is pumped by a spectrally broadened laser. Broadening the spectrum of the pump field increases the linewidth $\Delta\omega_p$ of the Stokes amplifying resonance, thereby increasing the slow-light bandwidth. One physical bandwidth limitation occurs when the linewidth becomes several times larger than the Brillouin frequency shift $\Omega_B$ so that the anti-Stokes absorbing resonance cancels out substantially the Stokes amplifying resonance and hence the slow-light effect. We find that partial overlap of the Stokes and anti-Stokes resonances can actually lead to an enhancement of the slow-light delay - bandwidth product when $\Delta\omega_p \simeq 1.3 \Omega_B$. Using this general approach, we increase the Brillouin slow-light bandwidth to over 12 GHz from its nominal linewidth of $\sim$30 MHz obtained for monochromatic pumping. We controllably delay 75-ps-long pulses by up to 47 ps and study the data pattern dependence of the broadband SBS slow-light system.
physics.optics:We present a theoretical study of electromagnetic surface waves localized at an interface separating a conventional uniform medium and a semi-infinite 1-D photonic crystal made of alternate left-handed metamaterial and right-handed material which we refer to as left-handed photonic crystal. We find novel type of surface mode's structure, the so-called surface Tamm states and demonstrate that the presence of metamaterial in the photonic crystal structure allows for a flexible control of the dispersion properties of surface states, and can support the Tamm states with a backward energy flow and a vortex-like structure.
physics.optics:We investigate nonlinear transmission in a layered structure consisting of a slab of positive index material (PIM) with Kerr-type nonlinearity and a sub-wavelength layer of linear negative index material (NIM) sandwiched between semi-infinite linear dielectrics. We find that a thin layer of NIM leads to significant changes in the hysteresis width when the nonlinear slab is illuminated at an angle near that of total internal reflection. Unidirectional diode-like transmission with enhanced operational range is demonstrated. These results may be useful for NIMs characterization and for designing novel NIMs based devices.
physics.optics:The use of short lengths of large core phosphate glass fibre, doped with high concentrations of Er or Er:Yb represents an attractive route to achieving high power erbium doped fibre amplifiers (EDFAs) and lasers (EDFLs). With the aim of investigating the potential of achieving diffraction limited output from such large core fibres, we present experimental results of fundamental mode propagation through a 20 cm length of passive 300 micrometer core multimode fibre when the input is a well-aligned Gaussian beam. Through careful control of fibre geometry, input beam parameters and alignment, we measured an output M squared of 1.1 + - 0.05. The fibre had a numerical aperture of 0.389, implying a V number of 236.8. To our knowledge, this is the largest core fibre through which diffraction limited fundamental mode propagation has been demonstrated. Although the results presented here relate to undoped fibre, they do provide the practical basis for a new generation of EDFAs and EDFLs.
physics.optics:Achieving single-mode operation and highly directional (preferably unidirectional) in-plane light output from whispering-gallery (WG) mode semiconductor microdisk resonators without seriously degrading the mode Q-factor challenges designers of low-threshold microlasers. To address this problem, basic design rules to tune the spectral and emission characteristics of micro-scale optical cavity structures with nanoscale features by tailoring their geometry are formulated and discussed in this paper. The validity and usefulness of these rules is demonstrated by reviewing a number of previously studied cavity shapes with global and local deformations. The rules provide leads to novel improved WG-mode cavity designs, two of which are presented: a cross-shaped photonic molecule with introduced asymmetry and a photonic-crystal-assisted microdisk resonator. Both these designs yield degenerate mode splitting, as well as Q-factor enhancement and directional light output of one of the split modes.
physics.optics:Complex and interesting electromagnetic behavior can be found in spaces with non-flat topology. When considering the properties of an electromagnetic medium under an arbitrary coordinate transformation an alternative interpretation presents itself. The transformed material property tensors may be interpreted as a different set of material properties in a flat, Cartesian space. We describe the calculation of these material properties for coordinate transformations that describe spaces with spherical or cylindrical holes in them. The resulting material properties can then implement invisibility cloaks in flat space. We also describe a method for performing geometric ray tracing in these materials which are both inhomogeneous and anisotropic in their electric permittivity and magnetic permeability.
physics.optics:We describe a way to determine the total angular momentum, both spin and orbital, transferred to a particle trapped in optical tweezers. As an example an LG02 mode of a laser beam with varying degrees of circular polarisation is used to trap and rotate an elongated particle with a well defined geometry. The method successfully estimates the total optical torque applied to the particle. For this technique, there is no need to measure the viscous drag on the particle, as it is an optical measurement. Therefore, knowledge of the particle's size and shape, as well as the fluid's viscosity, is not required.
physics.optics:We suggest a novel type of photonic structures where the strength of diffraction can be managed in a very broad frequency range. We introduce optimized arrays of curved waveguides where light beams experience wavelength-independent normal, anomalous, or zero diffraction. Our results suggest novel opportunities for efficient self-collimation, focusing, and reshaping of beams produced by white-light and super-continuum sources. We also show how to manipulate light patterns through multicolor Talbot effect, which is possible neither in free space nor in conventional photonic lattices.
physics.optics:We demonstrate experimentally that lasing in a semiconductor microstadium can be optimized by controlling its shape. Under spatially uniform optical pumping, the first lasing mode in a GaAs microstadium with large major-to-minor-axis ratio usually corresponds to a high-quality scar mode consisting of several unstable periodic orbits. Interference of waves propagating along the constituent orbits may minimize light leakage at particular major-to-minor-axis ratio. By making stadium of the optimum shape, we are able to maximize the mode quality factor and align the mode frequency to the peak of the gain spectrum, thus minimizing the lasing threshold. This work opens the door to control chaotic microcavity lasers by tailoring the shape factor.
physics.optics:Optical microcavities trap light in compact volumes by the mechanisms of almost total internal reflection or distributed Bragg reflection, enable light amplification, and select out specific (resonant) frequencies of light that can be emitted or coupled into optical guides, and lower the thresholds of lasing. Such resonators have radii from 1 to 100 mkm and can be fabricated in a wide range of materials. Devices based on optical resonators are essential for cavity quantum-electro-dynamic experiments, frequency stabilization, optical filtering and switching, light generation, biosensing, and nonlinear optics.
physics.optics:Based on a coordinate transformation approach, Pendry {\it et al.} have reported electromagnetically anisotropic and inhomogeneous shells that, in theory, completely shield an interior structure of arbitrary size from electromagnetic fields without perturbing the external fields. We report full-wave simulations of the cylindrical version of this cloaking structure using ideal and nonideal (but physically realizable) electromagnetic parameters in an effort to understand the challenges of realizing such a structure in practice. The simulations indicate that the performance of the electromagnetic cloaking structure is not especially sensitive to modest permittivity and permeability variations. This is in contrast to other applications of engineered electromagnetic materials, such as subwavelength focusing using negative refractive index materials. The cloaking performance degrades smoothly with increasing loss, and effective low-reflection shielding can be achieved with a cylindrical shell composed of an eight (homogeneous) layer approximation of the ideal continuous medium.
physics.optics:By integrating the full Maxwell's equations we predict the existence of gap solitons in a quadratic, Fabry-Perot negative index cavity. An intense, fundamental pump pulse shifts the band structure that forms when magnetic and electric plasma frequencies are different so that a weak, second harmonic pulse initially tuned inside the gap is almost entirely transmitted. The process is due cascading, which occurs far from phase matching conditions, and causes pulse compression. A nonlinear polarization spawns a dark soliton, while a nonlinear magnetization produces a bright soliton.
physics.optics:The Bruggeman formalism for the homogenization of particulate composite materials is used to predict the effective permittivity dyadic of a two-constituent composite material with one constituent having the ability to display the Pockels effect. Scenarios wherein the constituent particles are randomly oriented, oriented spheres, and oriented spheroids are numerically explored. Thereby, homogenized composite materials (HCMs) are envisaged whose constitutive parameters may be continuously varied through the application of a low-frequency (dc) electric field. The greatest degree of control over the HCM constitutive parameters is achievable when the constituents comprise oriented and highly aspherical particles and have high electro-optic coefficients.
physics.optics:We propose an approach to far-field optical imaging beyond the diffraction limit. The proposed system allows image magnification, is robust with respect to material losses and can be fabricated by adapting existing metamaterial technologies in a cylindrical geometry.
physics.optics:Based on the Ewald-Oseen extinction theorem, the omnidirectional total transmission of waves incident from vacuum into an anisotropic dielectric-magnetic metamaterials is investigated. It is shown that the omnidirectional total transmission need not limit at the interface associated with the conventional nonmagnetic anisotropic medium. The recent advent of a new class of anisotropic dielectric-magnetic matermaterial make the omnidirectional total transmission become available. It is found that the inherent physics underlying the omnidirectional total transmission are collective contributions of the electric and magnetic responses.
physics.optics:We study second harmonic generation in a planar dielectric waveguide having a low-index, polymer core layer, bounded by two multilayer stacks. This geometry allows exceptionally strong confinement of the light at the fundamental wavelength inside the core region with virtually zero net propagation losses for distances that exceed several centimeters, provided material and scattering losses are neglected. A phase-matched configuration of the waveguide is reported in which the pump signal is the lowest-order mode of the waveguide, and the generated second harmonic signal corresponds to the third propagation mode of the waveguide. Using a polymer waveguide core, having chi(2)=100 pm/V, we predict a conversion efficiency of approximately 90% after a propagation distance of 2 mm, using peak pump intensities inside the core of the waveguide of 1.35 GW/cm^2. If the waveguide core contains polymer layers with different glass transition temperatures, the layers can be poled independently to maximize the overlap integral, and similar pump depletions may be achieved over a distance of approximately 500 microns.
physics.optics:The equations of motion of charged particles under the influence of short electromagnetic pulses are investigated. The subcycle regime is considered, and the delta function approximation is applied. The effects of the self force are also considered, and the threshold where radiation becomes important is discussed. A dimensionless parameter is defined that signals the onset of radiation reaction effects.
physics.optics:We show that in addition to well known Bessel, Hermite-Gauss, and Laguerre-Gauss beams of electromagnetic radiation, one may also construct exponential beams. These beams are characterized by a fall-off in the transverse direction described by an exponential function of rho. Exponential beams, like Bessel beams, carry definite angular momentum and are periodic along the direction of propagation, but unlike Bessel beams they have a finite energy per unit beam length. The analysis of these beams is greatly simplified by an extensive use of the Riemann-Silberstein vector and the Whittaker representation of the solutions of the Maxwell equations in terms of just one complex function. The connection between the Bessel beams and the exponential beams is made explicit by constructing the exponential beams as wave packets of Bessel beams.
physics.optics:We demonstrate that under certain conditions, fractional Talbot revivals can occur in heterostructures of composite metamaterials, such as multilayer positive and negative index media, metallodielectric stacks, and one-dimensional dielectric photonic crystals. Most importantly, without using the paraxial approximation we obtain Talbot images for the feature sizes of transverse patterns smaller than the illumination wavelength. A general expression for the Talbot distance in such structures is derived, and the conditions favorable for observing Talbot effects in layered heterostructures is discussed.
physics.optics:We study experimentally the dynamic tunability and self-induced nonlinearity of split-ring resonators incorporating variable capacitance diodes. We demonstrate that the eigenfrequencies of the resonators can be tuned over a wide frequency range, and significantly, we show that the self-induced nonlinear effects observed in the varactor-loaded split-ring resonator structures can appear at relatively low power levels.
physics.optics:We study second harmonic generation in a negative index material cavity. The transmission spectrum shows a gap between the electric and magnetic plasma frequencies, so that localized and anti-localized states are allowed at the band edges. The process is made efficient by local phase matching conditions between a forward-propagating pump and a backward-propagating second harmonic signal. By simultaneously exciting the cavity with counter-propagating pulses one is able to enhance or inhibit absorption and stimulated processes. The control parameter is the relative phase difference between the two incident pulses which determines the interference properties of the fields inside the cavity.
physics.optics:The problem of object restoration in the case of spatially incoherent illumination is considered. A regularized solution to the inverse problem is obtained through a probabilistic approach, and a numerical algorithm based on the statistical analysis of the noisy data is presented. Particular emphasis is placed on the question of the positivity constraint, which is incorporated into the probabilistically regularized solution by means of a quadratic programming technique. Numerical examples illustrating the main steps of the algorithm are also given.
physics.optics:We study the energy propagation in subwavelength waveguides and demonstrate that the mechanism of material gain, previously suggested for loss compensation, is also a powerful tool to manipulate dispersion and propagation characteristics of electromagnetic pulses at the nanoscale. We show theoretically that the group velocity in lossy nano-waveguides can be controlled from slow to superluminal values by the material gain and waveguide geometry and develop an analytical description of the relevant physics. We utilize the developed formalism to show that gain-assisted dispersion management can be used to control the transition between ``photonic-funnel'' and ``photonic-compressor'' regimes in tapered nano-waveguides. The phenomenon of strong modulation of group velocity in subwavelength structures can be realized in waveguides with different geometries, and is present for both volume and surface-modes.
physics.optics:Using transmission electron microscopy (TEM) to analyse the physical-chemical surface properties of subwavlength structured silver films and finite-difference time-domain (FDTD) numerical simulations of the optical response of these structures to plane-wave excitation, we report on the origin and nature of the persistent surface waves generated by a single slit-groove motif and recently measured by far-field optical interferometry. The surface analysis shows that the silver films are free of detectable oxide or sulfide contaminants, and the numerical simulations show very good agreement with the results previously reported.
physics.optics:Coherent responses of resonance atom layer to short optical pulse excitation are numerically considered. The inhomogeneous broadening of one-photon transition, the local field effect, and the substrate dispersion are involved into analysis. For a certain intensity of incident pulses a strong coherent interaction in the form of sharp spikes of superradiation is observed in transmitted radiation. The Lorentz field correction and the substrate dispersion weaken the effect, providing additional spectral shifts. Specific features of photon echo in the form of multiple responses to a double or triple pulse excitation is discussed.
physics.optics:We consider the nonspherically decaying radiation field that is generated by a polarization current with a superluminally rotating distribution pattern in vacuum, a field that decays with the distance $\subP{R}$ from its source as $\subP{R}^{-1/2}$, instead of $\subP{R}^{-1}$. It is shown (i) that the nonspherical decay of this emission remains in force at all distances from its source independently of the frequency of the radiation, (ii) that the part of the source that makes the main contribution toward the value of the nonspherically decaying field has a filamentary structure whose radial and azimuthal widths become narrower (as $\subP{R}^{-2}$ and $\subP{R}^{-3}$, respectively), the farther the observer is from the source, (iii) that the loci on which the waves emanating from this filament interfere constructively delineate a radiation `subbeam' that is nondiffracting in the polar direction, (iv) that the cross-sectional area of each nondiffracting subbeam increases as $\subP{R}$, instead of $\subP{R}^2$, so that the requirements of conservation of energy are met by the nonspherically decaying radiation automatically, and (v) that the overall radiation beam within which the field decays nonspherically consists, in general, of the incoherent superposition of such coherent nondiffracting subbeams. These findings are related to the recent construction and use of superluminal sources in the laboratory and numerical models of the emission from them. We also briefly discuss the relevance of these results to the giant pulses received from pulsars.
physics.optics:The Kramers-Kronig (KK) algorithm, useful for retrieving the phase of a spectrum based on the known spectral amplitude, is applied to reconstruct the impulse response of a diffusive medium. It is demonstrated by a simulation of a 1D scattering medium with realistic parameters that its impulse response can be generated from the KK method with high accuracy.
physics.optics:The strong waveguide dispersion in photonic crystal fibres provides unique opportunities for nonlinear optics with a zero-dispersion wavelength $\lambda_0$ far below the limit of ~1.3 micron set by the material dispersion of silica. By tuning the air-hole diameter d, the pitch Lambda, and the number of rings of air holes N, the strong waveguide dispersion can in principle be used to extend lambda_0 well into the visible, albeit to some extend at the cost of multimode operation. We study in detail the interplay of the zero-dispersion wavelength, the cut-off wavelength lambda_c, and the leakage loss in the parameter space spanned by d, Lambda, and N. As a particular result we identify values of d (~500 nm) and Lambda (~700 nm) which facilitate the shortest possible zero-dispersion wavelength (~700 nm) while the fibre is still single-mode for longer wavelengths.
physics.optics:Near-field enhancement and sub-wavelength imaging properties of a system comprising a coupled pair of two-dimensional arrays of resonant nanospheres are studied. The concept of using two coupled material sheets possessing surface mode resonances for evanescent field enhancement is already well established in the microwave region. This paper shows that the same principles can be applied also in the optical region, where the performance of the resonant sheets can be realized with the use of metallic nanoparticles. In this paper we present design of such structures and study the electric field distributions in the image plane of such superlens.
physics.optics:We observed new effect which we called photonic flame effect (PFE). Several 3-dimensional photonic crystals (artificial opals) were posed on Cu plate at the temperature of liquid nitrogen (77K). Typical distance between them was 1-5 centimeters. Long-continued optical luminescence was excited in one of them by the ruby laser pulse. Analogous visible luminescence manifesting time delay appeared in other samples of the crystals. Experiments were realized for opal crystals and for nanocomposites (opals filled with nonlinear liquids).
physics.optics:We examine some of the optical properties of a metamaterial consisting of thin layers of alternating metal and dielectric. We can model this material as a homogeneous effective medium with anisotropic dielectric permittivity. When the components of this permittivity have different signs, the behavior of the system becomes very interesting: the normally evanescent parts of a P-polarized incident field are now transmitted, and there is a preferred direction of propagation.   We show that a slab of this material can form an image with sub-wavelength details, at a position which depends on the frequency of light used. The quality of the image is affected by absorption and by the finite width of the layers; we go beyond the effective medium approximation to predict how thin the layers need to be in order to obtain subwavelength resolution.
physics.optics:In conventional approaches to the homogenization of random particulate composites, the component phase particles are often treated mathematically as vanishingly small, point-like entities. The electromagnetic responses of these component phase particles are provided by depolarization dyadics which derive from the singularity of the corresponding dyadic Green functions. Through neglecting the spatial extent of the depolarization region, important information may be lost, particularly relating to coherent scattering losses. We present an extension to the strong-property-fluctuation theory in which depolarization regions of nonzero volume and ellipsoidal geometry are accommodated. Therein, both the size and spatial distribution of the component phase particles are taken into account. The analysis is developed within the most general linear setting of bianisotropic homogenized composite mediums (HCMs). Numerical studies of the constitutive parameters are presented for representative examples of HCM; both Lorentz-reciprocal and Lorentz-nonreciprocal HCMs are considered. These studies reveal that estimates of the HCM constitutive parameters in relation to volume fraction, particle eccentricity, particle orientation and correlation length are all significantly influenced by the size of the component phase particles.
physics.optics:In this letter we present a new technique for pulse shaping. The desired pulse is shaped by two apodized chirped fiber Bragg gratings which dispersions are adjusted to be cancelled. This technique exploits the well-known property of linearly-chirped gratings, in which the apodization (amplitude) grating profile maps its spectral response. This technique presents inherent advantages of chirped fiber Bragg gratings and direct design in frequency domain.
physics.optics:This paper presents theory and finite-difference time-domain (FDTD) calculations for a single and arrays of sub-wavelength cylindrical holes in metallic films presenting large transmission. These calculations are in excellent agreement with experimental measurements. This effect has to be understood in terms of the properties exhibited by the dielectric constant of metals which cannot be treated as ideal metals for the purpose of transmission and diffraction of light. We discuss the cases of well-differentiated metals silver and tungsten. It is found that the effect of surface plasmons or other surface wave excitations due to a periodical set of holes or other roughness at the surface is marginal. The effect can enhance but also can depress the transmission of the arrays as shown by theory and experiments. The peak structure observed in experiments is a consequence of the interference of the wavefronts transmitted by each hole and is determined by the surface array period independently of the material. Without large transmission through a single hole there is no large transmission through the array. We found that in the case of Ag which at the discussed frequencies is a metal there are cylindrical plasmons at the wall of the hole, as reported by Economu et al 30 years ago, that enhanced the transmission. But it turns out, as will be explained, that for the case of W which behaves as a dielectric, there is also a large transmission when compared with that of an ideal metal waveguide. To deal with this problem one has to use the measured dielectric function of the metals. We discuss thoroughly all these cases and compare with the data.
physics.optics:We present an optical experiment which permits to evaluate the information exchange necessary to self-induce cooperatively a well-organized pattern in a randomly activated molecular assembly. A low-power coherent beam carrying polarization and wavelength information is used to organize a surface relief grating on a photochromic polymer thin film which is photo-activated by a powerful incoherent beam. We demonstrate experimentally that less than 1% of the molecules possessing information cooperatively transmit it to the entire photo-activated polymer film.
physics.optics:Semiconductor laser generation begins at a critical injection when the gain and loss spectra touch each other at a singular frequency. In the framework of the standard (Schawlow-Townes-Lax-Henry) theory, the finite linewidth results from the account of fluctuations associated with the random spontaneous emission processes. This approach is based on the assumption that in the mean-field approximation the singular frequency generation persists for injection levels higher than critical. We show that this assumption in the framework of the Boltzmann kinetic equation for electrons and photons is invalid and therefore the standard description of semiconductor laser linewidth lacks theoretical foundation. Experimental support of the standard theory is also questionable.
physics.optics:An erbium doped micro-laser is demonstrated utilizing $\mathrm{SiO_{2}}$ microdisk resonators on a silicon chip. Passive microdisk resonators exhibit whispering gallery type (WGM) modes with intrinsic optical quality factors of up to $6\times{10^{7}}$ and were doped with trivalent erbium ions (peak concentration $\mathrm{\sim3.8\times{10^{20}cm^{-3})}}$ using MeV ion implantation. Coupling to the fundamental WGM of the microdisk resonator was achieved by using a tapered optical fiber. Upon pumping of the $^{4}% I_{15/2}\longrightarrow$ $^{4}I_{13/2}$ erbium transition at 1450 nm, a gradual transition from spontaneous to stimulated emission was observed in the 1550 nm band. Analysis of the pump-output power relation yielded a pump threshold of 43 $\mathrm{\mu}$W and allowed measuring the spontaneous emission coupling factor: $\beta\approx1\times10^{-3}$.
physics.optics:Using exact three-dimensional vectorial simulations of radiation coupling into uncoated dielectric fiber probes, we calculate amplitude transfer functions for conical single-mode fiber tips at the light wavelength of 633 nm. The coupling efficiency of glass fiber tips is determined in a wide range of spatial frequencies of the incident radiation for opening angles varying from 30 to 120 degrees. The resolution in near-field imaging with these tips is considered for field distributions limited in both direct and spatial-frequency space. The characteristics of the transfer functions describing the relation between probed optical fields and near-field images are analyzed in detail. The importance of utilizing a perfectly sharp tip is also examined.
physics.optics:We introduce an improved and simplified structure made of periodic arrays of pairs of H-shaped metallic wires that offer a potentially simpler approach in building negative-index materials. Using simulations and microwave experiments, we have investigated the negative-index n properties of these structures. We have measured experimentally both the transmittance and the reflectance properties and found unambiguously that a negative refractive index with Re(n)<0 and Im(n) <Re(n). The same is true for epsilon and mu. Our results show that H-shaped wire pairs can be used very effectively in producing materials with negative refractive indices.
physics.optics:We report on a matrix-based diffraction integral that evaluates the focal field of any diffraction-limited axisymmetric complex system. This diffraction formula is a generalization of the Debye integral applied to apertured focused beams, which may be accommodated to broadband problems. Longitudinal chromatic aberration may limit the convenience of the Debye formulation and, additionally, spatial boundaries of validity around the focal point are provided. Fresnel number is reformulated in order to guarantee that the focal region is entirely into the region of validity of the Debye approximation when the Fresnel number of the focusing geometry largely exceeds unity. We have applied the matrix-based Debye integral to several examples. Concretely, we present an optical system for beam focusing with strong angular dispersion and free of longitudinal chromatic aberration. This simple formalism leaves an open door for analysis and design of focused beams with arbitrary angular dispersion. Our results are valid for ultrashort pulsed and polychromatic incoherent sources.
physics.optics:Optical interference holography has been proved to be a useful technique in fabricating periodic photonic crystals in which electromagnetic waves are forbidden in certain frequency bandgaps. Compared to periodic crystals quasicrystals, having higher point group symmetry, are more favourable in achieving complete bandgaps. In this report, we propose two seven-beam optical interference configurations based on the reciprocal vector space representations for quasicrystals to fabricate icosahedral quasicrystals. Interference simulations for the quasicrystals exhibit the full symmetry of an icosahedron. The result paves the path for the fabrication of photonic quasicrystals using holographic lithography.
physics.optics:We study the excitation, decay and interactions of novel, velocity locked three-wave parametric solitons in a medium with quadratic nonlinearity and dispersion. We analytically describe the particle-like scattering between stable or unstable soliton triplets with linear waves in terms of explicit solutions featuring accelerated or decelerated solitons.
physics.optics:We have demonstrated an up to seven-fold enhancement of photoluminescence from silicon-rich silicon nitride film due to a single photonic crystal cavity. The enhancement is partially attributed to the Purcell effect. Purcell factor predicted by FDTD calculations is 32 for a linear three-defect cavity mode with computed quality factor of 332 and mode volume of 0.785 cubic wavelengths. Experimentally measured cavity quality (Q) factors vary in the range of 200 to 300, showing excellent agreement with calculations. The emission peak can be tuned to any wavelength in the 600 to 800 nm range.
physics.optics:Here we discuss the theory and analyze in detail the guidance properties of linear arrays of metamaterial/plasmonic small particles as nano-scale optical nanotransmission lines, including the effect of material loss. Under the assumption of dipolar approximation for each particle, which is shown to be accurate in the geometry of interest here, we develop closed-form analytical expressions for the eigen-modal dispersion in such arrays. With the material loss included, the conditions for minimal absorption and maximum bandwidth are derived analytically by studying the properties of such dispersion relations. Numerical examples with realistic materials including their ohmic absorption and frequency dispersion are presented. The analytical properties discussed here also provide some further physical insights into the mechanisms underlying the sub-diffraction guidance in such arrays and their fundamental physical limits. Possibility of guiding beams with sub-wavelength lateral confinement and reasonably low decay is discussed offering the possible use of this technique at microwave, infrared and optical frequencies. Interpretation of these results in terms of nanocircuit concepts is presented, and possible extension to 2-D and 3-D nanotrasnsmission line optical metamaterials is also foreseen.
physics.optics:We present an electron interferometer based on near-field diffraction from two nanostructure gratings. Lau fringes are observed with an imaging detector, and revivals in the fringe visibility occur as the separation between gratings is increased from 0 to 3 mm. This verifies that electron beams diffracted by nanostructures remain coherent after propagating farther than the Talbot length $z_T = 2d^2/\lambda$ = 1.2 mm, and hence is a proof of principle for the function of a Talbot-Lau interferometer for electrons. Distorted fringes due to a phase object demonstrates an application for this new type of electron interferometer.
physics.optics:This paper introduces a new concept of one-dimensional hologram which represents one line image, and a new kind of display structure using it. This one-dimensional hologram is similar to a superpositioned diffraction lattice. And the interference patterns can be efficiently computed with a simple optical computing structure. This is a Proposal for a new kind of display method.
physics.optics:We study theoretically the effect of ultraslow phase and group velocities in an anisotropic metamaterial. The ultraslow phase propagation is induced by the hyperbolic dispersion relation. While the inherent physics underlying the slow group velocity are collective operations of the frequency and spatial dispersion. We show that a Gaussian wave packet exhibits simultaneous slow phase and group velocities which depend on the choice of incident angles and principal axis angles. The anisotrocpic metamaterial slab can be constructed and the ultraslow phase and group velocities can be measured experimentally.
physics.optics:We report the direct observation of periodic intensity modulation of a laser pulse propagating in a hollow core waveguide. A series of equally spaced plasma sparks along the gas-filled capillary is produced. This effect can be explained by the beating of different fiber modes, which are excited by controlling the size of the focal spot at the capillary entrance. As compared to an artificial modulated waveguide structure, our presented approach represents an easier and more flexible quasi-phase-matching scheme for nonlinear-optical frequency conversion.
physics.optics:We report a numerical investigation of surface plasmon (SP) propagation in ordered and disordered linear chains of metal nanospheres. In our simulations, SPs are excited at one end of a chain by a near-field tip. We then find numerically the SP amplitude as a function of propagation distance. Two types of SPs are discovered. The first SP, which we call the ordinary or quasistatic, is mediated by short-range, near-field electromagnetic interaction in the chain. This excitation is strongly affected by Ohmic losses in the metal and by disorder in the chain. These two effects result in spatial decay of the quasistatic SP by means of absorptive and radiative losses, respectively. The second SP is mediated by longer range, far-field interaction of nanospheres. We refer to this SP as the extraordinary or non-quasistatic. The non-quasistatic SP can not be effectively excited by a near-field probe due to the small integral weight of the associated spectral line. Because of that, at small propagation distances, this SP is dominated by the quasistatic SP. However, the non-quasistatic SP is affected by Ohmic and radiative losses to a much smaller extent than the quasistatic one. Because of that, the non-quasistatic SP becomes dominant sufficiently far from the exciting tip and can propagate with little further losses of energy to remarkable distances. The unique physical properties of the non-quasistatic SP can be utilized in all-optical integrated photonic systems.
physics.optics:It is shown that, even when the eigenmodes of an optical cavity are wave-chaotic, the frequency splitting due to the rotation of the cavity occurs and the frequency difference is proportional to the angular velocity although the splitting eigenmodes are still wave-chaotic and do not correspond to any unidirectionally-rotating waves.
physics.optics:We theoretically study channel plasmon-polaritons (CPPs) with a geometry similar to that in recent experiments at telecom wavelengths (Bozhevolnyi et al., Nature 440, 508 (2006)). The CPP modal shape, dispersion relation, and losses are simulated using the multiple multipole method and the finite difference time domain technique. It is shown that, with the increase of the wavelength, the fundamental CPP mode shifts progressively towards the groove opening, ceasing to be guided at the groove bottom and becoming hybridized with wedge plasmon-polaritons running along the groove edges.
physics.optics:We investigated the influence of the photonic mode density (PMD) on the triplet dynamics of individual chromophores on a dielectric interface by comparing their response in the presence and absence of a nearby gold film. Lifetimes of the excited singlet state were evaluated in ordet to measure directly the PMD at the molecules position. Triplet state lifetimes were simultaneously determined by statistical analysis of the detection time of the fluorescence photons. The observed singlet decay rates are in agreement with the predicted PMD for molecules with different orientations. The triplet decay rate is modified in a fashion correlated to the singlet decay rate. These results show that PMD engineering can lead to an important suppression of the fluorescence, introducing a novel aspect of the physical mechanism to enhance fluorescence intensity in PMD-enhancing systems such as plasmonic devices.
physics.optics:We present a theoretical analysis of light scattering from a layered metal-dielectric microsphere. The system consists of two spherical resonators, coupled through concentric embedding. Solving for the modes of this system we find that near an avoided crossing the scattering cross section is dramatically suppressed, exhibiting a tunable optical transparency. Similar to electromagnetically induced transparency, this phenomenon is associated with a large group delay, which in our system is manifest as flat azimuthal dispersion.
physics.optics:In analogy to a perturbed harmonic oscillator, we calculate the fundamental and some other higher order soliton solutions of the nonlocal nonlinear Schroedinger equation (NNLSE) in the second approximation in the generally nonlocal case. Comparing with numerical simulations we show that soliton solutions in the 2nd approximation can describe the generally nonlocal soliton states of the NNLSE more exactly than that in the zeroth approximation. We show that for the nonlocal case of an exponential-decay type nonlocal response the Gaussian-function-like soliton solutions can't describe the nonlocal soliton states exactly even in the strongly nonlocal case. The properties of such nonlocal solitons are investigated. In the strongly nonlocal limit, the soliton's power and phase constant are both in inverse proportion to the 4th power of its beam width for the nonlocal case of a Gaussian function type nonlocal response, and are both in inverse proportion to the 3th power of its beam width for the nonlocal case of an exponential-decay type nonlocal response.
physics.optics:In order to measure the radial displacements of facets on surface of a growing spherical Cu_{2-\delta}Se crystal with sub-nanometer resolution, we have investigated the reliability and accuracy of standard method of Fourier analysis of fringes obtained applying digital laser interferometry method. Guided by the realistic experimental parameters (density and orientation of fringes), starting from 2-dimensional model interferograms and using unconventional custom designed Gaussian filtering window and unwrapping procedure of the retrieved phase, we have demonstrated that for considerable portion of parameter space the non-negligible inherent phase retrieval error is present solely due to non-integer number of fringes within the digitally recorded image (using CCD camera). Our results indicate the range of experimentally adjustable parameters for which the generated error is acceptably small. We also introduce a modification of the (last part) of the usual phase retrieval algorithm which significantly reduces the error in the case of small fringe density.
physics.optics:We theoretically study the problem of detecting dipole radiation in an optical system of high numerical aperture in which the detector is sensitive to \textit{field amplitude}. In particular, we model the phase sensitive detector as a single-mode cylindrical optical fiber. We find that the maximum in collection efficiency of the dipole radiation does not coincide with the optimum resolution for the light gathering instrument. The calculated results are important for analyzing fiber-based confocal microscope performance in fluorescence and spectroscopic studies of single molecules and/or quantum dots.
physics.optics:We observe the weak coupling of lead sulphide nanocrystals to localized defect modes of 2-dimensional silicon nanocavities. Cavity resonances characterized with ensemble nanocrystals are verified with cold-cavity measurements using integrated waveguides. Polarization dependence of the cavity field modes is observed. The linewidths measured in coupling experiments are broadened in comparison to the cold-cavity characterization, partly due to large homogeneous linewidths of the nanocrystals. The calculated Purcell factor for a single exciton is 75, showing promise toward applications in single photon systems. These novel light sources operate near 1.55 micron wavelengths at room temperature, permitting integration with current fiber communications networks.
physics.optics:We present a quantitative shadowgraphic method which can measure the density of a laser-generated plasma in air with sensitivity and resolution comparable or better than traditional interferometric techniques. Simultaneous comparison of both shadowgraphy and interferometry has been carried out allowing the experimental evaluation of the reliability of the shadowgraphic method.
physics.optics:The present note establishes the self-averaging, radiative transfer limit for the two-frequency Wigner distribution for classical waves in random media. Depending on the ratio of the wavelength to the correlation length the limiting equation is either a Boltzmann-like integral equation or a Fokker-Planck-like differential equation in the phase space. The limiting equation is used to estimate three physical parameters: the spatial spread, the coherence length and the coherence bandwidth. In the longitudinal case, the Fokker-Planck-like equation can be solved exactly.
physics.optics:In this letter we show that equivalent circuits offer a qualitative and even quantitative simple explanation for the behavior of various types of left-handed (or negative index) meta-materials. This allows us to optimize design features and parameters, while avoiding trial and error simulations or fabrications. In particular we apply this unifying circuit approach in accounting for the features and in optimizing the structure employing parallel metallic bars on the two sides of a dielectric film.
physics.optics:In this paper we have put forth an innovative method of obtaining a stereographic image on a single frame using a single lens. This method has been verified experimentally. A preliminary prototype of the same is built with an optimized use of the material available in the laboratory. The prospective applications of this technique are also explored in brief. This method once commercialized, will reduce the expenses incurred in the stereo videography. We also propose a simplified method of obtaining anaglyph.
physics.optics:A key optical parameter characterizing the existence of negative refraction in a thin layer of a composite material is the effective refractive index of an equivalent, homogenized layer with the same physical thickness as the initial inhomogeneous composite. Measuring the complex transmission and reflection coefficients is one of the most rigorous ways to obtain this parameter. We dispute Grigorenko's statement (Optics Letters 31, 2483 (2006)) that measuring only the reflection intensity spectrum is sufficient for determining the effective refractive index. We discuss fundamental drawbacks of Grigorenko's technique of using a best-fit approximation with an a priori prescribed dispersive behavior for a given metamaterial and an 'effective optical thickness' that is smaller than the actual thickness of the sample. Our simulations do not confirm the Grigorenko paper conclusions regarding the negative refractive index and the negative permeability of the nanopillar sample in the visible spectral range.
physics.optics:Non-doped white organic light-emitting diodes using an ultrathin yellow-emitting layer of rubrene (5,6,11,12-tetraphenylnaphtacene) inserted on either side of the interface between a hole-transporting NPB (4,4'-bis[N-(1-naphtyl)-N-phenylamino]biphenyl) layer and a blue-emitting DPVBi (4,4'-bis(2,2'-diphenylvinyl)-1,1'-biphenyl) layer are described. Both the thickness and the position of the rubrene layer allow fine chromaticity tuning from deep-blue to pure-yellow via bright-white with CIE coordinates (x= 0.33, y= 0.32), a external quantum efficiency of 1.9%, and a color rendering index of 70. Such a structure also provides an accurate sensing tool to measure the exciton diffusion length in both DPVBi and NPB (8.7 and 4.9 nm respectively).
physics.optics:Diffractive screens are high-resolution elements with capability for generating holographic-like images from a sequence of planes where TV frames are seen oblique to it. If we project a sequence of contour lines of an object it could be seen in continuous horizontal parallax to a size of 1m3.
physics.optics:We calculate bisoliton solutions using a slowly varying stroboscopic equation. The system is characterized in terms of a single dimensionless parameter. We find two branches of solutions and describe the structure of the tails for the lower branch solutions.
physics.optics:the realization of high repetition rate passively Q-switched monolithic microlaser is a challenge since a decade. To achieve this goal, we report here on the first passively Q-switched diode-pumped microchip laser based on the association of a Nd:GdVO4 crystal and a Cr4+:YAG saturable absorber. The monolithic design consists of 1 mm long 1% doped Nd:GdVO4 optically contacted on a 0.4 mm long Cr4+:YAG leading to a plano-plano cavity. A repetition rate as high as 85 kHz is achieved. The average output power is approximately 400 mW for 2.2 W of absorbed pump power and the pulse length is 1.1 ns.
physics.optics:High performance of InP-based quantum cascade lasers emitting at $\lambda$ ~ 9$\mu$m are reported. Thick electroplated gold layer was deposited on top of the laser to improve heat dissipation. With one facet high reflection coated, the devices produce a maximum output power of 175mW at 40% duty cycle at room temperature and continuous-wave operation up to 278K.
physics.optics:The radiation forces on a Rayleigh dielectric sphere induced by a partially coherent light beam are greatly affected by the coherence of the light beam. The magnitude of the radiation forces on a dielectric sphere near the focus point greatly decreases as the coherence decreases. For the light beam with good coherence, the radiation force may be used to trap a particle; and for the light beam with intermediate coherence, the radiation force may be used to guide and accelerate a particle.
physics.optics:In this letter we show how the dispersion relation of surface plasmon polaritons (SPPs) propagating along a perfectly conducting wire can be tailored by corrugating its surface with a periodic array of radial grooves. In this way, highly localized SPPs can be sustained in the terahertz region of the electromagnetic spectrum. Importantly, the propagation characteristics of these spoof SPPs can be controlled by the surface geometry, opening the way to important applications such as energy concentration on cylindrical wires and superfocusing using conical structures.
physics.optics:Refraction of obliquely incident plane waves due to the interface of a vacuous half-space and a half-space occupied by a simply moving, nondissipative, isotropic dielectric-magnetic medium is considered, when the medium's velocity lies parallel to the interface and in the plane of incidence. Counterposition of the refracted wavevector and time-averaged Poynting vector occurs when the medium's velocity is sufficiently large in magnitude and makes an obtuse angle with the incident wavevector. The counterposition induced by relative motion occurs whether the refraction is negative or positive when the medium is at rest.
physics.optics:The perceived lateral position of a transmitted beam, upon propagating through a slab made of homogeneous, isotropic, dielectric material at an oblique angle, can be controlled through varying the velocity of the slab. In particular, by judiciously selecting the slab velocity, the transmitted beam can emerge from the slab with no lateral shift in position. Thereby, a degree of concealment can be achieved. This concealment is explored in numerical calculations based on a 2D Gaussian beam.
physics.optics:Theory predicts that with an ultrashort and extremely bright coherent X-ray pulse, a single diffraction pattern may be recorded from a large macromolecule, a virus, or a cell before the sample explodes and turns into a plasma. Here we report the first experimental demonstration of this principle using the FLASH soft X-ray free-electron laser. An intense 25 fs, 4 10^13 W/cm^2 pulse, containing 10^12 photons at 32 nm wavelength, produced a coherent diffraction pattern from a nano-structured non-periodic object, before destroying it at 60,000 K. A novel X-ray camera assured single photon detection sensitivity by filtering out parasitic scattering and plasma radiation. The reconstructed image, obtained directly from the coherent pattern by phase retrieval through oversampling, shows no measurable damage, and extends to diffraction-limited resolution. A three-dimensional data set may be assembled from such images when copies of a reproducible sample are exposed to the beam one by one.
physics.optics:We report direct and absolute temperature measurements in a diode-end-pumped Yb:YAG crystal, using a calibrated infrared camera, with a 60-$\mu$m spatial resolution. The heat transfer coefficient has been measured, for the first time to our knowledge, with four different types of thermal contact (H = 0.25, 0.28, 0.9 and 2.0 for bare contact, graphite layer, indium foil and heat sink grease respectively). The dynamics of thermal effects is also presented.
physics.optics:We demonstrate the operation of an ultra low repetition rate, high peak power, picosecond diode pumped Nd:YVO4 passively mode locked laser oscillator. Repetition rates even below 1 MHz were achieved with the use of a new design multiple-pass cavity and a semiconductor saturable absorber. Long term stable operation at 1.2 MHz, pulse duration of 16.3 ps and average output power of 470 mW corresponding to 24 KW peak power pulses is reported. This is, to our knowledge, the lowest repetition rate high peak power pulses ever generated directly from a picosecond laser resonator without cavity dumping.
physics.optics:Analytical solution for optical trapping force on a spherical dielectric particle for an arbitrary positioned focused beam is presented in a generalized Lorenz-Mie and vectorial diffraction theory. In this case the exact electromagnetic field is considered in the focal region. A double tweezers setup was employed to perform ultra sensitive force spectroscopy and observe the forces, demonstrating the selectively couple of the transverse electric (TE), transverse magnetic (TM) modes by means of the beam polarization and positioning, and to observe correspondent morphology-dependent resonances (MDR) as a change in the optical force. The theoretical prediction of the theory agrees well with the experimental results. The algorithm presented here can be easily extended to other beam geometries and scattering particles.
physics.optics:We report on new simple and efficient multipass amplifiers using prisms or corner cubes to perform several passes in different planes of incidence. This scheme provides an optimized overlap between the signal passes and the pumped volume. We investigated our amplification geometry with Nd:YAG and Nd: YVO4 crystals : the use of a low doped (0.3%) Nd:YVO4 crystal allowed better thermal behaviour and higher performances. We amplified a pulsed microlaser (110 mW of average power at 1064 nm) and obtained a diffraction-limited output beam with an average power of 5.7 W for 15 W of pump power and a small-signal gain of 56 dB in a 6-pass configuration.
physics.optics:This letter proposes a quasi-planar chiral resonator suitable for the design of negative refractive index matamaterial. It is presented an analytical model for the determination of its polarizabilities, and the viability of negative refraction in chiral and racemic arrangements with the proposed inclusions is analyzed. The present analysis is expected to pave the way to the design of negative refractive index matamaterials made of a single kind of inclusions feasible from standard photo-etching techniques.
physics.optics:We report the first demonstration to our knowledge of a continuously tunable picosecond laser operating around 1 MHz. The emission can be tuned from 640 to 685 nm and the repetition rate from 200 kHz to 1 MHz with a pulse duration of less than 200 ps. The system is based on a Nd:YVO4 passively Q-switched microchip laser providing a few tens of nJ per pulse. Two cascaded stages of amplification are then used to increase the pulse energy to several microJ . The frequency doubled radiation is then used to pump a periodically-poled-niobate-lithium (PPLN)-based optical parametric generator in an all-solid-state architecture. 20 nJ of tunable signal radiation are obtained. We also demonstrated 300-ps pulses generation in the UV (355 nm) at 1 MHz.
physics.optics:The boundary-value problem of the reflection and transmission of a plane wave due to a slab of an electro-optic structurally chiral material (SCM) is formulated in terms of a 4x4 matrix ordinary differential equation. The SCM slab can be locally endowed with one of 20 classes of point group symmetry, and is subjected to a dc voltage across its thickness. The enhancement (and, in some cases, the production) of the circular Bragg phenomenon (CBP) by the application of the dc voltage has either switching or circular-polarization-rejection applications in optics. The twin possibilities of thinner filters and electrical manipulation of the CBP, depending on the local crystallographic class as well as the constitutive parameters of the SCM, emerge.
physics.optics:We used a variety of nonlinear optical (NLO) spectroscopies to study the singlet excited states order, and primary photoexcitations in polyfluorene; an important blue emitting p-conjugated polymer. The polarized NLO spectroscopies include ultrafast pump-probe photomodulation, two-photon absorption, and electroabsorption. For completeness we also measured the linear absorption and photoluminescence spectra. We found that the primary photoexcitations in polyfluorene are singlet excitons.
physics.optics:We report on tunable terahertz resonant detection of two 1.55 &micro;m cw-lasers beating by plasma waves in AlGaAs/InGaAs/InP high-electron-mobility transistor. We show that the fundamental plasma resonant frequency and its odd harmonics can be tuned with the applied gate-voltage in the range 75-490 GHz. The observed frequency dependence on gate-bias is found to be in good agreement with the theoretical plasma waves dispersion law.
physics.optics:The refractive index of single microparticles is derived from precise measurement and rigorous modeling of the stiffness of a laser trap. We demonstrate the method for particles of four different materials with diameters from 1.6 to 5.2 microns and achieve an accuracy of better than 1%. The method greatly contributes as a new characterization technique because it works best under conditions (small particle size, polydispersion) where other methods, such as absorption spectroscopy, start to fail. Particles need not be transferred to a particular fluid, which prevents particle degradation or alteration common in index matching techniques. Our results also show that advanced modeling of laser traps accurately reproduces experimental reality.
physics.optics:Coupling of photonic crystal (PC) linear three-hole defect cavities (L3) to PC waveguides is theoretically and experimentally investigated. The systems are designed to increase the overlap between the evanescent cavity field and the waveguide mode, and to operate in the linear dispersion region of the waveguide. Our simulations indicate increased coupling when the cavity is tilted by 60 degrees with respect to the waveguide axis, which we have also confirmed by experiments. We obtained up to 90% coupling efficiency into the waveguide.
physics.optics:Experimental demonstration of a negative permeability due to near-field coupling of periodic thin silver strips is presented. Two samples with different strip thicknesses are fabricated; optical measurements of the samples confirm our initial design projections by showing the real part of permeability to be about -1 for the sample with thinner strips and -0.8 for the sample with thicker strips at wavelengths of 770 nm and 720 nm.
physics.optics:Novel nonlinear optical phenomena - photonic flame effect (PFE) and stimulated globular scattering (SGS) are discussed.PFE consisted in the appearance of the few seconds duration emission in blue-green spectral range under 20 ns ruby laser pulse excitation and simultaneous excitation of several spatially separated synthetic opal crystalls situated on the Cu plate. SGS was observed both in forward and backward directions. Spectral and energetical SGS characteristics were measured.
physics.optics:Two-frequency radiative transfer (2f-RT) theory is developed for classical waves in random media. Depending on the ratio of the wavelength to the scale of medium fluctuation 2f-RT equation is either a Boltzmann-like integral equation with a complex-valued kernel or a Fokker-Planck-like differential equation with complex-valued coefficients in the phase space. The 2f-RT equation is used to estimate three physical parameters: the spatial spread, the coherence length and the coherence bandwidth (Thouless frequency). A closed form solution is given for the boundary layer behavior of geometrical radiative transfer and shows highly nontrivial dependence of mutual coherence on the spatial displacement and frequency difference. It is shown that the paraxial form of 2f-RT arises naturally in anisotropic media which fluctuate slowly in the longitudinal direction.
physics.optics:We present an elementary proof concerning reciprocal transmittances and reflectances. The proof is direct, simple, and valid for the diverse objects that can be absorptive and induce diffraction and scattering, as long as the objects respond linearly and locally to electromagnetic waves. The proof enables students who understand the basics of classical electromagnetics to grasp the physical basis of reciprocal optical responses. In addition, we show an example to demonstrate reciprocal response numerically and experimentally.
physics.optics:We analyze phase matching with reference to frequency doubling in nanosized quadratic waveguides encompassing form birefringence and supporting cross-polarized fundamental and second-harmonic modes. In an AlGaAs rod with an air void, we show that phase-matched second-harmonic generation could be achieved in a wide spectral range employing state-of-the-art nanotechnology.
physics.optics:We analyze electromagnetic modes in multi-layered nano-composites and demonstrate that the response of a majority of realistic layered structures is strongly affected by the non-local effects originating from strong field oscillations across the system, and is not described by conventional effective-medium theories. We develop the analytical description of the relevant phenomena and confirm our results with numerical solutions of Maxwell equations. Finally, we use the developed formalism to demonstrate that multi-layered plasmonic nanostructures support high-index volume modes, confined to deep subwavelength areas, opening a wide class of applications in nanoscale light management.
physics.optics:The phase velocity of plane waves propagating in an isotropic chiral medium can be simultaneously positive for left-circularly polarized light and negative for right-circularly polarized light(or vice versa). The constitutive parameter regimes supporting this phenomenon are established for nondissipative and dissipative mediums. The boundary between positive and negative phase velocity is characterized by infinite phase velocity.
physics.optics:The quality factors of modes in nearly identical GaAs and Al_{0.18}Ga_{0.82}As microdisks are tracked over three wavelength ranges centered at 980 nm, 1460 nm, and 1600 nm, with quality factors measured as high as 6.62x10^5 in the 1600-nm band. After accounting for surface scattering, the remaining loss is due to sub-bandgap absorption in the bulk and on the surfaces. We observe the absorption is, on average, 80 percent greater in AlGaAs than in GaAs and in both materials is 540 percent higher at 980 nm than at 1600nm.
physics.optics:A novel focusing structure with fractal properties is presented. It is a photon sieve in which the pinholes are appropriately distributed over the zones of a fractal zone plate. The focusing properties of the fractal photon sieve are analyzed. The good performance of our proposal is demonstrated experimentally with a series of images obtained under white light illumination. It is shown that compared with a conventional photon sieve, the fractal photon sieve exhibits an extended depth of field and a reduced chromatic aberration.
physics.optics:The Zeeman splitting and the underlying value of the g-factor for conduction band electrons in GaAs/Al_xGa_{1-x}As quantum wells have been measured by spin-beat spectroscopy based on a time-resolved Kerr rotation technique. The experimental data are in good agreement with theoretical predictions. The model accurately accounts for the large electron energies above the GaAs conduction band bottom, resulting from the strong quantum confinement. In the tracked range of optical transition energies E from 1.52 to 2.0eV, the electron g-factor along the growth axis follows closely the universal dependence g_||(E)= -0.445 + 3.38(E-1.519)-2.21(E-1.519)^2 (with E measured in eV); and this universality also embraces Al_xGa_{1-x}As alloys. The in-plane g-factor component deviates notably from the universal curve, with the degree of deviation controlled by the structural anisotropy.
physics.optics:We give a detailed description of a novel method for time-resolved experiments on single non-luminescent nanoparticles. The method is based on the combination of pump-probe spectroscopy and a common-path interferometer. In our interferometer, probe and reference arms are separated in time and polarization by a birefringent crystal. The interferometer, fully described by an analytical model, allows us to separately detect the real and imaginary contributions to the signal. We demonstrate the possibilities of the setup by time-resolved detection of single gold nanoparticles as small as 10 nm in diameter, and of acoustic oscillations of particles larger than 40 nm in diameter.
physics.optics:In this communication we introduce a new design of the magnifying superlens and demonstrate it in the experiment.
physics.optics:In this comment, I point out to several mathematical mistakes in the above-referenced letter.
physics.optics:The possibility of soliton self-compression of ultrashort laser pulses down to the few-cycle regime in photonic crystal fibers is numerically investigated. We show that efficient sub-two-cycle temporal compression of nanojoule-level 800 nm pulses can be achieved by employing short (typically 5-mm-long) commercially available photonic crystal fibers and pulse durations of around 100 fs, regardless of initial linear chirp, and without the need of additional dispersion compensation techniques. We envisage applications in a new generation of compact and efficient sub-two cycle laser pulse sources.
physics.optics:The theory of summation of electromagnetic line transitions is used to tabulate the Taylor expansion of the refractive index of humid air over the basic independent parameters (temperature, pressure, humidity, wavelength) in five separate infrared regions from the H to the Q band at a fixed percentage of Carbon Dioxide. These are least-squares fits to raw, highly resolved spectra for a set of temperatures from 10 to 25 C, a set of pressures from 500 to 1023 hPa, and a set of relative humidities from 5 to 60%. These choices reflect the prospective application to characterize ambient air at mountain altitudes of astronomical telescopes.
physics.optics:Numerical examination of the solution of the boundary--value problem of the reflection and transmission of a plane wave due to a slab of an electro--optic structurally chiral material (SCM) indicates that the exhibition of the circular Bragg phenomenon by the SCM can be controlled not only by the sign and the magnitude of a dc electric field but also by its orientation in relation to axis of helicoidal nonhomogeneity of the SCM. Thereby, the possibility of electrical control of circular--polarization filters has been extended.
physics.optics:We have developed a self consistent technique to predict the behavior of plasmon resonances in multi-component systems as a function of wavelength. This approach, based on the tight lower bounds of the Bergman-Milton formulation, is able to predict experimental optical data, including the positions, shifts and shapes of plasmonic peaks in ternary nanocomposites without using any ftting parameters. Our approach is based on viewing the mixing of 3 components as the mixing of 2 binary mixtures, each in the same host. We obtained excellent predictions of the experimental optical behavior for mixtures of Ag:Cu:SiO2 and alloys of Au-Cu:SiO2 and Ag-Au:H2 O, suggesting that the essential physics of plasmonic behavior is captured by this approach.
physics.optics:Through examining the product of the mathematical variance of intensity with respect to time and frequency, we arrive at a temporal characterization of laser pulses through parameters for pulse duration, group delay dispersion and temporal form. These statistics, which are sufficient to predict subsequent pulse behavior, are recoverable in a simple experiment, measuring the two-photon-induced photocurrents in three nonlinear diodes. With only two photodiodes, we demonstrate that pulse durations as low as several tens of femtoseconds can be easily measured in a single shot if the usual assumptions of pulse form and dispersion are made as in the more difficult autocorrelation setup.
physics.optics:The transition from spatial to spatiotemporal dynamics in Kerr-driven beam collapse is modelled as the instability of the Townes profile. Coupled axial and conical radiation, temporal splitting and X waves appear as the effect of Y-shaped unstable modes, whose growth is experimentally detected.
physics.optics:A stochastical description is applied in order to understand how ferroelectric structures can be formed. The predictions are compared with experimental data of the so-called electrical fixing: Domains are patterned in photorefractive lithium niobate crystals by the combination of light-induced space-charge fields with externally applied electrical fields. In terms of our stochastical model the probability for domain nucleation is modulated according to the sum of external and internal fields. The model describes the shape of the domain pattern as well as the effective degree of modulation.
physics.optics:We present a solution to the problem of partial reflection and refraction of a polarized paraxial Gaussian beam at the interface between two transparent media. The Fedorov--Imbert transverse shift of the centers of gravity of the reflected and refracted beams are calculated. Our results differ in general case from ones derived previously by other authors. In particular, they obey general conservation law for the beams' total angular momentum but do not obey one-particle conservation laws for individual photons, which have been proposed by Onoda et al. [Phys. Rev. Lett. 93, 083901 (2004)]. We ascertain that these circumstances relate to the accepted in the literature artificial model for the polarized beam which does not fit to real beams. The present paper resolves the recent controversy and confirms results of our previous paper [Bliokh et al., Phys. Rev. Lett. 96, 073903 (2006)]. Besides, a diffraction effect of angular transverse shift of the reflected and refracted beams is predicted.
physics.optics:Luminescence is the phenomenon investigated and applied in many disciplines of science and technique. Spectral and kinetic measurements of luminescence provide much information concerning the mechanism of luminescent devices. Better understanding of the mechanism allows to improve the devices. It has been known for a long time that the spectral and kinetic properties of luminescence depend on the temperature. It is assumed that fluorescence does not depend on temperature but practically lifetime of fluorescence changes insignificantly with temperature. Until now, to our knowledge, nobody describes mathematically the dependence. Here is reported the model of photon emission permitting a description of the dependence. It is found that lifetime of fluorescence decreases with increasing the square root of temperature. The dependence is weaker than temperature dependence of phosphorescence. It agrees with experimental observations. In the model presented, the new application of Maxwell distribution is shown. This idea can be used in other investigations. Also the presented model can be applied, for example, in nuclear physics.
physics.optics:A unified theory is advanced to describe both the lateral Goos-H\"{a}nchen (GH) effect and the transverse Imbert-Fedorov (IF) effect, through representing the vector angular spectrum of a 3-dimensional light beam in terms of a 2-form angular spectrum consisting of its 2 orthogonal polarized components. From this theory, the quantization characteristics of the GH and IF displacements are obtained, and the Artmann formula for the GH displacement is derived. It is found that the eigenstates of the GH displacement are the 2 orthogonal linear polarizations in this 2-form representation, and the eigenstates of the IF displacement are the 2 orthogonal circular polarizations. The theoretical predictions are found to be in agreement with recent experimental results.
physics.optics:We demonstrate a conceptually new mechanism for sub-wavelength focusing at optical frequencies based upon the use of nano-hole quasi-periodic arrays in metal screens. Using coherent illumination at 660 nm and scanning near-field optical microscopy, ~290 nm "hot spots", were observed at a distance of ~12.5 mkm from the array. Even smaller hot-spots of about 200 nm in waist were observed closer to the plane of the array.
physics.optics:This paper presents the principles and experimental results of an optical fiber QKD system operating at 1550 nm, and using the BB84 protocol with QPSK signals. Our experimental setup consists of a time-multiplexed super-homodyne configuration using P.I.N detectors in a differential scheme as an alternative to avalanche photon counting. Transmission over 11km of optical fiber has been done using this detection scheme and major relevant characteristics such as noise, quantum efficiency and bit error rate (BER) are reported.
physics.optics:Using a microscopic theory based on the Maxwell-semiconductor Bloch equations, we investigate the possibility of an optically-assisted electrically-driven THz quantum cascade laser. Whereas in optical conversion schemes the power conversion efficiency is limited by the Manley-Rowe relation, the proposed optically-assisted scheme can achieve higher efficiency by coherently recovering the optical pump energy. Furthermore, due to quantum coherence effects the detrimental effects of scattering are mitigated.
physics.optics:We present a photon counting experiment designed for an undergraduate physics laboratory. The statistics of the number of photons of a pseudo-thermal light source is studied in two limiting cases: much longer and much shorter than the coherence time, giving Poisson and Bose-Einstein distributions, respectively. The experiment can be done in a reasonable time using a digital oscilloscope without the need of counting boards. The use of the oscilloscope has the advantage of allowing the storage of the data for further processing. The stochastic nature of the detection phenomena adds additional value because students are forced to do data processing and analysis.
physics.optics:The Bruggeman formalism provides an estimate of the effective permittivity of a composite material comprising two constituent materials, with each constituent material being composed of electrically small particles. When one of the constituent materials is silver and the other is an insulating material, the Bruggeman estimate of the effective permittivity of the composite exhibits resonances with respect to volume fraction that are not physically plausible.
physics.optics:We theoretically and experimentally investigate the mutual collapse dynamics of two spatially separated optical beams in a Kerr medium. Depending on the initial power, beam separation, and the relative phase, we observe repulsion or attraction, which in the latter case reveals a sharp transition to a single collapsing beam. This transition to fusion of the beams is accompanied by an increase in the collapse distance, indicating the effect of the nonlinear coupling on the individual collapse dynamics. Our results shed light on the basic nonlinear interaction between self-focused beams and provide a mechanism to control the collapse dynamics of such beams.
physics.optics:We report on design and fabrication of nano-composite metal-dielectric thin film coatings with high reflectance asymmetries. Applying basic dispersion engineering principles to model a broadband and large reflectance asymmetry, we obtain a model dielectric function for the metamaterial film, closely resembling the effective permittivity of disordered metal-dielectric nano-composites. Coatings realized using disordered nanocrystalline silver films deposited on glass substrates confirm the theoretical predictions, exhibiting symmetric transmittance, large reflectance asymmetries and a unique flat reflectance asymmetry.
physics.optics:By using the notion of wavelength- and angle-averaged reflectance, we assess in a systematic way the performance of finite omnidirectional reflectors. We put forward how this concept can be employed to optimize omnidirectional capabilities. We also apply it to give an alternate meaningful characterization of the bandwidth of these systems.
physics.optics:A system for studying microcavity resonators at cryogenic temperatures (~10 K) through evanescent coupling via optical fiber taper waveguides is reported, and efficient fiber coupling to AlGaAs microdisk cavities with embedded quantum dots is demonstrated. As an immediate application of this tool, we study high-resolution tuning of microdisk cavities through nitrogen gas adsorption, as first discussed by Mosor, et al. By proper regulation of the nitrogen gas flow and delivery of the gas to the sample surface, continuous tuning can be achieved with modest gas flows, and overall wavelength shifts as large as 4 nm are achieved.
physics.optics:The polarization-sensitive propagation in the anisotropic metamaterial (AMM) with double-sheeted hyperboloid dispersion relation is investigated from a purely wave propagation point of view. We show that TE and TM polarized waves present significantly different characteristics which depend on the polarization. The omnidirectional total reflection and oblique total transmission can occur in the interface associated with the AMM. If appropriate conditions are satisfied, one polarized wave exhibits the total refraction, while the other presents the total reflection. We find that the opposite amphoteric refractions can be realized by rotating the principle axis of AMM, such that one polarized wave performs the negative refraction, while the other undergoes positive refraction. The polarization-sensitive characteristics allow us to construct two types of efficient polarizing beam splitters under certain achievable conditions.
physics.optics:Electromagnetic localization and existence of gap solitons in nonlinear metamaterials, which exhibit a stop band in their linear spectral response, is theoretically investigated. For a self-focusing Kerr nonlinearity, the equation for the electric field envelope with carrier frequency in the stop band - where the magnetic permeability $\mu(\omega)$ is positive and the dielectric permittivity $\epsilon(\omega)$ is negative - is described by a nonlinear Klein-Gordon equation with a dispersive nonlinear term. A family of standing and moving localized waves for both electric and magnetic fields is found, and the role played by the nonlinear dispersive term on solitary wave stability is discussed.
physics.optics:We present a theoretical analysis of a super-resolving lens based on 1-dimensional metallo-dielectric photonic crystals composed of Ag/GaP multilayers. The lens contains a total of 10 optical skin depths of Ag, yet maintains a normal incidence transmittance of ~50% for propagating waves over the super-resolving wavelength range of 500-650 nm. The individual Ag layers are 22 nm thick and can be readily fabricated in conventional deposition systems. The importance of anti-reflection coatings for the transmission of evanescent and propagating waves is illustrated by comparison to periodic and symmetric structures without the coatings. In addition, the reflection for propagating waves is reduced to ~5% across the super-resolving wavelength band diminishing the interference between the object and the lens.
physics.optics:We propose a novel chirped structure consisting of a low index polymer core bounded by modulated multilayer claddings, to realize an optical field concentrator with virtually zero propagation losses in a wide spectral range, independent of wave polarization. In spite of the absence of the total internal reflection mechanism, properly designed multilayer claddings ensure the achievement of unitary transmittance in a wide spectral range, including the widely used wavelengths for optical communications. Several cladding geometries obtained by varying the thicknesses of the cladding layers are reported and discussed.
physics.optics:While adaptive optical systems are able to remove moderate wavefront distortions in scintillated optical beams, phase singularities that appear in strongly scintillated beams can severely degrade the performance of such an adaptive optical system. Therefore, the detection of these phase singularities is an important aspect of strong scintillation adaptive optics. We investigate the detection of phase singularities with the aid of a Shack-Hartmann wavefront sensor and show that, in spite of some systematical deficiencies inherent to the Shack-Hartmann wavefront sensor, it can be used for the reliable detection of phase singularities, irrespective of their morphologies. We provide full analytical results, together with numerical simulations of the detection process.
physics.optics:A comment on a recent Letter on the information capacity of nonlinear channels.
physics.optics:We present results on the realization of an all-taper coupled, multicolour microspherical glass light source fabricated from the erbium doped fluoride glass ZBLALiP. Whispering gallery mode lasing and upconversion processes give rise to laser and fluorescent emissions at multiple wavelengths from the ultraviolet to the infrared. A single-mode tapered fibre coupling scheme, which serves to both launch pump light at 980 nm into the microresonator cavity and collect the resulting infrared lasing, is employed. We demonstrate the use of a second separate fibre taper to outcouple the upconversion spectrum over several hundred nanometers. Thirteen discrete emissions ranging from 320 to 850 nm have been observed in the upconversion spectrum. The absorption and fluorescence properties are studied and the processes responsible for the generation of the observed wavelengths are outlined.
physics.optics:Current spectroscopic optical coherence tomography (OCT) methods rely on a posteriori numerical calculation. We present an alternative for accessing optically the spectroscopic information in OCT, i.e. without any post-processing, by using a grating based correlation and a wavelength demultiplexing system. Conventional A-scan and spectrally resolved A-scan are directly recorded on the image sensor. Furthermore, due to the grating based system, no correlation scan is necessary. In the frame of this paper we present the principle of the system as well as first experimental results.
physics.optics:Despite the current concentration on phase control in few-cycle pulses, it emerges that there exists a wide class of nonlinear optical interactions in which the carrier phase is essentially irrelevant, even for the shortest pulse profiles. Most parametric processes and most perturbative processes fall into this category, although others such as above threshold ionization (ATI) do not. In an envelope approach, the carrier oscillations are not part of the problem because they are removed at the outset. When they are reinstated at the end of the calculation, one is free to include arbitrary phase shifts -- within certain constraints. In many cases the constraints are relatively weak, and it follows that a single envelope solution can be used with an infinite range of choices for the carrier phase.
physics.optics:This paper has been withdrawn by the author due to a some errors in figures 2 and 3.
physics.optics:We construct combined electric and magnetic field variables which independently represent energy flows in the forward and backward directions respectively, and use these to re-formulate Maxwell's equations. These variables enable us to not only judge the effect and significance of backward-travelling field components, but also to discard them when appropriate. They thereby have the potential to simplify numerical simulations, leading to potential speed gains of up to 100% over standard FDTD or PSSD simulations. We present results for various illustrative situations, including an example application to second harmonic generation in periodically poled lithium niobate. These field variables are also used to derive both envelope equations useful for narrow-band pulse propagation, and a second order wave equation. Alternative definitions are also presented.
physics.optics:I construct combined electric and magnetic field variables which independently represent energy flows in the forward and backward directions respectively, and use these to re-formulate Maxwell's equations. The emphasis is on detailed calculations, with a more general overview being published in Phys. Rev. A72 and arXiv. These directional variables enable us to not only judge the effect and significance of backward-travelling field components, but also to discard them when appropriate. They thereby have the potential to simplify numerical simulations, leading to potential speed gains of up to 100% over standard FDTD or PSSD simulations. These field variables are also used to derive both envelope equations useful for narrow-band pulse propagation, and a second order wave equation. Alternative definitions are also presented, along with their associated wave equations.
physics.optics:We address the reflection of vector solitons, comprising several components that exhibit multiple field oscillations, at the interface between two nonlinear media. We reveal that reflection causes fission of the input signal into sets of solitons propagating at different angles. We find that the maximum number of solitons that arises upon the fission is given by the number of field oscillations in the highest-order input vector soliton.
physics.optics:Cooling of a 58 MHz micro-mechanical resonator from room temperature to 11 K is demonstrated using cavity enhanced radiation pressure. Detuned pumping of an optical resonance allows enhancement of the blue shifted motional sideband (caused by the oscillator's Brownian motion) with respect to the red-shifted sideband leading to cooling of the mechanical oscillator mode. The reported cooling mechanism is a manifestation of the effect of radiation pressure induced dynamical backaction. These results constitute an important step towards achieving ground state cooling of a mechanical oscillator.
physics.optics:Artificially structured metamaterials have enabled unprecedented flexibility in manipulating electromagnetic waves and producing new functionalities, including the cloak of invisibility based on coordinate transformation. Here we present the design of a non-magnetic cloak operating at optical frequencies. The principle and structure of the proposed cylindrical cloak are analyzed, and the general recipe for the implementation of such a device is provided. The cloaking performance is verified using full-wave finite-element simulations.
physics.optics:Highly dispersive photonic band-gap-edge optofluidic biosensors are studied theoretically. We demonstrate that these structures are strongly sensitive to the refractive index of the liquid, which is used to tune dispersion of the photonic crystal. The upper frequency band-gap edge shifts about 1.8 nm for dn=0.002, which is quite sensitive. Results from transmission spectra agree well with those obtained from the band structure theory.
physics.optics:We theoretically analyze the influence of the Gouy phase shift on the nonlinear interaction between waves of different frequencies. We focus on $\chi^{(2)}$ interaction of optical fields, e.g. through birefringent crystals, and show that focussing, stronger than suggested by the Boyd-Kleinman factor, can further improve nonlinear processes. An increased value of 3.32 for the optimal focussing parameter for a single pass process is found. The new value builds on the compensation of the Gouy phase shift by a spatially varying, instead constant, wave vector phase mismatch. We analyze the single-ended, singly resonant standing wave nonlinear cavity and show that in this case the Gouy phase shift leads to an additional phase during backreflection. Our numerical simulations may explain ill-understood experimental observations in such devices.
physics.optics:We study the scintillation index of N partially overlapping collimated lowest order Gaussian laser beams with different wavelengths in weak atmospheric turbulence. Using the Rytov approximation we calculate the initial beam separation that minimizes the longitudinal scintillation. Further reduction of the longitudinal scintillation is obtained by optimizing with respect to both beam separation and spot size. The longitudinal scintillation of the optimal N-beam configurations is inversely proportional to N, resulting in a 92% reduction for a 9-beam system compared with the single beam value. The radial scintillation values for the optimal N-beam configurations are significantly smaller than the corresponding single beam values.
physics.optics:Quadrate hole array is explored to study the influence of unsymmetrical periodicity on extraordinary optical transmission through periodic arrays of subwavelength holes. It is found that the transmission efficiency of light and the ratio between transmission efficiencies of horizontal and vertical polarized light can be continuously tuned by rotating the quadrate hole array. We can calculate out the transmission spectra (including the heights and locations of peaks) for any rotation angle $\theta$ with a simple theoretical model.
physics.optics:We propose a new type of waveguide optical amplifier. The device consists of collinearly propagating pump and amplified Stokes beams with periodic imaging of the Stokes beam due to the Talbot effect. The application of this device as an Image preamplifier for Mid Wave Infrared (MWIR) remote sensing is discussed and its performance is described. Silicon is the preferred material for this application in MWIR due to its excellent transmission properties, high thermal conductivity, high damage threshold and the mature fabrication technology. In these devices, the Raman amplification process also includes four-wave-mixing between various spatial modes of pump and Stokes signals. This phenomenon is unique to nonlinear interactions in multimode waveguides and places a limit on the maximum achievable gain, beyond which the image begins to distort. Another source of image distortion is the preferential amplification of Stokes modes that have the highest overlap with the pump. These effects introduce a tradeoff between the gain and image quality. We show that a possible solution to this trade-off is to restrict the pump into a single higher order waveguide mode.
physics.optics:Based on the basic theory of the microfiber loop resonator, we exploit the application of microfiber loop resonators in biochemical sensing. We set up a reliable theoretical model and optimize the structural parameters of microfiber loop resonators including the radius of the microrfiber, the radius of the loop and the length of the coupling region for higher sensitivity, wider dynamic measurement range, and lower detection limit. To show the convincible and realizable sensing ability we perform the simulation of sensing an extreme small variation of ambient refractive index by employing a set of experimental datas as the parameters in the expression of intensity transmission coefficient, and the detection limit reaches to a varation of ambient refractive index of 10-5 refractive index unit(RIU). This has superiority over the exsiting evanescent field-based subwavelength-diameter optical fiber refractive index sensor.
physics.optics:We demonstrate fast (up to 20 GHz), low power (5 $\mu W$) modulation of photonic crystal (PC) cavities in GaAs containing InAs quantum dots. Rapid modulation through blue-shifting of the cavity resonance is achieved via free carrier injection by an above-band picosecond laser pulse. Slow tuning by several linewidths due to laser-induced heating is also demonstrated.
physics.optics:We apply the technique of far-field interferometry to measure the properties of surface waves generated by two-dimensional (2D) single subwavelength slit-groove structures on gold films. The effective surface index of refraction measured for the surface wave propagating over a distance of more than 12 microns is determined to be 1.016 with a measurement uncertainty of 0.004, to within experimental uncertainty of the expected bound surface plasmon-polariton (SPP) value for a Au/Air interface of 1.018. We compare these measurements to finite-difference-time-domain (FDTD) numerical simulations of the optical field transmission through these devices. We find excellent agreement between the measurements and the simulations for the surface index of refraction. The measurements also show that the surface wave propagation parameter exhibits transient behavior close to the slit, evolving smoothly from greater values asymptotically toward the value expected for the SPP over the first 2-3 microns of slit-groove distance. This behavior is confirmed by the FDTD simulations.
physics.optics:We demonstrate that nonlinear directional coupler with special bending of waveguide axes can be used for all-optical switching of polychromatic light with very broad spectrum covering all visible region. The bandwidth of suggested device is enhanced five times compared to conventional couplers. Our results suggest novel opportunities for creation of all-optical logical gates and switches for polychromatic light with white-light and super-continuum spectrum.
physics.optics:The light transmission through square and rectangular holes in a metal film has been studied. By taking account of plasma response of real metal on hole walls as well as metal surface, an analytical result for the transmission has been deduced, which agrees well with the experiments. We show that the light transmission involves both the diffraction modes and the surface plasmons. Strong coupling of diffraction modes to surface plasmon polariton results in the transmission minima, whereas the coupling to cavity surface plasmon leads to the transmission maxima. The results suggest a dual-effect of the surface plasmons.
physics.optics:Codes were written to simulate the propagation of monochromatic light through a bare optical resonator, using a computational Fourier method to solve the Huygens-Fresnel integral. This was used, in the Fox-Li method, to find the lowest-loss eigenmodes of arbitrary cavity designs. An implicit shift `hopping' method was employed to allow a series of increasingly higher-loss eigenmodes to be found, limited in number by computational time.   Codes were confirmed in their accuracy against the literature, and were used to investigate a number of different cavity configurations.   In addition to confirming the fractal nature of eigenmodes imaged at the conjugate plane of a symmetric (g<-1) resonator, an initial study was made of how the (imperfect) quality of the fractal fit varied as the defining aperture was moved around the cavity.   A comparison was also made with the fractal-patterns produced by codes written to simulate basic video-feedback.
physics.optics:We discuss the claims of the comment at arXiv.org:physics/0609234. We show that A.V. Kildishev et al. misread our method of extracting of optical constants of nanostructured films. The theoretical calculations performed in the comment appear to be in a direct contradiction with an experiment. We demonstrate that the theoretical calculations suggested being free from ambiguities (V.M. Shalaev et al., Opt. Lett. 30, 3345 (2005)) require an additional experimental verification, which can be performed by observing physical effects of negative refraction.
physics.optics:Single-shot time resolved Coherent Anti-Stokes Raman Scattering (CARS) is presented as a viable method for fast measurements of molecular spectra. The method is based on the short spatial extension of femtosecond pulses and maps time delays between pulses onto the region of intersection between broad beams. The image of the emitted CARS signal contains full temporal information on the field-free molecular dynamics, from which spectral information is extracted. The method is demonstrated on liquid samples of CHBr3 and CHCl3 and the Raman spectrum of the low-lying vibrational states of these molecules is measured.
physics.optics:We propose the use of an optically trapped, dye doped polystyrene microsphere for spatial probing of the refractive index at any position in a fluid. We demonstrate the use of the dye embedded in the microsphere as an internal broadband excitation source, thus eliminating the need for a tunable excitation source. We measured the full width at half maximum of the TE and TM resonances, and their frequency spacing as a function of the refractive index of the immersion fluid. From these relations we obtained an absolute sensitivity of 5e-4 in local refractive index, even when the exact size of the microsphere was not a priori known.
physics.optics:We report the observation of a coherent multimode instability in quantum cascade lasers (QCLs), which is driven by the same fundamental mechanism of Rabi oscillations as the elusive Risken-Nummedal-Graham-Haken (RNGH) instability predicted 40 years ago for ring lasers. The threshold of the observed instability is significantly lower than in the original RNGH instability, which we attribute to saturable-absorption nonlinearity in the laser. Coherent effects, which cannot be reproduced by standard laser rate equations, can play therefore a key role in the multimode dynamics of QCLs, and in lasers with fast gain recovery in general.
physics.optics:The ultimate sensitivity of optical detection is limited by the signal-to-noise ratio (SNR). The first part of the paper shows that coherence plays an important role in the noise analysis. Although interference between an auxiliary wave and a signal wave makes the photo detector response to the signal stronger, the coherent noise also enhances. This makes insignificant the gain in the SNR. Pulsed-excitation gated-detection (PEGD) is described and analyzed in the second part to show that 1) a high brightness of detected particles is not a prerequisite for a high SNR, 2) optimized parameters of the PEGD protocol demonstrate interesting bifurcation making a sudden jump from an effectively continuous regime to PEGD, and 3) photo-physical properties of NV centers in nano crystals of diamond approach those ideal for PEGD.
physics.optics:Second-harmonic and sum-frequency mixing phenomena associated with 3D-localized photonic modes are studied in InP-based planar photonic crystal microcavities excited by short-pulse radiation near 1550 nm. Three-missing-hole microcavities that support two closely-spaced modes exhibit rich second-order scattering spectra that reflect intra- and inter-mode mixing via the bulk InP chi(2) during ring-down after excitation by the broadband, resonant pulse. Simultaneous excitation with a non-resonant source results in tunable second-order radiation from the microcavity.
physics.optics:We study the angular structure of polarization of light transmitted through a nematic liquid crystal (NLC) cell by theoretically analyzing the polarization state as a function of the incidence angles. For a uniformly aligned NLC cell, the $4\times 4$ matrix formalism and the orthogonality relations are used to derive the analytical expressions for the transmission and reflection matrices. The polarization resolved angular patterns in the two-dimensional projection plane are characterized in terms of the polarization singularities: C points (points of circular polarization) and L lines (lines of linear polarization). In the case of linearly polarized plane waves incident on the homeotropically aligned cell, we present the results of detailed theoretical analysis describing the structure of the polarization singularities. We apply the theory to compute the polarization patterns for various orientational structures in the NLC cell and discuss the effects induced by the director orientation and biaxiality.
physics.optics:The dynamics of photonic wavepacket in the effective oscillator potential is studied. The oscillator potential is constructed on a base of one dimensional photonic crystal with a period of unit cell adiabatically varied in space. The structure has a locally equidistant discrete spectrum. This leads to an echo effect, i.e. the periodical reconstruction of the packet shape. The effect can be observed in a nonlinear response of the system. Numerical estimations for porous-silicon based structures are presented for femtosecond Ti:Sapphire laser pump.
physics.optics:Together with the 74 lines belonging to (0,9,0)-(0,0,0) band, the high-resolution absorption spectrum of H2O+ was observed in the visible region of 16680 -17300 (cm-1) using optical heterodyne magnetic rotation enhanced velocity modulation spectroscopy for the first time, which verifies the high sensitivity and high signal to noise ratio (S/N) of this technique.
physics.optics:An implementation of the strong-permittivity-fluctuation theory (SPFT) is presented in order to estimate the constitutive parameters of a homogenized composite material (HCM) which is both cubically nonlinear and anisotropic. Unlike conventional approaches to homogenization, the particles which comprise the component material phases are herein assumed to be small but not vanishingly small. The influence of particle size on the estimates of the HCM constitutive parameters is illustrated by means of a representative numerical example. It is observed that, by taking the nonzero particle size into consideration, attenuation is predicted and nonlinearity enhancement is somewhat diminished. In these respects, the effect of particle size is similar to that of correlation length within the bilocally-approximated SPFT.
physics.optics:The optical properties of a concentric nanometer-sized spherical shell comprised of an (active) 3-level gain medium core and a surrounding plasmonic metal shell are investigated. Current research in optical metamaterials has demonstrated that including lossless plasmonic materials to achieve a negative permittivity in a nano-sized coated spherical particle can lead to novel optical properties such as resonant scattering as well as transparency or invisibility. However, in practice, plasmonic materials have high losses at optical frequencies. It is observed that with the introduction of active materials, the intrinsic absorption in the plasmonic shell can be overcome and new optical properties can be observed in the scattering and absorption cross-sections of these coated nano-sized spherical shell particles. In addition, a "super" resonance is observed with a magnitude that is greater than that for a tuned, resonant passive nano-sized coated spherical shell. This observation suggests the possibility of realizing a highly sub-wavelength laser with dimensions more than an order of magnitude below the traditional half-wavelength cavity length criteria. The operating characteristics of this coated nano-particle (CNP) laser are obtained numerically for a variety of configurations.
physics.optics:A coherent microwave radiation, concomitant with experiments of stimulated Raman scattering (SRS) in weakly compressed hydrogen, had been observed; qualitative and quantitative results had been obtained, and are given in this article. .An attempt to interpret this emission on a phenomenological basis is presented, on the basis of a model in which the influence of hydrogen molecule anharmonicity is taken into account, leading to two main effects:1) possible existence of an intermediate state involving a vibrationally excited quasiparticle system, the lifetime of which should be very short (0.1-1 ps); 2) nonlinear coupling between the Stokes light wave and the longitudinal propagating waves associated to the quasiparticle system. Moreover, several processes are also to be taken into account; existence of a macroscopic longitudinal electric field, due to coulombian interactions; electrostriction effect giving rise to a "piezoelectric" character for the quasiparticle system; Cherenkov-type and anomalous relativistic Doppler effects. Then, the source of the coherent microwave radiation might be the in phase quadrature part of the longitudinal polarisation associated to the quasiparticle waves, and its emission should appear like parametric instabilities. Concerning the SRS process itself, an additional contribution to the Raman gain should arise from the laser induced electro-optic effect.
physics.optics:This work is concerned with the experimental demonstration of a dual-band negative index metamaterial. The sample is double-negative (showing both a negative effective permeability and a negative effective permittivity) for wavelengths between 799 and 818 nm of linearly polarized light with a real part of refractive index of about -1.0 at 813 nm; the ratio -Re(n)/Im(n) is close to 1.3 at that wavelength. For an orthogonal polarization, the same sample also exhibits a negative refractive index in the visible (at 772 nm). The spectroscopic measurements of the material are in good agreement with the results obtained from a finite element electromagnetic solver for the actual geometry of the fabricated sample at both polarizations.
physics.optics:Spectra comprised of hundreds of time-components for absorption path lengths up to 130 km have been recorded around 1050 nm by combining two recent techniques, intracavity laser spectroscopy with vertical external cavity surface emitting multiple-quantum-well lasers and time-resolved Fourier transform spectroscopy. A sensitivity of 1 10^{-10} cm^{-1}.Hz^{-1/2} is achieved, for simultaneously acquired 10^4 spectral elements, three orders of magnitude better than the sensitivity obtained in previous similar experiments. Specific advantages of the method, especially for frequency and intensity metrology of weak absorption transitions, are discussed.
physics.optics:A family of coupled nano-strips with varying dimensions is fabricated to obtain optical magnetic responses across the whole visible spectrum, from red to blue. The proposed approach provides one with a universal building block and a general recipe for producing controllable optical magnetism for various practical implementations.
physics.optics:We use fluorescent nanospheres as scalar detectors for the electric-field intensity in order to probe the near-field of an optical tip used in aperture-type near-field scanning optical microscopy (NSOM). Surprisingly, the recorded fluorescence images show two intensity lobes if the sphere diameter is smaller that the aperture diameter, as expected only in the case of vector detectors like single molecules. We present a simple but realistic, analytical model for the electric field created by light emitted by a NSOM tip which is quantitative agreement with the experimental data.
physics.optics:We present the design of mid-infrared and THz quantum cascade laser cavities formed from planar photonic crystals with a complete in-plane photonic bandgap. The design is based on a honeycomb lattice, and achieves a full in-plane photonic gap for transverse-magnetic polarized light while preserving a connected pattern for efficient electrical injection. Candidate defects modes for lasing are identified. This lattice is then used as a model system to demonstrate a novel effect: under certain conditions - that are typically satisfied in the THz range - a complete photonic gap can be obtained by the sole patterning of the top metal contact. This possibility greatly reduces the required fabrication complexity and avoids potential damage of the semiconductor active region.
physics.optics:A thin metal slab is known to be able to focus the near fields of TM wave of a point source. Here, we show that a thin metal slab in fact possesses a far-field image and through a simple modification on the system by resonance tunneling, a double metal slab can give a bright image with both the far-field and the near-field details.
physics.optics:We have developed a numerical method based on the transfer matrix to calculate the quasimodes and lasing modes in one-dimensional random systems. Depending on the relative magnitude of the localization length versus the system size, there are two regimes in which the quasimodes are distinct in spatial profile and frequency distribution. In the presence of uniform gain, the lasing modes have one-to-one correspondence to the quasimodes in both regimes. Local excitation may enhance the weight of a mode within the gain region due to local amplification, especially in a weakly scattering system.
physics.optics:We present a new family of diffractive lenses whose structure is based on the combination of two concepts: photon sieve and fractal zone plates with variable lacunarity. The focusing properties of different members of this family are examined. It is shown that the sieves provide a smoothing effect on the higher order foci of a conventional lacunar fractal zone plate. However, the characteristic self-similar axial response of the fractal zone plates is always preserved.
physics.optics:Fresnel zone plates are diffractive elements that are essential to form images in many scientific and technological areas, especially where refractive optics is not available. One of the main shortcomings of Fresnel zone plates, which limit their utility with broadband sources, is their high chromatic aberration. Recently presented, Fractal Zone Plates (FZPs) are diffractive lenses characterized by an extended focal depth with a fractal structure. This behaviour predicts an improved performance of FZPs as image forming devices with an extended depth of field and a reduced chromatic aberration. Here we report the achievement of the first polychromatic images obtained with a FZP that confirm these predictions. We show that the polychromatic modulation transfer function (MTF) of a FZP affected by defocus is about two times better than one corresponding to a Fresnel zone plate. This result opens the possibility to improve imaging techniques that use conventional Fresnel zone plates over a wide range of the electromagnetic spectrum, ranging from x-rays to THz.
physics.optics:A new carbazole derivative with a 3,3'-bicarbazyl core 6,6'-substituted by dicyanovinylene groups (6,6'-bis(1-(2,2'-dicyano)vinyl)-N,N'-dioctyl-3,3'-bicarbazyl; named (OcCz2CN)2, was synthesized by carbonyl-methylene Knovenagel condensation, characterized and used as a component of multilayer organic light-emitting diodes (OLEDs). Due to its &#61552;-donor-acceptor type structure, (OcCz2CN)2 was found to emit a yellow light at &#61548;max=590 nm (with the CIE coordinates x=0.51; y = 0.47) and was used either as a dopant or as an ultra-thin layer in a blue-emitting matrix of 4,4'-bis(2,2'-diphenylvinyl)-1,1'-biphenyl (DPVBi). DPVBi (OcCz2CN)2-doped structure exhibited, at doping ratio of 1.5 weight %, a yellowish-green light with the CIE coordinates (x = 0.31; y = 0.51), an electroluminescence efficiency &#61544;EL=1.3 cd/A, an external quantum efficiency &#61544;ext= 0.4 % and a luminance L= 127 cd/m2 (at 10 mA/cm2) whereas for non-doped devices utilizing the carbazolic fluorophore as a thin neat layer, a warm white with CIE coordinates (x = 0.40; y= 0.43), &#61544;EL= 2.0 cd/A, &#61544;ext= 0.7 %, L = 197 cd/m2 (at 10 mA/cm2) and a color rendering index (CRI) of 74, were obtained. Electroluminescence performances of both the doped and non-doped devices were compared with those obtained with 5,6,11,12-tetraphenylnaphtacene (rubrene) taken as a reference of highly efficient yellow emitter.
physics.optics:A metallo-dielectric multilayer structure is proposed as a novel approach to the analysis of lactose malabsorption. When lactose intolerance occurs, the bacterial overgrowth in the intestine causes an increased spontaneous emission of H2 in the human breath. By monitoring the changes in the optical properties of a multilayer palladium-polymeric structure, one is able to detect the patient's disease and the level of lactose malabsorption with high sensitivity and rapid response.
physics.optics:Fresnel lens has a long history in optics. This concept at non-optical wavelengths is also applicable. In this paper we report design and fabrication of a half and quarter wave dielectric Fresnel lens made of Plexiglas, and a Fresnel reflector at 11.1 GHz frequency. We made two lenses and one reflector at same frequency and compare their gain and radiation pattern to simulated results. Some methods for better focusing action will be introduced.
physics.optics:In this Letter we report observations of optically induced self-organization of colloidal arrays in the presence of un-patterned counter-propagating evanescent waves. The colloidal arrays formed along the laser propagation-axis are shown to be linked to the break-up of the incident field into optical spatial solitons, the lateral spacing of the arrays being related to modulation instability of the soft condensed matter system.
physics.optics:The canonical perfect lens--comprising three slabs, each made of a linear, homogeneous, bianisotropic material with orthorhombic symmetry--is Lorentz covariant.
physics.optics:A metamaterial with a negative effective index of refraction is made from a three-dimensional hexagonal lattice photonic crystal with a metallic basis embedded in foam. It has been simulated with Ansoft HFSSTM in a frequency range from 7.0 to 12.0 GHz. Simulated results tested experimentally and negative refraction verified in some frequencies. Experimental results are in excellent agreement with simulations.
physics.optics:Quasicrystals, realized in metal alloys, are a class of lattices exhibiting symmetries that fall outside the usual classification for periodic crystals. They do not have translational symmetry and yet the lattice points are well ordered. Furthermore, they exhibit higher rotational symmetry than periodic crystals. Because of the higher symmetry (more spherical), they are more optimal than periodic crystals in achieving complete photonic bandgaps in a new class of materials called photonic crystals in which the propagation of light in certain frequency ranges is forbidden. The potential of quasicrystals has been demonstrated in two dimensions for the infrared range and, recently, in three-dimensional icosahedral quasicrystals fabricated using a stereo lithography method for the microwave range. Here, we report the fabrication and optical characterization of icosahedral quasicrystals using a holographic lithography method for the visible range. The icosahedral pattern, generated using a novel 7-beam optical interference holography, is recorded on photoresists and holographic plates. Electron micrographs of the photoresist samples show clearly the symmetry of the icosahedral quasicrytals in the submicron range, while the holographic plate samples exhibit bandgaps in the angular-dependent transmission spectra in the visible range. Calculations of the bandgaps due to reflection planes inside the icosahedral quasicrystal show good agreement with the experimental results.
physics.optics:The local electromagnetic field distribution over the dielectric (SiC) surface illuminated by mid-IR light of frequency close to the lattice resonance was directly mapped with an apertureless scattering near-field optical microscope. Half of the sample surface was covered with a metal layer, in which some small $(0.2-10 \mu m)$ holes were formed. It was found, that due to the eliminating of collective surface polarization effects, the amplitude of the electromagnetic field becomes several times higher over such holes than over infinitely open surface of the same dielectric.
physics.optics:We present scanning near-field extinction spectra of single molecules embedded in a solid matrix. By varying the molecule-tip separation, we modify the line shape of the spectra, demonstrating the coherent nature of the interaction between the incident laser light and the excited state of the molecule. We compare the measured data with the outcome of numerical calculations and find a very good agreement.
physics.optics:The negative shifts seen by Wang and Zhu are not due to the excitation of surface plasmons but to leaky modes of the slab propagating backward. Provided the characteristics of the LHM slab are chosen correctly, it is shown that a leaky surface plasmon can actually be excited using the KR configuration.
physics.optics:The effective permittivity dyadic of a composite material containing particulate constituent materials with one constituent having the ability to display the Pockels effect is computed, using an extended version of the strong-permittivity-fluctuation theory which takes account of both the distributional statistics of the constituent particles and their sizes. Scattering loss, thereby incorporated in the effective electromagnetic response of the homogenized composite material, is significantly affected by the application of a low-frequency (dc) electric field.
physics.optics:The successful development and optimisation of optically-driven micromachines will be greatly enhanced by the ability to computationally model the optical forces and torques applied to such devices. In principle, this can be done by calculating the light-scattering properties of such devices. However, while fast methods exist for scattering calculations for spheres and axisymmetric particles, optically-driven micromachines will almost always be more geometrically complex. Fortunately, such micromachines will typically possess a high degree of symmetry, typically discrete rotational symmetry. Many current designs for optically-driven micromachines are also mirror-symmetric about a plane. We show how such symmetries can be used to reduce the computational time required by orders of magnitude. Similar improvements are also possible for other highly-symmetric objects such as crystals. We demonstrate the efficacy of such methods by modelling the optical trapping of a cube, and show that even simple shapes can function as optically-driven micromachines.
physics.optics:The optical forces in optical tweezers can be robustly modeled over a broad range of parameters using generalsed Lorenz-Mie theory. We describe the procedure, and show how the combination of experimental measurement of properties of the trap coupled with computational modeling, can allow unknown parameters of the particle - in this case, the refractive index - to be determined.
physics.optics:We show that two-dimensional arrays of thin metallic wires can guide transverse electromagnetic (TEM) waves and focus them to the spatial dimensions much smaller that the vacuum wavelength. This guiding property is retained for the tapered wire bundles which can be used as multi-channel TEM endoscopes: they capture a detailed electromagnetic field profile created by deeply sub-wavelength features of the studied sample and magnify it for observation. The resulting imaging method is superior to the conventional scanning microscopy because of the parallel nature of the image acquisition by multiple metal wires. Possible applications include terahertz and mid-infrared endoscopy with nanoscale resolution.
physics.optics:We suggest a way to manipulate electromagnetic wave by introducing a rotation mapping of coordinates that can be realized by a specific transformation of permittivity and permeability of a shell surrounding an enclosed domain. Inside the enclosed domain, the information from outside will appear as if it comes from a different angle. Numerical simulations were performed to illustrate these properties.
physics.optics:We report the observation of the magnetic field induced circular differential deflection of light at the interface of a Faraday medium. The difference in the angles of refraction or reflection between the two circular polarization components is a function of the magnetic field strength and the Verdet constant. The reported phenomena permit the observation of the Faraday effect not via polarization rotation in transmission, but via changes in the propagation direction in refraction or in reflection. An unpolarized light beam is predicted to split into its two circular polarization components. The light deflection arises within a few wavelengths at the interface and is therefore independent of pathlength.
physics.optics:A small depression is created in a straight optical fiber taper to form a local probe suitable for studying closely spaced, planar microphotonic devices. The tension of the "dimpled" taper controls the probe-sample interaction length and the level of noise present during coupling measurements. Practical demonstrations with high-Q silicon microcavities include testing a dense array of undercut microdisks (maximum Q = 3.3x10^6) and a planar microring (Q = 4.8x10^6).
physics.optics:We study the formation of dark-bright vector soliton pairs in nonlocal Kerr-type nonlinear medium. We show, by analytical analysis and direct numerical calculation, that in addition to stabilize vector soliton pairs nonlocal nonlinearity also helps to reduce the threshold power for forming a guided bright soliton. With help of the nonlocality, it is expected that the observation of dark-bright vector soliton pairs in experiments becomes more workable.
physics.optics:We experimentally demonstrate optical bistability in Er3+-Yb3+ phosphate glass microspheres at 295 K. Bistability is associated with both Er3+ fluorescence and lasing behavior, and chromatic switching. The chromatic switching results from an intrinsic mechanism exploiting the thermal coupling of closely-spaced energy levels, and occurs simultaneously with the intensity switching. A contrast ratio of 3.2 has been obtained for chromatic switching, and the intensity switching shows ratios of 2.4 for 550 nm and, 1.8 for the 660 nm fluorescence emissions, and 11 for the IR lasing at 1.5 um. Concurrent with these observations, we investigate a temperature dependent absorption of pump power which exhibits bistable behavior. The influences of the host matrix on lasing and fluorescence mechanisms are highlighted.
physics.optics:When a strong magnetostatic field is present, vacuum effectively appears as a linear, uniaxial, dielectric-magnetic medium for small-magnitude optical fields. The availability of the frequency-domain dyadic Green function when the magnetostatic field is spatially uniform facilitates the formulation of an integral equation for the scattering of an optical field by a spatially varying magnetostatic field in vacuum. This integral equation can be numerically treated by using the method of moments as well as the coupled dipole method. Furthermore, the principle underlying the strong-property-fluctuation theory allows the homogenization of a spatially varying magnetostatic field in the context of light scattering.
physics.optics:The pronounced light-matter interactions in photonic crystals make them interesting as opto-fludic "building blocks" for lab-on-a-chip applications. We show how conducting electrolytes cause dissipation and smearing of the density-of-states, thus altering decay dynamics of excited bio-molecules dissolved in the electrolyte. Likewise, we find spatial damping of propagating modes, of the order dB/cm, for naturally occurring electrolytes such as drinking water or physiological salt water.
physics.optics:We examine how the interference of a coherent light-pulse with its slightly time-delayed copy may generate a pulse nearly identical to the original one and ahead of it. The simplicity of this 2-pulse system enabled us to obtain exact analytic expressions of the pulse distortion, valid for a wide class of pulse shapes. Explicit results are given for the pulses usually considered (gaussian, hyperbolic secant) but also for more realistic pulses of strictly limited duration. We finally show that the efficiency of the 2-pulse system is comparable to that of the other superluminal systems, at least for the pulse advancements actually demonstrated in the optical experiments.
physics.optics:This paper is withdrawn.
physics.optics:An analytical formulation for the band structure and Bloch modes in elliptically birefringent magnetophotonic crystals is presented. The model incorporates both the effects of gyrotropy and linear birefringence generally present in magneto-optic thin film devices. Full analytical expressions are obtained for the dispersion relation and Bloch modes in a layered stack photonic crystal and their properties are analyzed. It is shown that other models recently discussed in the literature are contained as special limiting cases of the formulation presented herein.
physics.optics:We have investigated the bleaching dynamics that occur in optofluidic dye lasers where the liquid laser dye in a microfluidic channel is locally bleached due to optical pumping. We find that for microfluidic devices, the dye bleaching may be compensated through diffusion of dye molecules alone. By relying on diffusion rather than convection to generate the necessary dye replenishment, our observation potentially allows for a significant simplification of optofluidic dye laser device layouts, omitting the need for cumbersome and costly external fluidic handling or on-chip microfluidic pumping devices.
physics.optics:We present effective optical constants of stratified metal-dielectric metameterial. The effective constants are determined by two complex reflectivity method (TCRM). TCRM reveals full components of effective permittivity and permeability tensors and indicates the remarkable anisotropy of metallic and dielectric components below effective plasma frequency. On the other hand, above the plasma frequency, one of the effective refractive indexes takes a positive value less than unity and is associated with small loss. The photonic states are confirmed by the distribution of electromagnetic fields.
physics.optics:We demonstrate that arrays of split ring resonators (SRR) can have negative refractive index at optical frequencies (~1.4 eV). Our calculations reveal that the electric fields of radiation interact strongly with even symmetric SRR at optical frequencies as the size of the SRR becomes of the order of the wavelength (~lambda /3) for practicably realizable structures. We also demonstrate by calculations the focussing of a line source by a flat slab of a (2D) SRR medium. The negative refractive index here is related to the plasmonic excitations of the SRR and cannot be directly explained by the usual paradigm of negative dielectric permittivity and negative magnetic permeability in these limits when homogenization breaks down.
physics.optics:The efficient generation of surface plasmons from free space optical waves is still an open problem in the field. Here we present a methodology and optimized design for a grating coupler. The photo-excitation of surface plasmons at an Ag-SiO2 interface is numerically demonstrated to yield greater than 50% coupling from a Gaussian beam into surface plasmon voltages and currents.
physics.optics:We have measured the evolution of the light intensity of a random laser during a nanosecond pump pulse. Relaxation oscillations in a titania random laser were observed in the time trace of the total emitted intensity. We compare our experimental results with a simple model, based on the four-level rate equations for a single mode laser.
physics.optics:It is commonly assumed that the long-wavelength limit of a metamaterial can always be described in terms of effective permeability and permittivity tensors. This assumption holds true in all metamaterials considered up to now. Here we report that this assumption is false--there exist an entirely new class of metamaterials consisting of multiple interlocking disconnected metal networks, for which the effective long-wavelength theory is local, but the effective field is non-Maxwellian, and possesses much more internal degrees of freedom than effective Maxwellian fields in a homogeneous medium.
physics.optics:Optofluidic sensors based on highly dispersive two-dimensional photonic crystal waveguides are theoretically studied. Results show that these structures are strongly sensitive to the refractive index of the infiltrated liquid (nl), which is used to tune dispersion of the photonic crystal waveguide. Waveguide mode-gap edge shifts about 1.2 nm for dnl=0.002. The shifts can be explained well by band structure theory combined with first-order perturbation theory. These devices are potentially interesting for chemical sensing applications.
physics.optics:We have expressed the nonlinear optical absorption of a semiconductor in terms of its linear spectrum. We determined that the two-photon absorption coefficient in a strong DC-electric field of a direct gap semiconductor can be expressed as the product of a differential operator times the convolution integral of the linear absorption without a DC-electric field and an Airy function. We have applied this formalism to calculate the two-photon absorption coefficient and nonlinear refraction for GaAs and ZnSe using their linear absorption and have found excellent agreement with available experimental data.
physics.optics:Partially-coherent, quasi-monochromatic optical fields are fully described by their Mutual Optical Intensity (MOI) or the phase-space equivalent, the Generalised Radiance (GR). This paper reports on the application of a propagation-based phase-space tomographic technique for determining both the MOI and the GR of wavefields. This method is applied to the reconstruction of the MOI and the GR of an optical wavefield propagated through a suspension of \~10micrometre diameter polystyrene spheres.
physics.optics:The refractive index of novel organosilica (nano/micro)material is determined using two methods. The first method is based on analysis of optical extinction efficiency of organosilica beads versus wavelength, which is obtained by a standard laboratory spectrometer. The second method relies on the measurable trapping potential of these beads in the focused light beam (laser tweezers). Polystyrene beads were used to test these methods, and the determined dispersion curves of refractive index values have been found accurate. The refractive index of organosilica beads has been determined to range from 1.60-1.51 over the wavelength range of 300-1100 nm.
physics.optics:Two devices for subwave length focusing of light are explored. The first one is a thin film of a well reflecting metal which the converging beam of surface plasmons with a wave number $h>>\omega_0/c$ is excited on. The waist of this beam has a cross section that is much smaller than $\lambda_0^2$. The second device is a linear nanoantenna with a gap in the middle. A resonance plasmon is excited in the antenna. The field in the gap concentrates on the spot that is much smaller than $\lambda_0^2$. In both cases the effect of field enhancement in the spot of subwave length focusing is greatly decreased because of propagation losses in the first case and because of a small excitation cross section in the second case. It is suggested to improve on the effect of excitation of the film and the nanoantenna by means of interaction with an adjacent atom or quantum dot.
physics.optics:We demonstrate a uniform high spectral brightness and peak power density all-fiber supercontinuum source. The source consists of a nanosecond Ytterbium fiber laser and an optimal length PCF producing a continuum with a peak power density of 2 W/nm and less than 5 dB of spectral variation between 590 to 1500 nm. The Watt level per nm peak power density enables the use of such sources for the characterization of nonlinear materials. Application of the source is demonstrated with the characterization of several periodically poled crystals.
physics.optics:We report on 33 % efficient generation of the first Stokes in a high concentration GeO2 fiber Raman laser pumped by a 22 W Thulium doped fiber laser. An output power of 4.6 W at 2.105 um is demonstrated.
physics.optics:An approach reported recently by Alexandrov et al. on optical scatter imaging, termed digital Fourier microscopy (DFM), represents an adaptation of digital Fourier holography to selective imaging of biological matter. Holographic mode of recording of the sample optical scatter enables reconstruction of the sample image. Form-factor of the sample constituents provides a basis for discrimination of these constituents implemented via flexible digital Fourier filtering at the post processing stage. Like in the dark-field microscopy, the DFM image contrast appears to improve due to the suppressed optical scatter from extended sample structures. In this paper, we present theoretical and experimental study of DFM using biological phantom that contains polymorphic scatterers.
physics.optics:Enhanced transmissions through a gold film with arrays of subwavelength holes are theoretically studied, employing the rigid full vectorial three dimensional finite difference time domain method. Influence of air-holes shape to the transmission is firstly studied, which confirms two different resonances attributing to the enhanced transmission: the localized waveguide resonance and periodic surface plasmon resonances. For the film coated with dielectric layers, calculated results show that in the wavelength region of interest the localized waveguide resonant mode attributes to sensing rather than the periodic gold-glass surface plasmon mode. Although the detected peak is fairly broad and the shift is not too pronounced, we emphasize the contribution for sensing from the localized waveguide resonant mode, which may opens up new ways to design surface plasmon based sensors.
physics.optics:It is hard for us humans to recognize things in nature until we have invented them ourselves. For image-forming optics, nature has made virtually every kind of lens humans have devised. But what about lensless "imaging"? Recently, we showed that a bare array of sensors on a curved substrate could achieve resolution not limited by diffraction- without any lens at all provided that the objects imaged conform to our a priori assumptions. Is it possible that somewhere in nature we will find this kind of vision system? We think so and provide examples that seem to make no sense whatever unless they are using something like our lensless imaging work.
physics.optics:We are addressing the optical speedup of movements of layers in moire patterns. We introduce a set of equations for computing curved patterns, where the formulas of optical speedup and moire periods are kept in their simplest form. We consider linear movements and rotations. In the presented notation, all periods are relative to the axis of movements of layers and moire bands.   KEYWORDS: moire patterns, line moire, superposition images, optical speedup, moire speedup, moire magnification, periodic moire
physics.optics:In total ignorance of what a scene contains, imaging systems are extremely useful. But if we know the scene will be comprised of no more than a few distant point sources, nonimaging systems may achieve better accuracy in a smaller, more rugged, and less expensive manner. We show here that those advantages can be realized in a wide variety of designs. All can beat the diffraction limit under the proper circumstances. We call these sensors "brainy" in analogy to anima; vision which uses poor optics processed by a wonderful computer - a brain.
physics.optics:A room temperature operating Vertical External Cavity Surface Emitting Laser is applied around 1550 nm to intracavity laser absorption spectroscopy analyzed by time-resolved Fourier transform interferometry. At an equivalent pathlength of 15 km, the high resolution spectrum of the semiconductor disk laser emission covers 17 nm simultaneously. A noise equivalent absorption coefficient at one second averaging equal to 1.5 10^{-10} cm^{-1}.Hz^{-1/2} per spectral element is reported for the 65 km longest path length employed.
cs.LG:This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.
cs.LG:Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.
cs.LG:The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.
cs.LG:We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.
cs.LG:Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a {\it natural} way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus.   Keywords: Kolmogorov complexity; Algorithmic information theory.
cs.LG:In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.
cs.LG:An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.
cs.LG:When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.   Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).   As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.
cs.LG:In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a {\it stigmergic} approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(\lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.
cs.LG:Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.
cs.LG:The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.
cs.LG:In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.
cs.LG:To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur.   We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree \Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures.   The complexity for searching the optimal Bayesian networks of in-degree \Delta increases only polynomially in the number of random varibales for constant \Delta and the optimal joint measure associated with a given graph can be found by convex optimization.
cs.LG:We make progress on two important problems regarding attribute efficient learnability.   First, we give an algorithm for learning decision lists of length $k$ over $n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time $n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight.   Second, we give an algorithm for learning an unknown parity function on $k$ out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$. For $k=o(\log n)$ this yields a polynomial time algorithm with sample complexity $o(n)$. This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.
cs.LG:Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.
cs.LG:It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipf's distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable "progress in studies".
cs.LG:For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.
cs.LG:We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) := 1/N sum_i (f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when y_i is a noisy version of f*(x_i) for some function f* in H, the output of the algorithm converges to f* as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N -> infinity. For the case where N -> infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we describe the limiting 'non-noisy', 'non-regularized' function f*, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.
cs.LG:We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2.   To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated.   Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.
cs.LG:We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space $[0,1]$ and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.
cs.LG:We consider a general class of forecasting protocols, called "linear protocols", and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecaster's predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptic's capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.
cs.LG:This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.
cs.LG:We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.
cs.LG:We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed "bag of components" representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects' structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.
cs.LG:This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for "reactive" experts problems, which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.
cs.LG:A main problem of "Follow the Perturbed Leader" strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t^(1/2)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t^2 log t) samples in each step.
cs.LG:Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.
cs.LG:We consider the problem of learning unions of rectangles over the domain $[b]^n$, in the uniform distribution membership query learning setting, where both b and n are "large". We obtain poly$(n, \log b)$-time algorithms for the following classes:   - poly$(n \log b)$-way Majority of $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   - Union of poly$(\log(n \log b))$ many $O(\frac{\log^2 (n \log b)} {(\log \log(n \log b) \log \log \log (n \log b))^2})$-dimensional rectangles.   - poly$(n \log b)$-way Majority of poly$(n \log b)$-Or of disjoint $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   Our main algorithmic tool is an extension of Jackson's boosting- and Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$, building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to obtain the results stated above are techniques from exact learning [Beimel, Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$ circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean functions as thresholds of parities [Klivans, Servedio 2001].
cs.LG:We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithm's performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is "universal" (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.
cs.LG:The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t - x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, >...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x*. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may impose practical limitations on an agent's performance, as O(eps^-4) queries are made before the queries converge to x* with eps accuracy.
cs.LG:A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).
cs.LG:We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a "regret term" of O(N^(-1/2)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.
cs.LG:Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.
cs.LG:It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.
cs.LG:Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.
cs.LG:Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m->infinity and V for k->infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.
cs.LG:Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.
cs.LG:Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.
cs.LG:A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their $L_1$ distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the $L_1$ metric into a distribution that is good under the KL-divergence.
cs.LG:In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.
cs.LG:In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.
cs.LG:We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as "learning from possibilities" that formalizes these ideas, then we present a more specific learning setting, referred to as "assumption-based learning" that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.
cs.LG:We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learner/classifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. "Nested tree," "embedded tree" and "recursive tree" are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.
cs.LG:We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a "leading prediction strategy", which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.
cs.LG:Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.
cs.LG:We present basic notions of Gold's "learnability in the limit" paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.
cs.LG:An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.
cs.LG:Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictor's goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.
cs.LG:We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon.   In this scenario, we give a poly(n/epsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the ``clustering'' model, where such assumptions are unavoidable.   Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.
cs.LG:Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for "hedging" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.
cs.LG:This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.
cs.LG:Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2^D \sqrt{n}), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.
cs.LG:We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean $n$-sphere $\s^n$ is $\Theta(n)$. We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the ``intrinsic dimensionality'' of Ch\'avez et al.)
cs.LG:This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.
cs.LG:We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.
cs.LG:Speaker identification is a powerful, non-invasive and in-expensive biometric technique. The recognition accuracy, however, deteriorates when noise levels affect a specific band of frequency. In this paper, we present a sub-band based speaker identification that intends to improve the live testing performance. Each frequency sub-band is processed and classified independently. We also compare the linear and non-linear merging techniques for the sub-bands recognizer. Support vector machines and Gaussian Mixture models are the non-linear merging techniques that are investigated. Results showed that the sub-band based method used with linear merging techniques enormously improved the performance of the speaker identification over the performance of wide-band recognizers when tested live. A live testing improvement of 9.78% was achieved
cs.LG:Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such "VC dimensions" exist for models taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations appropriate for the missing case, the one of models with values in R^Q. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.
cs.LG:We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.
cs.LG:Supervised learning deals with the inference of a distribution over an output or label space $\CY$ conditioned on points in an observation space $\CX$, given a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of applications of interest, acquisition of large amounts of observations is easy, while the process of generating labels is time-consuming or costly. One way to deal with this problem is {\em active} learning, where points to be labelled are selected with the aim of creating a model with better performance than that of an model trained on an equal number of randomly sampled points. In this paper, we instead propose to deal with the labelling cost directly: The learning goal is defined as the minimisation of a cost which is a function of the expected model performance and the total cost of the labels used. This allows the development of general strategies and specific algorithms for (a) optimal stopping, where the expected cost dictates whether label acquisition should continue (b) empirical evaluation, where the cost is used as a performance metric for a given combination of inference, stopping and sampling methods. Though the main focus of the paper is optimal stopping, we also aim to provide the background for further developments and discussion in the related field of active learning.
cs.LG:The method of defensive forecasting is applied to the problem of prediction with expert advice for binary outcomes. It turns out that defensive forecasting is not only competitive with the Aggregating Algorithm but also handles the case of "second-guessing" experts, whose advice depends on the learner's prediction; this paper assumes that the dependence on the learner's prediction is continuous.
cs.LG:Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms. There are two known varieties of defensive forecasting: "continuous", in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and "randomized", in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized. This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous.
cs.LG:The purpose of this note is to show how the method of maximum entropy in the mean (MEM) may be used to improve parametric estimation when the measurements are corrupted by large level of noise. The method is developed in the context on a concrete example: that of estimation of the parameter in an exponential distribution. We compare the performance of our method with the bayesian and maximum likelihood approaches.
cs.LG:We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.
cs.LG:Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.
cs.LG:Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is $O(n^3)$, where $n$ is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity $O(n^2)$. We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.
cs.LG:Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical
cs.LG:For a classification problem described by the joint density $P(\omega,x)$, models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have been shown to be an optimal similarity measure for nearest neighbor classification. This paper analyzes demonstrates several additional properties of that conditional distribution. The paper first shows that we can reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$ given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class labels, and gives an asymptotically Bayes-optimal classification procedure. It also shows, given such an optimal similarity measure, how to construct a classifier that outperforms the nearest neighbor classifier and achieves Bayes-optimal classification rates. The paper then analyzes Bayesian similarity in a framework where a classifier faces a number of related classification tasks (multitask learning) and illustrates that reconstruction of the class posterior distribution is not possible in general. Finally, the paper identifies a distinct class of classification problems using $P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to solve those problems is the Bayes optimal solution.
cs.LG:Learning machines which have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither the Bayes a posteriori distribution converges to the normal distribution nor the maximum likelihood estimator satisfies asymptotic normality. This is the main reason why it has been difficult to predict their generalization performances from trained states. In this paper, we study four errors, (1) Bayes generalization error, (2) Bayes training error, (3) Gibbs generalization error, and (4) Gibbs training error, and prove that there are mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that Bayes and Gibbs generalization errors are estimated by Bayes and Gibbs training errors, and propose widely applicable information criteria which can be applied to both regular and singular statistical models.
cs.LG:We consider the problem of choosing a density estimate from a set of distributions F, minimizing the L1-distance to an unknown distribution (Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the problem: Scheffe tournament winner and minimum distance estimate. The Scheffe tournament estimate requires fewer computations than the minimum distance estimate, but has strictly weaker guarantees than the latter.   We focus on the computational aspect of density estimation. We present two algorithms, both with the same guarantee as the minimum distance estimate. The first one, a modification of the minimum distance estimate, uses the same number (quadratic in |F|) of computations as the Scheffe tournament. The second one, called ``efficient minimum loss-weight estimate,'' uses only a linear number of computations, assuming that F is preprocessed.   We also give examples showing that the guarantees of the algorithms cannot be improved and explore randomized algorithms for density estimation.
cs.LG:Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.
cs.LG:The cross-entropy method is a simple but efficient method for global optimization. In this paper we provide two online variants of the basic CEM, together with a proof of convergence.
cs.LG:We prove that the optimal assignment kernel, proposed recently as an attempt to embed labeled graphs and more generally tuples of basic data to a Hilbert space, is in fact not always positive definite.
cs.LG:Given R groups of numerical variables X1, ... XR, we assume that each group is the result of one underlying latent variable, and that all latent variables are bound together through a linear equation system. Moreover, we assume that some explanatory latent variables may interact pairwise in one or more equations. We basically consider PLS Path Modelling's algorithm to estimate both latent variables and the model's coefficients. New "external" estimation schemes are proposed that draw latent variables towards strong group structures in a more flexible way. New "internal" estimation schemes are proposed to enable PLSPM to make good use of variable group complementarity and to deal with interactions. Application examples are given.
cs.LG:We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators from "users" to the "objects" they rate. Recent low-rank type matrix completion approaches to CF are shown to be special cases. However, unlike existing regularization based CF methods, our approach can be used to also incorporate information such as attributes of the users or the objects -- a limitation of existing regularization based CF methods. We then provide novel representer theorems that we use to develop new estimation methods. We provide learning algorithms based on low-rank decompositions, and test them on a standard CF dataset. The experiments indicate the advantages of generalizing the existing regularization based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.
cs.LG:We study the problem of learning k-juntas given access to examples drawn from a number of different product distributions. Thus we wish to learn a function f : {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best known algorithms for the general problem of learning a k-junta require running time of n^k * poly(n,2^k), we show that given access to k different product distributions with biases separated by \gamma>0, the functions may be learned in time poly(n,2^k,\gamma^{-k}). More generally, given access to t <= k different product distributions, the functions may be learned in time n^{k/t} * poly(n,2^k,\gamma^{-k}). Our techniques involve novel results in Fourier analysis relating Fourier expansions with respect to different biases and a generalization of Russo's formula.
cs.LG:The use of computational intelligence techniques for classification has been used in numerous applications. This paper compares the use of a Multi Layer Perceptron Neural Network and a new Relational Network on classifying the HIV status of women at ante-natal clinics. The paper discusses the architecture of the relational network and its merits compared to a neural network and most other computational intelligence classifiers. Results gathered from the study indicate comparable classification accuracies as well as revealed relationships between data features in the classification data. Much higher classification accuracies are recommended for future research in the area of HIV classification as well as missing data estimation.
cs.LG:This paper aims to showcase the measure of structural diversity of an ensemble of 9 classifiers and then map a relationship between this structural diversity and accuracy. The structural diversity was induced by having different architectures or structures of the classifiers The Genetical Algorithms (GA) were used to derive the relationship between diversity and the classification accuracy by evolving the classifiers and then picking 9 classifiers out on an ensemble of 60 classifiers. It was found that as the ensemble became diverse the accuracy improved. However at a certain diversity measure the accuracy began to drop. The Kohavi-Wolpert variance method is used to measure the diversity of the ensemble. A method of voting is used to aggregate the results from each classifier. The lowest error was observed at a diversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024 as maximum diversity measured. The parameters that were varied were: the number of hidden nodes, learning rate and the activation function.
cs.LG:Using a support vector machine requires to set two types of hyperparameters: the soft margin parameter C and the parameters of the kernel. To perform this model selection task, the method of choice is cross-validation. Its leave-one-out variant is known to produce an estimator of the generalization error which is almost unbiased. Its major drawback rests in its time requirement. To overcome this difficulty, several upper bounds on the leave-one-out error of the pattern recognition SVM have been derived. Among those bounds, the most popular one is probably the radius-margin bound. It applies to the hard margin pattern recognition SVM, and by extension to the 2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2, as a direct extension of the 2-norm SVM to the multi-class case. For this machine, a generalized radius-margin bound is then established.
cs.LG:This article considers constrained $\ell_1$ minimization methods for the recovery of high dimensional sparse signals in three settings: noiseless, bounded error and Gaussian noise. A unified and elementary treatment is given in these noise settings for two $\ell_1$ minimization methods: the Dantzig selector and $\ell_1$ minimization with an $\ell_2$ constraint. The results of this paper improve the existing results in the literature by weakening the conditions and tightening the error bounds. The improvement on the conditions shows that signals with larger support can be recovered accurately. This paper also establishes connections between restricted isometry property and the mutual incoherence property. Some results of Candes, Romberg and Tao (2006) and Donoho, Elad, and Temlyakov (2006) are extended.
cs.LG:We identify the classical Perceptron algorithm with margin as a member of a broader family of large margin classifiers which we collectively call the Margitron. The Margitron, (despite its) sharing the same update rule with the Perceptron, is shown in an incremental setting to converge in a finite number of updates to solutions possessing any desirable fraction of the maximum margin. Experiments comparing the Margitron with decomposition SVMs on tasks involving linear kernels and 2-norm soft margin are also reported.
cs.LG:This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of distributional stability which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm.
cs.LG:This article describes an approach to designing a distributed and modular neural classifier. This approach introduces a new hierarchical clustering that enables one to determine reliable regions in the representation space by exploiting supervised information. A multilayer perceptron is then associated with each of these detected clusters and charged with recognizing elements of the associated cluster while rejecting all others. The obtained global classifier is comprised of a set of cooperating neural networks and completed by a K-nearest neighbor classifier charged with treating elements rejected by all the neural networks. Experimental results for the handwritten digit recognition problem and comparison with neural and statistical nonmodular classifiers are given.
cs.LG:Nous pr\'esentons dans cette contribution une approche \`a la fois symbolique et probabiliste permettant d'extraire l'information sur la segmentation du signal de parole \`a partir d'information prosodique. Nous utilisons pour ce faire des grammaires probabilistes poss\'edant une structure hi\'erarchique minimale. La phase de construction des grammaires ainsi que leur pouvoir de pr\'ediction sont \'evalu\'es qualitativement ainsi que quantitativement.   -----   Methodologically oriented, the present work sketches an approach for prosodic information retrieval and speech segmentation, based on both symbolic and probabilistic information. We have recourse to probabilistic grammars, within which we implement a minimal hierarchical structure. Both the stages of probabilistic grammar building and its testing in prediction are explored and quantitatively and qualitatively evaluated.
cs.LG:Statistical learning theory chiefly studies restricted hypothesis classes, particularly those with finite Vapnik-Chervonenkis (VC) dimension. The fundamental quantity of interest is the sample complexity: the number of samples required to learn to a specified level of accuracy. Here we consider learning over the set of all computable labeling functions. Since the VC-dimension is infinite and a priori (uniform) bounds on the number of samples are impossible, we let the learning algorithm decide when it has seen sufficient samples to have learned. We first show that learning in this setting is indeed possible, and develop a learning algorithm. We then show, however, that bounding sample complexity independently of the distribution is impossible. Notably, this impossibility is entirely due to the requirement that the learning algorithm be computable, and not due to the statistical nature of the problem.
cs.LG:We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend on an unknown subset of k<<n variables (so-called k-juntas) is agnostically learnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k}, and log(1/delta). In other words, there is an algorithm with the claimed running time that, given epsilon, delta > 0 and access to a random walk on {-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with probability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f, where opt(f) denotes the distance of a closest k-junta to f.
cs.LG:The method of stable random projections is a tool for efficiently computing the $l_\alpha$ distances using low memory, where $0<\alpha \leq 2$ is a tuning parameter. The method boils down to a statistical estimation task and various estimators have been proposed, based on the geometric mean, the harmonic mean, and the fractional power etc.   This study proposes the optimal quantile estimator, whose main operation is selecting, which is considerably less expensive than taking fractional power, the main operation in previous estimators. Our experiments report that the optimal quantile estimator is nearly one order of magnitude more computationally efficient than previous estimators. For large-scale learning tasks in which storing and computing pairwise distances is a serious bottleneck, this estimator should be desirable.   In addition to its computational advantages, the optimal quantile estimator exhibits nice theoretical properties. It is more accurate than previous estimators when $\alpha>1$. We derive its theoretical error bounds and establish the explicit (i.e., no hidden constants) sample complexity bound.
cs.LG:Applications in machine learning and data mining require computing pairwise Lp distances in a data matrix A. For massive high-dimensional data, computing all pairwise distances of A can be infeasible. In fact, even storing A or all pairwise distances of A in the memory may be also infeasible. This paper proposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where p is even) distances into a sum of 2 marginal norms and p-1 ``inner products'' at different orders. Then we apply normal or sub-Gaussian random projections to approximate the resultant ``inner products,'' assuming that the marginal norms can be computed exactly by a linear scan. We propose two strategies for applying random projections. The basic projection strategy requires only one projection matrix but it is more difficult to analyze, while the alternative projection strategy requires p-1 projection matrices but its theoretical analysis is much easier. In terms of the accuracy, at least for p=4, the basic strategy is always more accurate than the alternative strategy if the data are non-negative, which is common in reality.
cs.LG:We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel \citep{GaeFlaWro03,BorOngSchVisetal05}, marginalized graph kernel \citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04}, and geometric kernel on graphs \citep{Gaertner02}. Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels \citep{KonLaf02}, regularization on graphs \citep{SmoKon03}, and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels \citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized to graphs reduce to the random walk graph kernel.
cs.LG:We study probability distributions over free algebras of trees. Probability distributions can be seen as particular (formal power) tree series [Berstel et al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. We argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. First, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. Note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. Second, the algebraic representation can be easily extended to deal with unranked trees (like XML trees where a symbol may have an unbounded number of children). Both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that Hidden Markov Models are equivalent to nondeterministic string automata); nowadays applications for Web Information Extraction, Web Services and document processing consider unranked trees.
cs.LG:We present a novel graphical framework for modeling non-negative sequential data with hierarchical structure. Our model corresponds to a network of coupled non-negative matrix factorization (NMF) modules, which we refer to as a positive factor network (PFN). The data model is linear, subject to non-negativity constraints, so that observation data consisting of an additive combination of individually representable observations is also representable by the network. This is a desirable property for modeling problems in computational auditory scene analysis, since distinct sound sources in the environment are often well-modeled as combining additively in the corresponding magnitude spectrogram. We propose inference and learning algorithms that leverage existing NMF algorithms and that are straightforward to implement. We present a target tracking example and provide results for synthetic observation data which serve to illustrate the interesting properties of PFNs and motivate their potential usefulness in applications such as music transcription, source separation, and speech recognition. We show how a target process characterized by a hierarchical state transition model can be represented as a PFN. Our results illustrate that a PFN which is defined in terms of a single target observation can then be used to effectively track the states of multiple simultaneous targets. Our results show that the quality of the inferred target states degrades gradually as the observation noise is increased. We also present results for an example in which meaningful hierarchical features are extracted from a spectrogram. Such a hierarchical representation could be useful for music transcription and source separation applications. We also propose a network for language modeling.
cs.LG:We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the inner product then the learned vector is a linear combination of the input data. This result, known as the {\em representer theorem}, is at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, thereby completing the characterization of kernel methods based on regularization. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition for these class of matrix regularizers and highlight them with some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing function.
cs.LG:In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.
cs.LG:We consider the task of learning a classifier from the feature space $\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the features can be partitioned into class-conditionally independent feature sets $\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classifier from $\mathcal{X}_2$ to $\mathcal{X}_1$ and 2) learning the class-conditional distribution of the feature set $\mathcal{X}_1$. This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples. We present experimental evaluation of the idea in two real world applications.
cs.LG:Maximum Variance Unfolding (MVU) and its variants have been very successful in embedding data-manifolds in lower dimensional spaces, often revealing the true intrinsic dimension. In this paper we show how to also incorporate supervised class information into an MVU-like method without breaking its convexity. We call this method the Isometric Separation Map and we show that the resulting kernel matrix can be used as a binary/multiclass Support Vector Machine-like method in a semi-supervised (transductive) framework. We also show that the method always finds a kernel matrix that linearly separates the training data exactly without projecting them in infinite dimensional spaces. In traditional SVMs we choose a kernel and hope that the data become linearly separable in the kernel space. In this paper we show how the hyperplane can be chosen ad-hoc and the kernel is trained so that data are always linearly separable. Comparisons with Large Margin SVMs show comparable performance.
cs.LG:In this paper, I expand Shannon's definition of entropy into a new form of entropy that allows integration of information from different random events. Shannon's notion of entropy is a special case of my more general definition of entropy. I define probability using a so-called performance function, which is de facto an exponential distribution. Assuming that my general notion of entropy reflects the true uncertainty about a probabilistic event, I understand that our perceived uncertainty differs. I claim that our perception is the result of two opposing forces similar to the two famous antagonists in Chinese philosophy: Yin and Yang. Based on this idea, I show that our perceived uncertainty matches the true uncertainty in points determined by the golden ratio. I demonstrate that the well-known sigmoid function, which we typically employ in artificial neural networks as a non-linear threshold function, describes the actual performance. Furthermore, I provide a motivation for the time dilation in Einstein's Special Relativity, basically claiming that although time dilation conforms with our perception, it does not correspond to reality. At the end of the paper, I show how to apply this theoretical framework to practical applications. I present recognition rates for a pattern recognition problem, and also propose a network architecture that can take advantage of general entropy to solve complex decision problems.
cs.LG:Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence.   This paper studies the scenario where the observations are drawn from a stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary phi-mixing and beta-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios.   We also illustrate the application of our phi-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios.
cs.LG:Ensemble classification is an emerging approach to land cover mapping whereby the final classification output is a result of a consensus of classifiers. Intuitively, an ensemble system should consist of base classifiers which are diverse i.e. classifiers whose decision boundaries err differently. In this paper ensemble feature selection is used to impose diversity in ensembles. The features of the constituent base classifiers for each ensemble were created through an exhaustive search algorithm using different separability indices. For each ensemble, the classification accuracy was derived as well as a diversity measure purported to give a measure of the inensemble diversity. The correlation between ensemble classification accuracy and diversity measure was determined to establish the interplay between the two variables. From the findings of this paper, diversity measures as currently formulated do not provide an adequate means upon which to constitute ensembles for land cover mapping.
cs.LG:The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.
cs.LG:We present a convex formulation of dictionary learning for sparse signal decomposition. Convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm. In particular, our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using a large set of synthetic examples, we compare the estimation abilities of the convex and non-convex approaches, showing that while the convex formulation has a single local minimum, this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation.
cs.LG:We introduce a simple and computationally trivial method for binary classification based on the evaluation of potential functions. We demonstrate that despite the conceptual and computational simplicity of the method its performance can match or exceed that of standard Support Vector Machine methods.
cs.LG:We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an $r$-dimensional random vector $\mathbf{Z} \in \mathbb{R}^r$, where $r \geq 2$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\Theta(r \sqrt{T})$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form $O(r \sqrt{T} \log^{3/2} T)$.
cs.LG:We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.
cs.LG:Cooperative decision making is a vision of future network management and control. Distributed connection preemption is an important example where nodes can make intelligent decisions on allocating resources and controlling traffic flows for multi-class service networks. A challenge is that nodal decisions are spatially dependent as traffic flows trespass multiple nodes in a network. Hence the performance-complexity trade-off becomes important, i.e., how accurate decisions are versus how much information is exchanged among nodes. Connection preemption is known to be NP-complete. Centralized preemption is optimal but computationally intractable. Decentralized preemption is computationally efficient but may result in a poor performance. This work investigates distributed preemption where nodes decide whether and which flows to preempt using only local information exchange with neighbors. We develop, based on the probabilistic graphical models, a near-optimal distributed algorithm. The algorithm is used by each node to make collectively near-optimal preemption decisions. We study trade-offs between near-optimal performance and complexity that corresponds to the amount of information-exchange of the distributed algorithm. The algorithm is validated by both analysis and simulation.
cs.LG:In statistical problems, a set of parameterized probability distributions is used to estimate the true probability distribution. If Fisher information matrix at the true distribution is singular, then it has been left unknown what we can estimate about the true distribution from random samples. In this paper, we study a singular regression problem and prove a limit theorem which shows the relation between the singular regression problem and two birational invariants, a real log canonical threshold and a singular fluctuation. The obtained theorem has an important application to statistics, because it enables us to estimate the generalization error from the training error without any knowledge of the true probability distribution.
cs.LG:Scenarios for the emergence or bootstrap of a lexicon involve the repeated interaction between at least two agents who must reach a consensus on how to name N objects using H words. Here we consider minimal models of two types of learning algorithms: cross-situational learning, in which the individuals determine the meaning of a word by looking for something in common across all observed uses of that word, and supervised operant conditioning learning, in which there is strong feedback between individuals about the intended meaning of the words. Despite the stark differences between these learning schemes, we show that they yield the same communication accuracy in the realistic limits of large N and H, which coincides with the result of the classical occupancy problem of randomly assigning N objects to H words.
cs.LG:In this paper, we propose a technique to extract constrained formal concepts.
cs.LG:Recently, different works proposed a new way to mine patterns in databases with pathological size. For example, experiments in genome biology usually provide databases with thousands of attributes (genes) but only tens of objects (experiments). In this case, mining the "transposed" database runs through a smaller search space, and the Galois connection allows to infer the closed patterns of the original database. We focus here on constrained pattern mining for those unusual databases and give a theoretical framework for database and constraint transposition. We discuss the properties of constraint transposition and look into classical constraints. We then address the problem of generating the closed patterns of the original database satisfying the constraint, starting from those mined in the "transposed" database. Finally, we show how to generate all the patterns satisfying the constraint from the closed ones.
cs.LG:We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -- that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.
cs.LG:This paper formalises the concept of learning symbolic rules from multisource data in a cardiac monitoring context. Our sources, electrocardiograms and arterial blood pressure measures, describe cardiac behaviours from different viewpoints. To learn interpretable rules, we use an Inductive Logic Programming (ILP) method. We develop an original strategy to cope with the dimensionality issues caused by using this ILP technique on a rich multisource language. The results show that our method greatly improves the feasibility and the efficiency of the process while staying accurate. They also confirm the benefits of using multiple sources to improve the diagnosis of cardiac arrhythmias.
cs.LG:The problem of completing a low-rank matrix from a subset of its entries is often encountered in the analysis of incomplete data sets exhibiting an underlying factor model with applications in collaborative filtering, computer vision and control. Most recent work had been focused on constructing efficient algorithms for exact or approximate recovery of the missing matrix entries and proving lower bounds for the number of known entries that guarantee a successful recovery with high probability. A related problem from both the mathematical and algorithmic point of view is the distance geometry problem of realizing points in a Euclidean space from a given subset of their pairwise distances. Rigidity theory answers basic questions regarding the uniqueness of the realization satisfying a given partial set of distances. We observe that basic ideas and tools of rigidity theory can be adapted to determine uniqueness of low-rank matrix completion, where inner products play the role that distances play in rigidity theory. This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the completion matrix, that serves as the analogue of the rigidity matrix.
cs.LG:We introduce a new protocol for prediction with expert advice in which each expert evaluates the learner's and his own performance using a loss function that may change over time and may be different from the loss functions used by the other experts. The learner's goal is to perform better or not much worse than each expert, as evaluated by that expert, for all experts simultaneously. If the loss functions used by the experts are all proper scoring rules and all mixable, we show that the defensive forecasting algorithm enjoys the same performance guarantee as that attainable by the Aggregating Algorithm in the standard setting and known to be optimal. This result is also applied to the case of "specialist" (or "sleeping") experts. In this case, the defensive forecasting algorithm reduces to a simple modification of the Aggregating Algorithm.
cs.LG:We present multiplicative updates for solving hard and soft margin support vector machines (SVM) with non-negative kernels. They follow as a natural extension of the updates for non-negative matrix factorization. No additional param- eter setting, such as choosing learning, rate is required. Ex- periments demonstrate rapid convergence to good classifiers. We analyze the rates of asymptotic convergence of the up- dates and establish tight bounds. We test the performance on several datasets using various non-negative kernels and report equivalent generalization errors to that of a standard SVM.
cs.LG:Collecting large labeled data sets is a laborious and expensive task, whose scaling up requires division of the labeling workload between many teachers. When the number of classes is large, miscorrespondences between the labels given by the different teachers are likely to occur, which, in the extreme case, may reach total inconsistency. In this paper we describe how globally consistent labels can be obtained, despite the absence of teacher coordination, and discuss the possible efficiency of this process in terms of human labor. We define a notion of label efficiency, measuring the ratio between the number of globally consistent labels obtained and the number of labels provided by distributed teachers. We show that the efficiency depends critically on the ratio alpha between the number of data instances seen by a single teacher, and the number of classes. We suggest several algorithms for the distributed labeling problem, and analyze their efficiency as a function of alpha. In addition, we provide an upper bound on label efficiency for the case of completely uncoordinated teachers, and show that efficiency approaches 0 as the ratio between the number of labels each teacher provides and the number of classes drops (i.e. alpha goes to 0).
cs.LG:This paper has been retracted.
cs.LG:A $p$-adic modification of the split-LBG classification method is presented in which first clusterings and then cluster centers are computed which locally minimise an energy function. The outcome for a fixed dataset is independent of the prime number $p$ with finitely many exceptions. The methods are applied to the construction of $p$-adic classifiers in the context of learning.
cs.LG:This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It also shows that a number of widely used transductive regression algorithms are in fact unstable. Finally, it reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, for one of the algorithms, in particular for determining the radius of the local neighborhood used by the algorithm.
cs.LG:Motivation: Several different threads of research have been proposed for modeling and mining temporal data. On the one hand, approaches such as dynamic Bayesian networks (DBNs) provide a formal probabilistic basis to model relationships between time-indexed random variables but these models are intractable to learn in the general case. On the other, algorithms such as frequent episode mining are scalable to large datasets but do not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature.   Results: We present a unification of these two seemingly diverse threads of research, by demonstrating how dynamic (discrete) Bayesian networks can be inferred from the results of frequent episode mining. This helps bridge the modeling emphasis of the former with the counting emphasis of the latter. First, we show how, under reasonable assumptions on data characteristics and on influences of random variables, the optimal DBN structure can be computed using a greedy, local, algorithm. Next, we connect the optimality of the DBN structure with the notion of fixed-delay episodes and their counts of distinct occurrences. Finally, to demonstrate the practical feasibility of our approach, we focus on a specific (but broadly applicable) class of networks, called excitatory networks, and show how the search for the optimal DBN structure can be conducted using just information from frequent episodes. Application on datasets gathered from mathematical models of spiking neurons as well as real neuroscience datasets are presented.   Availability: Algorithmic implementations, simulator codebases, and datasets are available from our website at http://neural-code.cs.vt.edu/dbn
cs.LG:Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).
cs.LG:In this paper, we consider the coherent theory of (epistemic) uncertainty of Walley, in which beliefs are represented through sets of probability distributions, and we focus on the problem of modeling prior ignorance about a categorical random variable. In this setting, it is a known result that a state of prior ignorance is not compatible with learning. To overcome this problem, another state of beliefs, called \emph{near-ignorance}, has been proposed. Near-ignorance resembles ignorance very closely, by satisfying some principles that can arguably be regarded as necessary in a state of ignorance, and allows learning to take place. What this paper does, is to provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is \emph{latent}. We argue that such a setting is by far the most common case in practice, and we provide, for the case of categorical latent variables (and general \emph{manifest} variables) a condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied even in the most common statistical problems. We regard these results as a strong form of evidence against the possibility to adopt a condition of prior near-ignorance in real statistical problems.
cs.LG:Engine assembly is a complex and heavily automated distributed-control process, with large amounts of faults data logged everyday. We describe an application of temporal data mining for analyzing fault logs in an engine assembly plant. Frequent episode discovery framework is a model-free method that can be used to deduce (temporal) correlations among events from the logs in an efficient manner. In addition to being theoretically elegant and computationally efficient, frequent episodes are also easy to interpret in the form actionable recommendations. Incorporation of domain-specific information is critical to successful application of the method for analyzing fault logs in the manufacturing domain. We show how domain-specific knowledge can be incorporated using heuristic rules that act as pre-filters and post-filters to frequent episode discovery. The system described here is currently being used in one of the engine assembly plants of General Motors and is planned for adaptation in other plants. To the best of our knowledge, this paper presents the first real, large-scale application of temporal data mining in the manufacturing domain. We believe that the ideas presented in this paper can help practitioners engineer tools for analysis in other similar or related application domains as well.
cs.LG:This paper presents a new hybrid learning algorithm for unsupervised classification tasks. We combined Fuzzy c-means learning algorithm and a supervised version of Minimerror to develop a hybrid incremental strategy allowing unsupervised classifications. We applied this new approach to a real-world database in order to know if the information contained in unlabeled features of a Geographic Information System (GIS), allows to well classify it. Finally, we compared our results to a classical supervised classification obtained by a multilayer perceptron.
cs.LG:We analyze the expected cost of a greedy active learning algorithm. Our analysis extends previous work to a more general setting in which different queries have different costs. Moreover, queries may have more than two possible responses and the distribution over hypotheses may be non uniform. Specific applications include active learning with label costs, active learning for multiclass and partial label queries, and batch mode active learning. We also discuss an approximate version of interest when there are very many queries.
cs.LG:We present three related ways of using Transfer Learning to improve feature selection. The three methods address different problems, and hence share different kinds of information between tasks or feature classes, but all three are based on the information theoretic Minimum Description Length (MDL) principle and share the same underlying Bayesian interpretation. The first method, MIC, applies when predictive models are to be built simultaneously for multiple tasks (``simultaneous transfer'') that share the same set of features. MIC allows each feature to be added to none, some, or all of the task models and is most beneficial for selecting a small set of predictive features from a large pool of features, as is common in genomic and biological datasets. Our second method, TPC (Three Part Coding), uses a similar methodology for the case when the features can be divided into feature classes. Our third method, Transfer-TPC, addresses the ``sequential transfer'' problem in which the task to which we want to transfer knowledge may not be known in advance and may have different amounts of data than the other tasks. Transfer-TPC is most beneficial when we want to transfer knowledge between tasks which have unequal amounts of labeled data, for example the data for disambiguating the senses of different verbs. We demonstrate the effectiveness of these approaches with experimental results on real world data pertaining to genomics and to Word Sense Disambiguation (WSD).
cs.LG:Many learning machines that have hierarchical structure or hidden variables are now being used in information science, artificial intelligence, and bioinformatics. However, several learning machines used in such fields are not regular but singular statistical models, hence their generalization performance is still left unknown. To overcome these problems, in the previous papers, we proved new equations in statistical learning, by which we can estimate the Bayes generalization loss from the Bayes training loss and the functional variance, on the condition that the true distribution is a singularity contained in a learning machine. In this paper, we prove that the same equations hold even if a true distribution is not contained in a parametric model. Also we prove that, the proposed equations in a regular case are asymptotically equivalent to the Takeuchi information criterion. Therefore, the proposed equations are always applicable without any condition on the unknown true distribution.
cs.LG:The problem of classifying sonar signals from rocks and mines first studied by Gorman and Sejnowski has become a benchmark against which many learning algorithms have been tested. We show that both the training set and the test set of this benchmark are linearly separable, although with different hyperplanes. Moreover, the complete set of learning and test patterns together, is also linearly separable. We give the weights that separate these sets, which may be used to compare results found by other algorithms.
cs.LG:Clusters of genes that have evolved by repeated segmental duplication present difficult challenges throughout genomic analysis, from sequence assembly to functional analysis. Improved understanding of these clusters is of utmost importance, since they have been shown to be the source of evolutionary innovation, and have been linked to multiple diseases, including HIV and a variety of cancers. Previously, Zhang et al. (2008) developed an algorithm for reconstructing parsimonious evolutionary histories of such gene clusters, using only human genomic sequence data. In this paper, we propose a probabilistic model for the evolution of gene clusters on a phylogeny, and an MCMC algorithm for reconstruction of duplication histories from genomic sequences in multiple species. Several projects are underway to obtain high quality BAC-based assemblies of duplicated clusters in multiple species, and we anticipate that our method will be useful in analyzing these valuable new data sets.
cs.LG:In this paper, we present two classes of Bayesian approaches to the two-sample problem. Our first class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as flexible nonparametric priors over the unknown distributions.
cs.LG:In this paper, we present the step by step knowledge acquisition process by choosing a structured method through using a questionnaire as a knowledge acquisition tool. Here we want to depict the problem domain as, how to evaluate teachers performance in higher education through the use of expert system technology. The problem is how to acquire the specific knowledge for a selected problem efficiently and effectively from human experts and encode it in the suitable computer format. Acquiring knowledge from human experts in the process of expert systems development is one of the most common problems cited till yet. This questionnaire was sent to 87 domain experts within all public and private universities in Pakistani. Among them 25 domain experts sent their valuable opinions. Most of the domain experts were highly qualified, well experienced and highly responsible persons. The whole questionnaire was divided into 15 main groups of factors, which were further divided into 99 individual questions. These facts were analyzed further to give a final shape to the questionnaire. This knowledge acquisition technique may be used as a learning tool for further research work.
cs.LG:We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.
cs.LG:This paper has been withdrawn due to an error found by Dana Angluin and Lev Reyzin.
cs.LG:We learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks. We exploit the intuition that for domain adaptation, we wish to share classifier structure, but for multitask learning, we wish to share covariance structure. Our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets.
cs.LG:We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to define distributions over the countably infinite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these "reference types") that are generic across all clusters. Inference in our framework, which requires integrating over infinitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple--but general--parameterization of our model based on a Gaussian assumption. We evaluate this model on one artificial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics.
cs.LG:Dirichlet process (DP) mixture models provide a flexible Bayesian framework for density estimation. Unfortunately, their flexibility comes at a cost: inference in DP mixture models is computationally expensive, even when conjugate distributions are used. In the common case when one seeks only a maximum a posteriori assignment of data points to clusters, we show that search algorithms provide a practical alternative to expensive MCMC and variational techniques. When a true posterior sample is desired, the solution found by search can serve as a good initializer for MCMC. Experimental results show that using these techniques is it possible to apply DP mixture models to very large data sets.
cs.LG:The maze traversal problem (finding the shortest distance to the goal from any position in a maze) has been an interesting challenge in computational intelligence. Recent work has shown that the cellular simultaneous recurrent neural network (CSRN) can solve this problem for simple mazes. This thesis focuses on exploiting relevant information about the maze to improve learning and decrease the training time for the CSRN to solve mazes. Appropriate variables are identified to create useful clusters using relevant information. The CSRN was next modified to allow for an additional external input. With this additional input, several methods were tested and results show that clustering the mazes improves the overall learning of the traversal problem for the CSRN.
cs.LG:We propose a randomized algorithm for training Support vector machines(SVMs) on large datasets. By using ideas from Random projections we show that the combinatorial dimension of SVMs is $O({log} n)$ with high probability. This estimate of combinatorial dimension is used to derive an iterative algorithm, called RandSVM, which at each step calls an existing solver to train SVMs on a randomly chosen subset of size $O({log} n)$. The algorithm has probabilistic guarantees and is capable of training SVMs with Kernels for both classification and regression problems. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up existing SVM learners, without loss of accuracy.
cs.LG:We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.
cs.LG:In Data Mining, the usefulness of association rules is strongly limited by the huge amount of delivered rules. In this paper we propose a new approach to prune and filter discovered rules. Using Domain Ontologies, we strengthen the integration of user knowledge in the post-processing task. Furthermore, an interactive and iterative framework is designed to assist the user along the analyzing task. On the one hand, we represent user domain knowledge using a Domain Ontology over database. On the other hand, a novel technique is suggested to prune and to filter discovered rules. The proposed framework was applied successfully over the client database provided by Nantes Habitat.
cs.LG:Gaussian processes (GPs) provide a probabilistic nonparametric representation of functions in regression, classification, and other problems. Unfortunately, exact learning with GPs is intractable for large datasets. A variety of approximate GP methods have been proposed that essentially map the large dataset into a small set of basis points. The most advanced of these, the variable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have its own length scale. However, VSGP was only derived for regression. We describe how VSGP can be applied to classification and other problems, by deriving it as an expectation propagation algorithm. In this view, sparse GP approximations correspond to a KL-projection of the true posterior onto a compact exponential family of GPs. VSGP constitutes one such family, and we show how to enlarge this family to get additional accuracy. In particular, we show that endowing each basis point with its own full covariance matrix provides a significant increase in approximation power.
cs.LG:In this paper we discuss the techniques involved in the design of the famous statistical spam filters that include Naive Bayes, Term Frequency-Inverse Document Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes Additive Regression Tree. We compare these techniques with each other in terms of accuracy, recall, precision, etc. Further, we discuss the effectiveness and limitations of statistical filters in filtering out various types of spam from legitimate e-mails.
cs.LG:We study the problem of online regression. We prove a theoretical bound on the square loss of Ridge Regression. We do not make any assumptions about input vectors or outcomes. We also show that Bayesian Ridge Regression can be thought of as an online algorithm competing with all the Gaussian linear experts.
cs.LG:We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on $n$-point nominal data. Anomalies are declared whenever the score of a test sample falls below $\alpha$, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, $\alpha$, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.
cs.LG:In this paper, we prove a crucial theorem called Mirroring Theorem which affirms that given a collection of samples with enough information in it such that it can be classified into classes and subclasses then (i) There exists a mapping which classifies and subclassifies these samples (ii) There exists a hierarchical classifier which can be constructed by using Mirroring Neural Networks (MNNs) in combination with a clustering algorithm that can approximate this mapping. Thus, the proof of the Mirroring theorem provides a theoretical basis for the existence and a practical feasibility of constructing hierarchical classifiers, given the maps. Our proposed Mirroring Theorem can also be considered as an extension to Kolmogrovs theorem in providing a realistic solution for unsupervised classification. The techniques we develop, are general in nature and have led to the construction of learning machines which are (i) tree like in structure, (ii) modular (iii) with each module running on a common algorithm (tandem algorithm) and (iv) selfsupervised. We have actually built the architecture, developed the tandem algorithm of such a hierarchical classifier and demonstrated it on an example problem.
cs.LG:This paper describes a methodology for detecting anomalies from sequentially observed and potentially noisy data. The proposed approach consists of two main elements: (1) {\em filtering}, or assigning a belief or likelihood to each successive measurement based upon our ability to predict it from previous noisy observations, and (2) {\em hedging}, or flagging potential anomalies by comparing the current belief against a time-varying and data-adaptive threshold. The threshold is adjusted based on the available feedback from an end user. Our algorithms, which combine universal prediction with recent work on online convex programming, do not require computing posterior distributions given all current observations and involve simple primal-dual parameter updates. At the heart of the proposed approach lie exponential-family models which can be used in a wide variety of contexts and applications, and which yield methods that achieve sublinear per-round regret against both static and slowly varying product distributions with marginals drawn from the same exponential family. Moreover, the regret against static distributions coincides with the minimax value of the corresponding online strongly convex game. We also prove bounds on the number of mistakes made during the hedging step relative to the best offline choice of the threshold with access to all estimated beliefs and feedback signals. We validate the theory on synthetic data drawn from a time-varying distribution over binary vectors of high dimensionality, as well as on the Enron email dataset.
cs.LG:We present in this paper a study on the ability and the benefits of using a keystroke dynamics authentication method for collaborative systems. Authentication is a challenging issue in order to guarantee the security of use of collaborative systems during the access control step. Many solutions exist in the state of the art such as the use of one time passwords or smart-cards. We focus in this paper on biometric based solutions that do not necessitate any additional sensor. Keystroke dynamics is an interesting solution as it uses only the keyboard and is invisible for users. Many methods have been published in this field. We make a comparative study of many of them considering the operational constraints of use for collaborative systems.
cs.LG:This document describes concisely the ubiquitous class of exponential family distributions met in statistics. The first part recalls definitions and summarizes main properties and duality with Bregman divergences (all proofs are skipped). The second part lists decompositions and related formula of common exponential family distributions. We recall the Fisher-Rao-Riemannian geometries and the dual affine connection information geometries of statistical manifolds. It is intended to maintain and update this document and catalog by adding new distribution items.
cs.LG:One of the most popular algorithms for clustering in Euclidean space is the $k$-means algorithm; $k$-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is {\em well-clustered}. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of $k$-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian -- or, in other words, when the input comes from a mixture of Gaussians.   We analyze three aspects of the $k$-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components.   Finally, we study the sample requirement of $k$-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of $k$-means is {\em near-optimal}.
cs.LG:In this paper, we consider delay-optimal power and subcarrier allocation design for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base station. There are $K$ queues at the base station for the downlink traffic to the $K$ mobiles with heterogeneous packet arrivals and delay requirements. We shall model the problem as a $K$-dimensional infinite horizon average reward Markov Decision Problem (MDP) where the control actions are assumed to be a function of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). This problem is challenging because it corresponds to a stochastic Network Utility Maximization (NUM) problem where general solution is still unknown. We propose an {\em online stochastic value iteration} solution using {\em stochastic approximation}. The proposed power control algorithm, which is a function of both the CSI and the QSI, takes the form of multi-level water-filling. We prove that under two mild conditions in Theorem 1 (One is the stepsize condition. The other is the condition on accessibility of the Markov Chain, which can be easily satisfied in most of the cases we are interested.), the proposed solution converges to the optimal solution almost surely (with probability 1) and the proposed framework offers a possible solution to the general stochastic NUM problem. By exploiting the birth-death structure of the queue dynamics, we obtain a reduced complexity decomposed solution with linear $\mathcal{O}(KN_F)$ complexity and $\mathcal{O}(K)$ memory requirement.
cs.LG:Association rule mining plays vital part in knowledge mining. The difficult task is discovering knowledge or useful rules from the large number of rules generated for reduced support. For pruning or grouping rules, several techniques are used such as rule structure cover methods, informative cover methods, rule clustering, etc. Another way of selecting association rules is based on interestingness measures such as support, confidence, correlation, and so on. In this paper, we study how rule clusters of the pattern Xi - Y are distributed over different interestingness measures.
cs.LG:This paper presents a tumor detection algorithm from mammogram. The proposed system focuses on the solution of two problems. One is how to detect tumors as suspicious regions with a very weak contrast to their background and another is how to extract features which categorize tumors. The tumor detection method follows the scheme of (a) mammogram enhancement. (b) The segmentation of the tumor area. (c) The extraction of features from the segmented tumor area. (d) The use of SVM classifier. The enhancement can be defined as conversion of the image quality to a better and more understandable level. The mammogram enhancement procedure includes filtering, top hat operation, DWT. Then the contrast stretching is used to increase the contrast of the image. The segmentation of mammogram images has been playing an important role to improve the detection and diagnosis of breast cancer. The most common segmentation method used is thresholding. The features are extracted from the segmented breast area. Next stage include, which classifies the regions using the SVM classifier. The method was tested on 75 mammographic images, from the mini-MIAS database. The methodology achieved a sensitivity of 88.75%.
cs.LG:Among all the partition based clustering algorithms K-means is the most popular and well known method. It generally shows impressive results even in considerably large data sets. The computational complexity of K-means does not suffer from the size of the data set. The main disadvantage faced in performing this clustering is that the selection of initial means. If the user does not have adequate knowledge about the data set, it may lead to erroneous results. The algorithm Automatic Initialization of Means (AIM), which is an extension to K-means, has been proposed to overcome the problem of initial mean generation. In this paper an attempt has been made to compare the performance of the algorithms through implementation
cs.LG:Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.
cs.LG:In this paper we consider the problem of reconstructing a hidden weighted hypergraph of constant rank using additive queries. We prove the following: Let $G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$ hyperedges. For any $m$ there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log n}{\log m}) $$ additive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal Query Complexity Bounds for Finding Graphs. {\em STOC}, 749--758,~2008].   When the weights of the hypergraph are integers that are less than $O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for unweighted hypergraphs) there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log \frac{n^d}{m}}{\log m}). $$ additive queries.   Using the information theoretic bound the above query complexities are tight.
cs.LG:Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.
cs.LG:Discovering latent representations of the observed world has become increasingly more relevant in data analysis. Much of the effort concentrates on building latent variables which can be used in prediction problems, such as classification and regression. A related goal of learning latent structure from data is that of identifying which hidden common causes generate the observations, such as in applications that require predicting the effect of policies. This will be the main problem tackled in our contribution: given a dataset of indicators assumed to be generated by unknown and unmeasured common causes, we wish to discover which hidden common causes are those, and how they generate our data. This is possible under the assumption that observed variables are linear functions of the latent causes with additive noise. Previous results in the literature present solutions for the case where each observed variable is a noisy function of a single latent variable. We show how to extend the existing results for some cases where observed variables measure more than one latent variable.
cs.LG:Bayes statistics and statistical physics have the common mathematical structure, where the log likelihood function corresponds to the random Hamiltonian. Recently, it was discovered that the asymptotic learning curves in Bayes estimation are subject to a universal law, even if the log likelihood function can not be approximated by any quadratic form. However, it is left unknown what mathematical property ensures such a universal law. In this paper, we define a renormalizable condition of the statistical estimation problem, and show that, under such a condition, the asymptotic learning curves are ensured to be subject to the universal law, even if the true distribution is unrealizable and singular for a statistical model. Also we study a nonrenormalizable case, in which the learning curves have the different asymptotic behaviors from the universal law.
cs.LG:Associative Classifier is a novel technique which is the integration of Association Rule Mining and Classification. The difficult task in building Associative Classifier model is the selection of relevant rules from a large number of class association rules (CARs). A very popular method of ordering rules for selection is based on confidence, support and antecedent size (CSA). Other methods are based on hybrid orderings in which CSA method is combined with other measures. In the present work, we study the effect of using different interestingness measures of Association rules in CAR rule ordering and selection for associative classifier.
cs.LG:This paper presents a framework aimed at monitoring the behavior of aircraft in a given airspace. Nominal trajectories are determined and learned using data driven methods. Standard procedures are used by air traffic controllers (ATC) to guide aircraft, ensure the safety of the airspace, and to maximize the runway occupancy. Even though standard procedures are used by ATC, the control of the aircraft remains with the pilots, leading to a large variability in the flight patterns observed. Two methods to identify typical operations and their variability from recorded radar tracks are presented. This knowledge base is then used to monitor the conformance of current operations against operations previously identified as standard. A tool called AirTrajectoryMiner is presented, aiming at monitoring the instantaneous health of the airspace, in real time. The airspace is "healthy" when all aircraft are flying according to the nominal procedures. A measure of complexity is introduced, measuring the conformance of current flight to nominal flight patterns. When an aircraft does not conform, the complexity increases as more attention from ATC is required to ensure a safe separation between aircraft.
cs.LG:The paper deals with on-line regression settings with signals belonging to a Banach lattice. Our algorithms work in a semi-online setting where all the inputs are known in advance and outcomes are unknown and given step by step. We apply the Aggregating Algorithm to construct a prediction method whose cumulative loss over all the input vectors is comparable with the cumulative loss of any linear functional on the Banach lattice. As a by-product we get an algorithm that takes signals from an arbitrary domain. Its cumulative loss is comparable with the cumulative loss of any predictor function from Besov and Triebel-Lizorkin spaces. We describe several applications of our setting.
cs.LG:The performance in higher secondary school education in India is a turning point in the academic lives of all students. As this academic performance is influenced by many factors, it is essential to develop predictive data mining model for students' performance so as to identify the slow learners and study the influence of the dominant factors on their academic performance. In the present investigation, a survey cum experimental methodology was adopted to generate a database and it was constructed from a primary and a secondary source. While the primary data was collected from the regular students, the secondary data was gathered from the school and office of the Chief Educational Officer (CEO). A total of 1000 datasets of the year 2006 from five different schools in three different districts of Tamilnadu were collected. The raw data was preprocessed in terms of filling up missing values, transforming values in one form into another and relevant attribute/ variable selection. As a result, we had 772 student records, which were used for CHAID prediction model construction. A set of prediction rules were extracted from CHIAD prediction model and the efficiency of the generated CHIAD prediction model was found. The accuracy of the present model was compared with other model and it has been found to be satisfactory.
cs.LG:The recent increase in dimensionality of data has thrown a great challenge to the existing dimensionality reduction methods in terms of their effectiveness. Dimensionality reduction has emerged as one of the significant preprocessing steps in machine learning applications and has been effective in removing inappropriate data, increasing learning accuracy, and improving comprehensibility. Feature redundancy exercises great influence on the performance of classification process. Towards the better classification performance, this paper addresses the usefulness of truncating the highly correlated and redundant attributes. Here, an effort has been made to verify the utility of dimensionality reduction by applying LVQ (Learning Vector Quantization) method on two Benchmark datasets of 'Pima Indian Diabetic patients' and 'Lung cancer patients'.
cs.LG:A key problem in sensor networks is to decide which sensors to query when, in order to obtain the most useful information (e.g., for performing accurate prediction), subject to constraints (e.g., on power and bandwidth). In many applications the utility function is not known a priori, must be learned from data, and can even change over time. Furthermore for large sensor networks solving a centralized optimization problem to select sensors is not feasible, and thus we seek a fully distributed solution. In this paper, we present Distributed Online Greedy (DOG), an efficient, distributed algorithm for repeatedly selecting sensors online, only receiving feedback about the utility of the selected sensors. We prove very strong theoretical no-regret guarantees that apply whenever the (unknown) utility function satisfies a natural diminishing returns property called submodularity. Our algorithm has extremely low communication requirements, and scales well to large sensor deployments. We extend DOG to allow observation-dependent sensor selection. We empirically demonstrate the effectiveness of our algorithm on several real-world sensing tasks.
cs.LG:Recently Kutin and Niyogi investigated several notions of algorithmic stability--a property of a learning map conceptually similar to continuity--showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.
cs.LG:We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.
cs.LG:We introduce a natural generalization of submodular set cover and exact active learning with a finite hypothesis class (query learning). We call this new problem interactive submodular set cover. Applications include advertising in social networks with hidden information. We give an approximation guarantee for a novel greedy algorithm and give a hardness of approximation result which matches up to constant factors. We also discuss negative results for simpler approaches and present encouraging early experimental results.
cs.LG:India is a multi-lingual country where Roman script is often used alongside different Indic scripts in a text document. To develop a script specific handwritten Optical Character Recognition (OCR) system, it is therefore necessary to identify the scripts of handwritten text correctly. In this paper, we present a system, which automatically separates the scripts of handwritten words from a document, written in Bangla or Devanagri mixed with Roman scripts. In this script separation technique, we first, extract the text lines and words from document pages using a script independent Neighboring Component Analysis technique. Then we have designed a Multi Layer Perceptron (MLP) based classifier for script separation, trained with 8 different wordlevel holistic features. Two equal sized datasets, one with Bangla and Roman scripts and the other with Devanagri and Roman scripts, are prepared for the system evaluation. On respective independent text samples, word-level script identification accuracies of 99.29% and 98.43% are achieved.
cs.LG:We address the problem of learning in an online, bandit setting where the learner must repeatedly select among $K$ actions, but only receives partial feedback based on its choices. We establish two new facts: First, using a new algorithm called Exp4.P, we show that it is possible to compete with the best in a set of $N$ experts with probability $1-\delta$ while incurring regret at most $O(\sqrt{KT\ln(N/\delta)})$ over $T$ time steps. The new algorithm is tested empirically in a large-scale, real-world dataset. Second, we give a new algorithm called VE that competes with a possibly infinite set of policies of VC-dimension $d$ while incurring regret at most $O(\sqrt{T(d\ln(T) + \ln (1/\delta))})$ with probability $1-\delta$. These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing supervised learning type guarantees for the contextual bandit setting.
cs.LG:We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared, and modify it only via a single time-dependent parameter. Our algorithm's regret bounds are worst-case optimal, and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent, which means they can exploit the structure of the actual problem instance. Critically, however, our algorithm does not need to know this structure in advance. Rather, we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.
cs.LG:Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.
cs.LG:Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled dataset. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional datasets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever.
cs.LG:A key issue in statistics and machine learning is to automatically select the "right" model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) - for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.
cs.LG:Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufficient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy.
cs.LG:We present an approach to semi-supervised learning based on an exponential family characterization. Our approach generalizes previous work on coupled priors for hybrid generative/discriminative models. Our model is more flexible and natural than previous approaches. Experimental results on several data sets show that our approach also performs better in practice.
cs.LG:In recent years, predicting the user's next request in web navigation has received much attention. An information source to be used for dealing with such problem is the left information by the previous web users stored at the web access log on the web servers. Purposed systems for this problem work based on this idea that if a large number of web users request specific pages of a website on a given session, it can be concluded that these pages are satisfying similar information needs, and therefore they are conceptually related. In this study, a new clustering approach is introduced that employs logical path storing of a website pages as another parameter which is regarded as a similarity parameter and conceptual relation between web pages. The results of simulation have shown that the proposed approach is more than others precise in determining the clusters.
cs.LG:Most Web page classification models typically apply the bag of words (BOW) model to represent the feature space. The original BOW representation, however, is unable to recognize semantic relationships between terms. One possible solution is to apply the topic model approach based on the Latent Dirichlet Allocation algorithm to cluster the term features into a set of latent topics. Terms assigned into the same topic are semantically related. In this paper, we propose a novel hierarchical classification method based on a topic model and by integrating additional term features from neighboring pages. Our hierarchical classification method consists of two phases: (1) feature representation by using a topic model and integrating neighboring pages, and (2) hierarchical Support Vector Machines (SVM) classification model constructed from a confusion matrix. From the experimental results, the approach of using the proposed hierarchical SVM model by integrating current page with neighboring pages via the topic model yielded the best performance with the accuracy equal to 90.33% and the F1 measure of 90.14%; an improvement of 5.12% and 5.13% over the original SVM model, respectively.
cs.LG:We apply the method of defensive forecasting, based on the use of game-theoretic supermartingales, to prediction with expert advice. In the traditional setting of a countable number of experts and a finite number of outcomes, the Defensive Forecasting Algorithm is very close to the well-known Aggregating Algorithm. Not only the performance guarantees but also the predictions are the same for these two methods of fundamentally different nature. We discuss also a new setting where the experts can give advice conditional on the learner's future decision. Both the algorithms can be adapted to the new setting and give the same performance guarantees as in the traditional setting. Finally, we outline an application of defensive forecasting to a setting with several loss functions.
cs.LG:This paper proposes a novel similarity measure for clustering sequential data. We first construct a common state-space by training a single probabilistic model with all the sequences in order to get a unified representation for the dataset. Then, distances are obtained attending to the transition matrices induced by each sequence in that state-space. This approach solves some of the usual overfitting and scalability issues of the existing semi-parametric techniques, that rely on training a model for each sequence. Empirical studies on both synthetic and real-world datasets illustrate the advantages of the proposed similarity measure for clustering sequences.
cs.LG:In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to $2\lambda/n$, where $\lambda$ is the real log canonical threshold and $n$ is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.
cs.LG:We present a solution to the problem of understanding a system that produces a sequence of temporally ordered observations. Our solution is based on generating and interpreting a set of temporal decision rules. A temporal decision rule is a decision rule that can be used to predict or retrodict the value of a decision attribute, using condition attributes that are observed at times other than the decision attribute's time of observation. A rule set, consisting of a set of temporal decision rules with the same decision attribute, can be interpreted by our Temporal Investigation Method for Enregistered Record Sequences (TIMERS) to signify an instantaneous, an acausal or a possibly causal relationship between the condition attributes and the decision attribute. We show the effectiveness of our method, by describing a number of experiments with both synthetic and real temporal data.
cs.LG:In this work we investigate the relationship between Bregman distances and regularized Logistic Regression model. We present a detailed study of Bregman Distance minimization, a family of generalized entropy measures associated with convex functions. We convert the L1-regularized logistic regression into this more general framework and propose a primal-dual method based algorithm for learning the parameters. We pose L1-regularized logistic regression into Bregman distance minimization and then apply non-linear constrained optimization techniques to estimate the parameters of the logistic model.
cs.LG:We describe and analyze efficient algorithms for learning a linear predictor from examples when the learner can only view a few attributes of each training example. This is the case, for instance, in medical research, where each patient participating in the experiment is only willing to go through a small number of tests. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on each training example. We demonstrate the efficiency of our algorithms by showing that when running on digit recognition data, they obtain a high prediction accuracy even when the learner gets to see only four pixels of each image.
cs.LG:We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.
cs.LG:In Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the {\it effective} sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning.
cs.LG:In this paper, we consider the distributive queue-aware power and subband allocation design for a delay-optimal OFDMA uplink system with one base station, $K$ users and $N_F$ independent subbands. Each mobile has an uplink queue with heterogeneous packet arrivals and delay requirements. We model the problem as an infinite horizon average reward Markov Decision Problem (MDP) where the control actions are functions of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the subband allocation Q-factor by the sum of the per-user subband allocation Q-factor and derive a distributive online stochastic learning algorithm to estimate the per-user Q-factor and the Lagrange multipliers (LM) simultaneously and determine the control actions using an auction mechanism. We show that under the proposed auction mechanism, the distributive online learning converges almost surely (with probability 1). For illustration, we apply the proposed distributive stochastic learning framework to an application example with exponential packet size distribution. We show that the delay-optimal power control has the {\em multi-level water-filling} structure where the CSI determines the instantaneous power allocation and the QSI determines the water-level. The proposed algorithm has linear signaling overhead and computational complexity $\mathcal O(KN)$, which is desirable from an implementation perspective.
cs.LG:This paper presents a method for automated healing as part of off-line automated troubleshooting. The method combines statistical learning with constraint optimization. The automated healing aims at locally optimizing radio resource management (RRM) or system parameters of cells with poor performance in an iterative manner. The statistical learning processes the data using Logistic Regression (LR) to extract closed form (functional) relations between Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. These functional relations are then processed by an optimization engine which proposes new parameter values. The advantage of the proposed formulation is the small number of iterations required by the automated healing method to converge, making it suitable for off-line implementation. The proposed method is applied to heal an Inter-Cell Interference Coordination (ICIC) process in a 3G Long Term Evolution (LTE) network which is based on soft-frequency reuse scheme. Numerical simulations illustrate the benefits of the proposed approach.
cs.LG:Although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications such as communications. In this work, we focus our attention on the complex gaussian kernel and its possible application in the complex Kernel LMS algorithm. In order to derive the gradients needed to develop the complex kernel LMS (CKLMS), we employ the powerful tool of Wirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, the notion of Writinger's calculus is extended to include complex RKHSs. Experiments verify that the CKLMS offers significant performance improvements over the traditional complex LMS or Widely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. However, so far, the emphasis has been on batch techniques. It is only recently, that online adaptive techniques have been considered in the context of signal processing tasks. To the best of our knowledge, no kernel-based strategy has been developed, so far, that is able to deal with complex valued signals. In this paper, we take advantage of a technique called complexification of real RKHSs to attack this problem. In order to derive gradients and subgradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool ofWirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, in this paper, the notion of Writinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be used to derive nonlinear stable algorithms, which offer significant performance improvements over the traditional complex LMS orWidely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Semi-supervised support vector machines (S3VMs) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data. Though S3VMs have been found helpful in many situations, they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only. In this paper, we try to reduce the chance of performance degeneration of S3VMs. Our basic idea is that, rather than exploiting all unlabeled data, the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited, while some highly risky unlabeled instances are avoided. We propose the S3VM-\emph{us} method by using hierarchical clustering to select the unlabeled instances. Experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of S3VM-\emph{us} is much smaller than that of existing S3VMs.
cs.LG:We study prediction with expert advice in the setting where the losses are accumulated with some discounting---the impact of old losses may gradually vanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm for Regression to this case, propose a suitable new variant of exponential weights algorithm, and prove respective loss bounds.
cs.LG:In this paper, we formulate a novel problem for finding blackhole and volcano patterns in a large directed graph. Specifically, a blackhole pattern is a group which is made of a set of nodes in a way such that there are only inlinks to this group from the rest nodes in the graph. In contrast, a volcano pattern is a group which only has outlinks to the rest nodes in the graph. Both patterns can be observed in real world. For instance, in a trading network, a blackhole pattern may represent a group of traders who are manipulating the market. In the paper, we first prove that the blackhole mining problem is a dual problem of finding volcanoes. Therefore, we focus on finding the blackhole patterns. Along this line, we design two pruning schemes to guide the blackhole finding process. In the first pruning scheme, we strategically prune the search space based on a set of pattern-size-independent pruning rules and develop an iBlackhole algorithm. The second pruning scheme follows a divide-and-conquer strategy to further exploit the pruning results from the first pruning scheme. Indeed, a target directed graphs can be divided into several disconnected subgraphs by the first pruning scheme, and thus the blackhole finding can be conducted in each disconnected subgraph rather than in a large graph. Based on these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally, experimental results on real-world data show that the iBlackhole-DC algorithm can be several orders of magnitude faster than the iBlackhole algorithm, which has a huge computational advantage over a brute-force method.
cs.LG:We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is "similar" to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work.
cs.LG:We study online learning when individual instances are corrupted by adversarially chosen random noise. We assume the noise distribution is unknown, and may change over time with no restriction other than having zero mean and bounded variance. Our technique relies on a family of unbiased estimators for non-linear functions, which may be of independent interest. We show that a variant of online gradient descent can learn functions in any dot-product (e.g., polynomial) or Gaussian kernel space with any analytic convex loss function. Our variant uses randomized estimates that need to query a random number of noisy copies of each instance, where with high probability this number is upper bounded by a constant. Allowing such multiple queries cannot be avoided: Indeed, we show that online learning is in general impossible when only one noisy copy of each instance can be accessed.
cs.LG:We consider the question of the stability of evolutionary algorithms to gradual changes, or drift, in the target concept. We define an algorithm to be resistant to drift if, for some inverse polynomial drift rate in the target function, it converges to accuracy 1 -- \epsilon , with polynomial resources, and then stays within that accuracy indefinitely, except with probability \epsilon , at any one time. We show that every evolution algorithm, in the sense of Valiant (2007; 2009), can be converted using the Correlational Query technique of Feldman (2008), into such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean conjunctions, we give bounds on the rates of drift that they can resist. We develop some new evolution algorithms that are resistant to significant drift. In particular, we give an algorithm for evolving linear separators over the spherically symmetric distribution that is resistant to a drift rate of O(\epsilon /n), and another algorithm over the more general product normal distributions that resists a smaller drift rate.   The above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations.
cs.LG:We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the \emph{zero-one} loss function. Unlike most previous formulations which rely on surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural zero-one loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time $\poly(\exp(L\log(L/\epsilon)))$, for $\emph{any}$ distribution, where $L$ is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most $\epsilon$. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in $L$.
cs.LG:This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method.
cs.LG:The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\widetilde{O}(\log\frac{1}{\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\epsilon$ is related to the parameter in Tsybakov noise.
cs.LG:In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.
cs.LG:Exchangeable random variables form an important and well-studied generalization of i.i.d. variables, however simple examples show that no nontrivial concept or function classes are PAC learnable under general exchangeable data inputs $X_1,X_2,\ldots$. Inspired by the work of Berti and Rigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new paradigm, adequate for learning from exchangeable data: predictive PAC learnability. A learning rule $\mathcal L$ for a function class $\mathscr F$ is predictive PAC if for every $\e,\delta>0$ and each function $f\in {\mathscr F}$, whenever $\abs{\sigma}\geq s(\delta,\e)$, we have with confidence $1-\delta$ that the expected difference between $f(X_{n+1})$ and the image of $f\vert\sigma$ under $\mathcal L$ does not exceed $\e$ conditionally on $X_1,X_2,\ldots,X_n$. Thus, instead of learning the function $f$ as such, we are learning to a given accuracy $\e$ the predictive behaviour of $f$ at the future points $X_i(\omega)$, $i>n$ of the sample path. Using de Finetti's theorem, we show that if a universally separable function class $\mathscr F$ is distribution-free PAC learnable under i.i.d. inputs, then it is distribution-free predictive PAC learnable under exchangeable inputs, with a slightly worse sample complexity.
cs.LG:The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.
cs.LG:In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present the first model for dyadic prediction that satisfies several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) it is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to very large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are ordinal and ignore side-information when it is present. Experimental results show that the new method is competitive with state-of-the-art methods for the special cases of collaborative filtering and link prediction, and that it makes accurate predictions on nominal data.
cs.LG:We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. The primary mathematical tool employed in these methods is the notion of the Reproducing Kernel Hilbert Space. However, so far, the emphasis has been on batch techniques. It is only recently, that online techniques have been considered in the context of adaptive signal processing tasks. Moreover, these efforts have only been focussed on real valued data sequences. To the best of our knowledge, no adaptive kernel-based strategy has been developed, so far, for complex valued signals. Furthermore, although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications that deal with complex signals, with Communications being a typical example. In this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called \textit{complexification} of real RKHSs, or complex reproducing kernels, highlighting the use of the complex gaussian kernel. In order to derive gradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool of Wirtinger's Calculus, which has recently attracted attention in the signal processing community. To this end, in this paper, the notion of Wirtinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive several realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS offers significant performance improvements over several linear and nonlinear algorithms, when dealing with nonlinearities.
cs.LG:This paper studies the MINLIP estimator for the identification of Wiener systems consisting of a sequence of a linear FIR dynamical model, and a monotonically increasing (or decreasing) static function. Given $T$ observations, this algorithm boils down to solving a convex quadratic program with $O(T)$ variables and inequality constraints, implementing an inference technique which is based entirely on model complexity control. The resulting estimates of the linear submodel are found to be almost consistent when no noise is present in the data, under a condition of smoothness of the true nonlinearity and local Persistency of Excitation (local PE) of the data. This result is novel as it does not rely on classical tools as a 'linearization' using a Taylor decomposition, nor exploits stochastic properties of the data. It is indicated how to extend the method to cope with noisy data, and empirical evidence contrasts performance of the estimator against other recently proposed techniques.
cs.LG:In response to a 1997 problem of M. Vidyasagar, we state a necessary and sufficient condition for distribution-free PAC learnability of a concept class $\mathscr C$ under the family of all non-atomic (diffuse) measures on the domain $\Omega$. Clearly, finiteness of the classical Vapnik-Chervonenkis dimension of $\mathscr C$ is a sufficient, but no longer necessary, condition. Besides, learnability of $\mathscr C$ under non-atomic measures does not imply the uniform Glivenko-Cantelli property with regard to non-atomic measures. Our learnability criterion is stated in terms of a combinatorial parameter $\VC({\mathscr C}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$ modulo countable sets. The new parameter is obtained by ``thickening up'' single points in the definition of VC dimension to uncountable ``clusters''. Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if every countable subclass of $\mathscr C$ has VC dimension $\leq d$ outside a countable subset of $\Omega$. The new parameter can be also expressed as the classical VC dimension of $\mathscr C$ calculated on a suitable subset of a compactification of $\Omega$. We do not make any measurability assumptions on $\mathscr C$, assuming instead the validity of Martin's Axiom (MA).
cs.LG:We present a new latent-variable model employing a Gaussian mixture integrated with a feature selection procedure (the Bernoulli part of the model) which together form a "Latent Bernoulli-Gauss" distribution. The model is applied to MAP estimation, clustering, feature selection and collaborative filtering and fares favorably with the state-of-the-art latent-variable models.
cs.LG:We address in this paper the problem of multi-channel signal sequence labeling. In particular, we consider the problem where the signals are contaminated by noise or may present some dephasing with respect to their labels. For that, we propose to jointly learn a SVM sample classifier with a temporal filtering of the channels. This will lead to a large margin filtering that is adapted to the specificity of each channel (noise and time-lag). We derive algorithms to solve the optimization problem and we discuss different filter regularizations for automated scaling or selection of channels. Our approach is tested on a non-linear toy example and on a BCI dataset. Results show that the classification performance on these problems can be improved by learning a large margin filtering.
cs.LG:We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a non-recursive function, etc. We further observe that Sontag's ANN is not Glivenko-Cantelli under any input distribution having a non-atomic part.
cs.LG:This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.
cs.LG:Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query `independence day' usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.
cs.LG:Recently, applying the novel data mining techniques for evaluating enterprise financial distress has received much research alternation. Support Vector Machine (SVM) and back propagation neural (BPN) network has been applied successfully in many areas with excellent generalization results, such as rule extraction, classification and evaluation. In this paper, a model based on SVM with Gaussian RBF kernel is proposed here for enterprise financial distress evaluation. BPN network is considered one of the simplest and are most general methods used for supervised training of multilayered neural network. The comparative results show that through the difference between the performance measures is marginal; SVM gives higher precision and lower error rates.
cs.LG:Most image-search approaches today are based on the text based tags associated with the images which are mostly human generated and are subject to various kinds of errors. The results of a query to the image database thus can often be misleading and may not satisfy the requirements of the user. In this work we propose our approach to automate this tagging process of images, where image results generated can be fine filtered based on a probabilistic tagging mechanism. We implement a tool which helps to automate the tagging process by maintaining a training database, wherein the system is trained to identify certain set of input images, the results generated from which are used to create a probabilistic tagging mechanism. Given a certain set of segments in an image it calculates the probability of presence of particular keywords. This probability table is further used to generate the candidate tags for input images.
cs.LG:We present a framework for discriminative sequence classification where the learner works directly in the high dimensional predictor space of all subsequences in the training set. This is possible by employing a new coordinate-descent algorithm coupled with bounding the magnitude of the gradient for selecting discriminative subsequences fast. We characterize the loss functions for which our generic learning algorithm can be applied and present concrete implementations for logistic regression (binomial log-likelihood loss) and support vector machines (squared hinge loss). Application of our algorithm to protein remote homology detection and remote fold recognition results in performance comparable to that of state-of-the-art methods (e.g., kernel support vector machines). Unlike state-of-the-art classifiers, the resulting classification models are simply lists of weighted discriminative subsequences and can thus be interpreted and related to the biological problem.
cs.LG:We present three generalisations of Kernel Principal Components Analysis (KPCA) which incorporate knowledge of the class labels of a subset of the data points. The first, MV-KPCA, penalises within class variances similar to Fisher discriminant analysis. The second, LSKPCA is a hybrid of least squares regression and kernel PCA. The final LR-KPCA is an iteratively reweighted version of the previous which achieves a sigmoid loss function on the labeled points. We provide a theoretical risk bound as well as illustrative experiments on real and toy data sets.
cs.LG:In this paper the sequential prediction problem with expert advice is considered for the case where losses of experts suffered at each step cannot be bounded in advance. We present some modification of Kalai and Vempala algorithm of following the perturbed leader where weights depend on past losses of the experts. New notions of a volume and a scaled fluctuation of a game are introduced. We present a probabilistic algorithm protected from unrestrictedly large one-step losses. This algorithm has the optimal performance in the case when the scaled fluctuations of one-step losses of experts of the pool tend to zero.
cs.LG:In prediction with expert advice the goal is to design online prediction algorithms that achieve small regret (additional loss on the whole data) compared to a reference scheme. In the simplest such scheme one compares to the loss of the best expert in hindsight. A more ambitious goal is to split the data into segments and compare to the best expert on each segment. This is appropriate if the nature of the data changes between segments. The standard fixed-share algorithm is fast and achieves small regret compared to this scheme.   Fixed share treats the experts as black boxes: there are no assumptions about how they generate their predictions. But if the experts are learning, the following question arises: should the experts learn from all data or only from data in their own segment? The original algorithm naturally addresses the first case. Here we consider the second option, which is more appropriate exactly when the nature of the data changes between segments. In general extending fixed share to this second case will slow it down by a factor of T on T outcomes. We show, however, that no such slowdown is necessary if the experts are hidden Markov models.
cs.LG:A problem posed by Freund is how to efficiently track a small pool of experts out of a much larger set. This problem was solved when Bousquet and Warmuth introduced their mixing past posteriors (MPP) algorithm in 2001.   In Freund's problem the experts would normally be considered black boxes. However, in this paper we re-examine Freund's problem in case the experts have internal structure that enables them to learn. In this case the problem has two possible interpretations: should the experts learn from all data or only from the subsequence on which they are being tracked? The MPP algorithm solves the first case. Our contribution is to generalise MPP to address the second option. The results we obtain apply to any expert structure that can be formalised using (expert) hidden Markov models. Curiously enough, for our interpretation there are \emph{two} natural reference schemes: freezing and sleeping. For each scheme, we provide an efficient prediction strategy and prove the relevant loss bound.
cs.LG:We propose a novel feature selection strategy to discover language-independent acoustic features that tend to be responsible for emotions regardless of languages, linguistics and other factors. Experimental results suggest that the language-independent feature subset discovered yields the performance comparable to the full feature set on various emotional speech corpora.
cs.LG:The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results demonstrate the efficiency and effectiveness of the proposed algorithm.
cs.LG:This paper introduces an approach to Reinforcement Learning Algorithm by comparing their immediate rewards using a variation of Q-Learning algorithm. Unlike the conventional Q-Learning, the proposed algorithm compares current reward with immediate reward of past move and work accordingly. Relative reward based Q-learning is an approach towards interactive learning. Q-Learning is a model free reinforcement learning method that used to learn the agents. It is observed that under normal circumstances algorithm take more episodes to reach optimal Q-value due to its normal reward or sometime negative reward. In this new form of algorithm agents select only those actions which have a higher immediate reward signal in comparison to previous one. The contribution of this article is the presentation of new Q-Learning Algorithm in order to maximize the performance of algorithm and reduce the number of episode required to reach optimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20 Grid world deterministic environment and the result for the two forms of Q-Learning Algorithms is given.
cs.LG:We study three families of online convex optimization algorithms: follow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual averaging (RDA), and composite-objective mirror descent. We first prove equivalence theorems that show all of these algorithms are instantiations of a general FTRL update. This provides theoretical insight on previous experimental observations. In particular, even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly, it has been observed that RDA is even more effective at producing sparsity. Our results demonstrate that FOBOS uses subgradient approximations to the L1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two, and outperforms both on a large, real-world dataset.   Our second contribution is a unified analysis which produces regret bounds that match (up to logarithmic terms) or improve the best previously known bounds. This analysis also extends these algorithms in two important ways: we support a more general type of composite objective and we analyze implicit updates, which replace the subgradient approximation of the current loss function with an exact optimization.
cs.LG:We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels - specifically, the gap in per observation probabilities between the most likely labels. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs.   We demonstrate empirically that the hybrid loss typically performs as least as well as - and often better than - both of its constituent losses on variety of tasks. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction and the effects of label dominance on these results.
cs.LG:In this paper we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in a top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy to assess the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node, are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree.
cs.LG:Margin theory provides one of the most popular explanations to the success of \texttt{AdaBoost}, where the central point lies in the recognition that \textit{margin} is the key for characterizing the performance of \texttt{AdaBoost}. This theory has been very influential, e.g., it has been used to argue that \texttt{AdaBoost} usually does not overfit since it tends to enlarge the margin even after the training error reaches zero. Previously the \textit{minimum margin bound} was established for \texttt{AdaBoost}, however, \cite{Breiman1999} pointed out that maximizing the minimum margin does not necessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006} emphasized that the margin distribution rather than minimum margin is crucial to the performance of \texttt{AdaBoost}. In this paper, we first present the \textit{$k$th margin bound} and further study on its relationship to previous work such as the minimum margin bound and Emargin bound. Then, we improve the previous empirical Bernstein bounds \citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on such findings, we defend the margin-based explanation against Breiman's doubts by proving a new generalization error bound that considers exactly the same factors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than \cite{Breiman1999}'s minimum margin bound. By incorporating factors such as average margin and variance, we present a generalization error bound that is heavily related to the whole margin distribution. We also provide margin distribution bounds for generalization error of voting classifiers in finite VC-dimension space.
cs.LG:In this work, we propose a new optimization framework for multiclass boosting learning. In the literature, AdaBoost.MO and AdaBoost.ECC are the two successful multiclass boosting algorithms, which can use binary weak learners. We explicitly derive these two algorithms' Lagrange dual problems based on their regularized loss functions. We show that the Lagrange dual formulations enable us to design totally-corrective multiclass algorithms by using the primal-dual optimization technique. Experiments on benchmark data sets suggest that our multiclass boosting can achieve a comparable generalization capability with state-of-the-art, but the convergence speed is much faster than stage-wise gradient descent boosting. In other words, the new totally corrective algorithms can maximize the margin more aggressively.
cs.LG:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for empirical risk minimization with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n}, this translates to a learning rate of O(RH/n) in the separable (L*=0) case and O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization with a smooth non-negative objective.
cs.LG:Sparse learning has recently received increasing attention in many areas including machine learning, statistics, and applied mathematics. The mixed-norm regularization based on the L1/Lq norm with q > 1 is attractive in many applications of regression and classification in that it facilitates group sparsity in the model. The resulting optimization problem is, however, challenging to solve due to the structure of the L1/Lq -regularization. Existing work deals with special cases including q = 2,infinity, and they cannot be easily extended to the general case. In this paper, we propose an efficient algorithm based on the accelerated gradient method for solving the L1/Lq -regularized problem, which is applicable for all values of q larger than 1, thus significantly extending existing work. One key building block of the proposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our theoretical analysis reveals the key properties of EP1q and illustrates why EP1q for the general q is significantly more challenging to solve than the special cases. Based on our theoretical analysis, we develop an efficient algorithm for EP1q by solving two zero finding problems. Experimental results demonstrate the efficiency of the proposed algorithm.
cs.LG:An instance-weighted variant of the support vector machine (SVM) has attracted considerable attention recently since they are useful in various machine learning tasks such as non-stationary data analysis, heteroscedastic data modeling, transfer learning, learning to rank, and transduction. An important challenge in these scenarios is to overcome the computational bottleneck---instance weights often change dynamically or adaptively, and thus the weighted SVM solutions must be repeatedly computed. In this paper, we develop an algorithm that can efficiently and exactly update the weighted SVM solutions for arbitrary change of instance weights. Technically, this contribution can be regarded as an extension of the conventional solution-path algorithm for a single regularization parameter to multiple instance-weight parameters. However, this extension gives rise to a significant problem that breakpoints (at which the solution path turns) have to be identified in high-dimensional space. To facilitate this, we introduce a parametric representation of instance weights. We also provide a geometric interpretation in weight space using a notion of critical region: a polyhedron in which the current affine solution remains to be optimal. Then we find breakpoints at intersections of the solution path and boundaries of polyhedrons. Through extensive experiments on various practical applications, we demonstrate the usefulness of the proposed algorithm.
cs.LG:Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the model's estimate of the objective and the uncertainty at any given point. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm's performance.
cs.LG:We consider the problem of energy-efficient point-to-point transmission of delay-sensitive data (e.g. multimedia data) over a fading channel. Existing research on this topic utilizes either physical-layer centric solutions, namely power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM); however, there is currently no rigorous and unified framework for simultaneously utilizing both physical-layer centric and system-level techniques to achieve the minimum possible energy consumption, under delay constraints, in the presence of stochastic and a priori unknown traffic and channel conditions. In this report, we propose such a framework. We formulate the stochastic optimization problem as a Markov decision process (MDP) and solve it online using reinforcement learning. The advantages of the proposed online method are that (i) it does not require a priori knowledge of the traffic arrival and channel statistics to determine the jointly optimal power-control, AMC, and DPM policies; (ii) it exploits partial information about the system so that less information needs to be learned than when using conventional reinforcement learning algorithms; and (iii) it obviates the need for action exploration, which severely limits the adaptation speed and run-time performance of conventional reinforcement learning algorithms. Our results show that the proposed learning algorithms can converge up to two orders of magnitude faster than a state-of-the-art learning algorithm for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms.
cs.LG:We propose a focus of attention mechanism to speed up the Perceptron algorithm. Focus of attention speeds up the Perceptron algorithm by lowering the number of features evaluated throughout training and prediction. Whereas the traditional Perceptron evaluates all the features of each example, the Attentive Perceptron evaluates less features for easy to classify examples, thereby achieving significant speedups and small losses in prediction accuracy. Focus of attention allows the Attentive Perceptron to stop the evaluation of features at any interim point and filter the example. This creates an attentive filter which concentrates computation at examples that are hard to classify, and quickly filters examples that are easy to classify.
cs.LG:In this paper, we consider a queue-aware distributive resource control algorithm for two-hop MIMO cooperative systems. We shall illustrate that relay buffering is an effective way to reduce the intrinsic half-duplex penalty in cooperative systems. The complex interactions of the queues at the source node and the relays are modeled as an average-cost infinite horizon Markov Decision Process (MDP). The traditional approach solving this MDP problem involves centralized control with huge complexity. To obtain a distributive and low complexity solution, we introduce a linear structure which approximates the value function of the associated Bellman equation by the sum of per-node value functions. We derive a distributive two-stage two-winner auction-based control policy which is a function of the local CSI and local QSI only. Furthermore, to estimate the best fit approximation parameter, we propose a distributive online stochastic learning algorithm using stochastic approximation theory. Finally, we establish technical conditions for almost-sure convergence and show that under heavy traffic, the proposed low complexity distributive control is global optimal.
cs.LG:To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.
cs.LG:This paper describes algorithms for nonnegative matrix factorization (NMF) with the beta-divergence (beta-NMF). The beta-divergence is a family of cost functions parametrized by a single shape parameter beta that takes the Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito divergence as special cases (beta = 2,1,0, respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization (MM) algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a beta-dependent power exponent. The monotonicity of the heuristic algorithm can however be proven for beta in (0,1) using the proposed auxiliary function. Then we introduce the concept of majorization-equalization (ME) algorithm which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate the faster convergence of the ME approach. The paper also describes how the proposed algorithms can be adapted to two common variants of NMF : penalized NMF (i.e., when a penalty function of the factors is added to the criterion function) and convex-NMF (when the dictionary is assumed to belong to a known subspace).
cs.LG:Hardness results for maximum agreement problems have close connections to hardness results for proper learning in computational learning theory. In this paper we prove two hardness results for the problem of finding a low degree polynomial threshold function (PTF) which has the maximum possible agreement with a given set of labeled examples in $\R^n \times \{-1,1\}.$ We prove that for any constants $d\geq 1, \eps > 0$,   {itemize}   Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a degree-$d$ PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a degree-$d$ PTF that is consistent with a $1-\eps$ fraction of the examples.   It is $\NP$-hard to find a degree-2 PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a halfspace (degree-1 PTF) that is consistent with a $1 - \eps$ fraction of the examples.   {itemize}   These results immediately imply the following hardness of learning results: (i) Assuming the Unique Games Conjecture, there is no better-than-trivial proper learning algorithm that agnostically learns degree-$d$ PTFs under arbitrary distributions; (ii) There is no better-than-trivial learning algorithm that outputs degree-2 PTFs and agnostically learns halfspaces (i.e. degree-1 PTFs) under arbitrary distributions.
cs.LG:A general framework based on Gaussian models and a MAP-EM algorithm is introduced in this paper for solving matrix/table completion problems. The numerical experiments with the standard and challenging movie ratings data show that the proposed approach, based on probably one of the simplest probabilistic models, leads to the results in the same ballpark as the state-of-the-art, at a lower computational cost.
cs.LG:This paper considers the clustering problem for large data sets. We propose an approach based on distributed optimization. The clustering problem is formulated as an optimization problem of maximizing the classification gain. We show that the optimization problem can be reformulated and decomposed into small-scale sub optimization problems by using the Dantzig-Wolfe decomposition method. Generally speaking, the Dantzig-Wolfe method can only be used for convex optimization problems, where the duality gaps are zero. Even though, the considered optimization problem in this paper is non-convex, we prove that the duality gap goes to zero, as the problem size goes to infinity. Therefore, the Dantzig-Wolfe method can be applied here. In the proposed approach, the clustering problem is iteratively solved by a group of computers coordinated by one center processor, where each computer solves one independent small-scale sub optimization problem during each iteration, and only a small amount of data communication is needed between the computers and center processor. Numerical results show that the proposed approach is effective and efficient.
cs.LG:We give sublinear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L2-SVM, for which sublinear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sublinear time algorithms achieving arbitrary approximation factor.
cs.LG:Nesterov's accelerated gradient methods (AGM) have been successfully applied in many machine learning areas. However, their empirical performance on training max-margin models has been inferior to existing specialized solvers. In this paper, we first extend AGM to strongly convex and composite objective functions with Bregman style prox-functions. Our unifying framework covers both the $\infty$-memory and 1-memory styles of AGM, tunes the Lipschiz constant adaptively, and bounds the duality gap. Then we demonstrate various ways to apply this framework of methods to a wide range of machine learning problems. Emphasis will be given on their rate of convergence and how to efficiently compute the gradient and optimize the models. The experimental results show that with our extensions AGM outperforms state-of-the-art solvers on max-margin models.
cs.LG:An importance weight quantifies the relative importance of one example over another, coming up in applications of boosting, asymmetric classification costs, reductions, and active learning. The standard approach for dealing with importance weights in gradient descent is via multiplication of the gradient. We first demonstrate the problems of this approach when importance weights are large, and argue in favor of more sophisticated ways for dealing with them. We then develop an approach which enjoys an invariance property: that updating twice with importance weight $h$ is equivalent to updating once with importance weight $2h$. For many important losses this has a closed form update which satisfies standard regret guarantees when all examples have $h=1$. We also briefly discuss two other reasonable approaches for handling large importance weights. Empirically, these approaches yield substantially superior prediction with similar computational performance while reducing the sensitivity of the algorithm to the exact setting of the learning rate. We apply these to online active learning yielding an extraordinarily fast active learning algorithm that works even in the presence of adversarial noise.
cs.LG:The note presents a modified proof of a loss bound for the exponentially weighted average forecaster with time-varying potential. The regret term of the algorithm is upper-bounded by sqrt{n ln(N)} (uniformly in n), where N is the number of experts and n is the number of steps.
cs.LG:Recommendation systems are emerging as an important business application with significant economic impact. Currently popular systems include Amazon's book recommendations, Netflix's movie recommendations, and Pandora's music recommendations. In this paper we address the problem of estimating probabilities associated with recommendation system data using non-parametric kernel smoothing. In our estimation we interpret missing items as randomly censored observations and obtain efficient computation schemes using combinatorial properties of generating functions. We demonstrate our approach with several case studies involving real world movie recommendation data. The results are comparable with state-of-the-art techniques while also providing probabilistic preference estimates outside the scope of traditional recommender systems.
cs.LG:We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.
cs.LG:In this paper, we propose a two-timescale delay-optimal dynamic clustering and power allocation design for downlink network MIMO systems. The dynamic clustering control is adaptive to the global queue state information (GQSI) only and computed at the base station controller (BSC) over a longer time scale. On the other hand, the power allocations of all the BSs in one cluster are adaptive to both intra-cluster channel state information (CCSI) and intra-cluster queue state information (CQSI), and computed at the cluster manager (CM) over a shorter time scale. We show that the two-timescale delay-optimal control can be formulated as an infinite-horizon average cost Constrained Partially Observed Markov Decision Process (CPOMDP). By exploiting the special problem structure, we shall derive an equivalent Bellman equation in terms of Pattern Selection Q-factor to solve the CPOMDP. To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the Pattern Selection Q-factor by the sum of Per-cluster Potential functions and propose a novel distributive online learning algorithm to estimate the Per-cluster Potential functions (at each CM) as well as the Lagrange multipliers (LM) (at each BS). We show that the proposed distributive online learning algorithm converges almost surely (with probability 1). By exploiting the birth-death structure of the queue dynamics, we further decompose the Per-cluster Potential function into sum of Per-cluster Per-user Potential functions and formulate the instantaneous power allocation as a Per-stage QSI-aware Interference Game played among all the CMs. We also propose a QSI-aware Simultaneous Iterative Water-filling Algorithm (QSIWFA) and show that it can achieve the Nash Equilibrium (NE).
cs.LG:To attain the best learning accuracy, people move on with difficulties and frustrations. Though one can optimize the empirical objective using a given set of samples, its generalization ability to the entire sample distribution remains questionable. Even if a fair generalization guarantee is offered, one still wants to know what is to happen if the regularizer is removed, and/or how well the artificial loss (like the hinge loss) relates to the accuracy.   For such reason, this report surveys four different trials towards the learning accuracy, embracing the major advances in supervised learning theory in the past four years. Starting from the generic setting of learning, the first two trials introduce the best optimization and generalization bounds for convex learning, and the third trial gets rid of the regularizer. As an innovative attempt, the fourth trial studies the optimization when the objective is exactly the accuracy, in the special case of binary classification. This report also analyzes the last trial through experiments.
cs.LG:This report explores the use of machine learning techniques to accurately predict travel times in city streets and highways using floating car data (location information of user vehicles on a road network). The aim of this report is twofold, first we present a general architecture of solving this problem, then present and evaluate few techniques on real floating car data gathered over a month on a 5 Km highway in New Delhi.
cs.LG:This article discusses in detail the rating system that won the kaggle competition "Chess Ratings: Elo vs the rest of the world". The competition provided a historical dataset of outcomes for chess games, and aimed to discover whether novel approaches can predict the outcomes of future games, more accurately than the well-known Elo rating system. The winning rating system, called Elo++ in the rest of the article, builds upon the Elo rating system. Like Elo, Elo++ uses a single rating per player and predicts the outcome of a game, by using a logistic curve over the difference in ratings of the players. The major component of Elo++ is a regularization technique that avoids overfitting these ratings. The dataset of chess games and outcomes is relatively small and one has to be careful not to draw "too many conclusions" out of the limited data. Many approaches tested in the competition showed signs of such an overfitting. The leader-board was dominated by attempts that did a very good job on a small test dataset, but couldn't generalize well on the private hold-out dataset. The Elo++ regularization takes into account the number of games per player, the recency of these games and the ratings of the opponents. Finally, Elo++ employs a stochastic gradient descent scheme for training the ratings, and uses only two global parameters (white's advantage and regularization constant) that are optimized using cross-validation.
cs.LG:An important part of problems in statistical physics and computer science can be expressed as the computation of marginal probabilities over a Markov Random Field. The belief propagation algorithm, which is an exact procedure to compute these marginals when the underlying graph is a tree, has gained its popularity as an efficient way to approximate them in the more general case. In this paper, we focus on an aspect of the algorithm that did not get that much attention in the literature, which is the effect of the normalization of the messages. We show in particular that, for a large class of normalization strategies, it is possible to focus only on belief convergence. Following this, we express the necessary and sufficient conditions for local stability of a fixed point in terms of the graph structure and the beliefs values at the fixed point. We also explicit some connexion between the normalization constants and the underlying Bethe Free Energy.
cs.LG:We consider a retailer selling a single product with limited on-hand inventory over a finite selling season. Customer demand arrives according to a Poisson process, the rate of which is influenced by a single action taken by the retailer (such as price adjustment, sales commission, advertisement intensity, etc.). The relationship between the action and the demand rate is not known in advance. However, the retailer is able to learn the optimal action "on the fly" as she maximizes her total expected revenue based on the observed demand reactions.   Using the pricing problem as an example, we propose a dynamic "learning-while-doing" algorithm that only involves function value estimation to achieve a near-optimal performance. Our algorithm employs a series of shrinking price intervals and iteratively tests prices within that interval using a set of carefully chosen parameters. We prove that the convergence rate of our algorithm is among the fastest of all possible algorithms in terms of asymptotic "regret" (the relative loss comparing to the full information optimal solution). Our result closes the performance gaps between parametric and non-parametric learning and between a post-price mechanism and a customer-bidding mechanism. Important managerial insight from this research is that the values of information on both the parametric form of the demand function as well as each customer's exact reservation price are less important than prior literature suggests. Our results also suggest that firms would be better off to perform dynamic learning and action concurrently rather than sequentially.
cs.LG:This article presents a model which is capable of learning and abstracting new concepts based on comparing observations and finding the resemblance between the observations. In the model, the new observations are compared with the templates which have been derived from the previous experiences. In the first stage, the objects are first represented through a geometric description which is used for finding the object boundaries and a descriptor which is inspired by the human visual system and then they are fed into the model. Next, the new observations are identified through comparing them with the previously-learned templates and are used for producing new templates. The comparisons are made based on measures like Euclidean or correlation distance. The new template is created by applying onion-pealing algorithm. The algorithm consecutively uses convex hulls which are made by the points representing the objects. If the new observation is remarkably similar to one of the observed categories, it is no longer utilized in creating a new template. The existing templates are used to provide a description of the new observation. This description is provided in the templates space. Each template represents a dimension of the feature space. The degree of the resemblance each template bears to each object indicates the value associated with the object in that dimension of the templates space. In this way, the description of the new observation becomes more accurate and detailed as the time passes and the experiences increase. We have used this model for learning and recognizing the new polygons in the polygon space. Representing the polygons was made possible through employing a geometric method and a method inspired by human visual system. Various implementations of the model have been compared. The evaluation results of the model prove its efficiency in learning and deriving new templates.
cs.LG:It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet from a regularization perspective: the EigenNet has an adaptive eigenspace-based composite regularizer, which naturally generalizes the $l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and real data show that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.
cs.LG:Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.
cs.LG:We consider the problem of learning an unknown product distribution $X$ over $\{0,1\}^n$ using samples $f(X)$ where $f$ is a \emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework.   Information-theoretic arguments show that for every transformation function $f$ the corresponding learning problem can be solved to accuracy $\eps$, using $\tilde{O}(n/\eps^2)$ examples, by a generic algorithm whose running time may be exponential in $n.$ We show that this learning problem can be computationally intractable even for constant $\eps$ and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function $f(x)=w \cdot x$ with integer weights $w_i \leq n$ and prove that the corresponding learning problem requires $\Omega(n)$ samples.   As our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function $f(x)= \sum_{i=1}^n x_i$. Our algorithm learns to $\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\eps)$ number of samples that is independent of $n.$ We also give an efficient algorithm that uses $\log n \cdot \poly(1/\eps)$ samples but has running time that is only $\poly(\log n, 1/\eps).$
cs.LG:Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a two-layer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-of-the-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM$^{perf}$ in terms of $F_1$-score.
cs.LG:In this work we study parallelization of online learning, a core primitive in machine learning. In a parallel environment all known approaches for parallel online learning lead to delayed updates, where the model is updated using out-of-date information. In the worst case, or when examples are temporally correlated, delay can have a very adverse effect on the learning algorithm. Here, we analyze and present preliminary empirical results on a set of learning architectures based on a feature sharding approach that present various tradeoffs between delay, degree of parallelism, representation power and empirical performance.
cs.LG:Supervised learning is all about the ability to generalize knowledge. Specifically, the goal of the learning is to train a classifier using training data, in such a way that it will be capable of classifying new unseen data correctly. In order to acheive this goal, it is important to carefully design the learner, so it will not overfit the training data. The later can is done usually by adding a regularization term. The statistical learning theory explains the success of this method by claiming that it restricts the complexity of the learned model. This explanation, however, is rather abstract and does not have a geometric intuition. The generalization error of a classifier may be thought of as correlated with its robustness to perturbations of the data: a classifier that copes with disturbance is expected to generalize well. Indeed, Xu et al. [2009] have shown that the SVM formulation is equivalent to a robust optimization (RO) formulation, in which an adversary displaces the training and testing points within a ball of pre-determined radius. In this work we explore a different kind of robustness, namely changing each data point with a Gaussian cloud centered at the sample. Loss is evaluated as the expectation of an underlying loss function on the cloud. This setup fits the fact that in many applications, the data is sampled along with noise. We develop an RO framework, in which the adversary chooses the covariance of the noise. In our algorithm named GURU, the tuning parameter is a spectral bound on the noise, thus it can be estimated using physical or applicative considerations. Our experiments show that this framework performs as well as SVM and even slightly better in some cases. Generalizations for Mercer kernels and for the multiclass case are presented as well. We also show that our framework may be further generalized, using the technique of convex perspective functions.
cs.LG:We propose a new clustering technique that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.
cs.LG:A fundamental result of statistical learnig theory states that a concept class is PAC learnable if and only if it is a uniform Glivenko-Cantelli class if and only if the VC dimension of the class is finite. However, the theorem is only valid under special assumptions of measurability of the class, in which case the PAC learnability even becomes consistent. Otherwise, there is a classical example, constructed under the Continuum Hypothesis by Dudley and Durst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a concept class of VC dimension one which is neither uniform Glivenko-Cantelli nor consistently PAC learnable. We show that, rather surprisingly, under an additional set-theoretic hypothesis which is much milder than the Continuum Hypothesis (Martin's Axiom), PAC learnability is equivalent to finite VC dimension for every concept class.
cs.LG:Q-learning is a reliable but inefficient off-policy temporal-difference method, backing up reward only one step at a time. Replacing traces, using a recency heuristic, are more efficient but less reliable. In this work, we introduce model-free, off-policy temporal difference methods that make better use of experience than Watkins' Q(\lambda). We introduce both Optimistic Q(\lambda) and the temporal second difference trace (TSDT). TSDT is particularly powerful in deterministic domains. TSDT uses neither recency nor frequency heuristics, storing (s,a,r,s',\delta) so that off-policy updates can be performed after apparently suboptimal actions have been taken. There are additional advantages when using state abstraction, as in MAXQ. We demonstrate that TSDT does significantly better than both Q-learning and Watkins' Q(\lambda) in a deterministic cliff-walking domain. Results in a noisy cliff-walking domain are less advantageous for TSDT, but demonstrate the efficacy of Optimistic Q(\lambda), a replacing trace with some of the advantages of TSDT.
cs.LG:In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.
cs.LG:In this paper we present methods for attacking and defending $k$-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior's or process' $k$-order statistics to build a stochastic process that has those same $k$-order stationary statistics but possesses different, deliberately designed, $(k+1)$-order statistics if desired. Such a model realizes a "complexification" of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed $(k+1)$-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the $k$-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an {\em arms race} in the sense that the advantage goes to the party that has more computing resources applied to the problem.
cs.LG:We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a \emph{perturbed optimization problem} from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.
cs.LG:We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that "dogs are similar to dogs" even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.
cs.LG:We introduce an algorithm that, given n objects, learns a similarity matrix over all n^2 pairs, from crowdsourced data alone. The algorithm samples responses to adaptively chosen triplet-based relative-similarity queries. Each query has the form "is object 'a' more similar to 'b' or to 'c'?" and is chosen to be maximally informative given the preceding responses. The output is an embedding of the objects into Euclidean space (like MDS); we refer to this as the "crowd kernel." SVMs reveal that the crowd kernel captures prominent and subtle features across a number of domains, such as "is striped" among neckties and "vowel vs. consonant" among letters.
cs.LG:In this short note we prove a maximal concentration lemma for sub-Gaussian random variables stating that for independent sub-Gaussian random variables we have \[P<(\max_{1\le i\le N}S_{i}>\epsilon>) \le\exp<(-\frac{1}{N^2}\sum_{i=1}^{N}\frac{\epsilon^{2}}{2\sigma_{i}^{2}}>), \] where $S_i$ is the sum of $i$ zero mean independent sub-Gaussian random variables and $\sigma_i$ is the variance of the $i$th random variable.
cs.LG:We provide a natural learning process in which a financial trader without a risk receives a gain in case when Stock Market is inefficient. In this process, the trader rationally choose his gambles using a prediction made by a randomized calibrated algorithm. Our strategy is based on Dawid's notion of calibration with more general changing checking rules and on some modification of Kakade and Foster's randomized algorithm for computing calibrated forecasts.
cs.LG:We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale e, are explained and a few examples of their calculations are given with proofs. We then explain Sauer's Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distribution-free PAC learnable and it having finite VC dimension.   As the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale e of this new function class in terms of the Fat Shattering dimensions of the collection's classes.   We conclude this report by providing a few open questions and future research topics involving the PAC learning model.
cs.LG:In batch learning, stability together with existence and uniqueness of the solution corresponds to well-posedness of Empirical Risk Minimization (ERM) methods; recently, it was proved that CV_loo stability is necessary and sufficient for generalization and consistency of ERM. In this note, we introduce CV_on stability, which plays a similar note in online learning. We show that stochastic gradient descent (SDG) with the usual hypotheses is CVon stable and we then discuss the implications of CV_on stability for convergence of SGD.
cs.LG:Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our method both outperforms baseline methods and, in comparison to them, is faster and consumes less memory. We then demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.
cs.LG:We propose a nonparametric generalization of belief propagation, Kernel Belief Propagation (KBP), for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS), and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution, nor must their relations take any particular parametric form. Rather, the relations between variables are represented implicitly, and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (Rd, strings, groups), even where explicit parametric models are not known, or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments, we apply KBP to image denoising, depth prediction from still images, and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude, in some cases), while providing significantly more accurate results.
cs.LG:The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.
cs.LG:Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing $x^*$ that minimizes the expression $\sum_{f\in F}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where we wish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$ corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$, and we seek a shape $x$ that minimizes the sum of distances from each point in $P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and $f(x)$ is the distance from $p$ to its closest shape in $x$.   Our main result is a unified framework for constructing {\em coresets} and {\em approximate clustering} for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of $\varepsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation" of the input set $F$. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model).   We show how to generalize the results of our framework for squared distances (as in $k$-mean), distances to the $q$th power, and deterministic constructions.
cs.LG:The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.
cs.LG:This paper introduces a machine learning based collaborative multi-band spectrum sensing policy for cognitive radios. The proposed sensing policy guides secondary users to focus the search of unused radio spectrum to those frequencies that persistently provide them high data rate. The proposed policy is based on machine learning, which makes it adaptive with the temporally and spatially varying radio spectrum. Furthermore, there is no need for dynamic modeling of the primary activity since it is implicitly learned over time. Energy efficiency is achieved by minimizing the number of assigned sensors per each subband under a constraint on miss detection probability. It is important to control the missed detections because they cause collisions with primary transmissions and lead to retransmissions at both the primary and secondary user. Simulations show that the proposed machine learning based sensing policy improves the overall throughput of the secondary network and improves the energy efficiency while controlling the miss detection probability.
cs.LG:This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. In particular, we observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure between the observed variables - separating out the spurious interactions caused by the (marginalizing out of the) latent variables' time series. We develop a new method, based on convex optimization, to do so in the case when the number of latent variables is smaller than the number of observed ones. For the case when the dependency structure between the observed variables is sparse, we theoretically establish a high-dimensional scaling result for structure recovery. We verify our theoretical result with both synthetic and real data (from the stock market).
cs.LG:Estimator algorithms in learning automata are useful tools for adaptive, real-time optimization in computer science and engineering applications. This paper investigates theoretical convergence properties for a special case of estimator algorithms: the pursuit learning algorithm. In this note, we identify and fill a gap in existing proofs of probabilistic convergence for pursuit learning. It is tradition to take the pursuit learning tuning parameter to be fixed in practical applications, but our proof sheds light on the importance of a vanishing sequence of tuning parameters in a theoretical convergence analysis.
cs.LG:One of the major challenges of ECoG-based Brain-Machine Interfaces is the movement prediction of a human subject. Several methods exist to predict an arm 2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is to predict individual finger movements (5-D trajectory). The difficulty lies in the fact that there is no simple relation between ECoG signals and finger movement. We propose in this paper to decode finger flexions using switching models. This method permits to simplify the system as it is now described as an ensemble of linear models depending on an internal state. We show that an interesting accuracy prediction can be obtained by such a model.
cs.LG:Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.
cs.LG:This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using epsilon-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.
cs.LG:Motivated by the amount of code that goes unidentified on the web, we introduce a practical method for algorithmically identifying the programming language of source code. Our work is based on supervised learning and intelligent statistical features. We also explored, but abandoned, a grammatical approach. In testing, our implementation greatly outperforms that of an existing tool that relies on a Bayesian classifier. Code is written in Python and available under an MIT license.
cs.LG:Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.
cs.LG:Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.
cs.LG:We present IBSEAD or distributed autonomous entity systems based Interaction - a learning algorithm for the computer to self-evolve in a self-obsessed manner. This learning algorithm will present the computer to look at the internal and external environment in series of independent entities, which will interact with each other, with and/or without knowledge of the computer's brain. When a learning algorithm interacts, it does so by detecting and understanding the entities in the human algorithm. However, the problem with this approach is that the algorithm does not consider the interaction of the third party or unknown entities, which may be interacting with each other. These unknown entities in their interaction with the non-computer entities make an effect in the environment that influences the information and the behaviour of the computer brain. Such details and the ability to process the dynamic and unsettling nature of these interactions are absent in the current learning algorithm such as the decision tree learning algorithm. IBSEAD is able to evaluate and consider such algorithms and thus give us a better accuracy in simulation of the highly evolved nature of the human brain. Processes such as dreams, imagination and novelty, that exist in humans are not fully simulated by the existing learning algorithms. Also, Hidden Markov models (HMM) are useful in finding "hidden" entities, which may be known or unknown. However, this model fails to consider the case of unknown entities which maybe unclear or unknown. IBSEAD is better because it considers three types of entities- known, unknown and invisible. We present our case with a comparison of existing algorithms in known environments and cases and present the results of the experiments using dry run of the simulated runs of the existing machine learning algorithms versus IBSEAD.
cs.LG:In this paper, we correct an upper bound, presented in~\cite{hs-11}, on the generalisation error of classifiers learned through multiple kernel learning. The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive} dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a \emph{multiplicative} dependence on the logarithm of the number of kernels and the margin achieved by the classifier.
cs.LG:Machine Learning (ML) techniques are indispensable in a wide range of fields. Unfortunately, the exponential increase of dataset sizes are rapidly extending the runtime of sequential algorithms and threatening to slow future progress in ML. With the promise of affordable large-scale parallel computing, Cloud systems offer a viable platform to resolve the computational challenges in ML. However, designing and implementing efficient, provably correct distributed ML algorithms is often prohibitively challenging. To enable ML researchers to easily and efficiently use parallel systems, we introduced the GraphLab abstraction which is designed to represent the computational patterns in ML algorithms while permitting efficient parallel and distributed implementations. In this paper we provide a formal description of the GraphLab parallel abstraction and present an efficient distributed implementation. We conduct a comprehensive evaluation of GraphLab on three state-of-the-art ML algorithms using real large-scale data and a 64 node EC2 cluster of 512 processors. We find that GraphLab achieves orders of magnitude performance gains over Hadoop while performing comparably or superior to hand-tuned MPI implementations.
cs.LG:For large scale learning problems, it is desirable if we can obtain the optimal model parameters by going through the data in only one pass. Polyak and Juditsky (1992) showed that asymptotically the test performance of the simple average of the parameters obtained by stochastic gradient descent (SGD) is as good as that of the parameters which minimize the empirical cost. However, to our knowledge, despite its optimal asymptotic convergence rate, averaged SGD (ASGD) received little attention in recent research on large scale learning. One possible reason is that it may take a prohibitively large number of training samples for ASGD to reach its asymptotic region for most real problems. In this paper, we present a finite sample analysis for the method of Polyak and Juditsky (1992). Our analysis shows that it indeed usually takes a huge number of samples for ASGD to reach its asymptotic region for improperly chosen learning rate. More importantly, based on our analysis, we propose a simple way to properly set learning rate so that it takes a reasonable amount of data for ASGD to reach its asymptotic region. We compare ASGD using our proposed learning rate with other well known algorithms for training large scale linear classifiers. The experiments clearly show the superiority of ASGD.
cs.LG:Discovering pattern sets or global patterns is an attractive issue from the pattern mining community in order to provide useful information. By combining local patterns satisfying a joint meaning, this approach produces patterns of higher level and thus more useful for the data analyst than the usual local patterns, while reducing the number of patterns. In parallel, recent works investigating relationships between data mining and constraint programming (CP) show that the CP paradigm is a nice framework to model and mine such patterns in a declarative and generic way. We present a constraint-based language which enables us to define queries addressing patterns sets and global patterns. The usefulness of such a declarative approach is highlighted by several examples coming from the clustering based on associations. This language has been implemented in the CP framework.
cs.LG:We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.
cs.LG:This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a "value iteration" scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP.
cs.LG:We provide a formal, simple and intuitive theory of rational decision making including sequential decisions that affect the environment. The theory has a geometric flavor, which makes the arguments easy to visualize and understand. Our theory is for complete decision makers, which means that they have a complete set of preferences. Our main result shows that a complete rational decision maker implicitly has a probabilistic model of the environment. We have a countable version of this result that brings light on the issue of countable vs finite additivity by showing how it depends on the geometry of the space which we have preferences over. This is achieved through fruitfully connecting rationality with the Hahn-Banach Theorem. The theory presented here can be viewed as a formalization and extension of the betting odds approach to probability of Ramsey and De Finetti.
cs.LG:Building biological models by inferring functional dependencies from experimental data is an im- portant issue in Molecular Biology. To relieve the biologist from this traditionally manual process, various approaches have been proposed to increase the degree of automation. However, available ap- proaches often yield a single model only, rely on specific assumptions, and/or use dedicated, heuris- tic algorithms that are intolerant to changing circumstances or requirements in the view of the rapid progress made in Biotechnology. Our aim is to provide a declarative solution to the problem by ap- peal to Answer Set Programming (ASP) overcoming these difficulties. We build upon an existing approach to Automatic Network Reconstruction proposed by part of the authors. This approach has firm mathematical foundations and is well suited for ASP due to its combinatorial flavor providing a characterization of all models explaining a set of experiments. The usage of ASP has several ben- efits over the existing heuristic algorithms. First, it is declarative and thus transparent for biological experts. Second, it is elaboration tolerant and thus allows for an easy exploration and incorporation of biological constraints. Third, it allows for exploring the entire space of possible models. Finally, our approach offers an excellent performance, matching existing, special-purpose systems.
cs.LG:Detecting changes in high-dimensional time series is difficult because it involves the comparison of probability densities that need to be estimated from finite samples. In this paper, we present the first feature extraction method tailored to change point detection, which is based on an extended version of Stationary Subspace Analysis. We reduce the dimensionality of the data to the most non-stationary directions, which are most informative for detecting state changes in the time series. In extensive simulations on synthetic data we show that the accuracy of three change point detection algorithms is significantly increased by a prior feature extraction step. These findings are confirmed in an application to industrial fault monitoring.
cs.LG:We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.
cs.LG:We consider online learning in partial-monitoring games against an oblivious adversary. We show that when the number of actions available to the learner is two and the game is nontrivial then it is reducible to a bandit-like game and thus the minimax regret is $\Theta(\sqrt{T})$.
cs.LG:Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.
cs.LG:One of the most prominent challenges in clustering is "the user's dilemma," which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others.   Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of center-based approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight.
cs.LG:There has been increased interest in devising learning techniques that combine unlabeled data with labeled data ? i.e. semi-supervised learning. However, to the best of our knowledge, no study has been performed across various techniques and different types and amounts of labeled and unlabeled data. Moreover, most of the published work on semi-supervised learning techniques assumes that the labeled and unlabeled data come from the same distribution. It is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different. Not correcting for such bias can result in biased function approximation with potentially poor performance. In this paper, we present an empirical study of various semi-supervised learning techniques on a variety of datasets. We attempt to answer various questions such as the effect of independence or relevance amongst features, the effect of the size of the labeled and unlabeled sets and the effect of noise. We also investigate the impact of sample-selection bias on the semi-supervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias.
cs.LG:The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnows behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.
cs.LG:In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed.
cs.LG:We consider a bandit problem over a graph where the rewards are not directly observed. Instead, the decision maker can compare two nodes and receive (stochastic) information pertaining to the difference in their value. The graph structure describes the set of possible comparisons. Consequently, comparing between two nodes that are relatively far requires estimating the difference between every pair of nodes on the path between them. We analyze this problem from the perspective of sample complexity: How many queries are needed to find an approximately optimal node with probability more than $1-\delta$ in the PAC setup? We show that the topology of the graph plays a crucial in defining the sample complexity: graphs with a low diameter have a much better sample complexity.
cs.LG:User profiling is a useful primitive for constructing personalised services, such as content recommendation. In the present paper we investigate the feasibility of user profiling in a distributed setting, with no central authority and only local information exchanges between users. We compute a profile vector for each user (i.e., a low-dimensional vector that characterises her taste) via spectral transformation of observed user-produced ratings for items. Our two main contributions follow: i) We consider a low-rank probabilistic model of user taste. More specifically, we consider that users and items are partitioned in a constant number of classes, such that users and items within the same class are statistically identical. We prove that without prior knowledge of the compositions of the classes, based solely on few random observed ratings (namely $O(N\log N)$ such ratings for $N$ users), we can predict user preference with high probability for unrated items by running a local vote among users with similar profile vectors. In addition, we provide empirical evaluations characterising the way in which spectral profiling performance depends on the dimension of the profile space. Such evaluations are performed on a data set of real user ratings provided by Netflix. ii) We develop distributed algorithms which provably achieve an embedding of users into a low-dimensional space, based on spectral transformation. These involve simple message passing among users, and provably converge to the desired embedding. Our method essentially relies on a novel combination of gossiping and the algorithm proposed by Oja and Karhunen.
cs.LG:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper represents LDA as a factor graph within the Markov random field (MRF) framework, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Although two commonly-used approximate inference methods, such as variational Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in learning LDA, the proposed BP is competitive in both speed and accuracy as validated by encouraging experimental results on four large-scale document data sets. Furthermore, the BP algorithm has the potential to become a generic learning scheme for variants of LDA-based topic models. To this end, we show how to learn two typical variants of LDA-based topic models, such as author-topic models (ATM) and relational topic models (RTM), using BP based on the factor graph representation.
cs.LG:In machine learning, distance-based algorithms, and other approaches, use information that is represented by propositional data. However, this kind of representation can be quite restrictive and, in many cases, it requires more complex structures in order to represent data in a more natural way. Terms are the basis for functional and logic programming representation. Distances between terms are a useful tool not only to compare terms, but also to determine the search space in many of these applications. This dissertation applies distances between terms, exploiting the features of each distance and the possibility to compare from propositional data types to hierarchical representations. The distances between terms are applied through the k-NN (k-nearest neighbor) classification algorithm using XML as a common language representation. To be able to represent these data in an XML structure and to take advantage of the benefits of distance between terms, it is necessary to apply some transformations. These transformations allow the conversion of flat data into hierarchical data represented in XML, using some techniques based on intuitive associations between the names and values of variables and associations based on attribute similarity.   Several experiments with the distances between terms of Nienhuys-Cheng and Estruch et al. were performed. In the case of originally propositional data, these distances are compared to the Euclidean distance. In all cases, the experiments were performed with the distance-weighted k-nearest neighbor algorithm, using several exponents for the attraction function (weighted distance). It can be seen that in some cases, the term distances can significantly improve the results on approaches applied to flat representations.
cs.LG:In this paper we explore noise tolerant learning of classifiers. We formulate the problem as follows. We assume that there is an ${\bf unobservable}$ training set which is noise-free. The actual training set given to the learning algorithm is obtained from this ideal data set by corrupting the class label of each example. The probability that the class label of an example is corrupted is a function of the feature vector of the example. This would account for most kinds of noisy data one encounters in practice. We say that a learning method is noise tolerant if the classifiers learnt with the ideal noise-free data and with noisy data, both have the same classification accuracy on the noise-free data. In this paper we analyze the noise tolerance properties of risk minimization (under different loss functions), which is a generic method for learning classifiers. We show that risk minimization under 0-1 loss function has impressive noise tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant. We conclude the paper with some discussion on implications of these theoretical results.
cs.LG:Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain. We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept. In this paper we make several contributions. First, we introduce Co-Testing, which is the first approach to multi-view active learning. Second, we extend the multi-view learning framework by also exploiting weak views, which are adequate only for learning a concept that is more general/specific than the target concept. Finally, we empirically show that Co-Testing outperforms existing active learners on a variety of real world domains such as wrapper induction, Web page classification, advertisement removal, and discourse tree parsing.
cs.LG:Recently, a unified framework for adaptive kernel based signal processing of complex data was presented by the authors, which, besides offering techniques to map the input data to complex Reproducing Kernel Hilbert Spaces, developed a suitable Wirtinger-like Calculus for general Hilbert Spaces. In this short paper, the extended Wirtinger's calculus is adopted to derive complex kernel-based widely-linear estimation filters. Furthermore, we illuminate several important characteristics of the widely linear filters. We show that, although in many cases the gains from adopting widely linear estimation filters, as alternatives to ordinary linear ones, are rudimentary, for the case of kernel based widely linear filters significant performance improvements can be obtained.
cs.LG:Matrix factorization from a small number of observed entries has recently garnered much attention as the key ingredient of successful recommendation systems. One unresolved problem in this area is how to adapt current methods to handle changing user preferences over time. Recent proposals to address this issue are heuristic in nature and do not fully exploit the time-dependent structure of the problem. As a principled and general temporal formulation, we propose a dynamical state space model of matrix factorization. Our proposal builds upon probabilistic matrix factorization, a Bayesian model with Gaussian priors. We utilize results in state tracking, such as the Kalman filter, to provide accurate recommendations in the presence of both process and measurement noise. We show how system parameters can be learned via expectation-maximization and provide comparisons to current published techniques.
cs.LG:The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones.   We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only fails to guarantee any useful bounds for these problems.   The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypotheses and some fixed "pivotal" hypothesis to within an absolute error of at most $\eps$ times the
cs.LG:The analysis of physiological processes over time are often given by spectrometric or gene expression profiles over time with only few time points but a large number of measured variables. The analysis of such temporal sequences is challenging and only few methods have been proposed. The information can be encoded time independent, by means of classical expression differences for a single time point or in expression profiles over time. Available methods are limited to unsupervised and semi-supervised settings. The predictive variables can be identified only by means of wrapper or post-processing techniques. This is complicated due to the small number of samples for such studies. Here, we present a supervised learning approach, termed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a supervised mapping of the temporal sequences onto a low dimensional grid. We utilize a hidden markov model (HMM) to account for the time domain and relevance learning to identify the relevant feature dimensions most predictive over time. The learned mapping can be used to visualize the temporal sequences and to predict the class of a new sequence. The relevance learning permits the identification of discriminating masses or gen expressions and prunes dimensions which are unnecessary for the classification task or encode mainly noise. In this way we obtain a very efficient learning system for temporal sequences. The results indicate that using simultaneous supervised learning and metric adaptation significantly improves the prediction accuracy for synthetically and real life data in comparison to the standard techniques. The discriminating features, identified by relevance learning, compare favorably with the results of alternative methods. Our method permits the visualization of the data on a low dimensional grid, highlighting the observed temporal structure.
cs.LG:Bayesian optimization (BO) algorithms try to optimize an unknown function that is expensive to evaluate using minimum number of evaluations/experiments. Most of the proposed algorithms in BO are sequential, where only one experiment is selected at each iteration. This method can be time inefficient when each experiment takes a long time and more than one experiment can be ran concurrently. On the other hand, requesting a fix-sized batch of experiments at each iteration causes performance inefficiency in BO compared to the sequential policies. In this paper, we present an algorithm that asks a batch of experiments at each time step t where the batch size p_t is dynamically determined in each step. Our algorithm is based on the observation that the sequence of experiments selected by the sequential policy can sometimes be almost independent from each other. Our algorithm identifies such scenarios and request those experiments at the same time without degrading the performance. We evaluate our proposed method using the Expected Improvement policy and the results show substantial speedup with little impact on the performance in eight real and synthetic benchmarks.
cs.LG:This report considers how to inject external candidate solutions into the CMA-ES algorithm. The injected solutions might stem from a gradient or a Newton step, a surrogate model optimizer or any other oracle or search mechanism. They can also be the result of a repair mechanism, for example to render infeasible solutions feasible. Only small modifications to the CMA-ES are necessary to turn injection into a reliable and effective method: too long steps need to be tightly renormalized. The main objective of this report is to reveal this simple mechanism. Depending on the source of the injected solutions, interesting variants of CMA-ES arise. When the best-ever solution is always (re-)injected, an elitist variant of CMA-ES with weighted multi-recombination arises. When \emph{all} solutions are injected from an \emph{external} source, the resulting algorithm might be viewed as \emph{adaptive encoding} with step-size control. In first experiments, injected solutions of very good quality lead to a convergence speed twice as fast as on the (simple) sphere function without injection. This means that we observe an impressive speed-up on otherwise difficult to solve functions. Single bad injected solutions on the other hand do no significant harm.
cs.LG:We propose a method to efficiently construct data-dependent kernels which can make use of large quantities of (unlabeled) data. Our construction makes an approximation in the standard construction of semi-supervised kernels in Sindhwani et al. 2005. In typical cases these kernels can be computed in nearly-linear time (in the amount of data), improving on the cubic time of the standard construction, enabling large scale semi-supervised learning in a variety of contexts. The methods are validated on semi-supervised and unsupervised problems on data sets containing upto 64,000 sample points.
cs.LG:Recent work in signal processing and statistics have focused on defining new regularization functions, which not only induce sparsity of the solution, but also take into account the structure of the problem. We present in this paper a class of convex penalties introduced in the machine learning community, which take the form of a sum of l_2 and l_infinity-norms over groups of variables. They extend the classical group-sparsity regularization in the sense that the groups possibly overlap, allowing more flexibility in the group design. We review efficient optimization methods to deal with the corresponding inverse problems, and their application to the problem of learning dictionaries of natural image patches: On the one hand, dictionary learning has indeed proven effective for various signal processing tasks. On the other hand, structured sparsity provides a natural framework for modeling dependencies between dictionary elements. We thus consider a structured sparse regularization to learn dictionaries embedded in a particular structure, for instance a tree or a two-dimensional grid. In the latter case, the results we obtain are similar to the dictionaries produced by topographic independent component analysis.
cs.LG:In this paper, we describe our approach to the Wikipedia Participation Challenge which aims to predict the number of edits a Wikipedia editor will make in the next 5 months. The best submission from our team, "zeditor", achieved 41.7% improvement over WMF's baseline predictive model and the final rank of 3rd place among 96 teams. An interesting characteristic of our approach is that only temporal dynamics features (i.e., how the number of edits changes in recent periods, etc.) are used in a self-supervised learning framework, which makes it easy to be generalised to other application domains.
cs.LG:This paper presents a method of choosing number of states of a HMM based on number of critical points of the motion capture data. The choice of Hidden Markov Models(HMM) parameters is crucial for recognizer's performance as it is the first step of the training and cannot be corrected automatically within HMM. In this article we define predictor of number of states based on number of critical points of the sequence and test its effectiveness against sample data.
cs.LG:We develop a new tool for data-dependent analysis of the exploration-exploitation trade-off in learning under limited feedback. Our tool is based on two main ingredients. The first ingredient is a new concentration inequality that makes it possible to control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. The second ingredient is an application of this inequality to the exploration-exploitation trade-off via importance weighted sampling. We apply the new tool to the stochastic multiarmed bandit problem, however, the main importance of this paper is the development and understanding of the new tool rather than improvement of existing algorithms for stochastic multiarmed bandits. In the follow-up work we demonstrate that the new tool can improve over state-of-the-art in structurally richer problems, such as stochastic multiarmed bandits with side information (Seldin et al., 2011a).
cs.LG:Structured classification tasks such as sequence labeling and dependency parsing have seen much interest by the Natural Language Processing and the machine learning communities. Several online learning algorithms were adapted for structured tasks such as Perceptron, Passive- Aggressive and the recently introduced Confidence-Weighted learning . These online algorithms are easy to implement, fast to train and yield state-of-the-art performance. However, unlike probabilistic models like Hidden Markov Model and Conditional random fields, these methods generate models that output merely a prediction with no additional information regarding confidence in the correctness of the output. In this work we fill the gap proposing few alternatives to compute the confidence in the output of non-probabilistic algorithms.We show how to compute confidence estimates in the prediction such that the confidence reflects the probability that the word is labeled correctly. We then show how to use our methods to detect mislabeled words, trade recall for precision and active learning. We evaluate our methods on four noun-phrase chunking and named entity recognition sequence labeling tasks, and on dependency parsing for 14 languages.
cs.LG:In this paper we propose and study a generalization of the standard active-learning model where a more general type of query, class conditional query, is allowed. Such queries have been quite useful in applications, but have been lacking theoretical understanding. In this work, we characterize the power of such queries under two well-known noise models. We give nearly tight upper and lower bounds on the number of queries needed to learn both for the general agnostic setting and for the bounded noise model. We further show that our methods can be made adaptive to the (unknown) noise rate, with only negligible loss in query complexity.
cs.LG:In this paper, we consider the problem of multi-armed bandits with a large, possibly infinite number of correlated arms. We assume that the arms have Bernoulli distributed rewards, independent across time, where the probabilities of success are parametrized by known attribute vectors for each arm, as well as an unknown preference vector, each of dimension $n$. For this model, we seek an algorithm with a total regret that is sub-linear in time and independent of the number of arms. We present such an algorithm, which we call the Two-Phase Algorithm, and analyze its performance. We show upper bounds on the total regret which applies uniformly in time, for both the finite and infinite arm cases. The asymptotics of the finite arm bound show that for any $f \in \omega(\log(T))$, the total regret can be made to be $O(n \cdot f(T))$. In the infinite arm case, the total regret is $O(\sqrt{n^3 T})$.
cs.LG:We present a framework for performing efficient regression in general metric spaces. Roughly speaking, our regressor predicts the value at a new point by computing a Lipschitz extension --- the smoothest function consistent with the observed data --- after performing structural risk minimization to avoid overfitting. We obtain finite-sample risk bounds with minimal structural and noise assumptions, and a natural speed-precision tradeoff. The offline (learning) and online (prediction) stages can be solved by convex programming, but this naive approach has runtime complexity $O(n^3)$, which is prohibitive for large datasets. We design instead a regression algorithm whose speed and generalization performance depend on the intrinsic dimension of the data, to which the algorithm adapts. While our main innovation is algorithmic, the statistical results may also be of independent interest.
cs.LG:Spectral clustering is a novel clustering method which can detect complex shapes of data clusters. However, it requires the eigen decomposition of the graph Laplacian matrix, which is proportion to $O(n^3)$ and thus is not suitable for large scale systems. Recently, many methods have been proposed to accelerate the computational time of spectral clustering. These approximate methods usually involve sampling techniques by which a lot information of the original data may be lost. In this work, we propose a fast and accurate spectral clustering approach using an approximate commute time embedding, which is similar to the spectral embedding. The method does not require using any sampling technique and computing any eigenvector at all. Instead it uses random projection and a linear time solver to find the approximate embedding. The experiments in several synthetic and real datasets show that the proposed approach has better clustering quality and is faster than the state-of-the-art approximate spectral clustering methods.
cs.LG:In this paper we propose a framework for solving constrained online convex optimization problem. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set $\mathcal{K}$ from which the decisions are made. While for simple shapes (e.g. Euclidean ball) the projection is straightforward, for arbitrary complex sets this is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring decisions belong to $\mathcal{K}$ for all rounds, we only require that the constraints which define the set $\mathcal{K}$ be satisfied in the long run. We show that our framework can be utilized to solve a relaxed version of online learning with side constraints addressed in \cite{DBLP:conf/colt/MannorT06} and \cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and $\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting $\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on the Mirror Prox method \citep{nemirovski-2005-prox} to solve variational inequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound for both regret and the violation of constraints when the domain $\K$ can be described by a finite number of linear constraints. Finally, we extend the result to the setting where we only have partial access to the convex set $\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm.
cs.LG:In citep{Hazan-2008-extract}, the authors showed that the regret of online linear optimization can be bounded by the total variation of the cost vectors. In this paper, we extend this result to general online convex optimization. We first analyze the limitations of the algorithm in \citep{Hazan-2008-extract} when applied it to online convex optimization. We then present two algorithms for online convex optimization whose regrets are bounded by the variation of cost functions. We finally consider the bandit setting, and present a randomized algorithm for online bandit convex optimization with a variation-based regret bound. We show that the regret bound for online bandit convex optimization is optimal when the variation of cost functions is independent of the number of trials.
cs.LG:Although exploratory behaviors are ubiquitous in the animal kingdom, their computational underpinnings are still largely unknown. Behavioral Psychology has identified learning as a primary drive underlying many exploratory behaviors. Exploration is seen as a means for an animal to gather sensory data useful for reducing its ignorance about the environment. While related problems have been addressed in Data Mining and Reinforcement Learning, the computational modeling of learning-driven exploration by embodied agents is largely unrepresented.   Here, we propose a computational theory for learning-driven exploration based on the concept of missing information that allows an agent to identify informative actions using Bayesian inference. We demonstrate that when embodiment constraints are high, agents must actively coordinate their actions to learn efficiently. Compared to earlier approaches, our exploration policy yields more efficient learning across a range of worlds with diverse structures. The improved learning in turn affords greater success in general tasks including navigation and reward gathering. We conclude by discussing how the proposed theory relates to previous information-theoretic objectives of behavior, such as predictive information and the free energy principle, and how it might contribute to a general theory of exploratory behavior.
cs.LG:This paper derives an identity connecting the square loss of ridge regression in on-line mode with the loss of the retrospectively best regressor. Some corollaries about the properties of the cumulative loss of on-line ridge regression are also obtained.
cs.LG:Unsupervised aggregation of independently built univariate predictors is explored as an alternative regularization approach for noisy, sparse datasets. Bipartite ranking algorithm Smooth Rank implementing this approach is introduced. The advantages of this algorithm are demonstrated on two types of problems. First, Smooth Rank is applied to two-class problems from bio-medical field, where ranking is often preferable to classification. In comparison against SVMs with radial and linear kernels, Smooth Rank had the best performance on 8 out of 12 benchmark benchmarks. The second area of application is survival analysis, which is reduced here to bipartite ranking in a way which allows one to use commonly accepted measures of methods performance. In comparison of Smooth Rank with Cox PH regression and CoxPath methods, Smooth Rank proved to be the best on 9 out of 10 benchmark datasets.
cs.LG:We investigate a recently proposed family of positive-definite kernels that mimic the computation in large neural networks. We examine the properties of these kernels using tools from differential geometry; specifically, we analyze the geometry of surfaces in Hilbert space that are induced by these kernels. When this geometry is described by a Riemannian manifold, we derive results for the metric, curvature, and volume element. Interestingly, though, we find that the simplest kernel in this family does not admit such an interpretation. We explore two variations of these kernels that mimic computation in neural networks with different activation functions. We experiment with these new kernels on several data sets and highlight their general trends in performance for classification.
cs.LG:We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF), a popular unsupervised learning algorithm for dimensionality reduction. In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts; they are also guaranteed at each iteration to decrease their loss function---a weighted sum of I-divergences that captures the trade-off between unsupervised and supervised learning. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification. In this role, we find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space.
cs.LG:This paper provides a theoretical support for clustering aspect of the nonnegative matrix factorization (NMF). By utilizing the Karush-Kuhn-Tucker optimality conditions, we show that NMF objective is equivalent to graph clustering objective, so clustering aspect of the NMF has a solid justification. Different from previous approaches which usually discard the nonnegativity constraints, our approach guarantees the stationary point being used in deriving the equivalence is located on the feasible region in the nonnegative orthant. Additionally, since clustering capability of a matrix decomposition technique can sometimes imply its latent semantic indexing (LSI) aspect, we will also evaluate LSI aspect of the NMF by showing its capability in solving the synonymy and polysemy problems in synthetic datasets. And more extensive evaluation will be conducted by comparing LSI performances of the NMF and the singular value decomposition (SVD), the standard LSI method, using some standard datasets.
cs.LG:The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.
cs.LG:We propose a new, nonparametric approach to estimating the value function in reinforcement learning. This approach makes use of a recently developed representation of conditional distributions as functions in a reproducing kernel Hilbert space. Such representations bypass the need for estimating transition probabilities, and apply to any domain on which kernels can be defined. Our approach avoids the need to approximate intractable integrals since expectations are represented as RKHS inner products whose computation has linear complexity in the sample size. Thus, we can efficiently perform value function estimation in a wide variety of settings, including finite state spaces, continuous states spaces, and partially observable tasks where only sensor measurements are available. A second advantage of the approach is that we learn the conditional distribution representation from a training sample, and do not require an exhaustive exploration of the state space. We prove convergence of our approach either to the optimal policy, or to the closest projection of the optimal policy in our model class, under reasonable assumptions. In experiments, we demonstrate the performance of our algorithm on a learning task in a continuous state space (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. We compare with least-squares policy iteration where a Gaussian process is used for value function estimation. Our algorithm achieves better performance in both tasks.
cs.LG:Selecting the best classifier among the available ones is a difficult task, especially when only instances of one class exist. In this work we examine the notion of combining one-class classifiers as an alternative for selecting the best classifier. In particular, we propose two new one-class classification performance measures to weigh classifiers and show that a simple ensemble that implements these measures can outperform the most popular one-class ensembles. Furthermore, we propose a new one-class ensemble scheme, TUPSO, which uses meta-learning to combine one-class classifiers. Our experiments demonstrate the superiority of TUPSO over all other tested ensembles and show that the TUPSO performance is statistically indistinguishable from that of the hypothetical best classifier.
cs.LG:We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.
cs.LG:Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a {linear} dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.
cs.LG:Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, Involving Few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved, the quality of any RL method---in real-world settings---depends less on how well it scales with the number of states than on how well it scales with the number of actions. This is where our new method T-Learning excels, by evaluating the relatively few possible transits from one state to another in a policy-independent way, rather than a huge number of state-action pairs, or states in traditional policy-dependent ways. Illustrative experiments demonstrate that performance improvements of T-Learning over Q-learning can be arbitrarily large.
cs.LG:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.
cs.LG:Leveraging the power of increasing amounts of data to analyze customer base for attracting and retaining the most valuable customers is a major problem facing companies in this information age. Data mining technologies extract hidden information and knowledge from large data stored in databases or data warehouses, thereby supporting the corporate decision making process. CRM uses data mining (one of the elements of CRM) techniques to interact with customers. This study investigates the use of a technique, semi-supervised learning, for the management and analysis of customer-related data warehouse and information. The idea of semi-supervised learning is to learn not only from the labeled training data, but to exploit also the structural information in additionally available unlabeled data. The proposed semi-supervised method is a model by means of a feed-forward neural network trained by a back propagation algorithm (multi-layer perceptron) in order to predict the category of an unknown customer (potential customers). In addition, this technique can be used with Rapid Miner tools for both labeled and unlabeled data.
cs.LG:Diabetes is a major health problem in both developing and developed countries and its incidence is rising dramatically. In this study, we investigate a novel automatic approach to diagnose Diabetes disease based on Feature Weighted Support Vector Machines (FW-SVMs) and Modified Cuckoo Search (MCS). The proposed model consists of three stages: Firstly, PCA is applied to select an optimal subset of features out of set of all the features. Secondly, Mutual Information is employed to construct the FWSVM by weighting different features based on their degree of importance. Finally, since parameter selection plays a vital role in classification accuracy of SVMs, MCS is applied to select the best parameter values. The proposed MI-MCS-FWSVM method obtains 93.58% accuracy on UCI dataset. The experimental results demonstrate that our method outperforms the previous methods by not only giving more accurate results but also significantly speeding up the classification procedure.
cs.LG:We present a novel approach to learn a kernel-based regression function. It is based on the useof conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.
cs.LG:Teaching is one of the most important factors affecting any education system. Many research efforts have been conducted to facilitate the presentation modes used by instructors in classrooms as well as provide means for students to review lectures through web browsers. Other studies have been made to provide acoustical design recommendations for classrooms like room size and reverberation times. However, using acoustical features of classrooms as a way to provide education systems with feedback about the learning process was not thoroughly investigated in any of these studies. We propose a system that extracts different sound features of students and instructors, and then uses machine learning techniques to evaluate the acoustical quality of any learning environment. We infer conclusions about the students' satisfaction with the quality of lectures. Using classifiers instead of surveys and other subjective ways of measures can facilitate and speed such experiments which enables us to perform them continuously. We believe our system enables education systems to continuously review and improve their teaching strategies and acoustical quality of classrooms.
cs.LG:We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of $O(1/T)$ {assuming that the proximal step can be efficiently solved}, significantly faster than a standard subgradient descent method that has an $O(1/\sqrt{T})$ convergence rate. Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.
cs.LG:In the current competitive world, industrial companies seek to manufacture products of higher quality which can be achieved by increasing reliability, maintainability and thus the availability of products. On the other hand, improvement in products lifecycle is necessary for achieving high reliability. Typically, maintenance activities are aimed to reduce failures of industrial machinery and minimize the consequences of such failures. So the industrial companies try to improve their efficiency by using different fault detection techniques. One strategy is to process and analyze previous generated data to predict future failures. The purpose of this paper is to detect wasted parts using different data mining algorithms and compare the accuracy of these algorithms. A combination of thermal and physical characteristics has been used and the algorithms were implemented on Ahanpishegan's current data to estimate the availability of its produced parts.   Keywords: Data Mining, Fault Detection, Availability, Prediction Algorithms.
cs.LG:Clustering is considered a non-supervised learning setting, in which the goal is to partition a collection of data points into disjoint clusters. Often a bound $k$ on the number of clusters is given or assumed by the practitioner. Many versions of this problem have been defined, most notably $k$-means and $k$-median.   An underlying problem with the unsupervised nature of clustering it that of determining a similarity function. One approach for alleviating this difficulty is known as clustering with side information, alternatively, semi-supervised clustering. Here, the practitioner incorporates side information in the form of "must be clustered" or "must be separated" labels for data point pairs. Each such piece of information comes at a "query cost" (often involving human response solicitation). The collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints, possibly adding a pairwise constraint penalty function to the chosen clustering objective.   Our work is mostly related to clustering with side information. We ask how to choose the pairs of data points. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. Roughly speaking, we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, hence we don't know the bias. Using the recently introduced method of $\eps$-smooth relative regret approximations of Ailon, Begleiter and Ezra, we can show an iterative process that improves both the clustering and the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm selecting pairs uniformly.
cs.LG:Contextual bandit learning is a reinforcement learning problem where the learner repeatedly receives a set of features (context), takes an action and receives a reward based on the action and context. We consider this problem under a realizability assumption: there exists a function in a (known) function class, always capable of predicting the expected reward, given the action and context. Under this assumption, we show three things. We present a new algorithm---Regressor Elimination--- with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that for any set of policies (mapping contexts to actions), there is a distribution over rewards (given context) such that our new algorithm has constant regret unlike the previous approaches.
cs.LG:Inverse reinforcement learning (IRL) addresses the problem of recovering a task description given a demonstration of the optimal policy used to solve such a task. The optimal policy is usually provided by an expert or teacher, making IRL specially suitable for the problem of apprenticeship learning. The task description is encoded in the form of a reward function of a Markov decision process (MDP). Several algorithms have been proposed to find the reward function corresponding to a set of demonstrations. One of the algorithms that has provided best results in different applications is a gradient method to optimize a policy squared error criterion. On a parallel line of research, other authors have presented recently a gradient approximation of the maximum likelihood estimate of the reward signal. In general, both approaches approximate the gradient estimate and the criteria at different stages to make the algorithm tractable and efficient. In this work, we provide a detailed description of the different methods to highlight differences in terms of reward estimation, policy similarity and computational costs. We also provide experimental results to evaluate the differences in performance of the methods.
cs.LG:We study upper and lower bounds on the sample-complexity of learning near-optimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.
cs.LG:In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ||C||. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid's inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.
cs.LG:In this paper, we study the application of GIST SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores.
cs.LG:This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.
cs.LG:Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational data mining is used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.
cs.LG:We investigate adaptive mixture methods that linearly combine outputs of $m$ constituent filters running in parallel to model a desired signal. We use "Bregman divergences" and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of $m$ constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.
cs.LG:The past century was era of linear systems. Either systems (especially industrial ones) were simple (quasi)linear or linear approximations were accurate enough. In addition, just at the ending decades of the century profusion of computing devices were available, before then due to lack of computational resources it was not easy to evaluate available nonlinear system studies. At the moment both these two conditions changed, systems are highly complex and also pervasive amount of computation strength is cheap and easy to achieve. For recent era, a new branch of supervised learning well known as surrogate modeling (meta-modeling, surface modeling) has been devised which aimed at answering new needs of modeling realm. This short literature survey is on to introduce surrogate modeling to whom is familiar with the concepts of supervised learning. Necessity, challenges and visions of the topic are considered.
cs.LG:Bayesian model averaging (BMA) is an approach to average over alternative models; yet, it usually gets excessively concentrated around the single most probable model, therefore achieving only sub-optimal classification performance. The compression-based approach (Boulle, 2007) overcomes this problem, averaging over the different models by applying a logarithmic smoothing over the models' posterior probabilities. This approach has shown excellent performances when applied to ensembles of naive Bayes classifiers. AODE is another ensemble of models with high performance (Webb, 2005), based on a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs via BMA rather than by arithmetic mean deteriorates the performance; instead, we aggregate the SPODEs via the compression coefficients and we show that the resulting classifier obtains a slight but consistent improvement over AODE. However, an important issue in any Bayesian ensemble of models is the arbitrariness in the choice of the prior over the models. We address this problem by the paradigm of credal classification, namely by substituting the unique prior with a set of priors. Credal classifier automatically recognize the prior-dependent instances, namely the instances whose most probable class varies, when different priors are considered; in these cases, credal classifiers remain reliable by returning a set of classes rather than a single class. We thus develop the credal version of both the BMA-based and the compression-based ensemble of SPODEs, substituting the single prior over the models by a set of priors. Experiments show that both credal classifiers provide higher classification reliability than their determinate counterparts; moreover the compression-based credal classifier compares favorably to previous credal classifiers.
cs.LG:We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives.
cs.LG:Probabilistic generative modeling of data distributions can potentially exploit hidden information which is useful for discriminative classification. This observation has motivated the development of approaches that couple generative and discriminative models for classification. In this paper, we propose a new approach to couple generative and discriminative models in an unified framework based on PAC-Bayes risk theory. We first derive the model-parameter-independent stochastic feature mapping from a practical MAP classifier operating on generative models. Then we construct a linear stochastic classifier equipped with the feature mapping, and derive the explicit PAC-Bayes risk bounds for such classifier for both supervised and semi-supervised learning. Minimizing the risk bound, using an EM-like iterative procedure, results in a new posterior over hidden variables (E-step) and the update rules of model parameters (M-step). The derivation of the posterior is always feasible due to the way of equipping feature mapping and the explicit form of bounding risk. The derived posterior allows the tuning of generative models and subsequently the feature mappings for better classification. The derived update rules of the model parameters are same to those of the uncoupled models as the feature mapping is model-parameter-independent. Our experiments show that the coupling between data modeling generative model and the discriminative classifier via a stochastic feature mapping in this framework leads to a general classification tool with state-of-the-art performance.
cs.LG:Nowadays, supervised learning is commonly used in many domains. Indeed, many works propose to learn new knowledge from examples that translate the expected behaviour of the considered system. A key issue of supervised learning concerns the description language used to represent the examples. In this paper, we propose a method to evaluate the feature set used to describe them. Our method is based on the computation of the consistency of the example base. We carried out a case study in the domain of geomatic in order to evaluate the sets of measures used to characterise geographic objects. The case study shows that our method allows to give relevant evaluations of measure sets.
cs.LG:Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.
cs.LG:Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster.
cs.LG:We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.
cs.LG:Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.
cs.LG:Despite the widespread use of Clustering, there is distressingly little general theory of clustering available. Questions like "What distinguishes a clustering of data from other data partitioning?", "Are there any principles governing all clustering paradigms?", "How should a user choose an appropriate clustering algorithm for a particular task?", etc. are almost completely unanswered by the existing body of clustering literature. We consider an axiomatic approach to the theory of Clustering. We adopt the framework of Kleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, we sidestep his impossibility result and arrive at a consistent set of axioms. We suggest to extend these axioms, aiming to provide an axiomatic taxonomy of clustering paradigms. Such a taxonomy should provide users some guidance concerning the choice of the appropriate clustering paradigm for a given task. The main result of this paper is a set of abstract properties that characterize the Single-Linkage clustering function. This characterization result provides new insight into the properties of desired data groupings that make Single-Linkage the appropriate choice. We conclude by considering a taxonomy of clustering functions based on abstract properties that each satisfies.
cs.LG:A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X = x) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with t = 1/2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any t. We then present a principled algorithm to solve the extended SVM classifier for all values of t simultaneously. This has two implications: First, one can recover the entire conditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.
cs.LG:We consider MAP estimators for structured prediction with exponential family models. In particular, we concentrate on the case that efficient algorithms for uniform sampling from the output space exist. We show that under this assumption (i) exact computation of the partition function remains a hard problem, and (ii) the partition function and the gradient of the log partition function can be approximated efficiently. Our main result is an approximation scheme for the partition function based on Markov Chain Monte Carlo theory. We also show that the efficient uniform sampling assumption holds in several application settings that are of importance in machine learning.
cs.LG:We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of ~O(HSpAT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.
cs.LG:We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.
cs.LG:We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker's cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard square root of k dependence, depending on the behavior of the arms' reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms.
cs.LG:We are concerned with the issue of how to calculate the normalized maximum likelihood (NML) code-length. There is a problem that the normalization term of the NML code-length may diverge when it is continuous and unbounded and a straightforward computation of it is highly expensive when the data domain is finite . In previous works it has been investigated how to calculate the NML code-length for specific types of distributions. We first propose a general method for computing the NML code-length for the exponential family. Then we specifically focus on Gaussian mixture model (GMM), and propose a new efficient method for computing the NML to them. We develop it by generalizing Rissanen's re-normalizing technique. Then we apply this method to the clustering issue, in which a clustering structure is modeled using a GMM, and the main task is to estimate the optimal number of clusters on the basis of the NML code-length. We demonstrate using artificial data sets the superiority of the NML-based clustering over other criteria such as AIC, BIC in terms of the data size required for high accuracy rate to be achieved.
cs.LG:This paper describes the method of visualization of periodic constituents and instability areas in series of measurements, being based on the algorithm of smoothing out and concept of one-dimensional cellular automata. A method can be used at the analysis of temporal series, related to the volumes of thematic publications in web-space.
cs.LG:We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.
cs.LG:In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.
cs.LG:This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.
cs.LG:We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.
cs.LG:Unsupervised models can provide supplementary soft constraints to help classify new, "target" data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered "source" data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by naively applying classifiers learnt on the original task to the target data.
cs.LG:Numerous data mining techniques have been developed to extract information and identify patterns and predict trends from large data sets. In this study, two classification techniques, the J48 implementation of the C4.5 algorithm and a Naive Bayes classifier are applied to predict lung cancer survivability from an extensive data set with fifteen years of patient records. The purpose of the project is to verify the predictive effectiveness of the two techniques on real, historical data. Besides the performance outcome that renders J48 marginally better than the Naive Bayes technique, there is a detailed description of the data and the required pre-processing activities. The performance results confirm expectations while some of the issues that appeared during experimentation, underscore the value of having domain-specific understanding to leverage any domain-specific characteristics inherent in the data.
cs.LG:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.
cs.LG:This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.
cs.LG:We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce. In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker. The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering. In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one.
cs.LG:In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost).
cs.LG:Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work "transductively", i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an"inductive"-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\"om method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposition. Empirical results demonstrate the efficacy and efficiency of the proposed method.
cs.LG:There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for "Path Integral Policy Improvement with Covariance Matrix Adaptation". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.
cs.LG:F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.
cs.LG:We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of $O(1/T)$ where $T$ is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm.
cs.LG:Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.
cs.LG:Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.
cs.LG:We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as \emph{embeddings} in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments.
cs.LG:We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.
cs.LG:Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.
cs.LG:We consider a dynamic pricing problem under unknown demand models. In this problem a seller offers prices to a stream of customers and observes either success or failure in each sale attempt. The underlying demand model is unknown to the seller and can take one of N possible forms. In this paper, we show that this problem can be formulated as a multi-armed bandit with dependent arms. We propose a dynamic pricing policy based on the likelihood ratio test. We show that the proposed policy achieves complete learning, i.e., it offers a bounded regret where regret is defined as the revenue loss with respect to the case with a known demand model. This is in sharp contrast with the logarithmic growing regret in multi-armed bandit with independent arms.
cs.LG:Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.
cs.LG:The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.
cs.LG:In this paper we consider the problem of collectively classifying entities where relational information is available across the entities. In practice inaccurate class distribution for each entity is often available from another (external) classifier. For example this distribution could come from a classifier built using content features or a simple dictionary. Given the relational and inaccurate external classifier information, we consider two graph based settings in which the problem of collective classification can be solved. In the first setting the class distribution is used to fix labels to a subset of nodes and the labels for the remaining nodes are obtained like in a transductive setting. In the other setting the class distributions of all nodes are used to define the fitting function part of a graph regularized objective function. We define a generalized objective function that handles both the settings. Methods like harmonic Gaussian field and local-global consistency (LGC) reported in the literature can be seen as special cases. We extend the LGC and weighted vote relational neighbor classification (WvRN) methods to support usage of external classifier information. We also propose an efficient least squares regularization (LSR) based method and relate it to information regularization methods. All the methods are evaluated on several benchmark and real world datasets. Considering together speed, robustness and accuracy, experimental results indicate that the LSR and WvRN-extension methods perform better than other methods.
cs.LG:Metric learning methods have been shown to perform well on different learning tasks. Many of them rely on target neighborhood relationships that are computed in the original feature space and remain fixed throughout learning. As a result, the learned metric reflects the original neighborhood relations. We propose a novel formulation of the metric learning problem in which, in addition to the metric, the target neighborhood relations are also learned in a two-step iterative approach. The new formulation can be seen as a generalization of many existing metric learning methods. The formulation includes a target neighbor assignment rule that assigns different numbers of neighbors to instances according to their quality; `high quality' instances get more neighbors. We experiment with two of its instantiations that correspond to the metric learning algorithms LMNN and MCML and compare it to other metric learning methods on a number of datasets. The experimental results show state-of-the-art performance and provide evidence that learning the neighborhood relations does improve predictive performance.
cs.LG:We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T^{1/2} log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.
cs.LG:Semi-supervised template update systems allow to automatically take into account the intra-class variability of the biometric data over time. Such systems can be inefficient by including too many impostor's samples or skipping too many genuine's samples. In the first case, the biometric reference drifts from the real biometric data and attracts more often impostors. In the second case, the biometric reference does not evolve quickly enough and also progressively drifts from the real biometric data. We propose a hybrid system using several biometric sub-references in order to increase per- formance of self-update systems by reducing the previously cited errors. The proposition is validated for a keystroke- dynamics authentication system (this modality suffers of high variability over time) on two consequent datasets from the state of the art.
cs.LG:Most keystroke dynamics studies have been evaluated using a specific kind of dataset in which users type an imposed login and password. Moreover, these studies are optimistics since most of them use different acquisition protocols, private datasets, controlled environment, etc. In order to enhance the accuracy of keystroke dynamics' performance, the main contribution of this paper is twofold. First, we provide a new kind of dataset in which users have typed both an imposed and a chosen pairs of logins and passwords. In addition, the keystroke dynamics samples are collected in a web-based uncontrolled environment (OS, keyboards, browser, etc.). Such kind of dataset is important since it provides us more realistic results of keystroke dynamics' performance in comparison to the literature (controlled environment, etc.). Second, we present a statistical analysis of well known assertions such as the relationship between performance and password size, impact of fusion schemes on system overall performance, and others such as the relationship between performance and entropy. We put into obviousness in this paper some new results on keystroke dynamics in realistic conditions.
cs.LG:The selection of the best classification algorithm for a given dataset is a very widespread problem. It is also a complex one, in the sense it requires to make several important methodological choices. Among them, in this work we focus on the measure used to assess the classification performance and rank the algorithms. We present the most popular measures and discuss their properties. Despite the numerous measures proposed over the years, many of them turn out to be equivalent in this specific case, to have interpretation problems, or to be unsuitable for our purpose. Consequently, classic overall success rate or marginal rates should be preferred for this specific task.
cs.LG:It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.
cs.LG:Electronic health records contain rich textual data which possess critical predictive information for machine-learning based diagnostic aids. However many traditional machine learning methods fail to simultaneously integrate both vector space data and text. We present a supervised method using Laplacian eigenmaps to augment existing machine-learning methods with low-dimensional representations of textual predictors which preserve the local similarities. The proposed implementation performs alternating optimization using gradient descent. For the evaluation we applied our method to over 2,000 patient records from a large single-center pediatric cardiology practice to predict if patients were diagnosed with cardiac disease. Our method was compared with latent semantic indexing, latent Dirichlet allocation, and local Fisher discriminant analysis. The results were assessed using AUC, MCC, specificity, and sensitivity. Results indicate supervised Laplacian eigenmaps was the highest performing method in our study, achieving 0.782 and 0.374 for AUC and MCC respectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over the baseline which excluded textual data and a 2.69% and 5.35% increase in AUC and MCC respectively over unsupervised Laplacian eigenmaps. This method allows many existing machine learning predictors to effectively and efficiently utilize the potential of textual predictors.
cs.LG:This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.
cs.LG:The real challenge in pattern recognition task and machine learning process is to train a discriminator using labeled data and use it to distinguish between future data as accurate as possible. However, most of the problems in the real world have numerous data, which labeling them is a cumbersome or even an impossible matter. Semi-supervised learning is one approach to overcome these types of problems. It uses only a small set of labeled with the company of huge remain and unlabeled data to train the discriminator. In semi-supervised learning, it is very essential that which data is labeled and depend on position of data it effectiveness changes. In this paper, we proposed an evolutionary approach called Artificial Immune System (AIS) to determine which data is better to be labeled to get the high quality data. The experimental results represent the effectiveness of this algorithm in finding these data points.
cs.LG:It is often the case that, within an online recommender system, multiple users share a common account. Can such shared accounts be identified solely on the basis of the user- provided ratings? Once a shared account is identified, can the different users sharing it be identified as well? Whenever such user identification is feasible, it opens the way to possible improvements in personalized recommendations, but also raises privacy concerns. We develop a model for composite accounts based on unions of linear subspaces, and use subspace clustering for carrying out the identification task. We show that a significant fraction of such accounts is identifiable in a reliable manner, and illustrate potential uses for personalized recommendation.
cs.LG:In this paper, we attempts to learn a single metric across two heterogeneous domains where source domain is fully labeled and has many samples while target domain has only a few labeled samples but abundant unlabeled samples. To the best of our knowledge, this task is seldom touched. The proposed learning model has a simple underlying motivation: all the samples in both the source and the target domains are mapped into a common space, where both their priors P(sample)s and their posteriors P(label|sample)s are forced to be respectively aligned as much as possible. We show that the two mappings, from both the source domain and the target domain to the common space, can be reparameterized into a single positive semi-definite(PSD) matrix. Then we develop an efficient Bregman Projection algorithm to optimize the PDS matrix over which a LogDet function is used to regularize. Furthermore, we also show that this model can be easily kernelized and verify its effectiveness in crosslanguage retrieval task and cross-domain object recognition task.
cs.LG:Schapire's margin theory provides a theoretical explanation to the success of boosting-type methods and manifests that a good margin distribution (MD) of training samples is essential for generalization. However the statement that a MD is good is vague, consequently, many recently developed algorithms try to generate a MD in their goodness senses for boosting generalization. Unlike their indirect control over MD, in this paper, we propose an alternative boosting algorithm termed Margin distribution Controlled Boosting (MCBoost) which directly controls the MD by introducing and optimizing a key adjustable margin parameter. MCBoost's optimization implementation adopts the column generation technique to ensure fast convergence and small number of weak classifiers involved in the final MCBooster. We empirically demonstrate: 1) AdaBoost is actually also a MD controlled algorithm and its iteration number acts as a parameter controlling the distribution and 2) the generalization performance of MCBoost evaluated on UCI benchmark datasets is validated better than those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost.
cs.LG:We present new algorithms for inverse reinforcement learning (IRL, or inverse optimal control) in convex optimization settings. We argue that finite-space IRL can be posed as a convex quadratic program under a Bayesian inference framework with the objective of maximum a posterior estimation. To deal with problems in large or even infinite state space, we propose a Gaussian process model and use preference graphs to represent observations of decision trajectories. Our method is distinguished from other approaches to IRL in that it makes no assumptions about the form of the reward function and yet it retains the promise of computationally manageable implementations for potential real-world applications. In comparison with an establish algorithm on small-scale numerical problems, our method demonstrated better accuracy in apprenticeship learning and a more robust dependence on the number of observations.
cs.LG:We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.
cs.LG:Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.
cs.LG:Scientists study trajectory data to understand trends in movement patterns, such as human mobility for traffic analysis and urban planning. There is a pressing need for scalable and efficient techniques for analyzing this data and discovering the underlying patterns. In this paper, we introduce a novel technique which we call vector-field $k$-means.   The central idea of our approach is to use vector fields to induce a similarity notion between trajectories. Other clustering algorithms seek a representative trajectory that best describes each cluster, much like $k$-means identifies a representative "center" for each cluster. Vector-field $k$-means, on the other hand, recognizes that in all but the simplest examples, no single trajectory adequately describes a cluster. Our approach is based on the premise that movement trends in trajectory data can be modeled as flows within multiple vector fields, and the vector field itself is what defines each of the clusters. We also show how vector-field $k$-means connects techniques for scalar field design on meshes and $k$-means clustering.   We present an algorithm that finds a locally optimal clustering of trajectories into vector fields, and demonstrate how vector-field $k$-means can be used to mine patterns from trajectory data. We present experimental evidence of its effectiveness and efficiency using several datasets, including historical hurricane data, GPS tracks of people and vehicles, and anonymous call records from a large phone company. We compare our results to previous trajectory clustering techniques, and find that our algorithm performs faster in practice than the current state-of-the-art in trajectory clustering, in some examples by a large margin.
cs.LG:This study deals with the missing link prediction problem: the problem of predicting the existence of missing connections between entities of interest. We address link prediction using coupled analysis of relational datasets represented as heterogeneous data, i.e., datasets in the form of matrices and higher-order tensors. We propose to use an approach based on probabilistic interpretation of tensor factorisation models, i.e., Generalised Coupled Tensor Factorisation, which can simultaneously fit a large class of tensor models to higher-order tensors/matrices with com- mon latent factors using different loss functions. Numerical experiments demonstrate that joint analysis of data from multiple sources via coupled factorisation improves the link prediction performance and the selection of right loss function and tensor model is crucial for accurately predicting missing links.
cs.LG:The k-means algorithm is one of the well-known and most popular clustering algorithms. K-means seeks an optimal partition of the data by minimizing the sum of squared error with an iterative optimization procedure, which belongs to the category of hill climbing algorithms. As we know hill climbing searches are famous for converging to local optimums. Since k-means can converge to a local optimum, different initial points generally lead to different convergence cancroids, which makes it important to start with a reasonable initial partition in order to achieve high quality clustering solutions. However, in theory, there exist no efficient and universal methods for determining such initial partitions. In this paper we tried to find an optimum initial partitioning for k-means algorithm. To achieve this goal we proposed a new improved version of downhill simplex search, and then we used it in order to find an optimal result for clustering approach and then compare this algorithm with Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved Genetic K-Means (IGKM) and k-means algorithms.
cs.LG:Feature selection is one of the most prominent learning tasks, especially in high-dimensional datasets in which the goal is to understand the mechanisms that underly the learning dataset. However most of them typically deliver just a flat set of relevant features and provide no further information on what kind of structures, e.g. feature groupings, might underly the set of relevant features. In this paper we propose a new learning paradigm in which our goal is to uncover the structures that underly the set of relevant features for a given learning problem. We uncover two types of features sets, non-replaceable features that contain important information about the target variable and cannot be replaced by other features, and functionally similar features sets that can be used interchangeably in learned models, given the presence of the non-replaceable features, with no change in the predictive performance. To do so we propose a new learning algorithm that learns a number of disjoint models using a model disjointness regularization constraint together with a constraint on the predictive agreement of the disjoint models. We explore the behavior of our approach on a number of high-dimensional datasets, and show that, as expected by their construction, these satisfy a number of properties. Namely, model disjointness, a high predictive agreement, and a similar predictive performance to models learned on the full set of relevant features. The ability to structure the set of relevant features in such a manner can become a valuable tool in different applications of scientific knowledge discovery.
cs.LG:Cost-sensitive learning relies on the availability of a known and fixed cost matrix. However, in some scenarios, the cost matrix is uncertain during training, and re-train a classifier after the cost matrix is specified would not be an option. For binary classification, this issue can be successfully addressed by methods maximizing the Area Under the ROC Curve (AUC) metric. Since the AUC can measure performance of base classifiers independent of cost during training, and a larger AUC is more likely to lead to a smaller total cost in testing using the threshold moving method. As an extension of AUC to multi-class problems, MAUC has attracted lots of attentions and been widely used. Although MAUC also measures performance of base classifiers independent of cost, it is unclear whether a larger MAUC of classifiers is more likely to lead to a smaller total cost. In fact, it is also unclear what kinds of post-processing methods should be used in multi-class problems to convert base classifiers into discrete classifiers such that the total cost is as small as possible. In the paper, we empirically explore the relationship between MAUC and the total cost of classifiers by applying two categories of post-processing methods. Our results suggest that a larger MAUC is also beneficial. Interestingly, simple calibration methods that convert the output matrix into posterior probabilities perform better than existing sophisticated post re-optimization methods.
cs.LG:In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are successively applied on materials informatics to classify the engineering materials into different classes for the selection of materials that suit the input design specifications. Here, the classifiers are analyzed individually and their performance evaluation is analyzed with confusion matrix predictive parameters and standard measures, the classification results are analyzed on different class of materials. Comparison of classifiers has found that naive Bayesian classifier is more accurate and better than the C4.5 DTC. The knowledge discovered by the naive bayesian classifier can be employed for decision making in materials selection in manufacturing industries.
cs.LG:Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications.
cs.LG:This paper investigates energy efficiency for two-tier femtocell networks through combining game theory and stochastic learning. With the Stackelberg game formulation, a hierarchical reinforcement learning framework is applied to study the joint average utility maximization of macrocells and femtocells subject to the minimum signal-to-interference-plus-noise-ratio requirements. The macrocells behave as the leaders and the femtocells are followers during the learning procedure. At each time step, the leaders commit to dynamic strategies based on the best responses of the followers, while the followers compete against each other with no further information but the leaders' strategy information. In this paper, we propose two learning algorithms to schedule each cell's stochastic power levels, leading by the macrocells. Numerical experiments are presented to validate the proposed studies and show that the two learning algorithms substantially improve the energy efficiency of the femtocell networks.
cs.LG:We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this "independence" approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.
cs.LG:Time series classification is a field which has drawn much attention over the past decade. A new approach for classification of time series uses classification trees based on shapelets. A shapelet is a subsequence extracted from one of the time series in the dataset. A disadvantage of this approach is the time required for building the shapelet-based classification tree. The search for the best shapelet requires examining all subsequences of all lengths from all time series in the training set.   A key goal of this work was to find an evaluation order of the shapelets space which enables fast convergence to an accurate model. The comparative analysis we conducted clearly indicates that a random evaluation order yields the best results. Our empirical analysis of the distribution of high-quality shapelets within the shapelets space provides insights into why randomized shapelets sampling is superior to alternative evaluation orders.   We present an algorithm for randomized model generation for shapelet-based classification that converges extremely quickly to a model with surprisingly high accuracy after evaluating only an exceedingly small fraction of the shapelets space.
cs.LG:In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data. To solve this problem effectively, we first reformulate it as a convex semi-infinite programming (SIP) problem and then propose an efficient \emph{feature generating paradigm}. In contrast with traditional gradient-based approaches that conduct optimization on all input features, the proposed method iteratively activates a group of features and solves a sequence of multiple kernel learning (MKL) subproblems of much reduced scale. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such an optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm can guarantee that the solution converges globally under mild conditions and achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world datasets containing tens of million data points with $O(10^{14})$ features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training efficiency.
cs.LG:In this paper we introduce the first application of the Belief Propagation (BP) algorithm in the design of recommender systems. We formulate the recommendation problem as an inference problem and aim to compute the marginal probability distributions of the variables which represent the ratings to be predicted. However, computing these marginal probability functions is computationally prohibitive for large-scale systems. Therefore, we utilize the BP algorithm to efficiently compute these functions. Recommendations for each active user are then iteratively computed by probabilistic message passing. As opposed to the previous recommender algorithms, BPRS does not require solving the recommendation problem for all the users if it wishes to update the recommendations for only a single active. Further, BPRS computes the recommendations for each user with linear complexity and without requiring a training period. Via computer simulations (using the 100K MovieLens dataset), we verify that BPRS iteratively reduces the error in the predicted ratings of the users until it converges. Finally, we confirm that BPRS is comparable to the state of art methods such as Correlation-based neighborhood model (CorNgbr) and Singular Value Decomposition (SVD) in terms of rating and precision accuracy. Therefore, we believe that the BP-based recommendation algorithm is a new promising approach which offers a significant advantage on scalability while providing competitive accuracy for the recommender systems.
cs.LG:We describe a bootstrapping algorithm to learn from partially labeled data, and the results of an empirical study for using it to improve performance of sentiment classification using up to 15 million unlabeled Amazon product reviews. Our experiments cover semi-supervised learning, domain adaptation and weakly supervised learning. In some cases our methods were able to reduce test error by more than half using such large amount of data.   NOTICE: This is only the supplementary material.
cs.LG:We analyze an online learning algorithm that adaptively combines outputs of two constituent algorithms (or the experts) running in parallel to model an unknown desired signal. This online learning algorithm is shown to achieve (and in some cases outperform) the mean-square error (MSE) performance of the best constituent algorithm in the mixture in the steady-state. However, the MSE analysis of this algorithm in the literature uses approximations and relies on statistical models on the underlying signals and systems. Hence, such an analysis may not be useful or valid for signals generated by various real life systems that show high degrees of nonstationarity, limit cycles and, in many cases, that are even chaotic. In this paper, we produce results in an individual sequence manner. In particular, we relate the time-accumulated squared estimation error of this online algorithm at any time over any interval to the time accumulated squared estimation error of the optimal convex mixture of the constituent algorithms directly tuned to the underlying signal in a deterministic sense without any statistical assumptions. In this sense, our analysis provides the transient, steady-state and tracking behavior of this algorithm in a strong sense without any approximations in the derivations or statistical assumptions on the underlying signals such that our results are guaranteed to hold. We illustrate the introduced results through examples.
cs.LG:We investigate online kernel algorithms which simultaneously process multiple classification tasks while a fixed constraint is imposed on the size of their active sets. We focus in particular on the design of algorithms that can efficiently deal with problems where the number of tasks is extremely high and the task data are large scale. Two new projection-based algorithms are introduced to efficiently tackle those issues while presenting different trade offs on how the available memory is managed with respect to the prior information about the learning tasks. Theoretically sound budget algorithms are devised by coupling the Randomized Budget Perceptron and the Forgetron algorithms with the multitask kernel. We show how the two seemingly contrasting properties of learning from multiple tasks and keeping a constant memory footprint can be balanced, and how the sharing of the available space among different tasks is automatically taken care of. We propose and discuss new insights on the multitask kernel. Experiments show that online kernel multitask algorithms running on a budget can efficiently tackle real world learning problems involving multiple tasks.
cs.LG:We introduce semi-supervised data classification algorithms based on total variation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine (SVM), Cheeger cut, labeled and unlabeled data points. We design binary and multi-class semi-supervised classification algorithms. We compare the TV-based classification algorithms with the related Laplacian-based algorithms, and show that TV classification perform significantly better when the number of labeled data is small.
cs.LG:The expectation-maximization (EM) algorithm can compute the maximum-likelihood (ML) or maximum a posterior (MAP) point estimate of the mixture models or latent variable models such as latent Dirichlet allocation (LDA), which has been one of the most popular probabilistic topic modeling methods in the past decade. However, batch EM has high time and space complexities to learn big LDA models from big data streams. In this paper, we present a fast online EM (FOEM) algorithm that infers the topic distribution from the previously unseen documents incrementally with constant memory requirements. Within the stochastic approximation framework, we show that FOEM can converge to the local stationary point of the LDA's likelihood function. By dynamic scheduling for the fast speed and parameter streaming for the low memory usage, FOEM is more efficient for some lifelong topic modeling tasks than the state-of-the-art online LDA algorithms to handle both big data and big models (aka, big topic modeling) on just a PC.
cs.LG:In this paper we derive an efficient algorithm to learn the parameters of structured predictors in general graphical models. This algorithm blends the learning and inference tasks, which results in a significant speedup over traditional approaches, such as conditional random fields and structured support vector machines. For this purpose we utilize the structures of the predictors to describe a low dimensional structured prediction task which encourages local consistencies within the different structures while learning the parameters of the model. Convexity of the learning task provides the means to enforce the consistencies between the different parts. The inference-learning blending algorithm that we propose is guaranteed to converge to the optimum of the low dimensional primal and dual programs. Unlike many of the existing approaches, the inference-learning blending allows us to learn efficiently high-order graphical models, over regions of any size, and very large number of parameters. We demonstrate the effectiveness of our approach, while presenting state-of-the-art results in stereo estimation, semantic segmentation, shape reconstruction, and indoor scene understanding.
cs.LG:Boosting methods combine a set of moderately accurate weaklearners to form a highly accurate predictor. Despite the practical importance of multi-class boosting, it has received far less attention than its binary counterpart. In this work, we propose a fully-corrective multi-class boosting formulation which directly solves the multi-class problem without dividing it into multiple binary classification problems. In contrast, most previous multi-class boosting algorithms decompose a multi-boost problem into multiple binary boosting problems. By explicitly deriving the Lagrange dual of the primal optimization problem, we are able to construct a column generation-based fully-corrective approach to boosting which directly optimizes multi-class classification performance. The new approach not only updates all weak learners' coefficients at every iteration, but does so in a manner flexible enough to accommodate various loss functions and regularizations. For example, it enables us to introduce structural sparsity through mixed-norm regularization to promote group sparsity and feature sharing. Boosting with shared features is particularly beneficial in complex prediction problems where features can be expensive to compute. Our experiments on various data sets demonstrate that our direct multi-class boosting generalizes as well as, or better than, a range of competing multi-class boosting methods. The end result is a highly effective and compact ensemble classifier which can be trained in a distributed fashion.
cs.LG:We consider continuous-time sparse stochastic processes from which we have only a finite number of noisy/noiseless samples. Our goal is to estimate the noiseless samples (denoising) and the signal in-between (interpolation problem).   By relying on tools from the theory of splines, we derive the joint a priori distribution of the samples and show how this probability density function can be factorized. The factorization enables us to tractably implement the maximum a posteriori and minimum mean-square error (MMSE) criteria as two statistical approaches for estimating the unknowns. We compare the derived statistical methods with well-known techniques for the recovery of sparse signals, such as the $\ell_1$ norm and Log ($\ell_1$-$\ell_0$ relaxation) regularization methods. The simulation results show that, under certain conditions, the performance of the regularization techniques can be very close to that of the MMSE estimator.
cs.LG:In this paper, we consider the general scenario of resource sharing in a decentralized system when the resource rewards/qualities are time-varying and unknown to the users, and using the same resource by multiple users leads to reduced quality due to resource sharing. Firstly, we consider a user-independent reward model with no communication between the users, where a user gets feedback about the congestion level in the resource it uses. Secondly, we consider user-specific rewards and allow costly communication between the users. The users have a cooperative goal of achieving the highest system utility. There are multiple obstacles in achieving this goal such as the decentralized nature of the system, unknown resource qualities, communication, computation and switching costs. We propose distributed learning algorithms with logarithmic regret with respect to the optimal allocation. Our logarithmic regret result holds under both i.i.d. and Markovian reward models, as well as under communication, computation and switching costs.
cs.LG:We define a hierarchical clustering method: $\alpha$-unchaining single linkage or $SL(\alpha)$. The input of this algorithm is a finite metric space and a certain parameter $\alpha$. This method is sensitive to the density of the distribution and offers some solution to the so called chaining effect. We also define a modified version, $SL^*(\alpha)$, to treat the chaining through points or small blocks. We study the theoretical properties of these methods and offer some theoretical background for the treatment of chaining effects.
cs.LG:This work concerns a comparison of SVM kernel methods in text categorization tasks. In particular I define a kernel function that estimates the similarity between two objects computing by their compressed lengths. In fact, compression algorithms can detect arbitrarily long dependencies within the text strings. Data text vectorization looses information in feature extractions and is highly sensitive by textual language. Furthermore, these methods are language independent and require no text preprocessing. Moreover, the accuracy computed on the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than Gaussian, linear and polynomial kernels. The method limits are represented by computational time complexity of the Gram matrix and by very poor performance on non-textual datasets.
cs.LG:Transductive SVM (TSVM) is a well known semi-supervised large margin learning method for binary text classification. In this paper we extend this method to multi-class and hierarchical classification problems. We point out that the determination of labels of unlabeled examples with fixed classifier weights is a linear programming problem. We devise an efficient technique for solving it. The method is applicable to general loss functions. We demonstrate the value of the new method using large margin loss on a number of multi-class and hierarchical classification datasets. For maxent loss we show empirically that our method is better than expectation regularization/constraint and posterior regularization methods, and competitive with the version of entropy regularization method which uses label constraints.
cs.LG:In this paper, we present a novel algorithm for piecewise linear regression which can learn continuous as well as discontinuous piecewise linear functions. The main idea is to repeatedly partition the data and learn a liner model in in each partition. While a simple algorithm incorporating this idea does not work well, an interesting modification results in a good algorithm. The proposed algorithm is similar in spirit to $k$-means clustering algorithm. We show that our algorithm can also be viewed as an EM algorithm for maximum likelihood estimation of parameters under a reasonable probability model. We empirically demonstrate the effectiveness of our approach by comparing its performance with the state of art regression learning algorithms on some real world datasets.
cs.LG:This paper presents algorithm for missing values imputation in categorical data. The algorithm is based on using association rules and is presented in three variants. Experimental shows better accuracy of missing values imputation using the algorithm then using most common attribute value.
cs.LG:Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x^* are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x^*. In particular, regret with respect to x^* = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.
cs.LG:Random projection has been widely used in data classification. It maps high-dimensional data into a low-dimensional subspace in order to reduce the computational cost in solving the related optimization problem. While previous studies are focused on analyzing the classification performance of using random projection, in this work, we consider the recovery problem, i.e., how to accurately recover the optimal solution to the original optimization problem in the high-dimensional space based on the solution learned from the subspace spanned by random projections. We present a simple algorithm, termed Dual Random Projection, that uses the dual solution of the low-dimensional optimization problem to recover the optimal solution to the original problem. Our theoretical analysis shows that with a high probability, the proposed algorithm is able to accurately recover the optimal solution to the original problem, provided that the data matrix is of low rank or can be well approximated by a low rank matrix.
cs.LG:There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.
cs.LG:Improving students academic performance is not an easy task for the academic community of higher learning. The academic performance of engineering and science students during their first year at university is a turning point in their educational path and usually encroaches on their General Point Average,GPA in a decisive manner. The students evaluation factors like class quizzes mid and final exam assignment lab work are studied. It is recommended that all these correlated information should be conveyed to the class teacher before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students. In this paper, we present a hybrid procedure based on Decision Tree of Data mining method and Data Clustering that enables academicians to predict students GPA and based on that instructor can take necessary step to improve student academic performance.
cs.LG:In many practical applications of supervised learning the task involves the prediction of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are continuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classification domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called Stacked Single-Target and Ensemble of Regressor Chains, by adapting two popular multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods - a discrepancy of the values of the additional input variables between training and prediction - and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent improvements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods including regularization-based multi-task learning methods and a multi-objective random forest approach.
cs.LG:After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.
cs.LG:We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.
cs.LG:In data mining applications, feature selection is an essential process since it reduces a model's complexity. The cost of obtaining the feature values must be taken into consideration in many domains. In this paper, we study the cost-sensitive feature selection problem on numerical data with measurement errors, test costs and misclassification costs. The major contributions of this paper are four-fold. First, a new data model is built to address test costs and misclassification costs as well as error boundaries. Second, a covering-based rough set with measurement errors is constructed. Given a confidence interval, the neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in a three-dimension space, etc. Third, a new cost-sensitive feature selection problem is defined on this covering-based rough set. Fourth, both backtracking and heuristic algorithms are proposed to deal with this new problem. The algorithms are tested on six UCI (University of California - Irvine) data sets. Experimental results show that (1) the pruning techniques of the backtracking algorithm help reducing the number of operations significantly, and (2) the heuristic algorithm usually obtains optimal results. This study is a step toward realistic applications of cost-sensitive learning.
cs.LG:Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-the-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms.
cs.LG:In this paper, we present our work on clustering and prediction of temporal dynamics of global congestion configurations in large-scale road networks. Instead of looking into temporal traffic state variation of individual links, or of small areas, we focus on spatial congestion configurations of the whole network. In our work, we aim at describing the typical temporal dynamic patterns of this network-level traffic state and achieving long-term prediction of the large-scale traffic dynamics, in a unified data-mining framework. To this end, we formulate this joint task using Non-negative Tensor Factorization (NTF), which has been shown to be a useful decomposition tools for multivariate data sequences. Clustering and prediction are performed based on the compact tensor factorization results. Experiments on large-scale simulated data illustrate the interest of our method with promising results for long-term forecast of traffic evolution.
cs.LG:This paper presents a new hybrid Fuzzy-ART based K-Means Clustering technique to solve the part machine grouping problem in cellular manufacturing systems considering operational time. The performance of the proposed technique is tested with problems from open literature and the results are compared to the existing clustering models such as simple K-means algorithm and modified ART1 algorithm using an efficient modified performance measure known as modified grouping efficiency (MGE) as found in the literature. The results support the better performance of the proposed algorithm. The Novelty of this study lies in the simple and efficient methodology to produce quick solutions for shop floor managers with least computational efforts and time.
cs.LG:We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.
cs.LG:One of the ultimate goals of Manifold Learning (ML) is to reconstruct an unknown nonlinear low-dimensional manifold embedded in a high-dimensional observation space by a given set of data points from the manifold. We derive a local lower bound for the maximum reconstruction error in a small neighborhood of an arbitrary point. The lower bound is defined in terms of the distance between tangent spaces to the original manifold and the estimated manifold at the considered point and reconstructed point, respectively. We propose an amplification of the ML, called Tangent Bundle ML, in which the proximity not only between the original manifold and its estimator but also between their tangent spaces is required. We present a new algorithm that solves this problem and gives a new solution for the ML also.
cs.LG:Organizing data into semantically more meaningful is one of the fundamental modes of understanding and learning. Cluster analysis is a formal study of methods for understanding and algorithm for learning. K-mean clustering algorithm is one of the most fundamental and simple clustering algorithms. When there is no prior knowledge about the distribution of data sets, K-mean is the first choice for clustering with an initial number of clusters. In this paper a novel distance metric called Design Specification (DS) distance measure function is integrated with K-mean clustering algorithm to improve cluster accuracy. The K-means algorithm with proposed distance measure maximizes the cluster accuracy to 99.98% at P = 1.525, which is determined through the iterative procedure. The performance of Design Specification (DS) distance measure function with K - mean algorithm is compared with the performances of other standard distance functions such as Euclidian, squared Euclidean, City Block, and Chebshew similarity measures deployed with K-mean algorithm.The proposed method is evaluated on the engineering materials database. The experiments on cluster analysis and the outlier profiling show that these is an excellent improvement in the performance of the proposed method.
cs.LG:Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.
cs.LG:This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error-correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i) no constraints on the adversary other than an upper-bound on the number of errors, and (ii) some regularity properties for the original data. We present a simple and practical error-correction algorithm called SubSVMs that learns individual SVMs on several small-size (log-size), class-balanced, random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. Experimental results on synthetic as well as benchmark UCI data demonstrate the effectiveness of our approach. In addition to noise-tolerance, log-size subsampled bagging also yields significant run-time benefits over standard SVMs.
cs.LG:This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multi-armed bandit problems. The algorithm, also known as Thompson Sampling, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.
cs.LG:We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.
cs.LG:We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other's connections. We show how frequency/orientation "columns" as well as topographic filter maps follow naturally from training the model on image pairs. The model also helps explain why square-pooling models yield feature groups with similar grouping properties. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model.
cs.LG:Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.
cs.LG:In this correspondence, we will point out a problem with testing adaptive classifiers on autocorrelated data. In such a case random change alarms may boost the accuracy figures. Hence, we cannot be sure if the adaptation is working well.
cs.LG:We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods.
cs.LG:We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.
cs.LG:We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems.
cs.LG:Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.
cs.LG:Simultaneously solving multiple related learning tasks is beneficial under a variety of circumstances, but the prior knowledge necessary to correctly model task relationships is rarely available in practice. In this paper, we develop a novel kernel-based multi-task learning technique that automatically reveals structural inter-task relationships. Building over the framework of output kernel learning (OKL), we introduce a method that jointly learns multiple functions and a low-rank multi-task kernel by solving a non-convex regularization problem. Optimization is carried out via a block coordinate descent strategy, where each subproblem is solved using suitable conjugate gradient (CG) type iterative methods for linear operator equations. The effectiveness of the proposed approach is demonstrated on pharmacological and collaborative filtering data.
cs.LG:Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains. Since graph sparsification via spanning trees retains enough information while making the task much easier, trees are an important special case of this problem. Although it is known how to predict the nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm is not available yet. We fill this hole and introduce an efficient node predictor, Shazoo, which is nearly optimal on any weighted tree. Moreover, we show that Shazoo can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines. Experiments on real-world datasets confirm that Shazoo performs well in that it fully exploits the structure of the input tree, and gets very close to (and sometimes better than) less scalable energy minimization methods.
cs.LG:In online learning the performance of an algorithm is typically compared to the performance of a fixed function from some class, with a quantity called regret. Forster proposed a last-step min-max algorithm which was somewhat simpler than the algorithm of Vovk, yet with the same regret. In fact the algorithm he analyzed assumed that the choices of the adversary are bounded, yielding artificially only the two extreme cases. We fix this problem by weighing the examples in such a way that the min-max problem will be well defined, and provide analysis with logarithmic regret that may have better multiplicative factor than both bounds of Forster and Vovk. We also derive a new bound that may be sub-logarithmic, as a recent bound of Orabona et.al, but may have better multiplicative factor. Finally, we analyze the algorithm in a weak-type of non-stationary setting, and show a bound that is sub-linear if the non-stationarity is sub-linear as well.
cs.LG:In this paper, we propose a data representation model that demonstrates hierarchical feature learning using nsNMF. We extend unit algorithm into several layers. Experiments with document and image data successfully discovered feature hierarchies. We also prove that proposed method results in much better classification and reconstruction performance, especially for small number of features. feature hierarchies.
cs.LG:Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users to find relevant information, recommendations, and their preferred items. Slightly improvement of the accuracy of these recommenders can highly affect the quality of recommendations. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization which adds general neighborhood information on the recommendation model. Users and items are clustered into different categories to see how these categories share preferences. We then employ these shared interests of categories in a fusion by Biased Matrix Factorization to achieve more accurate recommendations. This is a complement for the current neighborhood aware matrix factorization models which rely on using direct neighborhood information of users and items. The proposed model is tested on two well-known recommendation system datasets: Movielens100k and Netflix. Our experiment shows applying the general latent features of categories into factorized recommender models improves the accuracy of recommendations. The current neighborhood-aware models need a great number of neighbors to acheive good accuracies. To the best of our knowledge, the proposed model is better than or comparable with the current neighborhood-aware models when they consider fewer number of neighbors.
cs.LG:In this paper, a novel approach for the optimal combination of binary classifiers is proposed. The classifier combination problem is approached from a Game Theory perspective. The proposed framework of adapted weighted majority rules (WMR) is tested against common rank-based, Bayesian and simple majority models, as well as two soft-output averaging rules. Experiments with ensembles of Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) and weighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate that this new adaptive WMR model, employing local accuracy estimators and the analytically computed optimal weights outperform all the other simple combination rules.
cs.LG:We propose a novel boosting approach to multi-class classification problems, in which multiple classes are distinguished by a set of random projection matrices in essence. The approach uses random projections to alleviate the proliferation of binary classifiers typically required to perform multi-class classification. The result is a multi-class classifier with a single vector-valued parameter, irrespective of the number of classes involved. Two variants of this approach are proposed. The first method randomly projects the original data into new spaces, while the second method randomly projects the outputs of learned weak classifiers. These methods are not only conceptually simple but also effective and easy to implement. A series of experiments on synthetic, machine learning and visual recognition data sets demonstrate that our proposed methods compare favorably to existing multi-class boosting algorithms in terms of both the convergence rate and classification accuracy.
cs.LG:Canonical correlation analysis is a statistical technique that is used to find relations between two sets of variables. An important extension in pattern analysis is to consider more than two sets of variables. This problem can be expressed as a quadratically constrained quadratic program (QCQP), commonly referred to Multi-set Canonical Correlation Analysis (MCCA). This is a non-convex problem and so greedy algorithms converge to local optima without any guarantees on global optimality. In this paper, we show that despite being highly structured, finding the optimal solution is NP-Hard. This motivates our relaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, can be solved reasonably efficiently and comes with both absolute and output-sensitive approximation quality. In addition to theoretical guarantees, we do an extensive comparison of the QCQP method and the SDP relaxation on a variety of synthetic and real world data. Finally, we present two useful extensions: we incorporate kernel methods and computing multiple sets of canonical vectors.
cs.LG:We consider two scenarios of multiclass online learning of a hypothesis class $H\subseteq Y^X$. In the {\em full information} scenario, the learner is exposed to instances together with their labels. In the {\em bandit} scenario, the true label is not exposed, but rather an indication whether the learner's prediction is correct or not. We show that the ratio between the error rates in the two scenarios is at most $8\cdot|Y|\cdot \log(|Y|)$ in the realizable case, and $\tilde{O}(\sqrt{|Y|})$ in the agnostic case. The results are tight up to a logarithmic factor and essentially answer an open question from (Daniely et. al. - Multiclass learnability and the erm principle).   We apply these results to the class of $\gamma$-margin multiclass linear classifiers in $\reals^d$. We show that the bandit error rate of this class is $\tilde{\Theta}(\frac{|Y|}{\gamma^2})$ in the realizable case and $\tilde{\Theta}(\frac{1}{\gamma}\sqrt{|Y|T})$ in the agnostic case. This resolves an open question from (Kakade et. al. - Efficient bandit algorithms for online multiclass prediction).
cs.LG:In this paper we consider learning in passive setting but with a slight modification. We assume that the target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. Unlike most studies in the learning theory that only incorporate the prior knowledge into the generalization bounds, we are able to explicitly utilize the target risk in the learning process. Our analysis reveals a surprising result on the sample complexity of learning: by exploiting the target risk in the learning algorithm, we show that when the loss function is both strongly convex and smooth, the sample complexity reduces to $\O(\log (\frac{1}{\epsilon}))$, an exponential improvement compared to the sample complexity $\O(\frac{1}{\epsilon})$ for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful.
cs.LG:We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. The standard benchmark is the loss of the best strategy chosen from a bounded comparator set. When the the comparison set and the adversary's gradients satisfy L_infinity bounds, we give the value of the game in closed form and prove it approaches sqrt(2T/pi) as T -> infinity.   Interesting algorithms result when we consider soft constraints on the comparator, rather than restricting it to a bounded set. As a warmup, we analyze the game with a quadratic penalty. The value of this game is exactly T/2, and this value is achieved by perhaps the simplest online algorithm of all: unprojected gradient descent with a constant learning rate.   We then derive a minimax-optimal algorithm for a much softer penalty function. This algorithm achieves good bounds under the standard notion of regret for any comparator point, without needing to specify the comparator set in advance. The value of this game converges to sqrt{e} as T ->infinity; we give a closed-form for the exact value as a function of T. The resulting algorithm is natural in unconstrained investment or betting scenarios, since it guarantees at worst constant loss, while allowing for exponential reward against an "easy" adversary.
cs.LG:We propose a tree ensemble method, referred to as time series forest (TSF), for time series classification. TSF employs a combination of the entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain criterion improves the accuracy of TSF. TSF randomly samples features at each tree node and has a computational complexity linear in the length of a time series and can be built using parallel computing techniques such as multi-core computing used here. The temporal importance curve is also proposed to capture the important temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, deviation and slope outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping, is computationally efficient, and can provide insights into the temporal characteristics.
cs.LG:Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchys knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach.
cs.LG:We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities.
cs.LG:The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T^{2/3} where T is the horizon time.
cs.LG:We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order $O(T^{2/3})$ with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after $T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This is optimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting of learning in a (single discrete) MDP.
cs.LG:Distance metric learning is of fundamental interest in machine learning because the distance metric employed can significantly affect the performance of many learning methods. Quadratic Mahalanobis metric learning is a popular approach to the problem, but typically requires solving a semidefinite programming (SDP) problem, which is computationally expensive. Standard interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with $D$ the dimension of input data), and can thus only practically solve problems exhibiting less than a few thousand variables. Since the number of variables is $D (D+1) / 2 $, this implies a limit upon the size of problem that can practically be solved of around a few hundred dimensions. The complexity of the popular quadratic Mahalanobis metric learning approach thus limits the size of problem to which metric learning can be applied. Here we propose a significantly more efficient approach to the metric learning problem based on the Lagrange dual formulation of the problem. The proposed formulation is much simpler to implement, and therefore allows much larger Mahalanobis metric learning problems to be solved. The time complexity of the proposed method is $O (D ^ 3) $, which is significantly lower than that of the SDP approach. Experiments on a variety of datasets demonstrate that the proposed method achieves an accuracy comparable to the state-of-the-art, but is applicable to significantly larger problems. We also show that the proposed method can be applied to solve more general Frobenius-norm regularized SDP problems approximately.
cs.LG:Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have some limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.   We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the \emph{bandit survey problem}. This model is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations. Our approach is based in our experience conducting relevance evaluation for a large commercial search engine.
cs.LG:Boosting is a method for learning a single accurate predictor by linearly combining a set of less accurate weak learners. Recently, structured learning has found many applications in computer vision. Inspired by structured support vector machines (SSVM), here we propose a new boosting algorithm for structured output prediction, which we refer to as StructBoost. StructBoost supports nonlinear structured learning by combining a set of weak structured learners. As SSVM generalizes SVM, our StructBoost generalizes standard boosting approaches such as AdaBoost, or LPBoost to structured learning. The resulting optimization problem of StructBoost is more challenging than SSVM in the sense that it may involve exponentially many variables and constraints. In contrast, for SSVM one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we formulate an equivalent $ 1 $-slack formulation and solve it using a combination of cutting planes and column generation. We show the versatility and usefulness of StructBoost on a range of problems such as optimizing the tree loss for hierarchical multi-class classification, optimizing the Pascal overlap criterion for robust visual tracking and learning conditional random field parameters for image segmentation.
cs.LG:Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.
cs.LG:We investigate the generalizability of learned binary relations: functions that map pairs of instances to a logical indicator. This problem has application in numerous areas of machine learning, such as ranking, entity resolution and link prediction. Our learning framework incorporates an example labeler that, given a sequence $X$ of $n$ instances and a desired training size $m$, subsamples $m$ pairs from $X \times X$ without replacement. The challenge in analyzing this learning scenario is that pairwise combinations of random variables are inherently dependent, which prevents us from using traditional learning-theoretic arguments. We present a unified, graph-based analysis, which allows us to analyze this dependence using well-known graph identities. We are then able to bound the generalization error of learned binary relations using Rademacher complexity and algorithmic stability. The rate of uniform convergence is partially determined by the labeler's subsampling process. We thus examine how various assumptions about subsampling affect generalization; under a natural random subsampling process, our bounds guarantee $\tilde{O}(1/\sqrt{n})$ uniform convergence.
cs.LG:In adaptive dynamic programming, neurocontrol and reinforcement learning, the objective is for an agent to learn to choose actions so as to minimise a total cost function. In this paper we show that when discretized time is used to model the motion of the agent, it can be very important to do "clipping" on the motion of the agent in the final time step of the trajectory. By clipping we mean that the final time step of the trajectory is to be truncated such that the agent stops exactly at the first terminal state reached, and no distance further. We demonstrate that when clipping is omitted, learning performance can fail to reach the optimum; and when clipping is done properly, learning performance can improve significantly.   The clipping problem we describe affects algorithms which use explicit derivatives of the model functions of the environment to calculate a learning gradient. These include Backpropagation Through Time for Control, and methods based on Dual Heuristic Dynamic Programming. However the clipping problem does not significantly affect methods based on Heuristic Dynamic Programming, Temporal Differences or Policy Gradient Learning algorithms. Similarly, the clipping problem does not affect fixed-length finite-horizon problems.
cs.LG:We propose a version of the follow-the-perturbed-leader online prediction algorithm in which the cumulative losses are perturbed by independent symmetric random walks. The forecaster is shown to achieve an expected regret of the optimal order O(sqrt(n log N)) where n is the time horizon and N is the number of experts. More importantly, it is shown that the forecaster changes its prediction at most O(sqrt(n log N)) times, in expectation. We also extend the analysis to online combinatorial optimization and show that even in this more general setting, the forecaster rarely switches between experts while having a regret of near-optimal order.
cs.LG:This paper addresses the problem of expressing a signal as a sum of frequency components (sinusoids) wherein each sinusoid may exhibit abrupt changes in its amplitude and/or phase. The Fourier transform of a narrow-band signal, with a discontinuous amplitude and/or phase function, exhibits spectral and temporal spreading. The proposed method aims to avoid such spreading by explicitly modeling the signal of interest as a sum of sinusoids with time-varying amplitudes. So as to accommodate abrupt changes, it is further assumed that the amplitude/phase functions are approximately piecewise constant (i.e., their time-derivatives are sparse). The proposed method is based on a convex variational (optimization) approach wherein the total variation (TV) of the amplitude functions are regularized subject to a perfect (or approximate) reconstruction constraint. A computationally efficient algorithm is derived based on convex optimization techniques. The proposed technique can be used to perform band-pass filtering that is relatively insensitive to narrow-band amplitude/phase jumps present in data, which normally pose a challenge (due to transients, leakage, etc.). The method is illustrated using both synthetic signals and human EEG data for the purpose of band-pass filtering and the estimation of phase synchrony indexes.
cs.LG:In this paper we address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop effective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithm's performances asymptotically approaches the performance of the best ARMA model in hindsight.
cs.LG:The framework of online learning with memory naturally captures learning problems with temporal constraints, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement our theoretic results with an application to statistical arbitrage in finance: we devise algorithms for constructing mean-reverting portfolios.
cs.LG:We consider online similarity prediction problems over networked data. We begin by relating this task to the more standard class prediction problem, showing that, given an arbitrary algorithm for class prediction, we can construct an algorithm for similarity prediction with "nearly" the same mistake bound, and vice versa. After noticing that this general construction is computationally infeasible, we target our study to {\em feasible} similarity prediction algorithms on networked data. We initially assume that the network structure is {\em known} to the learner. Here we observe that Matrix Winnow \cite{w07} has a near-optimal mistake guarantee, at the price of cubic prediction time per round. This motivates our effort for an efficient implementation of a Perceptron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time. Our focus then turns to the challenging case of networks whose structure is initially {\em unknown} to the learner. In this novel setting, where the network structure is only incrementally revealed, we obtain a mistake-bounded algorithm with a quadratic prediction time per round.
cs.LG:Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.
cs.LG:Sparse Subspace Clustering (SSC) has achieved state-of-the-art clustering quality by performing spectral clustering over a $\ell^{1}$-norm based similarity graph. However, SSC is a transductive method which does not handle with the data not used to construct the graph (out-of-sample data). For each new datum, SSC requires solving $n$ optimization problems in O(n) variables for performing the algorithm over the whole data set, where $n$ is the number of data points. Therefore, it is inefficient to apply SSC in fast online clustering and scalable graphing. In this letter, we propose an inductive spectral clustering algorithm, called inductive Sparse Subspace Clustering (iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts the assumption that high-dimensional data actually lie on the low-dimensional manifold such that out-of-sample data could be grouped in the embedding space learned from in-sample data. Experimental results show that iSSC is promising in clustering out-of-sample data.
cs.LG:In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WellSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WellSVM is also readily applicable on large data sets.
cs.LG:We propose a modular framework for multi-relational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques.
cs.LG:Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem.
cs.LG:Multitask clustering tries to improve the clustering performance of multiple tasks simultaneously by taking their relationship into account. Most existing multitask clustering algorithms fall into the type of generative clustering, and none are formulated as convex optimization problems. In this paper, we propose two convex Discriminative Multitask Clustering (DMTC) algorithms to address the problems. Specifically, we first propose a Bayesian DMTC framework. Then, we propose two convex DMTC objectives within the framework. The first one, which can be seen as a technical combination of the convex multitask feature learning and the convex Multiclass Maximum Margin Clustering (M3C), aims to learn a shared feature representation. The second one, which can be seen as a combination of the convex multitask relationship learning and M3C, aims to learn the task relationship. The two objectives are solved in a uniform procedure by the efficient cutting-plane algorithm. Experimental results on a toy problem and two benchmark datasets demonstrate the effectiveness of the proposed algorithms.
cs.LG:One important classifier ensemble for multiclass classification problems is Error-Correcting Output Codes (ECOCs). It bridges multiclass problems and binary-class classifiers by decomposing multiclass problems to a serial binary-class problems. In this paper, we present a heuristic ternary code, named Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). It starts with an arbitrary valid ECOC and iterates the following two steps until the training risk converges. The first step, named Layered Clustering based ECOC (LC-ECOC), constructs multiple strong classifiers on the most confusing binary-class problem. The second step adds the new classifiers to ECOC by a novel Optimized Weighted (OW) decoding algorithm, where the optimization problem of the decoding is solved by the cutting plane algorithm. Technically, LC-ECOC makes the heuristic training process not blocked by some difficult binary-class problem. OW decoding guarantees the non-increase of the training risk for ensuring a small code length. Results on 14 UCI datasets and a music genre classification problem demonstrate the effectiveness of WOLC-ECOC.
cs.LG:The goal of a learner in standard online learning is to maintain an average loss close to the loss of the best-performing single function in some class. In many real-world problems, such as rating or ranking items, there is no single best target function during the runtime of the algorithm, instead the best (local) target function is drifting over time. We develop a novel last-step minmax optimal algorithm in context of a drift. We analyze the algorithm in the worst-case regret framework and show that it maintains an average loss close to that of the best slowly changing sequence of linear functions, as long as the total of drift is sublinear. In some situations, our bound improves over existing bounds, and additionally the algorithm suffers logarithmic regret when there is no drift. We also build on the H_infinity filter and its bound, and develop and analyze a second algorithm for drifting setting. Synthetic simulations demonstrate the advantages of our algorithms in a worst-case constant drift setting.
cs.LG:Quorum sensing is a decentralized biological process, through which a community of cells with no global awareness coordinate their functional behaviors based solely on cell-medium interactions and local decisions. This paper draws inspirations from quorum sensing and colony competition to derive a new algorithm for data clustering. The algorithm treats each data as a single cell, and uses knowledge of local connectivity to cluster cells into multiple colonies simultaneously. It simulates auto-inducers secretion in quorum sensing to tune the influence radius for each cell. At the same time, sparsely distributed core cells spread their influences to form colonies, and interactions between colonies eventually determine each cell's identity. The algorithm has the flexibility to analyze not only static but also time-varying data, which surpasses the capacity of many existing algorithms. Its stability and convergence properties are established. The algorithm is tested on several applications, including both synthetic and real benchmarks data sets, alleles clustering, community detection, image segmentation. In particular, the algorithm's distinctive capability to deal with time-varying data allows us to experiment it on novel applications such as robotic swarms grouping and switching model identification. We believe that the algorithm's promising performance would stimulate many more exciting applications.
cs.LG:In imbalanced multi-class classification problems, the misclassification rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassification rate: misclassification costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classification problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the fine-grain informations contained in the matrix, especially in the case of imbalanced classes. Our first contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method --- namely AdaBoost.MM --- to the imbalanced class problem, by greedily minimizing the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods.
cs.LG:Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this p aper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods.
cs.LG:We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.
cs.LG:The k-support norm has been recently introduced to perform correlated sparsity regularization. Although Argyriou et al. only reported experiments using squared loss, here we apply it to several other commonly used settings resulting in novel machine learning algorithms with interesting and familiar limit cases. Source code for the algorithms described here is available.
cs.LG:Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexity of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE .
cs.LG:Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as positive semi-definite cones, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batch, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of convergence with only $O(\log T)$ projections. Our empirical study verifies the theoretical result.
cs.LG:Distance metric learning (DML) is an important task that has found applications in many domains. The high computational cost of DML arises from the large number of variables to be determined and the constraint that a distance metric has to be a positive semi-definite (PSD) matrix. Although stochastic gradient descent (SGD) has been successfully applied to improve the efficiency of DML, it can still be computationally expensive because in order to ensure that the solution is a PSD matrix, it has to, at every iteration, project the updated distance metric onto the PSD cone, an expensive operation. We address this challenge by developing two strategies within SGD, i.e. mini-batch and adaptive sampling, to effectively reduce the number of updates (i.e., projections onto the PSD cone) in SGD. We also develop hybrid approaches that combine the strength of adaptive sampling with that of mini-batch online learning techniques to further improve the computational efficiency of SGD for DML. We prove the theoretical guarantees for both adaptive sampling and mini-batch based approaches for DML. We also conduct an extensive empirical study to verify the effectiveness of the proposed algorithms for DML.
cs.LG:Applications of non-linear kernel Support Vector Machines (SVMs) to large datasets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training dataset. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine datasets compared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM \citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$ \citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection dataset, AESVM training was almost $10^3$ times faster than LIBSVM and LASVM and more than forty times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times.
cs.LG:Online learning algorithms are fast, memory-efficient, easy to implement, and applicable to many prediction problems, including classification, regression, and ranking. Several online algorithms were proposed in the past few decades, some based on additive updates, like the Perceptron, and some on multiplicative updates, like Winnow. A unifying perspective on the design and the analysis of online algorithms is provided by online mirror descent, a general prediction strategy from which most first-order algorithms can be obtained as special cases. We generalize online mirror descent to time-varying regularizers with generic updates. Unlike standard mirror descent, our more general formulation also captures second order algorithms, algorithms for composite losses and algorithms for adaptive filtering. Moreover, we recover, and sometimes improve, known regret bounds as special cases of our analysis using specific regularizers. Finally, we show the power of our approach by deriving a new second order algorithm with a regret bound invariant with respect to arbitrary rescalings of individual features.
cs.LG:Many studies in data mining have proposed a new learning called semi-Supervised. Such type of learning combines unlabeled and labeled data which are hard to obtain. However, in unsupervised methods, the only unlabeled data are used. The problem of significance and the effectiveness of semi-supervised clustering results is becoming of main importance. This paper pursues the thesis that muchgreater accuracy can be achieved in such clustering by improving the similarity computing. Hence, we introduce a new approach of semisupervised clustering using an innovative new homogeneity measure of generated clusters. Our experimental results demonstrate significantly improved accuracy as a result.
cs.LG:In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.
cs.LG:We consider the stochastic and adversarial settings of continuum armed bandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d -> R are assumed to intrinsically depend on at most k coordinate variables implying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknown i_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R with exponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed across time, we propose a simple modification of the CAB1 algorithm where we construct the discrete set of sampling points to obtain a bound of O(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the regret, with C(k,d) depending at most polynomially in k and sub-logarithmically in d. The construction is based on creating partitions of {1,..,d} into k disjoint subsets and is probabilistic, hence our result holds with high probability. Secondly we extend our results to also handle the more general case where (i_1,...,i_k) can change over time and derive regret bounds for the same.
cs.LG:We consider the problem of embedding entities and relations of knowledge bases in low-dimensional vector spaces. Unlike most existing approaches, which are primarily efficient for modeling equivalence relations, our approach is designed to explicitly model irreflexive relations, such as hierarchies, by interpreting them as translations operating on the low-dimensional embeddings of the entities. Preliminary experiments show that, despite its simplicity and a smaller number of parameters than previous approaches, our approach achieves state-of-the-art performance according to standard evaluation protocols on data from WordNet and Freebase.
cs.LG:Fractals are self-similar recursive structures that have been used in modeling several real world processes. In this work we study how "fractal-like" processes arise in a prediction game where an adversary is generating a sequence of bits and an algorithm is trying to predict them. We will see that under a certain formalization of the predictive payoff for the algorithm it is most optimal for the adversary to produce a fractal-like sequence to minimize the algorithm's ability to predict. Indeed it has been suggested before that financial markets exhibit a fractal-like behavior. We prove that a fractal-like distribution arises naturally out of an optimization from the adversary's perspective.   In addition, we give optimal trade-offs between predictability and expected deviation (i.e. sum of bits) for our formalization of predictive payoff. This result is motivated by the observation that several time series data exhibit higher deviations than expected for a completely random walk.
cs.LG:We consider the unsupervised learning problem of assigning labels to unlabeled data. A naive approach is to use clustering methods, but this works well only when data is properly clustered and each cluster corresponds to an underlying class. In this paper, we first show that this unsupervised labeling problem in balanced binary cases can be solved if two unlabeled datasets having different class balances are available. More specifically, estimation of the sign of the difference between probability densities of two unlabeled datasets gives the solution. We then introduce a new method to directly estimate the sign of the density difference without density estimation. Finally, we demonstrate the usefulness of the proposed method against several clustering methods on various toy problems and real-world datasets.
cs.LG:We present a brief survey of existing mistake bounds and introduce novel bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds generalize beyond standard margin-loss type bounds, allow for any convex and Lipschitz loss function, and admit a very simple proof.
cs.LG:Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.
cs.LG:In this study, a novel machine learning algorithm, restricted Boltzmann machine (RBM), is introduced. The algorithm is applied for the spectral classification in astronomy. RBM is a bipartite generative graphical model with two separate layers (one visible layer and one hidden layer), which can extract higher level features to represent the original data. Despite generative, RBM can be used for classification when modified with a free energy and a soft-max function. Before spectral classification, the original data is binarized according to some rule. Then we resort to the binary RBM to classify cataclysmic variables (CVs) and non-CVs (one half of all the given data for training and the other half for testing). The experiment result shows state-of-the-art accuracy of 100%, which indicates the efficiency of the binary RBM algorithm.
cs.LG:Methods for analyzing or learning from "fuzzy data" have attracted increasing attention in recent years. In many cases, however, existing methods (for precise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner, and without carefully considering the interpretation of a fuzzy set when being used for modeling data. Distinguishing between an ontic and an epistemic interpretation of fuzzy set-valued data, and focusing on the latter, we argue that a "fuzzification" of learning algorithms based on an application of the generic extension principle is not appropriate. In fact, the extension principle fails to properly exploit the inductive bias underlying statistical and machine learning methods, although this bias, at least in principle, offers a means for "disambiguating" the fuzzy data. Alternatively, we therefore propose a method which is based on the generalization of loss functions in empirical risk minimization, and which performs model identification and data disambiguation simultaneously. Elaborating on the fuzzification of specific types of losses, we establish connections to well-known loss functions in regression and classification. We compare our approach with related methods and illustrate its use in logistic regression for binary classification.
cs.LG:Representation learning and unsupervised learning are two central topics of machine learning and signal processing. Deep learning is one of the most effective unsupervised representation learning approach. The main contributions of this paper to the topics are as follows. (i) We propose to view the representative deep learning approaches as special cases of the knowledge reuse framework of clustering ensemble. (ii) We propose to view sparse coding when used as a feature encoder as the consensus function of clustering ensemble, and view dictionary learning as the training process of the base clusterings of clustering ensemble. (ii) Based on the above two views, we propose a very simple deep learning algorithm, named deep random model ensemble (DRME). It is a stack of random model ensembles. Each random model ensemble is a special k-means ensemble that discards the expectation-maximization optimization of each base k-means but only preserves the default initialization method of the base k-means. (iv) We propose to select the most powerful representation among the layers by applying DRME to clustering where the single-linkage is used as the clustering algorithm. Moreover, the DRME based clustering can also detect the number of the natural clusters accurately. Extensive experimental comparisons with 5 representation learning methods on 19 benchmark data sets demonstrate the effectiveness of DRME.
cs.LG:We consider the classical question of predicting binary sequences and study the {\em optimal} algorithms for obtaining the best possible regret and payoff functions for this problem. The question turns out to be also equivalent to the problem of optimal trade-offs between the regrets of two experts in an "experts problem", studied before by \cite{kearns-regret}. While, say, a regret of $\Theta(\sqrt{T})$ is known, we argue that it important to ask what is the provably optimal algorithm for this problem --- both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term.   In the basic setting, the result essentially follows from a classical result of Cover from '65. Here instead, we focus on another standard setting, of time-discounted payoffs, where the final "stopping time" is not specified. We exhibit an explicit characterization of the optimal regret for this setting.   To obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation. It turns out that characterization of the payoff function is qualitatively different from the classical (non-discounted) setting, and, namely, there's essentially a unique optimal solution.
cs.LG:AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.
cs.LG:In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, data-preprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.
cs.LG:In the internet era there has been an explosion in the amount of digital text information available, leading to difficulties of scale for traditional inference algorithms for topic models. Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method. We show connections between collapsed variational Bayesian inference and MAP estimation for LDA, and leverage these connections to prove convergence properties of the proposed algorithm. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than the previous method. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.
cs.LG:We consider the problem of online combinatorial optimization under semi-bandit feedback. The goal of the learner is to sequentially select its actions from a combinatorial decision set so as to minimize its cumulative loss. We propose a learning algorithm for this problem based on combining the Follow-the-Perturbed-Leader (FPL) prediction method with a novel loss estimation procedure called Geometric Resampling (GR). Contrary to previous solutions, the resulting algorithm can be efficiently implemented for any decision set where efficient offline combinatorial optimization is possible at all. Assuming that the elements of the decision set can be described with d-dimensional binary vectors with at most m non-zero entries, we show that the expected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As a side result, we also improve the best known regret bounds for FPL in the full information setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m) over previous bounds for this algorithm.
cs.LG:Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.
cs.LG:Auto-encoder is a special kind of neural network based on reconstruction. De-noising auto-encoder (DAE) is an improved auto-encoder which is robust to the input by corrupting the original data first and then reconstructing the original input by minimizing the reconstruction error function. And contractive auto-encoder (CAE) is another kind of improved auto-encoder to learn robust feature by introducing the Frobenius norm of the Jacobean matrix of the learned feature with respect to the original input. In this paper, we combine de-noising auto-encoder and contractive auto- encoder, and propose another improved auto-encoder, contractive de-noising auto- encoder (CDAE), which is robust to both the original input and the learned feature. We stack CDAE to extract more abstract features and apply SVM for classification. The experiment result on benchmark dataset MNIST shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method.
cs.LG:We present a novel approach for the construction of ensemble classifiers based on dimensionality reduction. Dimensionality reduction methods represent datasets using a small number of attributes while preserving the information conveyed by the original dataset. The ensemble members are trained based on dimension-reduced versions of the training set. These versions are obtained by applying dimensionality reduction to the original training set using different values of the input parameters. This construction meets both the diversity and accuracy criteria which are required to construct an ensemble classifier where the former criterion is obtained by the various input parameter values and the latter is achieved due to the decorrelation and noise reduction properties of dimensionality reduction. In order to classify a test sample, it is first embedded into the dimension reduced space of each individual classifier by using an out-of-sample extension algorithm. Each classifier is then applied to the embedded sample and the classification is obtained via a voting scheme. We present three variations of the proposed approach based on the Random Projections, the Diffusion Maps and the Random Subspaces dimensionality reduction algorithms. We also present a multi-strategy ensemble which combines AdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost, Rotation Forest ensemble classifiers and also with the base classifier which does not incorporate dimensionality reduction. Our experiments used seventeen benchmark datasets from the UCI repository. The results obtained by the proposed algorithms were superior in many cases to other algorithms.
cs.LG:Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).
cs.LG:Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.
cs.LG:The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for "cognitive tutors", i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an "outsider" to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory.
cs.LG:Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics.   In this work we propose the guided random forest (GRF) for feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. However, the trees in GRRF are built sequentially, are highly correlated and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in parallel. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), RF applied to features selected by GRF outperforms RF applied to all features on 9 data sets and 7 of them have significant differences at the 0.05 level. Therefore, both accuracy and interpretability are significantly improved. GRF selects more features than GRRF, however, leads to better classification accuracy. Note in this work the guided random forest is guided by the importance scores from an ordinary random forest, however, it can also be guided by other methods such as human insights (by specifying $\lambda_i$). GRF can be used in "RRF" v1.4 (and later versions), a package that also includes the regularized random forest methods.
cs.LG:We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.
cs.LG:Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics.
cs.LG:We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We also discuss a general competitive approach for auditing and possible modifications to the framework.
cs.LG:Learning an appropriate (dis)similarity function from the available data is a central problem in machine learning, since the success of many machine learning algorithms critically depends on the choice of a similarity function to compare examples. Despite many approaches for similarity metric learning have been proposed, there is little theoretical study on the links between similarity met- ric learning and the classification performance of the result classifier. In this paper, we propose a regularized similarity learning formulation associated with general matrix-norms, and establish their generalization bounds. We show that the generalization error of the resulting linear separator can be bounded by the derived generalization bound of similarity learning. This shows that a good gen- eralization of the learnt similarity function guarantees a good classification of the resulting linear classifier. Our results extend and improve those obtained by Bellet at al. [3]. Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrix- norm regularization. Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality enable us to establish bounds for regularized similarity learning formulations associated with general matrix-norms including sparse L 1 -norm and mixed (2,1)-norm.
cs.LG:We carefully investigate the on-line version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we analyze two popular on-line algorithms, Gradient Descent (GD) and Exponentiated Gradient (EG). We show that both algorithms are essentially optimal in the worst-case. This comes as a surprise, since EG is known to perform sub-optimally when the instances are sparse. This different behavior of EG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as function of a loss budget, EG remains optimal and strictly outperforms GD. Next, we study the extension of the PCA setting, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that EG is optimal and strictly better than GD in this setting.
cs.LG:We solve the COLT 2013 open problem of \citet{SCB} on minimizing regret in the setting of advice-efficient multiarmed bandits with expert advice. We give an algorithm for the setting of K arms and N experts out of which we are allowed to query and use only M experts' advices in each round, which has a regret bound of \tilde{O}\bigP{\sqrt{\frac{\min\{K, M\} N}{M} T}} after T rounds. We also prove that any algorithm for this problem must have expected regret at least \tilde{\Omega}\bigP{\sqrt{\frac{\min\{K, M\} N}{M}T}}, thus showing that our upper bound is nearly tight.
cs.LG:What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set. Our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework.
cs.LG:The work presented in this paper is part of a global framework which long term goal is to design a wireless sensor network able to support the observation of a population of endangered birds. We present the first stage for which we have conducted a knowledge discovery approach on a sample of acoustical data. We use MFCC features extracted from bird songs and we exploit two knowledge discovery techniques. One that relies on clustering-based approaches, that highlights the homogeneity in the songs of the species. The other, based on predictive modeling, that demonstrates the good performances of various machine learning techniques for the identification process. The knowledge elicited provides promising results to consider a widespread study and to elicit guidelines for designing a first version of the automatic approach for data collection based on acoustic sensors.
cs.LG:The feature space (including both input and output variables) characterises a data mining problem. In predictive (supervised) problems, the quality and availability of features determines the predictability of the dependent variable, and the performance of data mining models in terms of misclassification or regression error. Good features, however, are usually difficult to obtain. It is usual that many instances come with missing values, either because the actual value for a given attribute was not available or because it was too expensive. This is usually interpreted as a utility or cost-sensitive learning dilemma, in this case between misclassification (or regression error) costs and attribute tests costs. Both misclassification cost (MC) and test cost (TC) can be integrated into a single measure, known as joint cost (JC). We introduce methods and plots (such as the so-called JROC plots) that can work with any of-the-shelf predictive technique, including ensembles, such that we re-frame the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.
cs.LG:In multiclass semi-supervised learning (SSL), it is sometimes the case that the number of classes present in the data is not known, and hence no labeled examples are provided for some classes. In this paper we present variants of well-known semi-supervised multiclass learning methods that are robust when the data contains an unknown number of classes. In particular, we present an "exploratory" extension of expectation-maximization (EM) that explores different numbers of classes while learning. "Exploratory" SSL greatly improves performance on three datasets in terms of F1 on the classes with seed examples i.e., the classes which are expected to be in the data. Our Exploratory EM algorithm also outperforms a SSL method based non-parametric Bayesian clustering.
cs.LG:This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, $L_2$ regularization, {\em provides a bound for dropout training}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.
cs.LG:We describe the line search used in the minimum error rate training algorithm MERT as the "inside score" of a weighted proof forest under a semiring defined in terms of well-understood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation.
cs.LG:The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) the ability to tackle problems with a large number (say millions) of labels, and (b) the ability to handle data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to offer efficient algorithms. We further show that our learning framework admits formal excess risk bounds even in the presence of missing labels. Our risk bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as the Wikipedia dataset.
cs.LG:In this paper we study a generalized version of classical multi-armed bandits (MABs) problem by allowing for arbitrary constraints on constituent bandits at each decision point. The motivation of this study comes from many situations that involve repeatedly making choices subject to arbitrary constraints in an uncertain environment: for instance, regularly deciding which advertisements to display online in order to gain high click-through-rate without knowing user preferences, or what route to drive home each day under uncertain weather and traffic conditions. Assume that there are $K$ unknown random variables (RVs), i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. At each decision epoch, we select a strategy, i.e., a subset of RVs, subject to arbitrary constraints on constituent RVs.   We then gain a reward that is a linear combination of observations on selected RVs.   The performance of prior results for this problem heavily depends on the distribution of strategies generated by corresponding learning policy. For example, if the reward-difference between the best and second best strategy approaches zero, prior result may lead to arbitrarily large regret.   Meanwhile, when there are exponential number of possible strategies at each decision point, naive extension of a prior distribution-free policy would cause poor performance in terms of regret, computation and space complexity.   To this end, we propose an efficient Distribution-Free Learning (DFL) policy that achieves zero regret, regardless of the probability distribution of the resultant strategies.   Our learning policy has both $O(K)$ time complexity and $O(K)$ space complexity. In successive generations, we show that even if finding the optimal strategy at each decision point is NP-hard, our policy still allows for approximated solutions while retaining near zero-regret.
cs.LG:We present a scalable and effective classification model to train multi-class boosting for multi-class classification problems. Shen and Hao introduced a direct formulation of multi- class boosting in the sense that it directly maximizes the multi- class margin [C. Shen and Z. Hao, "A direct formulation for totally-corrective multi- class boosting", in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011]. The major problem of their approach is its high computational complexity for training, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise multi-class boosting method, which also directly maximizes the multi-class margin. Our approach of- fers a few advantages: 1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice of parameters and empirically demonstrates excellent generalization performance. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost compared to existing multi-class boosting.
cs.LG:In this work, we define cost-free learning (CFL) formally in comparison with cost-sensitive learning (CSL). The main difference between them is that a CFL approach seeks optimal classification results without requiring any cost information, even in the class imbalance problem. In fact, several CFL approaches exist in the related studies, such as sampling and some criteria-based pproaches. However, to our best knowledge, none of the existing CFL and CSL approaches are able to process the abstaining classifications properly when no information is given about errors and rejects. Based on information theory, we propose a novel CFL which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers. Using the strategy, we can deal with binary/multi-class classifications with/without abstaining. Significant features are observed from the new strategy. While the degree of class imbalance is changing, the proposed strategy is able to balance the errors and rejects accordingly and automatically. Another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the "equivalent" costs in binary classifications. The connection between rejection thresholds and ROC curve is explored. Empirical investigation is made on several benchmark data sets in comparison with other existing approaches. The classification results demonstrate a promising perspective of the strategy in machine learning.
cs.LG:In this paper Knockout Refinement Algorithm (KRA) is proposed to refine original clusters obtained by applying SOM and K-Means clustering algorithms. KRA Algorithm is based on Contingency Table concepts. Metrics are computed for the Original and Refined Clusters. Quality of Original and Refined Clusters are compared in terms of metrics. The proposed algorithm (KRA) is tested in the educational domain and results show that it generates better quality clusters in terms of improved metric values.
cs.LG:We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some known distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which "pretends" that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen, the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting.
cs.LG:The sequential minimal optimization (SMO) algorithm and variants thereof are the de facto standard method for solving large quadratic programs for support vector machine (SVM) training. In this paper we propose a simple yet powerful modification. The main emphasis is on an algorithm improving the SMO step size by planning-ahead. The theoretical analysis ensures its convergence to the optimum. Experiments involving a large number of datasets were carried out to demonstrate the superiority of the new algorithm.
cs.LG:We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning {\em symmetric} multiclass hypothesis classes---classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension.
cs.LG:Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.
cs.LG:It has been found that stochastic algorithms often find good solutions much more rapidly than inherently-batch approaches. Indeed, a very useful rule of thumb is that often, when solving a machine learning problem, an iterative technique which relies on performing a very large number of relatively-inexpensive updates will often outperform one which performs a smaller number of much "smarter" but computationally-expensive updates.   In this thesis, we will consider the application of stochastic algorithms to two of the most important machine learning problems. Part i is concerned with the supervised problem of binary classification using kernelized linear classifiers, for which the data have labels belonging to exactly two classes (e.g. "has cancer" or "doesn't have cancer"), and the learning problem is to find a linear classifier which is best at predicting the label. In Part ii, we will consider the unsupervised problem of Principal Component Analysis, for which the learning task is to find the directions which contain most of the variance of the data distribution.   Our goal is to present stochastic algorithms for both problems which are, above all, practical--they work well on real-world data, in some cases better than all known competing algorithms. A secondary, but still very important, goal is to derive theoretical bounds on the performance of these algorithms which are at least competitive with, and often better than, those known for other approaches.
cs.LG:We study the problem of predicting a set or list of options under knapsack constraint. The quality of such lists are evaluated by a submodular reward function that measures both quality and diversity. Similar to DAgger (Ross et al., 2010), by a reduction to online learning, we show how to adapt two sequence prediction models to imitate greedy maximization under knapsack constraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013). Experiments on extractive multi-document summarization show that our approach outperforms existing state-of-the-art methods.
cs.LG:This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem.
cs.LG:We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case.
cs.LG:We present an efficient distributed online learning scheme to classify data captured from distributed, heterogeneous, and dynamic data sources. Our scheme consists of multiple distributed local learners, that analyze different streams of data that are correlated to a common event that needs to be classified. Each learner uses a local classifier to make a local prediction. The local predictions are then collected by each learner and combined using a weighted majority rule to output the final prediction. We propose a novel online ensemble learning algorithm to update the aggregation rule in order to adapt to the underlying data dynamics. We rigorously determine a bound for the worst case misclassification probability of our algorithm which depends on the misclassification probabilities of the best static aggregation rule, and of the best local classifier. Importantly, the worst case misclassification probability of our algorithm tends asymptotically to 0 if the misclassification probability of the best static aggregation rule or the misclassification probability of the best local classifier tend to 0. Then we extend our algorithm to address challenges specific to the distributed implementation and we prove new bounds that apply to these settings. Finally, we test our scheme by performing an evaluation study on several data sets. When applied to data sets widely used by the literature dealing with dynamic data streams and concept drift, our scheme exhibits performance gains ranging from 34% to 71% with respect to state of the art solutions.
cs.LG:In this paper, we apply Classification Restricted Boltzmann Machine (ClassRBM) to the problem of predicting breast cancer recurrence. According to the Polish National Cancer Registry, in 2010 only, the breast cancer caused almost 25% of all diagnosed cases of cancer in Poland. We propose how to use ClassRBM for predicting breast cancer return and discovering relevant inputs (symptoms) in illness reappearance. Next, we outline a general probabilistic framework for learning Boltzmann machines with masks, which we refer to as Dropping. The fashion of generating masks leads to different learning methods, i.e., DropOut, DropConnect. We propose a new method called DropPart which is a generalization of DropConnect. In DropPart the Beta distribution instead of Bernoulli distribution in DropConnect is used. At the end, we carry out an experiment using real-life dataset consisting of 949 cases, provided by the Institute of Oncology Ljubljana.
cs.LG:In this work we consider the problem of learning a positive semidefinite kernel matrix from relative comparisons of the form: "object A is more similar to object B than it is to C", where comparisons are given by humans. Existing solutions to this problem assume many comparisons are provided to learn a high quality kernel. However, this can be considered unrealistic for many real-world tasks since relative assessments require human input, which is often costly or difficult to obtain. Because of this, only a limited number of these comparisons may be provided. In this work, we explore methods for aiding the process of learning a kernel with the help of auxiliary kernels built from more easily extractable information regarding the relationships among objects. We propose a new kernel learning approach in which the target kernel is defined as a conic combination of auxiliary kernels and a kernel whose elements are learned directly. We formulate a convex optimization to solve for this target kernel that adds only minor overhead to methods that use no auxiliary information. Empirical results show that in the presence of few training relative comparisons, our method can learn kernels that generalize to more out-of-sample comparisons than methods that do not utilize auxiliary information, as well as similar methods that learn metrics over objects.
cs.LG:We analyze the following group learning problem in the context of opinion diffusion: Consider a network with $M$ users, each facing $N$ options. In a discrete time setting, at each time step, each user chooses $K$ out of the $N$ options, and receive randomly generated rewards, whose statistics depend on the options chosen as well as the user itself, and are unknown to the users. Each user aims to maximize their expected total rewards over a certain time horizon through an online learning process, i.e., a sequence of exploration (sampling the return of each option) and exploitation (selecting empirically good options) steps.   Within this context we consider two group learning scenarios, (1) users with uniform preferences and (2) users with diverse preferences, and examine how a user should construct its learning process to best extract information from other's decisions and experiences so as to maximize its own reward. Performance is measured in {\em weak regret}, the difference between the user's total reward and the reward from a user-specific best single-action policy (i.e., always selecting the set of options generating the highest mean rewards for this user). Within each scenario we also consider two cases: (i) when users exchange full information, meaning they share the actual rewards they obtained from their choices, and (ii) when users exchange limited information, e.g., only their choices but not rewards obtained from these choices.
cs.LG:Most metric learning algorithms, as well as Fisher's Discriminant Analysis (FDA), optimize some cost function of different measures of within-and between-class distances. On the other hand, Support Vector Machines(SVMs) and several Multiple Kernel Learning (MKL) algorithms are based on the SVM large margin theory. Recently, SVMs have been analyzed from SVM and metric learning, and to develop new algorithms that build on the strengths of each. Inspired by the metric learning interpretation of SVM, we develop here a new metric-learning based SVM framework in which we incorporate metric learning concepts within SVM. We extend the optimization problem of SVM to include some measure of the within-class distance and along the way we develop a new within-class distance measure which is appropriate for SVM. In addition, we adopt the same approach for MKL and show that it can be also formulated as a Mahalanobis metric learning problem. Our end result is a number of SVM/MKL algorithms that incorporate metric learning concepts. We experiment with them on a set of benchmark datasets and observe important predictive performance improvements.
cs.LG:Recently a majorization method for optimizing partition functions of log-linear models was proposed alongside a novel quadratic variational upper-bound. In the batch setting, it outperformed state-of-the-art first- and second-order optimization methods on various learning tasks. We propose a stochastic version of this bound majorization method as well as a low-rank modification for high-dimensional data-sets. The resulting stochastic second-order method outperforms stochastic gradient descent (across variations and various tunings) both in terms of the number of iterations and computation time till convergence while finding a better quality parameter setting. The proposed method bridges first- and second-order stochastic optimization methods by maintaining a computational complexity that is linear in the data dimension and while exploiting second order information about the pseudo-global curvature of the objective function (as opposed to the local curvature in the Hessian).
cs.LG:Learning a distance metric from the given training samples plays a crucial role in many machine learning tasks, and various models and optimization algorithms have been proposed in the past decade. In this paper, we generalize several state-of-the-art metric learning methods, such as large margin nearest neighbor (LMNN) and information theoretic metric learning (ITML), into a kernel classification framework. First, doublets and triplets are constructed from the training samples, and a family of degree-2 polynomial kernel functions are proposed for pairs of doublets or triplets. Then, a kernel classification framework is established, which can not only generalize many popular metric learning methods such as LMNN and ITML, but also suggest new metric learning methods, which can be efficiently implemented, interestingly, by using the standard support vector machine (SVM) solvers. Two novel metric learning methods, namely doublet-SVM and triplet-SVM, are then developed under the proposed framework. Experimental results show that doublet-SVM and triplet-SVM achieve competitive classification accuracies with state-of-the-art metric learning methods such as ITML and LMNN but with significantly less training time.
cs.LG:We describe a primal-dual framework for the design and analysis of online convex optimization algorithms for {\em drifting regret}. Existing literature shows (nearly) optimal drifting regret bounds only for the $\ell_2$ and the $\ell_1$-norms. Our work provides a connection between these algorithms and the Online Mirror Descent ($\omd$) updates; one key insight that results from our work is that in order for these algorithms to succeed, it suffices to have the gradient of the regularizer to be bounded (in an appropriate norm). For situations (like for the $\ell_1$ norm) where the vanilla regularizer does not have this property, we have to {\em shift} the regularizer to ensure this. Thus, this helps explain the various updates presented in \cite{bansal10, buchbinder12}. We also consider the online variant of the problem with 1-lookahead, and with movement costs in the $\ell_2$-norm. Our primal dual approach yields nearly optimal competitive ratios for this problem.
cs.LG:In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical "high signal - high coupling" regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.
cs.LG:The k-Nearest Neighbor (k-NN) classification algorithm is one of the most widely-used lazy classifiers because of its simplicity and ease of implementation. It is considered to be an effective classifier and has many applications. However, its major drawback is that when sequential search is used to find the neighbors, it involves high computational cost. Speeding-up k-NN search is still an active research field. Hwang and Cho have recently proposed an adaptive cluster-based method for fast Nearest Neighbor searching. The effectiveness of this method is based on the adjustment of three parameters. However, the authors evaluated their method by setting specific parameter values and using only one dataset. In this paper, an extensive experimental study of this method is presented. The results, which are based on five real life datasets, illustrate that if the parameters of the method are carefully defined, one can achieve even better classification performance.
cs.LG:With the increasing number of mobile Apps developed, they are now closely integrated into daily life. In this paper, we develop a framework to predict mobile Apps that are most likely to be used regarding the current device status of a smartphone. Such an Apps usage prediction framework is a crucial prerequisite for fast App launching, intelligent user experience, and power management of smartphones. By analyzing real App usage log data, we discover two kinds of features: The Explicit Feature (EF) from sensing readings of built-in sensors, and the Implicit Feature (IF) from App usage relations. The IF feature is derived by constructing the proposed App Usage Graph (abbreviated as AUG) that models App usage transitions. In light of AUG, we are able to discover usage relations among Apps. Since users may have different usage behaviors on their smartphones, we further propose one personalized feature selection algorithm. We explore minimum description length (MDL) from the training data and select those features which need less length to describe the training data. The personalized feature selection can successfully reduce the log size and the prediction time. Finally, we adopt the kNN classification model to predict Apps usage. Note that through the features selected by the proposed personalized feature selection algorithm, we only need to keep these features, which in turn reduces the prediction time and avoids the curse of dimensionality when using the kNN classifier. We conduct a comprehensive experimental study based on a real mobile App usage dataset. The results demonstrate the effectiveness of the proposed framework and show the predictive capability for App usage prediction.
cs.LG:Multiple datasets containing different types of features may be available for a given task. For instance, users' profiles can be used to group users for recommendation systems. In addition, a model can also use users' historical behaviors and credit history to group users. Each dataset contains different information and suffices for learning. A number of clustering algorithms on multiple datasets were proposed during the past few years. These algorithms assume that at least one dataset is complete. So far as we know, all the previous methods will not be applicable if there is no complete dataset available. However, in reality, there are many situations where no dataset is complete. As in building a recommendation system, some new users may not have a profile or historical behaviors, while some may not have a credit history. Hence, no available dataset is complete. In order to solve this problem, we propose an approach called Collective Kernel Learning to infer hidden sample similarity from multiple incomplete datasets. The idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of the shared instances of the datasets. Furthermore, a clustering algorithm is proposed based on the kernel matrix. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information.
cs.LG:In many real-world tasks, particularly those involving data objects with complicated semantics such as images and texts, one object can be represented by multiple instances and simultaneously be associated with multiple labels. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems, and have been extensively studied during the past few years. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, in this paper we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 20K bags and 180K instances, MIMLfast is more than 100 times faster than existing MIML approaches. On a larger data set where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output label semantics.
cs.LG:In this paper, we present two localized graph filtering based methods for interpolating graph signals defined on the vertices of arbitrary graphs from only a partial set of samples. The first method is an extension of previous work on reconstructing bandlimited graph signals from partially observed samples. The iterative graph filtering approach very closely approximates the solution proposed in the that work, while being computationally more efficient. As an alternative, we propose a regularization based framework in which we define the cost of reconstruction to be a combination of smoothness of the graph signal and the reconstruction error with respect to the known samples, and find solutions that minimize this cost. We provide both a closed form solution and a computationally efficient iterative solution of the optimization problem. The experimental results on the recommendation system datasets demonstrate effectiveness of the proposed methods.
cs.LG:Graph-based Semi-supervised learning (SSL) algorithms have been successfully used in a large number of applications. These methods classify initially unlabeled nodes by propagating label information over the structure of graph starting from seed nodes. Graph-based SSL algorithms usually scale linearly with the number of distinct labels (m), and require O(m) space on each node. Unfortunately, there exist many applications of practical significance with very large m over large graphs, demanding better space and time complexity. In this paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which compactly stores label distribution on each node using Count-min Sketch, a randomized data structure. We present theoretical analysis showing that under mild conditions, MAD-SKETCH can reduce space complexity at each node from O(m) to O(log m), and achieve similar savings in time complexity as well. We support our analysis through experiments on multiple real world datasets. We observe that MAD-SKETCH achieves similar performance as existing state-of-the-art graph- based SSL algorithms, while requiring smaller memory footprint and at the same time achieving up to 10x speedup. We find that MAD-SKETCH is able to scale to datasets with one million labels, which is beyond the scope of existing graph- based SSL algorithms.
cs.LG:We present a general framework to learn functions in tensor product reproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a novel representer theorem suitable for existing as well as new spectral penalties for tensors. When the functions in the TP-RKHS are defined on the Cartesian product of finite discrete sets, in particular, our main problem formulation admits as a special case existing tensor completion problems. Other special cases include transfer learning with multimodal side information and multilinear multitask learning. For the latter case, our kernel-based view is instrumental to derive nonlinear extensions of existing model classes. We give a novel algorithm and show in experiments the usefulness of the proposed extensions.
cs.LG:We consider the multiarm bandit problems in the timevarying dynamic system for rich structural features. For the nonlinear dynamic model, we propose the approximate inference for the posterior distributions based on Laplace Approximation. For the context bandit problems, Thompson Sampling is adopted based on the underlying posterior distributions of the parameters. More specifically, we introduce the discount decays on the previous samples impact and analyze the different decay rates with the underlying sample dynamics. Consequently, the exploration and exploitation is adaptively tradeoff according to the dynamics in the system.
cs.LG:Clustering trajectory data attracted considerable attention in the last few years. Most of prior work assumed that moving objects can move freely in an euclidean space and did not consider the eventual presence of an underlying road network and its influence on evaluating the similarity between trajectories. In this paper, we present an approach to clustering such network-constrained trajectory data. More precisely we aim at discovering groups of road segments that are often travelled by the same trajectories. To achieve this end, we model the interactions between segments w.r.t. their similarity as a weighted graph to which we apply a community detection algorithm to discover meaningful clusters. We showcase our proposition through experimental results obtained on synthetic datasets.
cs.LG:In this paper we propose a multi-task linear classifier learning problem called D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance shared by all tasks to do multi-task knowledge transfer among different tasks. We formally define the learning problem of D-SVM and show two interpretations of this problem, from both the probabilistic and kernel perspectives. From the probabilistic perspective, we show that our learning formulation is actually a MAP estimation on all optimization variables. We also show its equivalence to a multiple kernel learning problem in which one is trying to find a re-weighting kernel for features from a dictionary of basis (despite the fact that only linear classifiers are learned). Finally, we describe an alternative optimization scheme to minimize the objective function and present empirical studies to valid our algorithm.
cs.LG:Second-price auctions with reserve play a critical role for modern search engine and popular online sites since the revenue of these companies often directly de- pends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real data demonstrating their effectiveness.
cs.LG:We present an extensive analysis of relative deviation bounds, including detailed proofs of two-sided inequalities and their implications. We also give detailed proofs of two-sided generalization bounds that hold in the general case of unbounded loss functions, under the assumption that a moment of the loss is bounded. These bounds are useful in the analysis of importance weighting and other learning tasks such as unbounded regression.
cs.LG:We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases and competitive results in the continuous case.
cs.LG:Principal Component Analysis (PCA) is a ubiquitous tool with many applications in machine learning including feature construction, subspace embedding, and outlier detection. In this paper, we present an algorithm for computing the top principal components of a dataset with a large number of rows (examples) and columns (features). Our algorithm leverages both structured and unstructured random projections to retain good accuracy while being computationally efficient. We demonstrate the technique on the winning submission the KDD 2010 Cup.
cs.LG:Sophisticated automatic incident detection (AID) technology plays a key role in contemporary transportation systems. Though many papers were devoted to study incident classification algorithms, few study investigated how to enhance feature representation of incidents to improve AID performance. In this paper, we propose to use an unsupervised feature learning algorithm to generate higher level features to represent incidents. We used real incident data in the experiments and found that effective feature mapping function can be learnt from the data crosses the test sites. With the enhanced features, detection rate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are significantly improved in all of the three representative cases. This approach also provides an alternative way to reduce the amount of labeled data, which is expensive to obtain, required in training better incident classifiers since the feature learning is unsupervised.
cs.LG:Scalable machine learning over big data is an important problem that is receiving a lot of attention in recent years. On popular distributed environments such as Hadoop running on a cluster of commodity machines, communication costs are substantial and algorithms need to be designed suitably considering those costs. In this paper we give a novel approach to the distributed training of linear classifiers (involving smooth losses and L2 regularization) that is designed to reduce the total communication costs. At each iteration, the nodes minimize locally formed approximate objective functions; then the resulting minimizers are combined to form a descent direction to move. Our approach gives a lot of freedom in the formation of the approximate objective function as well as in the choice of methods to solve them. The method is shown to have $O(log(1/\epsilon))$ time convergence. The method can be viewed as an iterative parameter mixing method. A special instantiation yields a parallel stochastic gradient descent method with strong convergence. When communication times between nodes are large, our method is much faster than the Terascale method (Agarwal et al., 2011), which is a state of the art distributed solver based on the statistical query model (Chuet al., 2006) that computes function and gradient values in a distributed fashion. We also evaluate against other recent distributed methods and demonstrate superior performance of our method.
cs.LG:We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We compare the methods against the state-of-the-art machine learning approaches on a set of heterogeneous multilabel benchmark problems, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that random graph ensembles are viable alternatives to flat multilabel and multitask learners.
cs.LG:Pattern recognition techniques have been employed in a myriad of industrial, medical, commercial and academic applications. To tackle such a diversity of data, many techniques have been devised. However, despite the long tradition of pattern recognition research, there is no technique that yields the best classification in all scenarios. Therefore, the consideration of as many as possible techniques presents itself as an fundamental practice in applications aiming at high accuracy. Typical works comparing methods either emphasize the performance of a given algorithm in validation tests or systematically compare various algorithms, assuming that the practical use of these methods is done by experts. In many occasions, however, researchers have to deal with their practical classification tasks without an in-depth knowledge about the underlying mechanisms behind parameters. Actually, the adequate choice of classifiers and parameters alike in such practical circumstances constitutes a long-standing problem and is the subject of the current paper. We carried out a study on the performance of nine well-known classifiers implemented by the Weka framework and compared the dependence of the accuracy with their configuration parameter configurations. The analysis of performance with default parameters revealed that the k-nearest neighbors method exceeds by a large margin the other methods when high dimensional datasets are considered. When other configuration of parameters were allowed, we found that it is possible to improve the quality of SVM in more than 20% even if parameters are set randomly. Taken together, the investigation conducted in this paper suggests that, apart from the SVM implementation, Weka's default configuration of parameters provides an performance close the one achieved with the optimal configuration.
cs.LG:We study exploration in Multi-Armed Bandits in a setting where $k$ players collaborate in order to identify an $\epsilon$-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the $k$ players to communicate only once, they are able to learn $\sqrt{k}$ times faster than a single player. That is, distributing learning to $k$ players gives rise to a factor $\sqrt{k}$ parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in $1/\epsilon$.
cs.LG:The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within $10^{-6}$ relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.
cs.LG:Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper.
cs.LG:It is surprising that last two decades many works in time series data mining and clustering were concerned with measures of similarity of time series but not with measures of association that can be used for measuring possible direct and inverse relationships between time series. Inverse relationships can exist between dynamics of prices and sell volumes, between growth patterns of competitive companies, between well production data in oilfields, between wind velocity and air pollution concentration etc. The paper develops a theoretical basis for analysis and construction of time series shape association measures. Starting from the axioms of time series shape association measures it studies the methods of construction of measures satisfying these axioms. Several general methods of construction of such measures suitable for measuring time series shape similarity and shape association are proposed. Time series shape association measures based on Minkowski distance and data standardization methods are considered. The cosine similarity and the Pearsons correlation coefficient are obtained as particular cases of the proposed general methods that can be used also for construction of new association measures in data analysis.
cs.LG:We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents' behaviors express key features of human behavior as predicted by prospect theory (Kahneman and Tversky, 1979), for example different risk-preferences for gains and losses as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework we apply it to quantify human behavior in a sequential investment task. We find, that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses which is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals show a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex and insula, which is not present if standard Q-values are used.
cs.LG:We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages.
cs.LG:Missing value imputation is an important practical problem. There is a large body of work on it, but there does not exist any work that formulates the problem in a structured output setting. Also, most applications have constraints on the imputed data, for example on the distribution associated with each variable. None of the existing imputation methods use these constraints. In this paper we propose a structured output approach for missing value imputation that also incorporates domain constraints. We focus on large margin models, but it is easy to extend the ideas to probabilistic models. We deal with the intractable inference step in learning via a piecewise training technique that is simple, efficient, and effective. Comparison with existing state-of-the-art and baseline imputation methods shows that our method gives significantly improved performance on the Hamming loss measure.
cs.LG:In structured output learning, obtaining labelled data for real-world applications is usually costly, while unlabelled examples are available in abundance. Semi-supervised structured classification has been developed to handle large amounts of unlabelled structured data. In this work, we consider semi-supervised structural SVMs with domain constraints. The optimization problem, which in general is not convex, contains the loss terms associated with the labelled and unlabelled examples along with the domain constraints. We propose a simple optimization approach, which alternates between solving a supervised learning problem and a constraint matching problem. Solving the constraint matching problem is difficult for structured prediction, and we propose an efficient and effective hill-climbing method to solve it. The alternating optimization is carried out within a deterministic annealing framework, which helps in effective constraint matching, and avoiding local minima which are not very useful. The algorithm is simple to implement and achieves comparable generalization performance on benchmark datasets.
cs.LG:The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a {\em computational} resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?   We give the first positive answer to this question for a {\em natural supervised learning problem} --- we consider agnostic PAC learning of halfspaces over $3$-sparse vectors in $\{-1,1,0\}^n$. This class is inefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random $\mathrm{3CNF}$ formulas is hard, it is impossible to efficiently learn this class using only $O\left(n/\epsilon^2\right)$ examples. We further show that under stronger hardness assumptions, even $O\left(n^{1.499}/\epsilon^2\right)$ examples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.
cs.LG:We consider the problem of quantitatively evaluating missing value imputation algorithms. Given a dataset with missing values and a choice of several imputation algorithms to fill them in, there is currently no principled way to rank the algorithms using a quantitative metric. We develop a framework based on treating imputation evaluation as a problem of comparing two distributions and show how it can be used to compute quantitative metrics. We present an efficient procedure for applying this framework to practical datasets, demonstrate several metrics derived from the existing literature on comparing distributions, and propose a new metric called Neighborhood-based Dissimilarity Score which is fast to compute and provides similar results. Results are shown on several datasets, metrics, and imputations algorithms.
cs.LG:The kernel $k$-means is an effective method for data clustering which extends the commonly-used $k$-means algorithm to work on a similarity matrix over complex data structures. The kernel $k$-means algorithm is however computationally very complex as it requires the complete data matrix to be calculated and stored. Further, the kernelized nature of the kernel $k$-means algorithm hinders the parallelization of its computations on modern infrastructures for distributed computing. In this paper, we are defining a family of kernel-based low-dimensional embeddings that allows for scaling kernel $k$-means on MapReduce via an efficient and unified parallelization strategy. Afterwards, we propose two methods for low-dimensional embedding that adhere to our definition of the embedding family. Exploiting the proposed parallelization strategy, we present two scalable MapReduce algorithms for kernel $k$-means. We demonstrate the effectiveness and efficiency of the proposed algorithms through an empirical evaluation on benchmark data sets.
cs.LG:The task of assigning label sequences to a set of observed sequences is common in computational linguistics. Several models for sequence labeling have been proposed over the last few years. Here, we focus on discriminative models for sequence labeling. Many batch and online (updating model parameters after visiting each example) learning algorithms have been proposed in the literature. On large datasets, online algorithms are preferred as batch learning methods are slow. These online algorithms were designed to solve either a primal or a dual problem. However, there has been no systematic comparison of these algorithms in terms of their speed, generalization performance (accuracy/likelihood) and their ability to achieve steady state generalization performance fast. With this aim, we compare different algorithms and make recommendations, useful for a practitioner. We conclude that the selection of an algorithm for sequence labeling depends on the evaluation criterion used and its implementation simplicity.
cs.LG:Echo State Networks (ESNs) are a special type of the temporally deep network model, the Recurrent Neural Network (RNN), where the recurrent matrix is carefully designed and both the recurrent and input matrices are fixed. An ESN uses the linearity of the activation function of the output units to simplify the learning of the output matrix. In this paper, we devise a special technique that take advantage of this linearity in the output units of an ESN, to learn the input and recurrent matrices. This has not been done in earlier ESNs due to their well known difficulty in learning those matrices. Compared to the technique of BackPropagation Through Time (BPTT) in learning general RNNs, our proposed method exploits linearity of activation function in the output units to formulate the relationships amongst the various matrices in an RNN. These relationships results in the gradient of the cost function having an analytical form and being more accurate. This would enable us to compute the gradients instead of obtaining them by recursion as in BPTT. Experimental results on phone state classification show that learning one or both the input and recurrent matrices in an ESN yields superior results compared to traditional ESNs that do not learn these matrices, especially when longer time steps are used.
cs.LG:Many researches have been devoted to learn a Mahalanobis distance metric, which can effectively improve the performance of kNN classification. Most approaches are iterative and computational expensive and linear rigidity still critically limits metric learning algorithm to perform better. We proposed a computational economical framework to learn multiple metrics in closed-form.
cs.LG:In this paper, we push forward the idea of machine learning systems whose operators can be modified and fine-tuned for each problem. This allows us to propose a learning paradigm where users can write (or adapt) their operators, according to the problem, data representation and the way the information should be navigated. To achieve this goal, data instances, background knowledge, rules, programs and operators are all written in the same functional language, Erlang. Since changing operators affect how the search space needs to be explored, heuristics are learnt as a result of a decision process based on reinforcement learning where each action is defined as a choice of operator and rule. As a result, the architecture can be seen as a 'system for writing machine learning systems' or to explore new operators where the policy reuse (as a kind of transfer learning) is allowed. States and actions are represented in a Q matrix which is actually a table, from which a supervised model is learnt. This makes it possible to have a more flexible mapping between old and new problems, since we work with an abstraction of rules and actions. We include some examples sharing reuse and the application of the system gErl to IQ problems. In order to evaluate gErl, we will test it against some structured problems: a selection of IQ test tasks and some experiments on some structured prediction problems (list patterns).
cs.LG:The covariate shift is a challenging problem in supervised learning that results from the discrepancy between the training and test distributions. An effective approach which recently drew a considerable attention in the research community is to reweight the training samples to minimize that discrepancy. In specific, many methods are based on developing Density-ratio (DR) estimation techniques that apply to both regression and classification problems. Although these methods work well for regression problems, their performance on classification problems is not satisfactory. This is due to a key observation that these methods focus on matching the sample marginal distributions without paying attention to preserving the separation between classes in the reweighted space. In this paper, we propose a novel method for Discriminative Density-ratio (DDR) estimation that addresses the aforementioned problem and aims at estimating the density-ratio of joint distributions in a class-wise manner. The proposed algorithm is an iterative procedure that alternates between estimating the class information for the test data and estimating new density ratio for each class. To incorporate the estimated class information of the test data, a soft matching technique is proposed. In addition, we employ an effective criterion which adopts mutual information as an indicator to stop the iterative procedure while resulting in a decision boundary that lies in a sparse region. Experiments on synthetic and benchmark datasets demonstrate the superiority of the proposed method in terms of both accuracy and robustness.
cs.LG:A hierarchical clustering method is stable if small perturbations on the data set produce small perturbations in the result. These perturbations are measured using the Gromov-Hausdorff metric. We study the problem of stability on linkage-based hierarchical clustering methods. We obtain that, under some basic conditions, standard linkage-based methods are semi-stable. This means that they are stable if the input data is close enough to an ultrametric space. We prove that, apart from exotic examples, introducing any unchaining condition in the algorithm always produces unstable methods.
cs.LG:Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches.
cs.LG:Challenging optimization problems, which elude acceptable solution via conventional calculus methods, arise commonly in different areas of industrial design and practice. Hard optimization problems are those who manifest the following behavior: a) high number of independent input variables; b) very complex or irregular multi-modal fitness; c) computational expensive fitness evaluation. This paper will focus on some theoretical issues that have strong implications for practice. I will stress how an interpretation of the No Free Lunch theorem leads naturally to a general Bayesian optimization framework. The choice of a prior over the space of functions is a critical and inevitable step in every black-box optimization.
cs.LG:Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison.
cs.LG:Novelty detection plays an important role in machine learning and signal processing. This paper studies novelty detection in a new setting where the data object is represented as a bag of instances and associated with multiple class labels, referred to as multi-instance multi-label (MIML) learning. Contrary to the common assumption in MIML that each instance in a bag belongs to one of the known classes, in novelty detection, we focus on the scenario where bags may contain novel-class instances. The goal is to determine, for any given instance in a new bag, whether it belongs to a known class or a novel class. Detecting novelty in the MIML setting captures many real-world phenomena and has many potential applications. For example, in a collection of tagged images, the tag may only cover a subset of objects existing in the images. Discovering an object whose class has not been previously tagged can be useful for the purpose of soliciting a label for the new object class. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed method, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in testing stage.
cs.LG:We study sequential prediction of real-valued, arbitrary and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios we achieve matching lower and upper bounds demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result we also prove that for the worst case scenario, the performance of randomized algorithms can be achieved by sequential algorithms so that randomized algorithms does not improve the performance.
cs.LG:We consider the problem of learning reject option classifiers. The goodness of a reject option classifier is quantified using $0-d-1$ loss function wherein a loss $d \in (0,.5)$ is assigned for rejection. In this paper, we propose {\em double ramp loss} function which gives a continuous upper bound for $(0-d-1)$ loss. Our approach is based on minimizing regularized risk under the double ramp loss using {\em difference of convex (DC) programming}. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets. Our approach performs better than the state of the art reject option classification approaches.
cs.LG:We introduce a novel family of adaptive filtering algorithms based on a relative logarithmic cost. The new family intrinsically combines the higher and lower order measures of the error into a single continuous update based on the error amount. We introduce important members of this family of algorithms such as the least mean logarithmic square (LMLS) and least logarithmic absolute difference (LLAD) algorithms that improve the convergence performance of the conventional algorithms. However, our approach and analysis are generic such that they cover other well-known cost functions as described in the paper. The LMLS algorithm achieves comparable convergence performance with the least mean fourth (LMF) algorithm and extends the stability bound on the step size. The LLAD and least mean square (LMS) algorithms demonstrate similar convergence performance in impulse-free noise environments while the LLAD algorithm is robust against impulsive interferences and outperforms the sign algorithm (SA). We analyze the transient, steady state and tracking performance of the introduced algorithms and demonstrate the match of the theoretical analyzes and simulation results. We show the extended stability bound of the LMLS algorithm and analyze the robustness of the LLAD algorithm against impulsive interferences. Finally, we demonstrate the performance of our algorithms in different scenarios through numerical examples.
cs.LG:TThe problem is to identify a probability associated with a set of natural numbers, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function involved (the target) belongs to a computably enumerable (c.e.) or co-computably enumerable (co-c.e.) set of computable probability mass functions, then there is an algorithm to almost surely identify the target in the limit. The technical tool is the strong law of large numbers. If the set is finite and the elements of the sequence are dependent while the sequence is typical in the sense of Martin-L\"of for at least one measure belonging to a c.e. or co-c.e. set of computable measures, then there is an algorithm to identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions.
cs.LG:The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, user's aggregated purchase history and competitive online travel agency information for each potential hotel choice. This paper describes the solution of team "binghsu & MLRush & BrickMover". We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model's output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition.
cs.LG:When belief propagation (BP) converges, it does so to a stationary point of the Bethe free energy $F$, and is often strikingly accurate. However, it may converge only to a local optimum or may not converge at all. An algorithm was recently introduced for attractive binary pairwise MRFs which is guaranteed to return an $\epsilon$-approximation to the global minimum of $F$ in polynomial time provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number of variables. Here we significantly improve this algorithm and derive several results including a new approach based on analyzing first derivatives of $F$, which leads to performance that is typically far superior and yields a fully polynomial-time approximation scheme (FPTAS) for attractive models without any degree restriction. Further, the method applies to general (non-attractive) models, though with no polynomial time guarantee in this case, leading to the important result that approximating $\log$ of the Bethe partition function, $\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may be reduced to a discrete MAP inference problem. We explore an application to predicting equipment failure on an urban power network and demonstrate that the Bethe approximation can perform well even when BP fails to converge.
cs.LG:Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p does not have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can achieve better accuracies than the previous formulations through the right choice of t.
cs.LG:Gaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high computational cost for big data. In this paper, we propose a new Bayesian approach, EigenGP, that learns both basis dictionary elements--eigenfunctions of a GP prior--and prior precisions in a sparse finite model. It is well known that, among all orthogonal basis functions, eigenfunctions can provide the most compact representation. Unlike other sparse Bayesian finite models where the basis function has a fixed form, our eigenfunctions live in a reproducing kernel Hilbert space as a finite linear combination of kernel functions. We learn the dictionary elements--eigenfunctions--and the prior precisions over these elements as well as all the other hyperparameters from data by maximizing the model marginal likelihood. We explore computational linear algebra to simplify the gradient computation significantly. Our experimental results demonstrate improved predictive performance of EigenGP over alternative sparse GP methods as well as relevance vector machine.
cs.LG:Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting the exploration of risky arms, MARAB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MARAB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is supported by extensive experimental validation of MIN and MARAB compared to UCB and state-of-art risk-aware MAB algorithms on artificial and real-world problems.
cs.LG:In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A fundamental aspect of music perception is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a novel reinforcement-learning framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a model of preferences for both songs and song transitions. The model is learned online and is uniquely adapted for each listener. To reduce exploration time, DJ-MC exploits user feedback to initialize a model, which it subsequently updates by reinforcement. We evaluate our framework with human participants using both real song and playlist data. Our results indicate that DJ-MC's ability to recommend sequences of songs provides a significant improvement over more straightforward approaches, which do not take transitions into account.
cs.LG:This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.
cs.LG:We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.
cs.LG:The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, nu-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented.
cs.LG:Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns.
cs.LG:In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours Regression (k-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures.
cs.LG:Multiclass problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC), have been studied, to decompose multiclass problems into binary problems. However, little study has been made to optimally aggregate binary problems to determine a final answer to the multiclass problem. In this paper we present a convex optimization method for an optimal aggregation of binary classifiers to estimate class membership probabilities in multiclass problems. We model the class membership probability as a softmax function which takes a conic combination of discrepancies induced by individual binary classifiers, as an input. With this model, we formulate the regularized maximum likelihood estimation as a convex optimization problem, which is solved by the primal-dual interior point method. Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Numerical experiments on synthetic and real-world data sets demonstrate that our method outperforms existing aggregation methods as well as direct methods, in terms of the classification accuracy and the quality of class membership probability estimates.
cs.LG:Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques in the context of kernel-based learning. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Solving different MKL formulations usually involves designing algorithms that are tailored to the problem at hand, which is, typically, a non-trivial accomplishment.   In this paper we present a general Multi-Task Multi-Kernel Learning (Multi-Task MKL) framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To demonstrate the flexibility of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation.
cs.LG:An extreme learning machine (ELM) can be regarded as a two stage feed-forward neural network (FNN) learning system which randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of our work is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part on this topic, we pointed out that via appropriate selection of the activation function, ELM does not degrade the generalization capability in the expectation sense. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exists an activation function such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM and give theoretical guidance concerning how to use ELM.
cs.LG:Non-negative least-mean-square (NNLMS) algorithm and its variants have been proposed for online estimation under non-negativity constraints. The transient behavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMS algorithms have been studied in our previous work. In this technical report, we derive closed-form expressions for the steady-state excess mean-square error (EMSE) for the four algorithms. Simulations results illustrate the accuracy of the theoretical results. This is a complementary material to our previous work.
cs.LG:Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. Simultaneously addressing all of these challenges i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based represen- tations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data.
cs.LG:Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. Our first contribution shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. We argue that semi-supervised anomaly detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. Although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we propose an active learning strategy to automatically filter candidates for labeling. In an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.
cs.LG:Recently, we have proposed a maximum likelihood iterative algorithm for estimation of the parameters of the Nakagami-m distribution. This technique performs better than state of art estimation techniques for this distribution. This could be of particular use in low data or block based estimation problems. In these scenarios, the estimator should be able to give accurate estimates in the mean square sense with less amounts of data. Also, the estimates should improve with the increase in number of blocks received. In this paper, we see through our simulations, that our proposal is well designed for such requirements. Further, it is well known in the literature that an efficient estimator does not exist for Nakagami-m distribution. In this paper, we derive a theoretical expression for the variance of our proposed estimator. We find that this expression clearly fits the experimental curve for the variance of the proposed estimator. This expression is pretty close to the cramer-rao lower bound(CRLB).
cs.LG:Many feature subset selection (FSS) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. At the same time, so far there is rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus, FSS algorithm automatic recommendation is very important and practically useful. In this paper, a meta learning based FSS algorithm automatic recommendation method is presented. The proposed method first identifies the data sets that are most similar to the one at hand by the k-nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. Then, it ranks all the candidate FSS algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. The proposed recommendation method is extensively tested on 115 real world data sets with 22 well-known and frequently-used different FSS algorithms for five representative classifiers. The results show the effectiveness of our proposed FSS algorithm recommendation method.
cs.LG:In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.
cs.LG:How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm.
cs.LG:Deep generative models (DGMs) have brought about a major breakthrough, as well as renewed interest, in generative latent variable models. However, DGMs do not allow for performing data-driven inference of the number of latent features needed to represent the observed data. Traditional linear formulations address this issue by resorting to tools from the field of nonparametric statistics. Indeed, linear latent variable models imposed an Indian Buffet Process (IBP) prior have been extensively studied by the machine learning community; inference for such models can been performed either via exact sampling or via approximate variational techniques. Based on this inspiration, in this paper we examine whether similar ideas from the field of Bayesian nonparametrics can be utilized in the context of modern DGMs in order to address the latent variable dimensionality inference problem. To this end, we propose a novel DGM formulation, based on the imposition of an IBP prior. We devise an efficient Black-Box Variational inference algorithm for our model, and exhibit its efficacy in a number of semi-supervised classification experiments. In all cases, we use popular benchmark datasets, and compare to state-of-the-art DGMs.
cs.LG:Let $f:\{-1,1\}^n$ be a polynomial with at most $s$ non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples from the uniform distribution on $\{-1,1\}^n$ that runs in time polynomial in $n$ and $2s$ and succeeds if the function satisfies the unique sign property: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of f is perturbed by a small random noise, or satisfied with high probability when s parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in $n$ and $2s$ is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials. Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset.
cs.LG:Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting.
cs.LG:In the election of a hierarchical clustering method, theoretic properties may give some insight to determine which method is the most suitable to treat a clustering problem. Herein, we study some basic properties of two hierarchical clustering methods: $\alpha$-unchaining single linkage or $SL(\alpha)$ and a modified version of this one, $SL^*(\alpha)$. We compare the results with the properties satisfied by the classical linkage-based hierarchical clustering methods.
cs.LG:We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups -- a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.
cs.LG:Semisupervised learning is a learning standard which deals with the study of how computers and natural systems such as human beings acquire knowledge in the presence of both labeled and unlabeled data. Semisupervised learning based methods are preferred when compared to the supervised and unsupervised learning because of the improved performance shown by the semisupervised approaches in the presence of large volumes of data. Labels are very hard to attain while unlabeled data are surplus, therefore semisupervised learning is a noble indication to shrink human labor and improve accuracy. There has been a large spectrum of ideas on semisupervised learning. In this paper we bring out some of the key approaches for semisupervised learning.
cs.LG:This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors.
cs.LG:To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.
cs.LG:In this paper, we consider a very general model for exploration-exploitation tradeoff which allows arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. This model subsumes the classic multi-armed bandit (MAB) model, and the Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the MAB model. We demonstrate that a natural and simple extension of the UCB family of algorithms for MAB provides a polynomial time algorithm that has near-optimal regret guarantees for this substantially more general model, and matches the bounds provided by Badanidiyuru et al.[2013] for the special case of BwK, which is quite surprising. We also provide computationally more efficient algorithms by establishing interesting connections between this problem and other well studied problems/algorithms such as the Blackwell approachability problem, online convex optimization, and the Frank-Wolfe technique for convex optimization. We give examples of several concrete applications, where this more general model of bandits allows for richer and/or more efficient formulations of the problem.
cs.LG:The objective of the GreenPAD project is to use green energy (wind, solar and biomass) for powering data-centers that are used to run HPC jobs. As a part of this it is important to predict the Renewable (Wind) energy for efficient scheduling (executing jobs that require higher energy when there is more green energy available and vice-versa). For predicting the wind energy we first analyze the historical data to find a statistical model that gives relation between wind energy and weather attributes. Then we use this model based on the weather forecast data to predict the green energy availability in the future. Using the green energy prediction obtained from the statistical model we are able to precompute job schedules for maximizing the green energy utilization in the future. We propose a model which uses live weather data in addition to machine learning techniques (which can predict future deviations in weather conditions based on current deviations from the forecast) to make on-the-fly changes to the precomputed schedule (based on green energy prediction).   For this we first analyze the data using histograms and simple statistical tools such as correlation. In addition we build (correlation) regression model for finding the relation between wind energy availability and weather attributes (temperature, cloud cover, air pressure, wind speed / direction, precipitation and sunshine). We also analyze different algorithms and machine learning techniques for optimizing the job schedules for maximizing the green energy utilization.
cs.LG:The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on an almost infinitely large training data set that captures all variations in the data distribution. In practical learning settings, however, we do not have infinite data and our predictors may overfit. Overfitting may be combatted, for example, by adding a regularizer to the training objective or by defining a prior over the model parameters and performing Bayesian inference. In this paper, we propose a third, alternative approach to combat overfitting: we extend the training set with infinitely many artificial training examples that are obtained by corrupting the original training data. We show that this approach is practical and efficient for a range of predictors and corruption models. Our approach, called marginalized corrupted features (MCF), trains robust predictors by minimizing the expected value of the loss function under the corruption model. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are also more robust to feature deletion at test time.
cs.LG:When dealing with datasets containing a billion instances or with simulations that require a supercomputer to execute, computational resources become part of the equation. We can improve the efficiency of learning and inference by exploiting their inherent statistical nature. We propose algorithms that exploit the redundancy of data relative to a model by subsampling data-cases for every update and reasoning about the uncertainty created in this process. In the context of learning we propose to test for the probability that a stochastically estimated gradient points more than 180 degrees in the wrong direction. In the context of MCMC sampling we use stochastic gradients to improve the efficiency of MCMC updates, and hypothesis tests based on adaptive mini-batches to decide whether to accept or reject a proposed parameter update. Finally, we argue that in the context of likelihood free MCMC one needs to store all the information revealed by all simulations, for instance in a Gaussian process. We conclude that Bayesian methods will remain to play a crucial role in the era of big data and big simulations, but only if we overcome a number of computational challenges.
cs.LG:We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to model a specific scenario emerging from research in sleep science. Scientists have segmented sleep into several stages and stage two is characterized by two patterns (or anomalies) in the EEG time series recorded on sleep subjects. These two patterns are sleep spindle (SS) and K-complex. The OSAD problem was introduced to design a residual system, where all anomalies (known and unknown) are detected but the system only triggers an alarm when non-SS anomalies appear. The solution of the OSAD problem required us to combine techniques from both machine learning and control theory. Experiments on data from real subjects attest to the effectiveness of our approach.
cs.LG:A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.
cs.LG:We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of $\mathcal{O}\Big(U \sqrt{T \log(U \sqrt{T} \log^2 T +1)}\Big)$, where $U$ is the $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to the player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ is known, we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown $T$ case, a Normal approximation to the conditional value of the game proves to be the key analysis tool.
cs.LG:In this paper we present methods for exemplar based clustering with outlier selection based on the facility location formulation. Given a distance function and the number of outliers to be found, the methods automatically determine the number of clusters and outliers. We formulate the problem as an integer program to which we present relaxations that allow for solutions that scale to large data sets. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable, i.e. it is easier to distinguish between outliers which are the result of data errors from those that may be indicative of a new pattern emergent in the data. We present and contrast three relaxations to the integer program formulation: (i) a linear programming formulation (LP) (ii) an extension of affinity propagation to outlier detection (APOC) and (iii) a Lagrangian duality based formulation (LD). Evaluation on synthetic as well as real data shows the quality and scalability of these different methods.
cs.LG:In the past few years co-clustering has emerged as an important data mining tool for two way data analysis. Co-clustering is more advantageous over traditional one dimensional clustering in many ways such as, ability to find highly correlated sub-groups of rows and columns. However, one of the overlooked benefits of co-clustering is that, it can be used to extract meaningful knowledge for various other knowledge extraction purposes. For example, building predictive models with high dimensional data and heterogeneous population is a non-trivial task. Co-clusters extracted from such data, which shows similar pattern in both the dimension, can be used for a more accurate predictive model building. Several applications such as finding patient-disease cohorts in health care analysis, finding user-genre groups in recommendation systems and community detection problems can benefit from co-clustering technique that utilizes the predictive power of the data to generate co-clusters for improved data analysis.   In this paper, we present the novel idea of Predictive Overlapping Co-Clustering (POCC) as an optimization problem for a more effective and improved predictive analysis. Our algorithm generates optimal co-clusters by maximizing predictive power of the co-clusters subject to the constraints on the number of row and column clusters. In this paper precision, recall and f-measure have been used as evaluation measures of the resulting co-clusters. Results of our algorithm has been compared with two other well-known techniques - K-means and Spectral co-clustering, over four real data set namely, Leukemia, Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate the effectiveness and utility of our algorithm POCC in practice.
cs.LG:In recent years the importance of finding a meaningful pattern from huge datasets has become more challenging. Data miners try to adopt innovative methods to face this problem by applying feature selection methods. In this paper we propose a new hybrid method in which we use a combination of resampling, filtering the sample domain and wrapper subset evaluation method with genetic search to reduce dimensions of Lung-Cancer dataset that we received from UCI Repository of Machine Learning databases. Finally, we apply some well- known classification algorithms (Na\"ive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) to the resulting dataset and compare the results and prediction rates before and after the application of our feature selection method on that dataset. The results show a substantial progress in the average performance of five classification algorithms simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost.
cs.LG:Cluster analysis has attracted more and more attention in the field of machine learning and data mining. Numerous clustering algorithms have been proposed and are being developed due to diverse theories and various requirements of emerging applications. Therefore, it is very worth establishing an unified axiomatic framework for data clustering. In the literature, it is an open problem and has been proved very challenging. In this paper, clustering results are axiomatized by assuming that an proper clustering result should satisfy categorization axioms. The proposed axioms not only introduce classification of clustering results and inequalities of clustering results, but also are consistent with prototype theory and exemplar theory of categorization models in cognitive science. Moreover, the proposed axioms lead to three principles of designing clustering algorithm and cluster validity index, which follow many popular clustering algorithms and cluster validity indices.
cs.LG:In this paper a hybrid feature selection method is proposed which takes advantages of wrapper subset evaluation with a lower cost and improves the performance of a group of classifiers. The method uses combination of sample domain filtering and resampling to refine the sample domain and two feature subset evaluation methods to select reliable features. This method utilizes both feature space and sample domain in two phases. The first phase filters and resamples the sample domain and the second phase adopts a hybrid procedure by information gain, wrapper subset evaluation and genetic search to find the optimal feature space. Experiments carried out on different types of datasets from UCI Repository of Machine Learning databases and the results show a rise in the average performance of five classifiers (Naive Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost.
cs.LG:High accuracy in cancer prediction is important to improve the quality of the treatment and to improve the rate of survivability of patients. As the data volume is increasing rapidly in the healthcare research, the analytical challenge exists in double. The use of effective sampling technique in classification algorithms always yields good prediction accuracy. The SEER public use cancer database provides various prominent class labels for prognosis prediction. The main objective of this paper is to find the effect of sampling techniques in classifying the prognosis variable and propose an ideal sampling method based on the outcome of the experimentation. In the first phase of this work the traditional random sampling and stratified sampling techniques have been used. At the next level the balanced stratified sampling with variations as per the choice of the prognosis class labels have been tested. Much of the initial time has been focused on performing the pre_processing of the SEER data set. The classification model for experimentation has been built using the breast cancer, respiratory cancer and mixed cancer data sets with three traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest Neighbor. The three prognosis factors survival, stage and metastasis have been used as class labels for experimental comparisons. The results shows a steady increase in the prediction accuracy of balanced stratified model as the sample size increases, but the traditional approach fluctuates before the optimum results.
cs.LG:We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens pre-viously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a correspond- ing FTRL update, which allows us to analyze any Mirror Descent algorithm in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight.
cs.LG:In many applications, the training data, from which one needs to learn a classifier, is corrupted with label noise. Many standard algorithms such as SVM perform poorly in presence of label noise. In this paper we investigate the robustness of risk minimization to label noise. We prove a sufficient condition on a loss function for the risk minimization under that loss to be tolerant to uniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss and probit loss satisfy this condition though none of the standard convex loss functions satisfy it. We also prove that, by choosing a sufficiently large value of a parameter in the loss function, the sigmoid loss, ramp loss and probit loss can be made tolerant to non-uniform label noise also if we can assume the classes to be separable under noise-free data distribution. Through extensive empirical studies, we show that risk minimization under the $0-1$ loss, the sigmoid loss and the ramp loss has much better robustness to label noise when compared to the SVM algorithm.
cs.LG:This work investigates the use of mixed-norm regularization for sensor selection in Event-Related Potential (ERP) based Brain-Computer Interfaces (BCI). The classification problem is cast as a discriminative optimization framework where sensor selection is induced through the use of mixed-norms. This framework is extended to the multi-task learning situation where several similar classification tasks related to different subjects are learned simultaneously. In this case, multi-task learning helps in leveraging data scarcity issue yielding to more robust classifiers. For this purpose, we have introduced a regularizer that induces both sensor selection and classifier similarities. The different regularization approaches are compared on three ERP datasets showing the interest of mixed-norm regularization in terms of sensor selection. The multi-task approaches are evaluated when a small number of learning examples are available yielding to significant performance improvements especially for subjects performing poorly.
cs.LG:This work considers the problem of estimating the parameters of negative mixture models, i.e. mixture models that possibly involve negative weights. The contributions of this paper are as follows. (i) We show that every rational probability distributions on strings, a representation which occurs naturally in spectral learning, can be computed by a negative mixture of at most two probabilistic automata (or HMMs). (ii) We propose a method to estimate the parameters of negative mixture models having a specific tensor structure in their low order observable moments. Building upon a recent paper on tensor decompositions for learning latent variable models, we extend this work to the broader setting of tensors having a symmetric decomposition with positive and negative weights. We introduce a generalization of the tensor power method for complex valued tensors, and establish theoretical convergence guarantees. (iii) We show how our approach applies to negative Gaussian mixture models, for which we provide some experiments.
cs.LG:Motivated by multi-distribution divergences, which originate in information theory, we propose a notion of `multi-point' kernels, and study their applications. We study a class of kernels based on Jensen type divergences and show that these can be extended to measure similarity among multiple points. We study tensor flattening methods and develop a multi-point (kernel) spectral clustering (MSC) method. We further emphasize on a special case of the proposed kernels, which is a multi-point extension of the linear (dot-product) kernel and show the existence of cubic time tensor flattening algorithm in this case. Finally, we illustrate the usefulness of our contributions using standard data sets and image segmentation tasks.
cs.LG:We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these contributions, the proposed approaches to fight the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called UMA (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, UMA furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic and real data. Keywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix
cs.LG:In many online learning problems we are interested in predicting local information about some universe of items. For example, we may want to know whether two items are in the same cluster rather than computing an assignment of items to clusters; we may want to know which of two teams will win a game rather than computing a ranking of teams. Although finding the optimal clustering or ranking is typically intractable, it may be possible to predict the relationships between items as well as if you could solve the global optimization problem exactly.   Formally, we consider an online learning problem in which a learner repeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarial payoff depending on those labels. The learner's goal is to receive a payoff nearly as good as the best fixed labeling of the items. We show that a simple algorithm based on semidefinite programming can obtain asymptotically optimal regret in the case where the number of possible labels is O(1), resolving an open problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technical contribution is a novel use and analysis of the log determinant regularizer, exploiting the observation that log det(A + I) upper bounds the entropy of any distribution with covariance matrix A.
cs.LG:We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.
cs.LG:We propose information-directed sampling -- a new approach to online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance.
cs.LG:This paper revisits the problem of learning a k-CNF Boolean function from examples in the context of online learning under the logarithmic loss. In doing so, we give a Bayesian interpretation to one of Valiant's celebrated PAC learning algorithms, which we then build upon to derive two efficient, online, probabilistic, supervised learning algorithms for predicting the output of an unknown k-CNF Boolean function. We analyze the loss of our methods, and show that the cumulative log-loss can be upper bounded, ignoring logarithmic factors, by a polynomial function of the size of each example.
cs.LG:This work investigates into cost behaviors of binary classification measures in a background of class-imbalanced problems. Twelve performance measures are studied, such as F measure, G-means in terms of accuracy rates, and of recall and precision, balance error rate (BER), Matthews correlation coefficient (MCC), Kappa coefficient, etc. A new perspective is presented for those measures by revealing their cost functions with respect to the class imbalance ratio. Basically, they are described by four types of cost functions. The functions provides a theoretical understanding why some measures are suitable for dealing with class-imbalanced problems. Based on their cost functions, we are able to conclude that G-means of accuracy rates and BER are suitable measures because they show "proper" cost behaviors in terms of "a misclassification from a small class will cause a greater cost than that from a large class". On the contrary, F1 measure, G-means of recall and precision, MCC and Kappa coefficient measures do not produce such behaviors so that they are unsuitable to serve our goal in dealing with the problems properly.
cs.LG:This paper presents an approximate method for performing Bayesian inference in models with conditional independence over a decentralized network of learning agents. The method first employs variational inference on each individual learning agent to generate a local approximate posterior, the agents transmit their local posteriors to other agents in the network, and finally each agent combines its set of received local posteriors. The key insight in this work is that, for many Bayesian models, approximate inference schemes destroy symmetry and dependencies in the model that are crucial to the correct application of Bayes' rule when combining the local posteriors. The proposed method addresses this issue by including an additional optimization step in the combination procedure that accounts for these broken dependencies. Experiments on synthetic and real data demonstrate that the decentralized method provides advantages in computational performance and predictive test likelihood over previous batch and distributed methods.
cs.LG:Many kernel methods suffer from high time and space complexities and are thus prohibitive in big-data applications. To tackle the computational challenge, the Nystr\"om method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nystr\"om method speedups computation by constructing an approximation of the kernel matrix using only a few columns of the matrix. Recently, a variant of the Nystr\"om method called the modified Nystr\"om method has demonstrated significant improvement over the standard Nystr\"om method in approximation accuracy, both theoretically and empirically.   In this paper, we propose two algorithms that make the modified Nystr\"om method practical. First, we devise a simple column selection algorithm with a provable error bound. Our algorithm is more efficient and easier to implement than and nearly as accurate as the state-of-the-art algorithm. Second, with the selected columns at hand, we propose an algorithm that computes the approximation in lower time complexity than the approach in the previous work. Furthermore, we prove that the modified Nystr\"om method is exact under certain conditions, and we establish a lower error bound for the modified Nystr\"om method.
cs.LG:In this paper, we present a computational technique to deal with uncertainty in dynamic continuous models in Social Sciences. Considering data from surveys, the method consists of determining the probability distribution of the survey output and this allows to sample data and fit the model to the sampled data using a goodness-of-fit criterion based on the chi-square-test. Taking the fitted parameters non-rejected by the chi-square-test, substituting them into the model and computing their outputs, we build 95% confidence intervals in each time instant capturing uncertainty of the survey data (probabilistic estimation). Using the same set of obtained model parameters, we also provide a prediction over the next few years with 95% confidence intervals (probabilistic prediction). This technique is applied to a dynamic social model describing the evolution of the attitude of the Basque Country population towards the revolutionary organization ETA.
cs.LG:The true process that generated data cannot be determined when multiple explanations are possible. Prediction requires a model of the probability that a process, chosen randomly from the set of candidate explanations, generates some future observation. The best model includes all of the information contained in the minimal description of the data that is not contained in the data. It is closely related to the Halting Problem and is logarithmic in the size of the data. Prediction is difficult because the ideal model is not computable, and the best computable model is not "findable." However, the error from any approximation can be bounded by the size of the description using the model.
cs.LG:The Bayesian Classification represents a supervised learning method as well as a statistical method for classification. Assumes an underlying probabilistic model and it allows us to capture uncertainty about the model in a principled way by determining probabilities of the outcomes. This Classification is named after Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian classification provides practical learning algorithms and prior knowledge and observed data can be combined. Bayesian Classification provides a useful perspective for understanding and evaluating many learning algorithms. It calculates explicit probabilities for hypothesis and it is robust to noise in input data. In statistical classification the Bayes classifier minimises the probability of misclassification. That was a visual intuition for a simple case of the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple Bayes
cs.LG:In this paper, we evaluate the performance of various parallel optimization methods for Kernel Support Vector Machines on multicore CPUs and GPUs. In particular, we provide the first comparison of algorithms with explicit and implicit parallelization. Most existing parallel implementations for multi-core or GPU architectures are based on explicit parallelization of Sequential Minimal Optimization (SMO)---the programmers identified parallelizable components and hand-parallelized them, specifically tuned for a particular architecture. We compare these approaches with each other and with implicitly parallelized algorithms---where the algorithm is expressed such that most of the work is done within few iterations with large dense linear algebra operations. These can be computed with highly-optimized libraries, that are carefully parallelized for a large variety of parallel platforms. We highlight the advantages and disadvantages of both approaches and compare them on various benchmark data sets. We find an approximate implicitly parallel algorithm which is surprisingly efficient, permits a much simpler implementation, and leads to unprecedented speedups in SVM training.
cs.LG:We present the \textit{hierarchical Dirichlet scaling process} (HDSP), a Bayesian nonparametric mixed membership model. The HDSP generalizes the hierarchical Dirichlet process (HDP) to model the correlation structure between metadata in the corpus and mixture components. We construct the HDSP based on the normalized gamma representation of the Dirichlet process, and this construction allows incorporating a scaling function that controls the membership probabilities of the mixture components. We develop two scaling methods to demonstrate that different modeling assumptions can be expressed in the HDSP. We also derive the corresponding approximate posterior inference algorithms using variational Bayes. Through experiments on datasets of newswire, medical journal articles, conference proceedings, and product reviews, we show that the HDSP results in a better predictive performance than labeled LDA, partially labeled LDA, and author topic model and a better negative review classification performance than the supervised topic model and SVM.
cs.LG:In this paper we have focused on an efficient feature selection method in classification of audio files. The main objective is feature selection and extraction. We have selected a set of features for further analysis, which represents the elements in feature vector. By extraction method we can compute a numerical representation that can be used to characterize the audio using the existing toolbox. In this study Gain Ratio (GR) is used as a feature selection measure. GR is used to select splitting attribute which will separate the tuples into different classes. The pulse clarity is considered as a subjective measure and it is used to calculate the gain of features of audio files. The splitting criterion is employed in the application to identify the class or the music genre of a specific audio file from testing database. Experimental results indicate that by using GR the application can produce a satisfactory result for music genre classification. After dimensionality reduction best three features have been selected out of various features of audio file and in this technique we will get more than 90% successful classification result.
cs.LG:Analysis of high dimensional noisy data is of essence across a variety of research fields. Feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection. Traditional (supervised) feature selection methods utilize label information to guide the identification of relevant feature subsets. In this paper, however, we consider the unsupervised feature selection problem. Without the label information, it is particularly difficult to identify a small set of relevant features due to the noisy nature of real-world data which corrupts the intrinsic structure of the data. Our Gradient-based Laplacian Feature Selection (GLFS) selects important features by minimizing the variance of the Laplacian regularized least squares regression model. With $\ell_1$ relaxation, GLFS can find a sparse subset of features that is relevant to the Laplacian manifolds. Extensive experiments on simulated, three real-world object recognition and two computational biology datasets, have illustrated the power and superior performance of our approach over multiple state-of-the-art unsupervised feature selection methods. Additionally, we show that GLFS selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology.
cs.LG:A traditional and intuitively appealing Multi-Task Multiple Kernel Learning (MT-MKL) method is to optimize the sum (thus, the average) of objective functions with (partially) shared kernel function, which allows information sharing amongst tasks. We point out that the obtained solution corresponds to a single point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO) problem, which considers the concurrent optimization of all task objectives involved in the Multi-Task Learning (MTL) problem. Motivated by this last observation and arguing that the former approach is heuristic, we propose a novel Support Vector Machine (SVM) MT-MKL framework, that considers an implicitly-defined set of conic combinations of task objectives. We show that solving our framework produces solutions along a path on the aforementioned PF and that it subsumes the optimization of the average of objective functions as a special case. Using algorithms we derived, we demonstrate through a series of experimental results that the framework is capable of achieving better classification performance, when compared to other similar MTL approaches.
cs.LG:This work presents a sound probabilistic method for enforcing adherence of the marginal probabilities of a multi-label model to automatically discovered deterministic relationships among labels. In particular we focus on discovering two kinds of relationships among the labels. The first one concerns pairwise positive entailement: pairs of labels, where the presence of one implies the presence of the other in all instances of a dataset. The second concerns exclusion: sets of labels that do not coexist in the same instances of the dataset. These relationships are represented with a Bayesian network. Marginal probabilities are entered as soft evidence in the network and adjusted through probabilistic inference. Our approach offers robust improvements in mean average precision compared to the standard binary relavance approach across all 12 datasets involved in our experiments. The discovery process helps interesting implicit knowledge to emerge, which could be useful in itself.
cs.LG:Ensemble classifier refers to a group of individual classifiers that are cooperatively trained on data set in a supervised classification problem. In this paper we present a review of commonly used ensemble classifiers in the literature. Some ensemble classifiers are also developed targeting specific applications. We also present some application driven ensemble classifiers in this paper.
cs.LG:Consider a Machine Learning Service Provider (MLSP) designed to rapidly create highly accurate learners for a never-ending stream of new tasks. The challenge is to produce task-specific learners that can be trained from few labeled samples, even if tasks are not uniquely identified, and the number of tasks and input dimensionality are large. In this paper, we argue that the MLSP should exploit knowledge from previous tasks to build a good representation of the environment it is in, and more precisely, that useful representations for such a service are ones that minimize generalization error for a new hypothesis trained on a new task. We formalize this intuition with a novel method that minimizes an empirical proxy of the intra-task small-sample generalization error. We present several empirical results showing state-of-the art performance on single-task transfer, multitask learning, and the full lifelong learning problem.
cs.LG:Stochastic variational inference makes it possible to approximate posterior distributions induced by large datasets quickly using stochastic optimization. The algorithm relies on the use of fully factorized variational distributions. However, this "mean-field" independence approximation limits the fidelity of the posterior approximation, and introduces local optima. We show how to relax the mean-field approximation to allow arbitrary dependencies between global parameters and local hidden variables, producing better parameter estimates by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.
cs.LG:Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs.
cs.LG:Machine learning algorithms have been applied to predict agent behaviors in real-world dynamic systems, such as advertiser behaviors in sponsored search and worker behaviors in crowdsourcing. The behavior data in these systems are generated by live agents: once the systems change due to the adoption of the prediction models learnt from the behavior data, agents will observe and respond to these changes by changing their own behaviors accordingly. As a result, the behavior data will evolve and will not be identically and independently distributed, posing great challenges to the theoretical analysis on the machine learning algorithms for behavior prediction. To tackle this challenge, in this paper, we propose to use Markov Chain in Random Environments (MCRE) to describe the behavior data, and perform generalization analysis of the machine learning algorithms on its basis. Since the one-step transition probability matrix of MCRE depends on both previous states and the random environment, conventional techniques for generalization analysis cannot be directly applied. To address this issue, we propose a novel technique that transforms the original MCRE into a higher-dimensional time-homogeneous Markov chain. The new Markov chain involves more variables but is more regular, and thus easier to deal with. We prove the convergence of the new Markov chain when time approaches infinity. Then we prove a generalization bound for the machine learning algorithms on the behavior data generated by the new Markov chain, which depends on both the Markovian parameters and the covering number of the function class compounded by the loss function for behavior prediction and the behavior prediction model. To the best of our knowledge, this is the first work that performs the generalization analysis on data generated by complex processes in real-world dynamic systems.
cs.LG:Multi-target regression is concerned with the simultaneous prediction of multiple continuous target variables based on the same set of input variables. It arises in several interesting industrial and environmental application domains, such as ecological modelling and energy forecasting. This paper presents an ensemble method for multi-target regression that constructs new target variables via random linear combinations of existing targets. We discuss the connection of our approach with multi-label classification algorithms, in particular RA$k$EL, which originally inspired this work, and a family of recent multi-label classification algorithms that involve output coding. Experimental results on 12 multi-target datasets show that it performs significantly better than a strong baseline that learns a single model for each target using gradient boosting and compares favourably to multi-objective random forest approach, which is a state-of-the-art approach. The experiments further show that our approach improves more when stronger unconditional dependencies exist among the targets.
cs.LG:Coactive learning is an online problem solving setting where the solutions provided by a solver are interactively improved by a domain expert, which in turn drives learning. In this paper we extend the study of coactive learning to problems where obtaining a globally optimal or near-optimal solution may be intractable or where an expert can only be expected to make small, local improvements to a candidate solution. The goal of learning in this new setting is to minimize the cost as measured by the expert effort over time. We first establish theoretical bounds on the average cost of the existing coactive Perceptron algorithm. In addition, we consider new online algorithms that use cost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved theoretical bounds. We provide an empirical evaluation of the learners in various domains, which show that the Perceptron based algorithms are quite effective and that unlike the case for online classification, the PA algorithms do not yield significant performance gains.
cs.LG:Multi-view learning leverages correlations between different sources of data to make predictions in one view based on observations in another view. A popular approach is to assume that, both, the correlations between the views and the view-specific covariances have a low-rank structure, leading to inter-battery factor analysis, a model closely related to canonical correlation analysis. We propose a convex relaxation of this model using structured norm regularization. Further, we extend the convex formulation to a robust version by adding an l1-penalized matrix to our estimator, similarly to convex robust PCA. We develop and compare scalable algorithms for several convex multi-view models. We show experimentally that the view-specific correlations are improving data imputation performances, as well as labeling accuracy in real-world multi-label prediction tasks.
cs.LG:In this paper, we present a learning method for sequence labeling tasks in which each example sequence has multiple label sequences. Our method learns multiple models, one model for each label sequence. Each model computes the joint probability of all label sequences given the example sequence. Although each model considers all label sequences, its primary focus is only one label sequence, and therefore, each model becomes a task-specific model, for the task belonging to that primary label. Such multiple models are learned {\it simultaneously} by facilitating the learning transfer among models through {\it explicit parameter sharing}. We experiment the proposed method on two applications and show that our method significantly outperforms the state-of-the-art method.
cs.LG:Using an optimization algorithm to solve a machine learning problem is one of mainstreams in the field of science. In this work, we demonstrate a comprehensive comparison of some state-of-the-art first-order optimization algorithms for convex optimization problems in machine learning. We concentrate on several smooth and non-smooth machine learning problems with a loss function plus a regularizer. The overall experimental results show the superiority of primal-dual algorithms in solving a machine learning problem from the perspectives of the ease to construct, running time and accuracy.
cs.LG:A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is "learned" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.
cs.LG:In this paper, we propose to study four meteorological and seasonal time series coupled with a multi-layer perceptron (MLP) modeling. We chose to combine two transfer functions for the nodes of the hidden layer, and to use a temporal indicator (time index as input) in order to take into account the seasonal aspect of the studied time series. The results of the prediction concern two years of measurements and the learning step, eight independent years. We show that this methodology can improve the accuracy of meteorological data estimation compared to a classical MLP modelling with a homogenous transfer function.
cs.LG:Hidden Markov models (HMMs) are widely used statistical models for modeling sequential data. The parameter estimation for HMMs from time series data is an important learning problem. The predominant methods for parameter estimation are based on local search heuristics, most notably the expectation-maximization (EM) algorithm. These methods are prone to local optima and oftentimes suffer from high computational and sample complexity. Recent years saw the emergence of spectral methods for the parameter estimation of HMMs, based on a method of moments approach. Two spectral learning algorithms as proposed by Hsu, Kakade and Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012 (arXiv:1203.0683) are assessed in this work. Using experiments with synthetic data, the algorithms are compared with each other. Furthermore, the spectral methods are compared to the Baum-Welch algorithm, a well-established method applying the EM algorithm to HMMs. The spectral algorithms are found to have a much more favorable computational and sample complexity. Even though the algorithms readily handle high dimensional observation spaces, instability issues are encountered in this regime. In view of learning from real-world experimental data, the representation of real-valued observations for the use in spectral methods is discussed, presenting possible methods to represent data for the use in the learning algorithms.
cs.LG:We investigate how different learning restrictions reduce learning power and how the different restrictions relate to one another. We give a complete map for nine different restrictions both for the cases of complete information learning and set-driven learning. This completes the picture for these well-studied \emph{delayable} learning restrictions. A further insight is gained by different characterizations of \emph{conservative} learning in terms of variants of \emph{cautious} learning.   Our analyses greatly benefit from general theorems we give, for example showing that learners with exclusively delayable restrictions can always be assumed total.
cs.LG:In this paper, we study the problem of learning a monotone DNF with at most $s$ terms of size (number of variables in each term) at most $r$ ($s$ term $r$-MDNF) from membership queries. This problem is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem motivated by applications arising in chemical reactions and genome sequencing.   We first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables $n$. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed $r$ and/or $s$.
cs.LG:We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm previously shown to outperform a number of algorithms for this task. Unlike many previous algorithms for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. We show that our algorithm benefits from a solid theoretical foundation and more favorable learning bounds than discrepancy minimization. We present a detailed description of our algorithm and give several efficient solutions for solving its optimization problem. We also report the results of several experiments showing that it outperforms discrepancy minimization.
cs.LG:We consider the problem of proper learning a Boolean Halfspace with integer weights $\{0,1,\ldots,t\}$ from membership queries only. The best known algorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$ membership queries where the best lower bound for the number of membership queries is $n^t$ [Learning Threshold Functions with Small Weights Using Membership Queries. COLT 1999]   In this paper we close this gap and give an adaptive proper learning algorithm with two rounds that asks $n^{O(t)}$ membership queries. We also give a non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membership queries.
cs.LG:The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of "how to learn"? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).
cs.LG:We prove the existence of a canonical form for semi-deterministic transducers with incomparable sets of output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge.
cs.LG:We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.
cs.LG:We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by (Charikar 2002) and to the max-norm ball, and the differences between their symmetric and asymmetric versions.
cs.LG:We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form "A is preferred to B" (as opposed to cardinal feedback like "A has value 2.5"), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions -- named $\Doubler$, $\MultiSbm$ and $\DoubleSbm$ -- provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For $\Doubler$ and $\MultiSbm$ we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of $\DoubleSbm$ which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.
cs.LG:The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012).
cs.LG:Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement.
cs.LG:We study a new class of online learning problems where each of the online algorithm's actions is assigned an adversarial value, and the loss of the algorithm at each step is a known and deterministic function of the values assigned to its recent actions. This class includes problems where the algorithm's loss is the minimum over the recent adversarial values, the maximum over the recent values, or a linear combination of the recent values. We analyze the minimax regret of this class of problems when the algorithm receives bandit feedback, and prove that when the minimum or maximum functions are used, the minimax regret is $\tilde \Omega(T^{2/3})$ (so called hard online learning problems), and when a linear function is used, the minimax regret is $\tilde O(\sqrt{T})$ (so called easy learning problems). Previously, the only online learning problem that was known to be provably hard was the multi-armed bandit with switching costs.
cs.LG:This paper concerns the distributed training of nonlinear kernel machines on Map-Reduce. We show that a re-formulation of Nystr\"om approximation based solution which is solved using gradient based techniques is well suited for this, especially when it is necessary to work with a large number of basis points. The main advantages of this approach are: avoidance of computing the pseudo-inverse of the kernel sub-matrix corresponding to the basis points; simplicity and efficiency of the distributed part of the computations; and, friendliness to stage-wise addition of basis points. We implement the method using an AllReduce tree on Hadoop and demonstrate its value on a few large benchmark datasets.
cs.LG:Distributed training of $l_1$ regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore and MPI platforms where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for $l_1$ regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods.
cs.LG:We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.
cs.LG:We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between "Follow the Regularized Leader" and "Follow the Perturbed Leader" up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.
cs.LG:Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.
cs.LG:In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified "safety" guarantees, and remains in a stable region of the parameter space (iii) how to design "off-policy" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators.   The key idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design "true" stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators.
cs.LG:BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.
cs.LG:K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown.
cs.LG:A useful strategy to deal with complex classification scenarios is the "divide and conquer" approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using $L1$ regularization, with a simultaneous expert selection. The experiments are still pending.
cs.LG:We prove that there is no class-dual for almost all sublinear models on graphs.
cs.LG:The performance of prediction models is often based on "abstract metrics" that estimate the model's ability to limit residual errors between the observed and predicted values. However, meaningful evaluation and selection of prediction models for end-user domains requires holistic and application-sensitive performance measures. Inspired by energy consumption prediction models used in the emerging "big data" domain of Smart Power Grids, we propose a suite of performance measures to rationally compare models along the dimensions of scale independence, reliability, volatility and cost. We include both application independent and dependent measures, the latter parameterized to allow customization by domain experts to fit their scenario. While our measures are generalizable to other domains, we offer an empirical analysis using real energy use data for three Smart Grid applications: planning, customer education and demand response, which are relevant for energy sustainability. Our results underscore the value of the proposed measures to offer a deeper insight into models' behavior and their impact on real applications, which benefit both data mining researchers and practitioners.
cs.LG:Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the beta-divergence family. Selecting the best beta then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate alpha-divergence in terms of beta-divergence, which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence. Furthermore, we show the connections between gamma and beta-divergences as well as R\'enyi and alpha-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.
cs.LG:In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a set of grammar rules we build trees that combine different rules, looking for branches which yield compositions that are analytically equivalent to a target expression, but of lower computational complexity. However, as the size of the trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.
cs.LG:We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.
cs.LG:Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.
cs.LG:We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.
cs.LG:Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufficiently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.
cs.LG:Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features.
cs.LG:Restricted Boltzmann machines (RBM) and its variants have become hot research topics recently, and widely applied to many classification problems, such as character recognition and document categorization. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we are interested in RBM with the hierarchical prior over classes. We assume parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree be orthogonal to those at its ancestors. We propose a hierarchical correlated RBM for classification problem, which generalizes the classification RBM with sharing information among different classes. In order to reduce the redundancy between node parameters in the hierarchy, we also introduce orthogonal restrictions to our objective function. We test our method on challenge datasets, and show promising results compared to competitive baselines.
cs.LG:We evaluate the following Machine Learning techniques for Green Energy (Wind, Solar) Prediction: Bayesian Inference, Neural Networks, Support Vector Machines, Clustering techniques (PCA). Our objective is to predict green energy using weather forecasts, predict deviations from forecast green energy, find correlation amongst different weather parameters and green energy availability, recover lost or missing energy (/ weather) data. We use historical weather data and weather forecasts for the same.
cs.LG:We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden.
cs.LG:Spectral learning recently generated lots of excitement in machine learning, largely because it is the first known method to produce consistent estimates (under suitable conditions) for several latent variable models. In contrast, maximum likelihood estimates may get trapped in local optima due to the non-convex nature of the likelihood function of latent variable models. In this paper, we do an empirical evaluation of spectral learning (SL) and expectation maximization (EM), which reveals an important gap between the theory and the practice. First, SL often leads to negative probabilities. Second, EM often yields better estimates than spectral learning and it does not seem to get stuck in local optima. We discuss how the rank of the model parameters and the amount of training data can yield negative probabilities. We also question the common belief that maximum likelihood estimators are necessarily inconsistent.
cs.LG:Data mining research into time series classification (TSC) has focussed on alternative distance measures for nearest neighbour classifiers. It is standard practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a straw man for comparison. As part of a wider investigation into elastic distance measures for TSC~\cite{lines14elastic}, we perform a series of experiments to test whether this standard practice is valid.   Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to standard classifiers, examine whether the performance of 1-NN Euclidean approaches that of 1-NN DTW as the number of cases increases, assess whether there is any benefit of setting $k$ for $k$-NN through cross validation whether it is worth setting the warping path for DTW through cross validation and finally is it better to use a window or weighting for DTW. Based on experiments on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to beat but 1-NN with DTW is not, if window size is set through cross validation.
cs.LG:Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a sparse representation. The sparser the representation, the better the dictionary. The resulting dictionary is in general a dense matrix, and its manipulation can be computationally costly both at the learning stage and later in the usage of this dictionary, for tasks such as sparse coding. Dictionary learning is thus limited to relatively small-scale problems. In this paper, inspired by usual fast transforms, we consider a general dictionary structure that allows cheaper manipulation, and propose an algorithm to learn such dictionaries --and their fast implementation-- over training data. The approach is demonstrated experimentally with the factorization of the Hadamard matrix and with synthetic dictionary learning experiments.
cs.LG:This paper proposes a new method of probabilistic prediction, which is based on conformal prediction. The method is applied to the standard USPS data set and gives encouraging results.
cs.LG:Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the Nystr\"om method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large-scale data, it is still useful to study its properties, on which the analysis of its extensions relies.   This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models.
cs.LG:We study the bandit problem where arms are associated with stationary phi-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve.
cs.LG:In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.
cs.LG:Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of $\Phi$-mixability where $\Phi$ is a general entropy (\ie, any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with $\Phi$-mixable losses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy.
cs.LG:Learning the parameters of graphical models using the maximum likelihood estimation is generally hard which requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation which are higher-order generalizations of the maximum pseudo-likelihood estimation. In this paper, we propose a composite likelihood method and investigate its property. Furthermore, we apply our composite likelihood method to restricted Boltzmann machines.
cs.LG:The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.
cs.LG:This paper examines the efficacy of different optimization techniques in a primal formulation of a support vector machine (SVM). Three main techniques are compared. The dataset used to compare all three techniques was the Sentiment Analysis on Movie Reviews dataset, from kaggle.com.
cs.LG:Structure learning of Conditional Random Fields (CRFs) can be cast into an L1-regularized optimization problem. To avoid optimizing over a fully linked model, gain-based or gradient-based feature selection methods start from an empty model and incrementally add top ranked features to it. However, for high-dimensional problems like statistical relational learning, training time of these incremental methods can be dominated by the cost of evaluating the gain or gradient of a large collection of candidate features. In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which only evaluates a subset of features that involve both variables with high signals (deviation from mean) and variables with high errors (residue). We prove that the gradient of candidate features can be represented solely as a function of signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real data sets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting.
cs.LG:We consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. No further assumption is made regarding the smoothness and the structure of the expected reward function. For these problems, we propose the Stochastic Pentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret and optimization error. In particular, we show that, for any expected reward function $\mu$ that behaves as $\mu(x)=\mu(x^\star)-C|x-x^\star|^\xi$ locally around its maximizer $x^\star$ for some $\xi, C>0$, the SP algorithm is order-optimal. Namely its regret and optimization error scale as $O(\sqrt{T\log(T)})$ and $O(\sqrt{\log(T)/T})$, respectively, when the time horizon $T$ grows large. These scalings are achieved without the knowledge of $\xi$ and $C$. Our algorithm is based on asymptotically optimal sequential statistical tests used to successively trim an interval that contains the best arm with high probability. To our knowledge, the SP algorithm constitutes the first sequential arm selection rule that achieves a regret and optimization error scaling as $O(\sqrt{T})$ and $O(1/\sqrt{T})$, respectively, up to a logarithmic factor for non-smooth expected reward functions, as well as for smooth functions with unknown smoothness.
cs.LG:Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent (OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.
cs.LG:Which ads should we display in sponsored search in order to maximize our revenue? How should we dynamically rank information sources to maximize the value of the ranking? These applications exhibit strong diminishing returns: Redundancy decreases the marginal utility of each ad or information source. We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one. We present an efficient algorithm for this general problem and analyze it in the no-regret model. Our algorithm possesses strong theoretical guarantees, such as a performance ratio that converges to the optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades. Finally, we present a second algorithm that handles the more general case in which the feasible sets are given by a matroid constraint, while still maintaining a 1 - 1/e asymptotic performance ratio.
cs.LG:Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some ground-truth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called MPU, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our MPU model consistently outperform other commonly-used baselines.
cs.LG:Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.
cs.LG:This document describes a novel learning algorithm that classifies "bags" of instances rather than individual instances. A bag is labeled positive if it contains at least one positive instance (which may or may not be specifically identified), and negative otherwise. This class of problems is known as multi-instance learning problems, and is useful in situations where the class label at an instance level may be unavailable or imprecise or difficult to obtain, or in situations where the problem is naturally posed as one of classifying instance groups. The algorithm described here is an ensemble-based method, wherein the members of the ensemble are lazy learning classifiers learnt using the Citation Nearest Neighbour method. Diversity among the ensemble members is achieved by optimizing their parameters using a multi-objective optimization method, with the objectives being to maximize Class 1 accuracy and minimize false positive rate. The method has been found to be effective on the Musk1 benchmark dataset.
cs.LG:The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order Boltzmann machine where multiplicative interactions are between one visible and two hidden units. There are two kinds of hidden units, namely, gate units and subspace units. The subspace units reflect variations of a pattern in data and the gate unit is responsible for activating the subspace units. Additionally, the gate unit can be seen as a pooling feature. We evaluate the behavior of subspaceRBM through experiments with MNIST digit recognition task, measuring reconstruction error and classification error.
cs.LG:No matter the expressive power and sophistication of supervised learning algorithms, their effectiveness is restricted by the features describing the data. This is not a new insight in ML and many methods for feature selection, transformation, and construction have been developed. But while this is on-going for general techniques for feature selection and transformation, i.e. dimensionality reduction, work on feature construction, i.e. enriching the data, is by now mainly the domain of image, particularly character, recognition, and NLP.   In this work, we propose a new general framework for feature construction. The need for feature construction in a data set is indicated by class outliers and discriminative pattern mining used to derive features on their k-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, and evaluate the usefulness of the derived features on a diverse collection of UCI data sets. The derived features are more often useful than ones derived by DC-Fringe, and our approach is much less likely to overfit. But while a weak learner, Naive Bayes, benefits strongly from the feature construction, the effect is less pronounced for C4.5, and almost vanishes for an SVM leaner.   Keywords: feature construction, classification, outlier detection
cs.LG:In the last several years, the intimate connection between convex optimization and learning problems, in both statistical and sequential frameworks, has shifted the focus of algorithmic machine learning to examine this interplay. In particular, on one hand, this intertwinement brings forward new challenges in reassessment of the performance of learning algorithms including generalization and regret bounds under the assumptions imposed by convexity such as analytical properties of loss functions (e.g., Lipschitzness, strong convexity, and smoothness). On the other hand, emergence of datasets of an unprecedented size, demands the development of novel and more efficient optimization algorithms to tackle large-scale learning problems.   The overarching goal of this thesis is to reassess the smoothness of loss functions in statistical learning, sequential prediction/online learning, and stochastic optimization and explicate its consequences. In particular we examine how smoothness of loss function could be beneficial or detrimental in these settings in terms of sample complexity, statistical consistency, regret analysis, and convergence rate, and investigate how smoothness can be leveraged to devise more efficient learning algorithms.
cs.LG:This paper presents a Fast Synchronization Clustering algorithm (FSynC), which is an improved version of SynC algorithm. In order to decrease the time complexity of the original SynC algorithm, we combine grid cell partitioning method and Red-Black tree to construct the near neighbor point set of every point. By simulated experiments of some artificial data sets and several real data sets, we observe that FSynC algorithm can often get less time than SynC algorithm for many kinds of data sets. At last, it gives some research expectations to popularize this algorithm.
cs.LG:We consider sequential decision making in a setting where regret is measured with respect to a set of stateful reference policies, and feedback is limited to observing the rewards of the actions performed (the so called "bandit" setting). If either the reference policies are stateless rather than stateful, or the feedback includes the rewards of all actions (the so called "expert" setting), previous work shows that the optimal regret grows like $\Theta(\sqrt{T})$ in terms of the number of decision rounds $T$.   The difficulty in our setting is that the decision maker unavoidably loses track of the internal states of the reference policies, and thus cannot reliably attribute rewards observed in a certain round to any of the reference policies. In fact, in this setting it is impossible for the algorithm to estimate which policy gives the highest (or even approximately highest) total reward. Nevertheless, we design an algorithm that achieves expected regret that is sublinear in $T$, of the form $O( T/\log^{1/4}{T})$. Our algorithm is based on a certain local repetition lemma that may be of independent interest. We also show that no algorithm can guarantee expected regret better than $O( T/\log^{3/2} T)$.
cs.LG:Many real-life data are described by categorical attributes without a pre-classification. A common data mining method used to extract information from this type of data is clustering. This method group together the samples from the data that are more similar than all other samples. But, categorical data pose a challenge when extracting information because: the calculation of two objects similarity is usually done by measuring the number of common features, but ignore a possible importance weighting; if the data may be divided differently according to different subsets of the features, the algorithm may find clusters with different meanings from each other, difficulting the post analysis. Data Co-Clustering of categorical data is the technique that tries to find subsets of samples that share a subset of features in common. By doing so, not only a sample may belong to more than one cluster but, the feature selection of each cluster describe its own characteristics. In this paper a novel Co-Clustering technique for categorical data is proposed by using Locality Sensitive Hashing technique in order to preprocess a list of Co-Clusters seeds based on a previous research. Results indicate this technique is capable of finding high quality Co-Clusters in many different categorical data sets and scales linearly with the data set size.
cs.LG:We propose to exploit {\em reconstruction} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.
cs.LG:With advances in data collection technologies, tensor data is assuming increasing prominence in many applications and the problem of supervised tensor learning has emerged as a topic of critical significance in the data mining and machine learning community. Conventional methods for supervised tensor learning mainly focus on learning kernels by flattening the tensor into vectors or matrices, however structural information within the tensors will be lost. In this paper, we introduce a new scheme to design structure-preserving kernels for supervised tensor learning. Specifically, we demonstrate how to leverage the naturally available structure within the tensorial representation to encode prior knowledge in the kernel. We proposed a tensor kernel that can preserve tensor structures based upon dual-tensorial mapping. The dual-tensorial mapping function can map each tensor instance in the input space to another tensor in the feature space while preserving the tensorial structure. Theoretically, our approach is an extension of the conventional kernels in the vector space to tensor space. We applied our novel kernel in conjunction with SVM to real-world tensor classification problems including brain fMRI classification for three different diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV). Extensive empirical studies demonstrate that our proposed approach can effectively boost tensor classification performances, particularly with small sample sizes.
cs.LG:We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline (\alpha,\beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability {\beta} generates an {\alpha} fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize (\alpha,\beta)-approximation regret, which is the difference between the \alpha{\beta} fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.
cs.LG:Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets.
cs.LG:We study the attainable regret for online linear optimization problems with bandit feedback, where unlike the full-information setting, the player can only observe its own loss rather than the full loss vector. We show that the price of bandit information in this setting can be as large as $d$, disproving the well-known conjecture that the regret for bandit linear optimization is at most $\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using "trivial" modifications of standard domains, which have no effect in the full-information setting. This and other results we present highlight some interesting differences between full-information and bandit learning, which were not considered in previous literature.
cs.LG:The VC dimension measures the capacity of a learning machine, and a low VC dimension leads to good generalization. While SVMs produce state-of-the-art learning performance, it is well known that the VC dimension of a SVM can be unbounded; despite good results in practice, there is no guarantee of good generalization. In this paper, we show how to learn a hyperplane classifier by minimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as the Minimal Complexity Machine (MCM), involves solving a simple linear programming problem. Experimental results show, that on a number of benchmark datasets, the proposed approach learns classifiers with error rates much less than conventional SVMs, while often using fewer support vectors. On many benchmark datasets, the number of support vectors is less than one-tenth the number used by SVMs, indicating that the MCM does indeed learn simpler representations.
cs.LG:In this paper, a robust online sequential extreme learning machine (ROS-ELM) is proposed. It is based on the original OS-ELM with an adaptive selective ensemble framework. Two novel insights are proposed in this paper. First, a novel selective ensemble algorithm referred to as particle swarm optimization selective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a general selective ensemble method which is applicable to any learning algorithms, including batch learning and online learning. Second, an adaptive selective ensemble framework for online learning is designed to balance the robustness and complexity of the algorithm. Experiments for both regression and classification problems with UCI data sets are carried out. Comparisons between OS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empirically show that ROS-ELM significantly improves the robustness and stability.
cs.LG:We propose a novel approach to sufficient dimension reduction in regression, based on estimating contour directions of negligible variation for the response surface. These directions span the orthogonal complement of the minimal space relevant for the regression, and can be extracted according to a measure of the variation in the response, leading to General Contour Regression(GCR). In comparison to exiisting sufficient dimension reduction techniques, this sontour-based mothology guarantees exhaustive estimation of the central space under ellipticity of the predictoor distribution and very mild additional assumptions, while maintaining vn-consisytency and somputational ease. Moreover, it proves to be robust to departures from ellipticity. We also establish some useful population properties for GCR. Simulations to compare performance with that of standard techniques such as ordinary least squares, sliced inverse regression, principal hessian directions, and sliced average variance estimation confirm the advntages anticipated by theoretical analyses. We also demonstrate the use of contour-based methods on a data set concerning grades of students from Massachusetts colleges.
cs.LG:Vehicular sensor data consists of multiple time-series arising from a number of sensors. Using such multi-sensor data we would like to detect occurrences of specific events that vehicles encounter, e.g., corresponding to particular maneuvers that a vehicle makes or conditions that it encounters. Events are characterized by similar waveform patterns re-appearing within one or more sensors. Further such patterns can be of variable duration. In this work, we propose a method for detecting such events in time-series data using a novel feature descriptor motivated by similar ideas in image processing. We define the shape histogram: a constant dimension descriptor that nevertheless captures patterns of variable duration. We demonstrate the efficacy of using shape histograms as features to detect events in an SVM-based, multi-sensor, supervised learning scenario, i.e., multiple time-series are used to detect an event. We present results on real-life vehicular sensor data and show that our technique performs better than available pattern detection implementations on our data, and that it can also be used to combine features from multiple sensors resulting in better accuracy than using any single sensor. Since previous work on pattern detection in time-series has been in the single series context, we also present results using our technique on multiple standard time-series datasets and show that it is the most versatile in terms of how it ranks compared to other published results.
cs.LG:AFP Algorithm is a learning algorithm for Horn formulas. We show that it does not improve the complexity of AFP Algorithm, if after each negative counterexample more that just one refinements are performed. Moreover, a canonical normal form for Horn formulas is presented, and it is proved that the output formula of AFP Algorithm is in this normal form.
cs.LG:Traditionally, Multi-task Learning (MTL) models optimize the average of task-related objective functions, which is an intuitive approach and which we will be referring to as Average MTL. However, a more general framework, referred to as Conic MTL, can be formulated by considering conic combinations of the objective functions instead; in this framework, Average MTL arises as a special case, when all combination coefficients equal 1. Although the advantage of Conic MTL over Average MTL has been shown experimentally in previous works, no theoretical justification has been provided to date. In this paper, we derive a generalization bound for the Conic MTL method, and demonstrate that the tightest bound is not necessarily achieved, when all combination coefficients equal 1; hence, Average MTL may not always be the optimal choice, and it is important to consider Conic MTL. As a byproduct of the generalization bound, it also theoretically explains the good experimental results of previous relevant works. Finally, we propose a new Conic MTL model, whose conic combination coefficients minimize the generalization bound, instead of choosing them heuristically as has been done in previous methods. The rationale and advantage of our model is demonstrated and verified via a series of experiments by comparing with several other methods.
cs.LG:We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve $\ell_2$-error fitting problems such as $k$-means clustering and subspace clustering. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for $k$-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.
cs.LG:Although multi-label learning can deal with many problems with label ambiguity, it does not fit some real applications well where the overall distribution of the importance of the labels matters. This paper proposes a novel learning paradigm named \emph{label distribution learning} (LDL) for such kind of applications. The label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is a more general learning framework which includes both single-label and multi-label learning as its special cases. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare the performance of the LDL algorithms, six representative and diverse evaluation measures are selected via a clustering analysis, and the first batch of label distribution datasets are collected and made publicly available. Experimental results on one artificial and fifteen real-world datasets show clear advantages of the specialized algorithms, which indicates the importance of special design for the characteristics of the LDL problem.
cs.LG:This paper describes the solution of Bazinga Team for Tmall Recommendation Prize 2014. With real-world user action data provided by Tmall, one of the largest B2C online retail platforms in China, this competition requires to predict future user purchases on Tmall website. Predictions are judged on F1Score, which considers both precision and recall for fair evaluation. The data set provided by Tmall contains more than half billion action records from over ten million distinct users. Such massive data volume poses a big challenge, and drives competitors to write every single program in MapReduce fashion and run it on distributed cluster. We model the purchase prediction problem as standard machine learning problem, and mainly employ regression and classification methods as single models. Individual models are then aggregated in a two-stage approach, using linear regression for blending, and finally a linear ensemble of blended models. The competition is approaching the end but still in running during writing this paper. In the end, our team achieves F1Score 6.11 and ranks 7th (out of 7,276 teams in total).
cs.LG:In this paper, we study the generalization performance of regularized multi-task learning (RMTL) in a vector-valued framework, where MTL is considered as a learning process for vector-valued functions. We are mainly concerned with two theoretical questions: 1) under what conditions does RMTL perform better with a smaller task sample size than STL? 2) under what conditions is RMTL generalizable and can guarantee the consistency of each task during simultaneous learning?   In particular, we investigate two types of task-group relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM), both of which detect the dependence between two groups of multiple related tasks (MRTs). We then introduce the Cartesian product-based uniform entropy number (CPUEN) to measure the complexities of vector-valued function classes. By applying the specific deviation and the symmetrization inequalities to the vector-valued framework, we obtain the generalization bound for RMTL, which is the upper bound of the joint probability of the event that there is at least one task with a large empirical discrepancy between the expected and empirical risks. Finally, we present a sufficient condition to guarantee the consistency of each task in the simultaneous learning process, and we discuss how task relatedness affects the generalization performance of RMTL. Our theoretical findings answer the aforementioned two questions.
cs.LG:Structural support vector machines (SSVMs) are amongst the best performing models for structured computer vision tasks, such as semantic image segmentation or human pose estimation. Training SSVMs, however, is computationally costly, because it requires repeated calls to a structured prediction subroutine (called \emph{max-oracle}), which has to solve an optimization problem itself, e.g. a graph cut.   In this work, we introduce a new algorithm for SSVM training that is more efficient than earlier techniques when the max-oracle is computationally expensive, as it is frequently the case in computer vision tasks. The main idea is to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm with efficient hyperplane caching, and (ii) use an automatic selection rule for deciding whether to call the exact max-oracle or to rely on an approximate one based on the cached hyperplanes.   We show experimentally that this strategy leads to faster convergence to the optimum with respect to the number of requires oracle calls, and that this translates into faster convergence with respect to the total runtime when the max-oracle is slow compared to the other steps of the algorithm.   A publicly available C++ implementation is provided at http://pub.ist.ac.at/~vnk/papers/SVM.html .
cs.LG:In this paper, the Dempster-Shafer method is employed as the theoretical basis for creating data classification systems. Testing is carried out using three popular (multiple attribute) benchmark datasets that have two, three and four classes. In each case, a subset of the available data is used for training to establish thresholds, limits or likelihoods of class membership for each attribute, and hence create mass functions that establish probability of class membership for each attribute of the test data. Classification of each data item is achieved by combination of these probabilities via Dempster's Rule of Combination. Results for the first two datasets show extremely high classification accuracy that is competitive with other popular methods. The third dataset is non-numerical and difficult to classify, but good results can be achieved provided the system and mass functions are designed carefully and the right attributes are chosen for combination. In all cases the Dempster-Shafer method provides comparable performance to other more popular algorithms, but the overhead of generating accurate mass functions increases the complexity with the addition of new attributes. Overall, the results suggest that the D-S approach provides a suitable framework for the design of classification systems and that automating the mass function design and calculation would increase the viability of the algorithm for complex classification problems.
cs.LG:This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications.
cs.LG:This paper presents a new similarity measure to be used for general tasks including supervised learning, which is represented by the K-nearest neighbor classifier (KNN). The proposed similarity measure is invariant to large differences in some dimensions in the feature space. The proposed metric is proved mathematically to be a metric. To test its viability for different applications, the KNN used the proposed metric for classifying test examples chosen from a number of real datasets. Compared to some other well known metrics, the experimental results show that the proposed metric is a promising distance measure for the KNN classifier with strong potential for a wide range of applications.
cs.LG:In this paper, we propose the problem of domain transfer structured output learn- ing and the first solution to solve it. The problem is defined on two different data domains sharing the same input and output spaces, named as source domain and target domain. The outputs are structured, and for the data samples of the source domain, the corresponding outputs are available, while for most data samples of the target domain, the corresponding outputs are missing. The input distributions of the two domains are significantly different. The problem is to learn a predictor for the target domain to predict the structured outputs from the input. Due to the limited number of outputs available for the samples form the target domain, it is difficult to directly learn the predictor from the target domain, thus it is necessary to use the output information available in source domain. We propose to learn the target domain predictor by adapting a auxiliary predictor trained by using source domain data to the target domain. The adaptation is implemented by adding a delta function on the basis of the auxiliary predictor. An algorithm is developed to learn the parameter of the delta function to minimize loss functions associat- ed with the predicted outputs against the true outputs of the data samples with available outputs of the target domain.
cs.LG:Much of the energy consumption in buildings is due to HVAC systems, which has motivated several recent studies on making these systems more energy- efficient. Occupancy and activity are two important aspects, which need to be correctly estimated for optimal HVAC control. However, state-of-the-art methods to estimate occupancy and classify activity require infrastructure and/or wearable sensors which suffers from lower acceptability due to higher cost. Encouragingly, with the advancement of the smartphones, these are becoming more achievable. Most of the existing occupancy estimation tech- niques have the underlying assumption that the phone is always carried by its user. However, phones are often left at desk while attending meeting or other events, which generates estimation error for the existing phone based occupancy algorithms. Similarly, in the recent days the emerging theory of Sparse Random Classifier (SRC) has been applied for activity classification on smartphone, however, there are rooms to improve the on-phone process- ing. We propose a novel sensor fusion method which offers almost 100% accuracy for occupancy estimation. We also propose an activity classifica- tion algorithm, which offers similar accuracy as of the state-of-the-art SRC algorithms while offering 50% reduction in processing.
cs.LG:The sensitivity of Adaboost to random label noise is a well-studied problem. LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be less sensitive to noise than AdaBoost. We present the results of experiments evaluating these algorithms on both synthetic and real datasets. We compare the performance on each of datasets when the labels are corrupted by different levels of independent label noise. In presence of random label noise, we found that BrownBoost and RobustBoost perform significantly better than AdaBoost and LogitBoost, while the difference between each pair of algorithms is insignificant. We provide an explanation for the difference based on the margin distributions of the algorithms.
cs.LG:In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio to audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better performance for the alignment.
cs.LG:A particularly successful role for Inductive Logic Programming (ILP) is as a tool for discovering useful relational features for subsequent use in a predictive model. Conceptually, the case for using ILP to construct relational features rests on treating these features as functions, the automated discovery of which necessarily requires some form of first-order learning. Practically, there are now several reports in the literature that suggest that augmenting any existing features with ILP-discovered relational features can substantially improve the predictive power of a model. While the approach is straightforward enough, much still needs to be done to scale it up to explore more fully the space of possible features that can be constructed by an ILP system. This is in principle, infinite and in practice, extremely large. Applications have been confined to heuristic or random selections from this space. In this paper, we address this computational difficulty by allowing features to be constructed in a distributed manner. That is, there is a network of computational units, each of which employs an ILP engine to construct some small number of features and then builds a (local) model. We then employ a consensus-based algorithm, in which neighboring nodes share information to update local models. For a category of models (those with convex loss functions), it can be shown that the algorithm will result in all nodes converging to a consensus model. In practice, it may be slow to achieve this convergence. Nevertheless, our results on synthetic and real datasets that suggests that in relatively short time the "best" node in the network reaches a model whose predictive accuracy is comparable to that obtained using more computational effort in a non-distributed setting (the best node is identified as the one whose weights converge first).
cs.LG:This work focuses on active learning of distance metrics from relative comparison information. A relative comparison specifies, for a data point triplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to $x_k$. Such constraints, when available, have been shown to be useful toward defining appropriate distance metrics. In real-world applications, acquiring constraints often require considerable human effort. This motivates us to study how to select and query the most useful relative comparisons to achieve effective metric learning with minimum user effort. Given an underlying class concept that is employed by the user to provide such constraints, we present an information-theoretic criterion that selects the triplet whose answer leads to the highest expected gain in information about the classes of a set of examples. Directly applying the proposed criterion requires examining $O(n^3)$ triplets with $n$ instances, which is prohibitive even for datasets of moderate size. We show that a randomized selection strategy can be used to reduce the selection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size problems. Experiments show that the proposed method consistently outperforms two baseline policies.
cs.LG:We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.
cs.LG:With the availability of high precision digital sensors and cheap storage medium, it is not uncommon to find large amounts of data collected on almost all measurable attributes, both in nature and man-made habitats. Weather in particular has been an area of keen interest for researchers to develop more accurate and reliable prediction models. This paper presents a set of experiments which involve the use of prevalent machine learning techniques to build models to predict the day of the week given the weather data for that particular day i.e. temperature, wind, rain etc., and test their reliability across four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The results provide a comparison of accuracy of these machine learning techniques and their reliability to predict the day of the week by analysing the weather data. We then apply the models to predict weather conditions based on the available data.
cs.LG:We consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability.
cs.LG:We consider \textit{anytime} linear prediction in the common machine learning setting, where features are in groups that have costs. We achieve anytime (or interruptible) predictions by sequencing the computation of feature groups and reporting results using the computed features at interruption. We extend Orthogonal Matching Pursuit (OMP) and Forward Regression (FR) to learn the sequencing greedily under this group setting with costs. We theoretically guarantee that our algorithms achieve near-optimal linear predictions at each budget when a feature group is chosen. With a novel analysis of OMP, we improve its theoretical bound to the same strength as that of FR. In addition, we develop a novel algorithm that consumes cost $4B$ to approximate the optimal performance of \textit{any} cost $B$, and prove that with cost less than $4B$, such an approximation is impossible. To our knowledge, these are the first anytime bounds at \textit{all} budgets. We test our algorithms on two real-world data-sets and evaluate them in terms of anytime linear prediction performance against cost-weighted Group Lasso and alternative greedy algorithms.
cs.LG:Subspace clustering (SC) is a promising clustering technology to identify clusters based on their associations with subspaces in high dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been extensively studied and well accepted by the scientific community, SSC algorithms are relatively new but gaining more attention in recent years due to better adaptability. In the paper, a comprehensive survey on existing SSC algorithms and the recent development are presented. The SSC algorithms are classified systematically into three main categories, namely, conventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The characteristics of these algorithms are highlighted and the potential future development of SSC is also discussed.
cs.LG:The traditional prototype based clustering methods, such as the well-known fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If the available data is limited or scarce, most of the existing prototype based clustering algorithms will no longer be effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding transfer clustering algorithms. Novel objective functions are proposed to integrate the knowledge of source domain with the data of target domain for clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets and the results demonstrate their effectiveness when compared with both the original prototype based fuzzy clustering algorithms and the related clustering algorithms like multi-task clustering and co-clustering.
cs.LG:This document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved.
cs.LG:We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design.
cs.LG:By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and some generalization to double-projection online learning algorithms, as a by-product.
cs.LG:Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximum-cut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account.
cs.LG:In this paper, a novel pattern classification approach is proposed by regularizing the classifier learning to maximize mutual information between the classification response and the true class label. We argue that, with the learned classifier, the uncertainty of the true class label of a data sample should be reduced by knowing its classification response as much as possible. The reduced uncertainty is measured by the mutual information between the classification response and the true class label. To this end, when learning a linear classifier, we propose to maximize the mutual information between classification responses and true class labels of training samples, besides minimizing the classification error and reduc- ing the classifier complexity. An objective function is constructed by modeling mutual information with entropy estimation, and it is optimized by a gradi- ent descend method in an iterative algorithm. Experiments on two real world pattern classification problems show the significant improvements achieved by maximum mutual information regularization.
cs.LG:In cognitive radio (CR) technology, the trend of sensing is no longer to only detect the presence of active primary users. A large number of applications demand for more comprehensive knowledge on primary user behaviors in spatial, temporal, and frequency domains. To satisfy such requirements, we study the statistical relationship among primary users by introducing a Bayesian network (BN) based framework. How to learn such a BN structure is a long standing issue, not fully understood even in the statistical learning community. Besides, another key problem in this learning scenario is that the CR has to identify how many variables are in the BN, which is usually considered as prior knowledge in statistical learning applications. To solve such two issues simultaneously, this paper proposes a BN structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification scheme. The proposed approach incurs significantly lower computational complexity compared with previous ones, and is capable of determining the structure without assuming much prior knowledge about variables. With this result, cognitive users could efficiently understand the statistical pattern of primary networks, such that more efficient cognitive protocols could be designed across different network layers.
cs.LG:Kernel-based approaches for sequence classification have been successfully applied to a variety of domains, including the text categorization, image classification, speech analysis, biological sequence analysis, time series and music classification, where they show some of the most accurate results.   Typical kernel functions for sequences in these domains (e.g., bag-of-words, mismatch, or subsequence kernels) are restricted to {\em discrete univariate} (i.e. one-dimensional) string data, such as sequences of words in the text analysis, codeword sequences in the image analysis, or nucleotide or amino acid sequences in the DNA and protein sequence analysis. However, original sequence data are often of real-valued multivariate nature, i.e. are not univariate and discrete as required by typical $k$-mer based sequence kernel functions.   In this work, we consider the problem of the {\em multivariate} sequence classification such as classification of multivariate music sequences, or multidimensional protein sequence representations. To this end, we extend {\em univariate} kernel functions typically used in sequence analysis and propose efficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) of each sequence dimension in the original {\em real-valued} multivariate sequences and (2) applying novel multivariate discrete kernel measures on these multivariate discrete DFQ sequence representations to more accurately capture similarity relationships among sequences and improve classification performance.   Experiments using the proposed MVDFQ-SK kernel method show excellent classification performance on three challenging music classification tasks as well as protein sequence classification with significant 25-40% improvements over univariate kernel methods and existing state-of-the-art sequence classification methods.
cs.LG:The Volterra series can be used to model a large subset of nonlinear, dynamic systems. A major drawback is the number of coefficients required model such systems. In order to reduce the number of required coefficients, Laguerre polynomials are used to estimate the Volterra kernels. Existing literature proposes algorithms for a fixed number of Volterra kernels, and Laguerre series. This paper presents a novel algorithm for generalized calculation of the finite order Volterra-Laguerre (VL) series for a MIMO system. An example addresses the utility of the algorithm in practical application.
cs.LG:We consider a setting where a system learns to rank a fixed set of $m$ items. The goal is produce good item rankings for users with diverse interests who interact online with the system for $T$ rounds. We consider a novel top-$1$ feedback model: at the end of each round, the relevance score for only the top ranked object is revealed. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For PairwiseLoss and DCG, two popular ranking measures, we prove that the minimax regret is $\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using an efficient strategy that only spends $O(m \log m)$ time per round. The same efficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$. Surprisingly, we show that for normalized versions of these ranking measures, i.e., AUC, NDCG \& MAP, no online ranking algorithm can have sublinear regret.
cs.LG:Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional log-likelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions.
cs.LG:Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize nonegative matrix factorization (NMF) for data representation problems. NMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank nonnegative matrices, has been a popular method for data representation due to its ability to explore the latent part-based structure of data. Recent study shows that lots of data distributions have manifold structures, and we should respect the manifold structure when the data are represented. Recently, manifold regularized NMF used a nearest neighbor graph to regulate the learning of factorization parameter matrices and has shown its advantage over traditional NMF methods for data representation problems. However, how to construct an optimal graph to present the manifold prop- erly remains a difficultproblem due to the graph modelselection, noisy features, and nonlinear distributed data. In this chapter, we introduce three effective methods to solve these problems of graph construction for manifold regularized NMF. Multiple graph learning is proposed to solve the problem of graph model selection, adaptive graph learning via feature selection is proposed to solve the problem of constructing a graph from noisy features, while multi-kernel learning-based graph construction is used to solve the problem of learning a graph from nonlinearly distributed data.
cs.LG:Conditional random fields (CRFs) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model. We implemented our approach as the D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while exploiting its dynamic programming mechanism for efficient probability computation. We tested D-PRISM with logistic regression, a linear-chain CRF and a CRF-CFG and empirically confirmed their excellent discriminative performance compared to their generative counterparts, i.e.\ naive Bayes, an HMM and a PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF versions of Bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in D-PRISM. We empirically showed that they outperform their generative counterparts as expected.
cs.LG:Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel Two-Layer Feature REduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method---called DPC (DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.
cs.LG:The capacity of a learning machine is measured by its Vapnik-Chervonenkis dimension, and learning machines with a low VC dimension generalize better. It is well known that the VC dimension of SVMs can be very large or unbounded, even though they generally yield state-of-the-art learning performance. In this paper, we show how to learn a hyperplane regressor by minimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as the Minimal Complexity Machine (MCM) Regressor, involves solving a simple linear programming problem. Experimental results show, that on a number of benchmark datasets, the proposed approach yields regressors with error rates much less than those obtained with conventional SVM regresssors, while often using fewer support vectors. On some benchmark datasets, the number of support vectors is less than one tenth the number used by SVMs, indicating that the MCM does indeed learn simpler representations.
cs.LG:Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes' probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction. In this article, we will look at the main concepts of naive Bayes classification in the context of document categorization.
cs.LG:This document provides a brief overview of different metrics and terminology that is used to measure the performance of binary classification systems.
cs.LG:In machine learning and pattern recognition, feature selection has been a hot topic in the literature. Unsupervised feature selection is challenging due to the loss of labels which would supply the related information.How to define an appropriate metric is the key for feature selection. We propose a filter method for unsupervised feature selection which is based on the Confidence Machine. Confidence Machine offers an estimation of confidence on a feature'reliability. In this paper, we provide the math model of Confidence Machine in the context of feature selection, which maximizes the relevance and minimizes the redundancy of the selected feature. We compare our method against classic feature selection methods Laplacian Score, Pearson Correlation and Principal Component Analysis on benchmark data sets. The experimental results demonstrate the efficiency and effectiveness of our method.
cs.LG:In this paper, we describe a new vector similarity measure associated with a convex cost function. Given two vectors, we determine the surface normals of the convex function at the vectors. The angle between the two surface normals is the similarity measure. Convex cost function can be the negative entropy function, total variation (TV) function and filtered variation function. The convex cost function need not be differentiable everywhere. In general, we need to compute the gradient of the cost function to compute the surface normals. If the gradient does not exist at a given vector, it is possible to use the subgradients and the normal producing the smallest angle between the two vectors is used to compute the similarity measure.
cs.LG:We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.
cs.LG:In this paper, we compare three initialization schemes for the KMEANS clustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and 3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k needs to be set by the user of the algorithms. (Kang 2013) recently proposed a novel use of determinantal point processes for sampling the initial centroids for the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide any evaluation establishing that KMEANSD++ is better than other algorithms. In this paper, we show that the performance of KMEANSD++ is comparable to KMEANS++ (both of which are better than KMEANSRAND) with KMEANSD++ having an additional that it can automatically approximate the value of k.
cs.LG:Feature selection involes identifying the most relevant subset of input features, with a view to improving generalization of predictive models by reducing overfitting. Directly searching for the most relevant combination of attributes is NP-hard. Variable selection is of critical importance in many applications, such as micro-array data analysis, where selecting a small number of discriminative features is crucial to developing useful models of disease mechanisms, as well as for prioritizing targets for drug discovery. The recently proposed Minimal Complexity Machine (MCM) provides a way to learn a hyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on its VC dimension. It is well known that a lower VC dimension contributes to good generalization. For a linear hyperplane classifier in the input space, the VC dimension is upper bounded by the number of features; hence, a linear classifier with a small VC dimension is parsimonious in the set of features it employs. In this paper, we use the linear MCM to learn a classifier in which a large number of weights are zero; features with non-zero weights are the ones that are chosen. Selected features are used to learn a kernel SVM classifier. On a number of benchmark datasets, the features chosen by the linear MCM yield comparable or better test set accuracy than when methods such as ReliefF and FCBF are used for the task. The linear MCM typically chooses one-tenth the number of attributes chosen by the other methods; on some very high dimensional datasets, the MCM chooses about $0.6\%$ of the features; in comparison, ReliefF and FCBF choose 70 to 140 times more features, thus demonstrating that minimizing the VC dimension may provide a new, and very effective route for feature selection and for learning sparse representations.
cs.LG:A Relational Dependency Network (RDN) is a directed graphical model widely used for multi-relational data. These networks allow cyclic dependencies, necessary to represent relational autocorrelations. We describe an approach for learning both the RDN's structure and its parameters, given an input relational database: First learn a Bayesian network (BN), then transform the Bayesian network to an RDN. Thus fast Bayes net learning can provide fast RDN learning. The BN-to-RDN transform comprises a simple, local adjustment of the Bayes net structure and a closed-form transform of the Bayes net parameters. This method can learn an RDN for a dataset with a million tuples in minutes. We empirically compare our approach to state-of-the art RDN learning methods that use functional gradient boosting, on five benchmark datasets. Learning RDNs via BNs scales much better to large datasets than learning RDNs with boosting, and provides competitive accuracy in predictions.
cs.LG:Standard Multi-Armed Bandit (MAB) problems assume that the arms are independent. However, in many application scenarios, the information obtained by playing an arm provides information about the remainder of the arms. Hence, in such applications, this informativeness can and should be exploited to enable faster convergence to the optimal solution. In this paper, we introduce and formalize the Global MAB (GMAB), in which arms are globally informative through a global parameter, i.e., choosing an arm reveals information about all the arms. We propose a greedy policy for the GMAB which always selects the arm with the highest estimated expected reward, and prove that it achieves bounded parameter-dependent regret. Hence, this policy selects suboptimal arms only finitely many times, and after a finite number of initial time steps, the optimal arm is selected in all of the remaining time steps with probability one. In addition, we also study how the informativeness of the arms about each other's rewards affects the speed of learning. Specifically, we prove that the parameter-free (worst-case) regret is sublinear in time, and decreases with the informativeness of the arms. We also prove a sublinear in time Bayesian risk bound for the GMAB which reduces to the well-known Bayesian risk bound for linearly parameterized bandits when the arms are fully informative. GMABs have applications ranging from drug and treatment discovery to dynamic pricing.
cs.LG:Estimating the parameters of probabilistic models of language such as maxent models and probabilistic neural models is computationally difficult since it involves evaluating partition functions by summing over an entire vocabulary, which may be millions of word types in size. Two closely related strategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al., 2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this computational problem, but some confusion remains as to which is more appropriate and when. This document explicates their relationships to each other and to other estimation techniques. The analysis shows that, although they are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while negative sampling is best understood as a family of binary classification models that are useful for learning word representations but not as a general-purpose estimator.
cs.LG:We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.
cs.LG:This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved.
cs.LG:We present Factorbird, a prototype of a parameter server approach for factorizing large matrices with Stochastic Gradient Descent-based algorithms. We designed Factorbird to meet the following desiderata: (a) scalability to tall and wide matrices with dozens of billions of non-zeros, (b) extensibility to different kinds of models and loss functions as long as they can be optimized using Stochastic Gradient Descent (SGD), and (c) adaptability to both batch and streaming scenarios. Factorbird uses a parameter server in order to scale to models that exceed the memory of an individual machine, and employs lock-free Hogwild!-style learning with a special partitioning scheme to drastically reduce conflicting updates. We also discuss other aspects of the design of our system such as how to efficiently grid search for hyperparameters at scale. We present experiments of Factorbird on a matrix built from a subset of Twitter's interaction graph, consisting of more than 38 billion non-zeros and about 200 million rows and columns, which is to the best of our knowledge the largest matrix on which factorization results have been reported in the literature.
cs.LG:CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the {\it full} matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only $O(n r\ln r)$ observed entries are needed by the proposed algorithm to perfectly recover a rank $r$ matrix of size $n\times n$, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.
cs.LG:The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by the spectral theorem is a foundational result in applied mathematics. Motivated by a shared structure found in inferential problems of recent interest---namely orthogonal tensor decompositions, Independent Component Analysis (ICA), topic models, spectral clustering, and Gaussian mixture learning---we generalize the eigendecomposition from quadratic forms to a broad class of "orthogonally decomposable" functions. We identify a key role of convexity in our extension, and we generalize two traditional characterizations of eigenvectors: First, the eigenvectors of a quadratic form arise from the optima structure of the quadratic form on the sphere. Second, the eigenvectors are the fixed points of the power iteration.   In our setting, we consider a simple first order generalization of the power method which we call gradient iteration. It leads to efficient and easily implementable methods for basis recovery. It includes influential Machine Learning methods such as cumulant-based FastICA and the tensor power iteration for orthogonally decomposable tensors as special cases.   We provide a complete theoretical analysis of gradient iteration using the structure theory of discrete dynamical systems to show almost sure convergence and fast (super-linear) convergence rates. The analysis also extends to the case when the observed function is only approximately orthogonally decomposable, with bounds that are polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices.
cs.LG:We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.
cs.LG:It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, "learning to learn" as they do so. In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal. Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm. Our aim is to learn new internal representations as the algorithm learns new target functions, that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning nonlinear Boolean combinations of features. Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural "anchor-set" assumption.
cs.LG:We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods.
cs.LG:A common phenomena in modern recommendation systems is the use of feedback from one user to infer the `value' of an item to other users. This results in an exploration vs. exploitation trade-off, in which items of possibly low value have to be presented to users in order to ascertain their value. Existing approaches to solving this problem focus on the case where the number of items are small, or admit some underlying structure -- it is unclear, however, if good recommendation is possible when dealing with content-rich settings with unstructured content.   We consider this problem under a simple natural model, wherein the number of items and the number of item-views are of the same order, and an `access-graph' constrains which user is allowed to see which item. Our main insight is that the presence of the access-graph in fact makes good recommendation possible -- however this requires the exploration policy to be designed to take advantage of the access-graph. Our results demonstrate the importance of `serendipity' in exploration, and how higher graph-expansion translates to a higher quality of recommendations; it also suggests a reason why in some settings, simple policies like Twitter's `Latest-First' policy achieve a good performance.   From a technical perspective, our model presents a way to study exploration-exploitation tradeoffs in settings where the number of `trials' and `strategies' are large (potentially infinite), and more importantly, of the same order. Our algorithms admit competitive-ratio guarantees which hold for the worst-case user, under both finite-population and infinite-horizon settings, and are parametrized in terms of properties of the underlying graph. Conversely, we also demonstrate that improperly-designed policies can be highly sub-optimal, and that in many settings, our results are order-wise optimal.
cs.LG:The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.
cs.LG:We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.
cs.LG:Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a new atom from a dictionary via the steepest gradient descent and build the estimator via orthogonal projecting the target function to the space spanned by the selected atoms in each greedy step. Here, "greed" means choosing a new atom according to the steepest gradient descent principle. OGL then avoids the overfitting/underfitting by selecting an appropriate iteration number. In this paper, we point out that the overfitting/underfitting can also be avoided via redefining "greed" in OGL. To this end, we introduce a new greedy metric, called $\delta$-greedy thresholds, to refine "greed" and theoretically verifies its feasibility. Furthermore, we reveals that such a greedy metric can bring an adaptive termination rule on the premise of maintaining the prominent learning performance of OGL. Our results show that the steepest gradient descent is not the unique greedy metric of OGL and some other more suitable metric may lessen the hassle of model-selection of OGL.
cs.LG:Consider a stationary discrete random process with alphabet size d, which is assumed to be the output process of an unknown stationary Hidden Markov Model (HMM). Given the joint probabilities of finite length strings of the process, we are interested in finding a finite state generative model to describe the entire process. In particular, we focus on two classes of models: HMMs and quasi-HMMs, which is a strictly larger class of models containing HMMs. In the main theorem, we show that if the random process is generated by an HMM of order less or equal than k, and whose transition and observation probability matrix are in general position, namely almost everywhere on the parameter space, both the minimal quasi-HMM realization and the minimal HMM realization can be efficiently computed based on the joint probabilities of all the length N strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to compare and connect the two lines of literature: realization theory of HMMs, and the recent development in learning latent variable models with tensor decomposition techniques.
cs.LG:Clinical trial adaptation refers to any adjustment of the trial protocol after the onset of the trial. The main goal is to make the process of introducing new medical interventions to patients more efficient by reducing the cost and the time associated with evaluating their safety and efficacy. The principal question is how should adaptation be performed so as to minimize the chance of distorting the outcome of the trial. We propose a novel method for achieving this. Unlike previous work our approach focuses on trial adaptation by sample size adjustment. We adopt a recently proposed stratification framework based on collected auxiliary data and show that this information together with the primary measured variables can be used to make a probabilistically informed choice of the particular sub-group a sample should be removed from. Experiments on simulated data are used to illustrate the effectiveness of our method and its application in practice.
cs.LG:An important use of private data is to build machine learning classifiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset.   In this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques significantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set.
cs.LG:In the convex optimization approach to online regret minimization, many methods have been developed to guarantee a $O(\sqrt{T})$ bound on regret for subdifferentiable convex loss functions with bounded subgradients, by using a reduction to linear loss functions. This suggests that linear loss functions tend to be the hardest ones to learn against, regardless of the underlying decision spaces. We investigate this question in a systematic fashion looking at the interplay between the set of possible moves for both the decision maker and the adversarial environment. This allows us to highlight sharp distinctive behaviors about the learnability of piecewise linear loss functions. On the one hand, when the decision set of the decision maker is a polyhedron, we establish $\Omega(\sqrt{T})$ lower bounds on regret for a large class of piecewise linear loss functions with important applications in online linear optimization, repeated zero-sum Stackelberg games, online prediction with side information, and online two-stage optimization. On the other hand, we exhibit $o(\sqrt{T})$ learning rates, achieved by the Follow-The-Leader algorithm, in online linear optimization when the boundary of the decision maker's decision set is curved and when $0$ does not lie in the convex hull of the environment's decision set. Hence, the curvature of the decision maker's decision set is a determining factor for the optimal learning rate. These results hold in a completely adversarial setting.
cs.LG:In many real-world applications, data are represented by matrices or high-order tensors. Despite the promising performance, the existing two-dimensional discriminant analysis algorithms employ a single projection model to exploit the discriminant information for projection, making the model less flexible. In this paper, we propose a novel Compound Rank-k Projection (CRP) algorithm for bilinear analysis. CRP deals with matrices directly without transforming them into vectors, and it therefore preserves the correlations within the matrix and decreases the computation complexity. Different from the existing two dimensional discriminant analysis algorithms, objective function values of CRP increase monotonically.In addition, CRP utilizes multiple rank-k projection models to enable a larger search space in which the optimal solution can be found. In this way, the discriminant ability is enhanced.
cs.LG:In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms.
cs.LG:Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2 , 1 -norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.
cs.LG:Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.
cs.LG:Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter free, distance consistent Locally Linear Embedding. The proposed distance consistent LLE promises that edges between closer data points have greater weight.Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art:1) label propagation,which propagates a node\'s labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state of the art spectral algorithms.
cs.LG:While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving state-of-the-art accuracies yet with substantially faster training speed.
cs.LG:We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previously best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than $\Omega(\sqrt{T})$. We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $O(\log T)$, an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several different scenarios.
cs.LG:Spectral clustering is a fundamental technique in the field of data mining and information processing. Most existing spectral clustering algorithms integrate dimensionality reduction into the clustering process assisted by manifold learning in the original space. However, the manifold in reduced-dimensional subspace is likely to exhibit altered properties in contrast with the original space. Thus, applying manifold information obtained from the original space to the clustering process in a low-dimensional subspace is prone to inferior performance. Aiming to address this issue, we propose a novel convex algorithm that mines the manifold structure in the low-dimensional subspace. In addition, our unified learning process makes the manifold learning particularly tailored for the clustering. Compared with other related methods, the proposed algorithm results in more structured clustering result. To validate the efficacy of the proposed algorithm, we perform extensive experiments on several benchmark datasets in comparison with some state-of-the-art clustering approaches. The experimental results demonstrate that the proposed algorithm has quite promising clustering performance.
cs.LG:The growing amount of high dimensional data in different machine learning applications requires more efficient and scalable optimization algorithms. In this work, we consider combining two techniques, parallelism and Nesterov's acceleration, to design faster algorithms for L1-regularized loss. We first simplify BOOM, a variant of gradient descent, and study it in a unified framework, which allows us to not only propose a refined measurement of sparsity to improve BOOM, but also show that BOOM is provably slower than FISTA. Moving on to parallel coordinate descent methods, we then propose an efficient accelerated version of Shotgun, improving the convergence rate from $O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysis compared to previous work, and also allows one to study several connected work in a unified way.
cs.LG:In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worst-case viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers.   Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with $m$ constraints and matrices of size $d$ by $d$ is roughly reduced from $\mathcal{O}(m^3+md^3+m^2d^2)$ to $\mathcal{O}(d^3)$ ($m>d$ in our case).
cs.LG:This report discusses two new indices for comparing clusterings of a set of points. The motivation for looking at new ways for comparing clusterings stems from the fact that the existing clustering indices are based on set cardinality alone and do not consider the positions of data points. The new indices, namely, the Random Walk index (RWI) and Variation of Information with Neighbors (VIN), are both inspired by the clustering metric Variation of Information (VI). VI possesses some interesting theoretical properties which are also desirable in a metric for comparing clusterings. We define our indices and discuss some of their explored properties which appear relevant for a clustering index. We also include the results of these indices on clusterings of some example data sets.
cs.LG:Matrix factorization is a popular approach for large-scale matrix completion. The optimization formulation based on matrix factorization can be solved very efficiently by standard algorithms in practice. However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of this formulation. In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization formulation, and recover the true low-rank matrix. We study the local geometry of a properly regularized factorization formulation and prove that any stationary point in a certain local region is globally optimal. A major difference of our work from the existing results is that we do not need resampling in either the algorithm or its analysis. Compared to other works on nonconvex optimization, one extra difficulty lies in analyzing nonconvex constrained optimization when the constraint (or the corresponding regularizer) is not "consistent" with the gradient direction. One technical contribution is the perturbation analysis for non-symmetric matrix factorization.
cs.LG:We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.
cs.LG:Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.
cs.LG:In this work we consider the learning setting where, in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. We focus on a broad class of ERM-based linear algorithms that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that, if the algorithm is fed with a good combination of source hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$ instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.
cs.LG:The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less.
cs.LG:In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, a threshold is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting (thresholded) classifier measured with respect to the squared AMS, is upperbounded by the regret of the underlying real-valued function measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.
cs.LG:In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.
cs.LG:A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the "maximum margin" linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation.
cs.LG:Deep learning has attracted great attention recently and yielded the state of the art performance in dimension reduction and classification problems. However, it cannot effectively handle the structured output prediction, e.g. sequential labeling. In this paper, we propose a deep learning structure, which can learn discriminative features for sequential labeling problems. More specifically, we add the inter-relationship between labels in our deep learning structure, in order to incorporate the context information from the sequential data. Thus, our model is more powerful than linear Conditional Random Fields (CRFs) because the objective function learns latent non-linear features so that target labeling can be better predicted. We pretrain the deep structure with stacked restricted Boltzmann machines (RBMs) for feature learning and optimize our objective function with online learning algorithm, a mixture of perceptron training and stochastic gradient descent. We test our model on different challenge tasks, and show that our model outperforms significantly over the completive baselines.
cs.LG:The purpose of this report is in examining the generalization performance of Support Vector Machines (SVM) as a tool for pattern recognition and object classification. The work is motivated by the growing popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. The method is implemented in MATLAB. SVMs based on various kernels are tested for classifying data from various domains.
cs.LG:In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, a $l_{2,1}$ norm constraint is introduced to make the transformation matrix in group sparsity. In addition, for multi-class classification tasks, we further intend to learn and leverage the correlation relationships among multiple class tasks for assisting in learning discriminative features. The experimental results demonstrate the power of the proposed method against the related state-of-the-art methods.
cs.LG:We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogeneous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality.   The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate.
cs.LG:Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for stream data. MORES can \emph{dynamically} learn the structure of the coefficients change in each update step to facilitate the model's continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to \emph{dynamically} learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to \emph{exactly} represent all the seen samples for \emph{incrementally} calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams' evolving characteristics quickly from the latest samples. Experiments on one synthetic dataset and three real-world datasets validate the effectiveness of the proposed method. In addition, the update speed of MORES is at least 2000 samples processed per second on the three real-world datasets, more than 15 times faster than the state-of-the-art online learning algorithm.
cs.LG:In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances.
cs.LG:The notion of metric plays a key role in machine learning problems such as classification, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a given metric. The theoretical framework of $(\epsilon, \gamma, \tau)$-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework.
cs.LG:Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.
cs.LG:We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.
cs.LG:Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM), and its staple training algorithm contrastive divergence (CD), have been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, CD has limited theoretical motivation, and can in some cases produce undesirable behavior. Here, we investigate the performance of Minimum Probability Flow (MPF) learning for training RBMs. Unlike CD, with its focus on approximating an intractable partition function via Gibbs sampling, MPF proposes a tractable, consistent, objective function defined in terms of a Taylor expansion of the KL divergence with respect to sampling dynamics. Here we propose a more general form for the sampling dynamics in MPF, and explore the consequences of different choices for these dynamics for training RBMs. Experimental results show MPF outperforming CD for various RBM configurations.
cs.LG:We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.
cs.LG:We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$ more similar to $x_j$ than to $x_k$? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don't know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don't know answers are considered; 2) the improved performance of the proposed method over state-of-the-art methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement.
cs.LG:Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps.
cs.LG:We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all real-life problems perfectly; this is supported by the no-free-lunch theorem
cs.LG:Support Vector Machine (SVM) is an effective model for many classification problems. However, SVM needs the solution of a quadratic program which require specialized code. In addition, SVM has many parameters, which affects the performance of SVM classifier. Recently, the Generalized Eigenvalue Proximal SVM (GEPSVM) has been presented to solve the SVM complexity. In real world applications data may affected by error or noise, working with this data is a challenging problem. In this paper, an approach has been proposed to overcome this problem. This method is called DSA-GEPSVM. The main improvements are carried out based on the following: 1) a novel fuzzy values in the linear case. 2) A new Kernel function in the nonlinear case. 3) Differential Search Algorithm (DSA) is reformulated to find near optimal values of the GEPSVM parameters and its kernel parameters. The experimental results show that the proposed approach is able to find the suitable parameter values, and has higher classification accuracy compared with some other algorithms.
cs.LG:Learning a kernel matrix from relative comparison human feedback is an important problem with applications in collaborative filtering, object retrieval, and search. For learning a kernel over a large number of objects, existing methods face significant scalability issues inhibiting the application of these methods to settings where a kernel is learned in an online and timely fashion. In this paper we propose a novel framework called Efficient online Relative comparison Kernel LEarning (ERKLE), for efficiently learning the similarity of a large set of objects in an online manner. We learn a kernel from relative comparisons via stochastic gradient descent, one query response at a time, by taking advantage of the sparse and low-rank properties of the gradient to efficiently restrict the kernel to lie in the space of positive semidefinite matrices. In addition, we derive a passive-aggressive online update for minimally satisfying new relative comparisons as to not disrupt the influence of previously obtained comparisons. Experimentally, we demonstrate a considerable improvement in speed while obtaining improved or comparable accuracy compared to current methods in the online learning setting.
cs.LG:High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.
cs.LG:In a variety of problems, the number and state of multiple moving targets are unknown and are subject to be inferred from their measurements obtained by a sensor with limited sensing ability. This type of problems is raised in a variety of applications, including monitoring of endangered species, cleaning, and surveillance. Particle filters are widely used to estimate target state from its prior information and its measurements that recently become available, especially for the cases when the measurement model and the prior distribution of state of interest are non-Gaussian. However, the problem of estimating number of total targets and their state becomes intractable when the number of total targets and the measurement-target association are unknown. This paper presents a novel Gaussian particle filter technique that combines Kalman filter and particle filter for estimating the number and state of total targets based on the measurement obtained online. The estimation is represented by a set of weighted particles, different from classical particle filter, where each particle is a Gaussian distribution instead of a point mass.
cs.LG:The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs.
cs.LG:We propose novel methods for max-cost Discrete Function Evaluation Problem (DFEP) under budget constraints. We are motivated by applications such as clinical diagnosis where a patient is subjected to a sequence of (possibly expensive) tests before a decision is made. Our goal is to develop strategies for minimizing max-costs. The problem is known to be NP hard and greedy methods based on specialized impurity functions have been proposed. We develop a broad class of \emph{admissible} impurity functions that admit monomials, classes of polynomials, and hinge-loss functions that allow for flexible impurity design with provably optimal approximation bounds. This flexibility is important for datasets when max-cost can be overly sensitive to "outliers." Outliers bias max-cost to a few examples that require a large number of tests for classification. We design admissible functions that allow for accuracy-cost trade-off and result in $O(\log n)$ guarantees of the optimal cost among trees with corresponding classification accuracy levels.
cs.LG:Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework -- a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines.
cs.LG:We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.
cs.LG:Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by Crammer and Singer. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times.
cs.LG:In this paper, we propose the problem of optimizing multivariate performance measures from multi-view data, and an effective method to solve it. This problem has two features: the data points are presented by multiple views, and the target of learning is to optimize complex multivariate performance measures. We propose to learn a linear discriminant functions for each view, and combine them to construct a overall multivariate mapping function for mult-view data. To learn the parameters of the linear dis- criminant functions of different views to optimize multivariate performance measures, we formulate a optimization problem. In this problem, we propose to minimize the complexity of the linear discriminant functions of each view, encourage the consistences of the responses of different views over the same data points, and minimize the upper boundary of a given multivariate performance measure. To optimize this problem, we employ the cutting-plane method in an iterative algorithm. In each iteration, we update a set of constrains, and optimize the mapping function parameter of each view one by one.
cs.LG:Assuming a view of the Random Forest as a special case of a nested ensemble of interchangeable modules, we construct a generalisation space allowing one to easily develop novel methods based on this algorithm. We discuss the role and required properties of modules at each level, especially in context of some already proposed RF generalisations.
cs.LG:In [1], a clustering algorithm was given to find the centers of clusters quickly. However, the accuracy of this algorithm heavily depend on the threshold value of d-c. Furthermore, [1] has not provided any efficient way to select the threshold value of d-c, that is, one can have to estimate the value of d_c depend on one's subjective experience. In this paper, based on the data field [2], we propose a new way to automatically extract the threshold value of d_c from the original data set by using the potential entropy of data field. For any data set to be clustered, the most reasonable value of d_c can be objectively calculated from the data set by using our proposed method. The same experiments in [1] are redone with our proposed method on the same experimental data set used in [1], the results of which shows that the problem to calculate the threshold value of d_c in [1] has been solved by using our method.
cs.LG:In this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two chal- lenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions.
cs.LG:Most of the existing classification methods are aimed at minimization of empirical risk (through some simple point-based error measured with loss function) with added regularization. We propose to approach this problem in a more information theoretic way by investigating applicability of entropy measures as a classification model objective function. We focus on quadratic Renyi's entropy and connected Cauchy-Schwarz Divergence which leads to the construction of Extreme Entropy Machines (EEM).   The main contribution of this paper is proposing a model based on the information theoretic concepts which on the one hand shows new, entropic perspective on known linear classifiers and on the other leads to a construction of very robust method competetitive with the state of the art non-information theoretic ones (including Support Vector Machines and Extreme Learning Machines).   Evaluation on numerous problems spanning from small, simple ones from UCI repository to the large (hundreads of thousands of samples) extremely unbalanced (up to 100:1 classes' ratios) datasets shows wide applicability of the EEM in real life problems and that it scales well.
cs.LG:Semi-supervised clustering is an very important topic in machine learning and computer vision. The key challenge of this problem is how to learn a metric, such that the instances sharing the same label are more likely close to each other on the embedded space. However, little attention has been paid to learn better representations when the data lie on non-linear manifold. Fortunately, deep learning has led to great success on feature learning recently. Inspired by the advances of deep learning, we propose a deep transductive semi-supervised maximum margin clustering approach. More specifically, given pairwise constraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum margin framework for clustering analysis. Thus, our model unifies transductive learning, feature learning and maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep network structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our objective function with gradient descent. By checking the most violated constraints, our approach updates the model parameters through error backpropagation, in which deep features are learned automatically. The experimental results shows that our model is significantly better than the state of the art on semi-supervised clustering.
cs.LG:In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping
cs.LG:Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel inner-product between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude---in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\"olkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper.
cs.LG:Coronary heart disease (CHD) caused by hardening of artery walls due to cholesterol known as atherosclerosis is responsible for large number of deaths world-wide. The disease progression is slow, asymptomatic and may lead to sudden cardiac arrest, stroke or myocardial infraction. Presently, imaging techniques are being employed to understand the molecular and metabolic activity of atherosclerotic plaques to estimate the risk. Though imaging methods are able to provide some information on plaque metabolism they lack the required resolution and sensitivity for detection. In this paper we consider the clinical observations and habits of individuals for predicting the risk factors of CHD. The identification of risk factors helps in stratifying patients for further intensive tests such as nuclear imaging or coronary angiography. We present a novel approach for predicting the risk factors of atherosclerosis with an in-built imputation algorithm and particle swarm optimization (PSO). We compare the performance of our methodology with other machine learning techniques on STULONG dataset which is based on longitudinal study of middle aged individuals lasting for twenty years. Our methodology powered by PSO search has identified physical inactivity as one of the risk factor for the onset of atherosclerosis in addition to other already known factors. The decision rules extracted by our methodology are able to predict the risk factors with an accuracy of $99.73%$ which is higher than the accuracies obtained by application of the state-of-the-art machine learning techniques presently being employed in the identification of atherosclerosis risk studies.
cs.LG:Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (per-block) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.
cs.LG:This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively.
cs.LG:Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures.   We first develop Constraints that machine learning imposes upon VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases ("The smart Brazilian girl") by binding sums of terms, in addition to simply binding the terms directly.   We show that matrix multiplication can be used as the binding operator for a VSA, and that matrix elements can be chosen at random. A consequence for living systems is that binding is mathematically possible without the need to specify, in advance, precise neuron-to-neuron connection properties for large numbers of synapses.   A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms), is described that satisfies all Constraints.   With respect to machine learning, for some types of problems appropriate VSA representations permit us to prove learnability, rather than relying on simulations. We also propose dividing machine (and neural) learning and representation into three Stages, with differing roles for learning in each stage.   For neural modeling, we give "representational reasons" for nervous systems to have many recurrent connections, as well as for the importance of phrases in language processing.   Sizing simulations and analyses suggest that VSAs in general, and MBAT in particular, are ready for real-world applications.
cs.LG:We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole. To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start. Experiments on both natural image patches and UCI data sets show that the proposed algorithm produces a better approximation with the same sparsity levels compared to the state-of-the-art algorithms.
cs.LG:We often encounter situations in which an experimenter wants to find, by sequential experimentation, $x_{max} = \arg\max_{x} f(x)$, where $f(x)$ is a (possibly unknown) function of a well controllable variable $x$. Taking inspiration from physics and engineering, we have designed a new method to address this problem. In this paper, we first introduce the method in continuous time, and then present two algorithms for use in sequential experiments. Through a series of simulation studies, we show that the method is effective for finding maxima of unknown functions by experimentation, even when the maximum of the functions drifts or when the signal to noise ratio is low.
cs.LG:In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations.
cs.LG:We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice - sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. If, in addition, the loss functions are quadratic, our method can be interpreted as a novel variant of the recently introduced Iterative Hessian Sketch.
cs.LG:The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear (or kernelized) classifiers, the minimization of the logistic loss is \textit{equivalent} to the minimization of an exponential \textit{rado}-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \textit{same} classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \textit{directly} used to classify \textit{observations}. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \textit{logistic loss} (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \textit{complete} set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados can compete with learning from random rados, and hence with batch learning from examples, achieving non-trivial privacy vs accuracy tradeoffs.
cs.LG:We present a mathematical construction for the restricted Boltzmann machine (RBM) that doesn't require specifying the number of hidden units. In fact, the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM to be sensitive to the ordering of its hidden units. Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As with RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behaviour of this infinite RBM, showing that its performance is competitive to that of the RBM, while not requiring the tuning of a hidden layer size.
cs.LG:We present a novel adaptive random subspace learning algorithm (RSSL) for prediction purpose. This new framework is flexible where it can be adapted with any learning technique. In this paper, we tested the algorithm for regression and classification problems. In addition, we provide a variety of weighting schemes to increase the robustness of the developed algorithm. These different wighting flavors were evaluated on simulated as well as on real-world data sets considering the cases where the ratio between features (attributes) and instances (samples) is large and vice versa. The framework of the new algorithm consists of many stages: first, calculate the weights of all features on the data set using the correlation coefficient and F-statistic statistical measurements. Second, randomly draw n samples with replacement from the data set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without replacement the indices of the chosen variables. The decision was taken based on the heuristic subspacing scheme. Fifth, call base learners and build the model. Sixth, use the model for prediction purpose on test set of the data. The results show the advancement of the adaptive RSSL algorithm in most of the cases compared with the synonym (conventional) machine learning algorithms.
cs.LG:We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. This optimal algorithm is not adaptive however. Using tools from online loss minimization, we derive an adaptive online boosting algorithm that is also parameter-free, but not optimal. Both algorithms work with base learners that can handle example importance weights directly, as well as by rejection sampling examples with probability defined by the booster. Results are complemented with an extensive experimental study.
cs.LG:We provide a summary of the mathematical and computational techniques that have enabled learning reductions to effectively address a wide class of problems, and show that this approach to solving machine learning problems can be broadly useful.
cs.LG:Modeling the dependence between outputs is a fundamental challenge in multilabel classification. In this work we show that a generic regularized nonlinearity mapping independent predictions to joint predictions is sufficient to achieve state-of-the-art performance on a variety of benchmark problems. Crucially, we compute the joint predictions without ever obtaining any independent predictions, while incorporating low-rank and smoothness regularization. We achieve this by leveraging randomized algorithms for matrix decomposition and kernel approximation. Furthermore, our techniques are applicable to the multiclass setting. We apply our method to a variety of multiclass and multilabel data sets, obtaining state-of-the-art results.
cs.LG:Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.
cs.LG:Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.
cs.LG:Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for Symmetric Positive Definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.
cs.LG:Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (CoCoA) for distributed optimization. Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes with convergence guarantees only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of CoCoA+ on several real-world distributed datasets, especially when scaling up the number of machines.
cs.LG:Stochastic alternating direction method of multipliers (ADMM), which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM. However, most stochastic methods can only achieve a convergence rate $O(1/\sqrt T)$ on general convex problems,where T is the number of iterations. Hence, these methods are not scalable with respect to convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve convergence rate $O(1/T)$ on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable with respect to storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve state-of-the-art performance in real applications
cs.LG:Bankruptcy is a legal procedure that claims a person or organization as a debtor. It is essential to ascertain the risk of bankruptcy at initial stages to prevent financial losses. In this perspective, different soft computing techniques can be employed to ascertain bankruptcy. This study proposes a bankruptcy prediction system to categorize the companies based on extent of risk. The prediction system acts as a decision support tool for detection of bankruptcy   Keywords: Bankruptcy, soft computing, decision support tool
cs.LG:We give a new deterministic algorithm that non-adaptively learns a hidden hypergraph from edge-detecting queries. All previous non-adaptive algorithms either run in exponential time or have non-optimal query complexity. We give the first polynomial time non-adaptive learning algorithm for learning hypergraph that asks almost optimal number of queries.
cs.LG:Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.
cs.LG:We present a deep learning approach to estimation of the bead parameters in welding tasks. Our model is based on a four-hidden-layer neural network architecture. More specifically, the first three hidden layers of this architecture utilize Sigmoid function to produce their respective intermediate outputs. On the other hand, the last hidden layer uses a linear transformation to generate the final output of this architecture. This transforms our deep network architecture from a classifier to a non-linear regression model. We compare the performance of our deep network with a selected number of results in the literature to show a considerable improvement in reducing the errors in estimation of these values. Furthermore, we show its scalability on estimating the weld bead parameters with same level of accuracy on combination of datasets that pertain to different welding techniques. This is a nontrivial result that is counter-intuitive to the general belief in this field of research.
cs.LG:The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission.   In this work, we introduce a notion of "leaderboard accuracy" tailored to the format of a competition. We introduce a natural algorithm called "the Ladder" and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from an actual competition hosted by Kaggle.   Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.
cs.LG:Errors in data are usually unwelcome and so some means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data. We then exploit the fact that this abstract transformation, which we call a deep transform (DT), inherently rejects information (errors) existing outside of the abstract feature space. Using the DT to perform probabilistic re-synthesis, we demonstrate the recovery of data that has been subject to extreme degradation.
cs.LG:The majority of machine learning algorithms assumes that objects are represented as vectors. But often the objects we want to learn on are more naturally represented by other data structures such as sequences and time series. For these representations many standard learning algorithms are unavailable. We generalize gradient-based learning algorithms to time series under dynamic time warping. To this end, we introduce elastic functions, which extend functions on time series to matrix spaces. Necessary conditions are presented under which generalized gradient learning on time series is consistent. We indicate how results carry over to arbitrary elastic distance functions and to sequences consisting of symbolic elements. Specifically, four linear classifiers are extended to time series under dynamic time warping and applied to benchmark datasets. Results indicate that generalized gradient learning via elastic functions have the potential to complement the state-of-the-art in statistical pattern recognition on time series.
cs.LG:Motivated by the problem of computing investment portfolio weightings we investigate various methods of clustering as alternatives to traditional mean-variance approaches. Such methods can have significant benefits from a practical point of view since they remove the need to invert a sample covariance matrix, which can suffer from estimation error and will almost certainly be non-stationary. The general idea is to find groups of assets which share similar return characteristics over time and treat each group as a single composite asset. We then apply inverse volatility weightings to these new composite assets. In the course of our investigation we devise a method of clustering based on triangular potentials and we present associated theoretical results as well as various examples based on synthetic data.
cs.LG:Supervised classification approaches can predict labels for unknown data because of the supervised training process. The success of classification is heavily dependent on the labeled training data. Differently, clustering is effective in revealing the aggregation property of unlabeled data, but the performance of most clustering methods is limited by the absence of labeled data. In real applications, however, it is time-consuming and sometimes impossible to obtain labeled data. The combination of clustering and classification is a promising and active approach which can largely improve the performance. In this paper, we propose an innovative and effective clustering framework based on self-adaptive labeling (CSAL) which integrates clustering and classification on unlabeled data. Clustering is first employed to partition data and a certain proportion of clustered data are selected by our proposed labeling approach for training classifiers. In order to refine the trained classifiers, an iterative process of Expectation-Maximization algorithm is devised into the proposed clustering framework CSAL. Experiments are conducted on publicly data sets to test different combinations of clustering algorithms and classification models as well as various training data labeling methods. The experimental results show that our approach along with the self-adaptive method outperforms other methods.
cs.LG:In this paper we study the problem of learning from multiple modal data for purpose of document classification. In this problem, each document is composed two different modals of data, i.e., an image and a text. Cross-modal factor analysis (CFA) has been proposed to project the two different modals of data to a shared data space, so that the classification of a image or a text can be performed directly in this space. A disadvantage of CFA is that it has ignored the supervision information. In this paper, we improve CFA by incorporating the supervision information to represent and classify both image and text modals of documents. We project both image and text data to a shared data space by factor analysis, and then train a class label predictor in the shared space to use the class label information. The factor analysis parameter and the predictor parameter are learned jointly by solving one single objective function. With this objective function, we minimize the distance between the projections of image and text of the same document, and the classification error of the projection measured by hinge loss function. The objective function is optimized by an alternate optimization strategy in an iterative algorithm. Experiments in two different multiple modal document data sets show the advantage of the proposed algorithm over other CFA methods.
cs.LG:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.
cs.LG:We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm (Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. (2009) and Chernov and Vovk (2010). Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen (2014) on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.
cs.LG:Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. In this paper we show how a variant of SDCA can be applied for non-convex losses. We prove linear convergence rate even if individual loss functions are non-convex as long as the expected loss is convex.
cs.LG:In this work we study the quantitative relation between VC-dimension and two other basic parameters related to learning and teaching. Namely, the quality of sample compression schemes and of teaching sets for classes of low VC-dimension. Let $C$ be a binary concept class of size $m$ and VC-dimension $d$. Prior to this work, the best known upper bounds for both parameters were $\log(m)$, while the best lower bounds are linear in $d$. We present significantly better upper bounds on both as follows. Set $k = O(d 2^d \log \log |C|)$.   We show that there always exists a concept $c$ in $C$ with a teaching set (i.e. a list of $c$-labeled examples uniquely identifying $c$ in $C$) of size $k$. This problem was studied by Kuhlmann (1999). Our construction implies that the recursive teaching (RT) dimension of $C$ is at most $k$ as well. The RT-dimension was suggested by Zilles et al. and Doliwa et al. (2010). The same notion (under the name partial-ID width) was independently studied by Wigderson and Yehudayoff (2013). An upper bound on this parameter that depends only on $d$ is known just for the very simple case $d=1$, and is open even for $d=2$. We also make small progress towards this seemingly modest goal.   We further construct sample compression schemes of size $k$ for $C$, with additional information of $k \log(k)$ bits. Roughly speaking, given any list of $C$-labelled examples of arbitrary length, we can retain only $k$ labeled examples in a way that allows to recover the labels of all others examples in the list, using additional $k\log (k)$ information bits. This problem was first suggested by Littlestone and Warmuth (1986).
cs.LG:We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context. Roughly, the learner's goal is to find the best policy, or way of behaving, in some space of policies, although "best" is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space.
cs.LG:A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the amount of context (the arity of the factors in a graphical model) be chosen "at run-time" by reifying it---that is, letting this choice itself be a random variable inside the model. Empirically, we show that our approach obtains expressivity and coverage on three natural language tasks.
cs.LG:Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the approximations can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.
cs.LG:Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.
cs.LG:Given a set $X$ and a function $h:X\longrightarrow\{0,1\}$ which labels each element of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ to measure the similarity of pairs of points in $X$ according to $h$. Specifically, for $h\in \{0,1\}^X$ we define $h^{(s)}\in \{0,1\}^{X\times X}$ by $h^{(s)}(w,x):= \mathbb{1}[h(w) = h(x)]$. This idea can be extended to a set of functions, or hypothesis space $\mathcal{H} \subseteq \{0,1\}^X$ by defining a similarity hypothesis space $\mathcal{H}^{(s)}:=\{h^{(s)}:h\in\mathcal{H}\}$. We show that ${{vc-dimension}}(\mathcal{H}^{(s)}) \in \Theta({{vc-dimension}}(\mathcal{H}))$.
cs.LG:We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced $T$-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with $\widetilde\Theta(\alpha^{1/2} T^{1/2})$ minimax regret, where $\alpha$ is the independence number of the underlying graph; the second class induces problems with $\widetilde\Theta(\delta^{1/3}T^{2/3})$ minimax regret, where $\delta$ is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.
cs.LG:Efficiently learning mixture of Gaussians is a fundamental problem in statistics and learning theory. Given samples coming from a random one out of k Gaussian distributions in Rn, the learning problem asks to estimate the means and the covariance matrices of these Gaussians. This learning problem arises in many areas ranging from the natural sciences to the social sciences, and has also found many machine learning applications. Unfortunately, learning mixture of Gaussians is an information theoretically hard problem: in order to learn the parameters up to a reasonable accuracy, the number of samples required is exponential in the number of Gaussian components in the worst case. In this work, we show that provided we are in high enough dimensions, the class of Gaussian mixtures is learnable in its most general form under a smoothed analysis framework, where the parameters are randomly perturbed from an adversarial starting point. In particular, given samples from a mixture of Gaussians with randomly perturbed parameters, when n > {\Omega}(k^2), we give an algorithm that learns the parameters with polynomial running time and using polynomial number of samples. The central algorithmic ideas consist of new ways to decompose the moment tensor of the Gaussian mixture by exploiting its structural properties. The symmetries of this tensor are derived from the combinatorial structure of higher order moments of Gaussian distributions (sometimes referred to as Isserlis' theorem or Wick's theorem). We also develop new tools for bounding smallest singular values of structured random matrices, which could be useful in other smoothed analysis settings.
cs.LG:\emph{Semi-Automated Text Classification} (SATC) may be defined as the task of ranking a set $\mathcal{D}$ of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of $\mathcal{D}$ with the goal of increasing the overall labelling accuracy of $\mathcal{D}$, the expected increase is maximized. An obvious SATC strategy is to rank $\mathcal{D}$ so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of \emph{validation gain}, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error.
cs.LG:We provide a simple and efficient algorithm for the projection operator for weighted $\ell_1$-norm regularization subject to a sum constraint, together with an elementary proof. The implementation of the proposed algorithm can be downloaded from the author's homepage.
cs.LG:We provide a simple and efficient algorithm for computing the Euclidean projection of a point onto the capped simplex---a simplex with an additional uniform bound on each coordinate---together with an elementary proof. Both the MATLAB and C++ implementations of the proposed algorithm can be downloaded at https://eng.ucmerced.edu/people/wwang5.
cs.LG:This paper presents an unsupervised learning approach for simultaneous sample and feature selection, which is in contrast to existing works which mainly tackle these two problems separately. In fact the two tasks are often interleaved with each other: noisy and high-dimensional features will bring adverse effect on sample selection, while informative or representative samples will be beneficial to feature selection. Specifically, we propose a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the features are highly representative. In particular, our method runs in one-shot without the procedure of iterative sample selection for progressive labeling. Thus, our model is especially suitable when there are few labeled samples or even in the absence of supervision, which is a particular challenge for existing methods. As the joint learning problem is NP-hard, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experimental results on publicly available datasets corroborate the efficacy of our method compared with the state-of-the-art.
cs.LG:We consider classification problems in which the label space has structure. A common example is hierarchical label spaces, corresponding to the case where one label subsumes another (e.g., animal subsumes dog). But labels can also be mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy and exclusion) graph was introduced in [7]. This combined a conditional random field (CRF) with a deep neural network (DNN), resulting in state of the art results when applied to visual object classification problems where the training labels were drawn from different levels of the ImageNet hierarchy (e.g., an image might be labeled with the basic level category "dog", rather than the more specific label "husky"). In this paper, we extend the HEX model to allow for soft or probabilistic relations between labels, which is useful when there is uncertainty about the relationship between two labels (e.g., an antelope is "sort of" furry, but not to the same degree as a grizzly bear). We call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can be converted to an Ising model, which allows us to use existing off-the-shelf inference methods (in contrast to the HEX method, which needed specialized inference algorithms). Experimental results show significant improvements in a number of large-scale visual object classification tasks, outperforming the previous HEX model.
cs.LG:The detection of very similar patterns in a time series, commonly called motifs, has received continuous and increasing attention from diverse scientific communities. In particular, recent approaches for discovering similar motifs of different lengths have been proposed. In this work, we show that such variable-length similarity-based motifs cannot be directly compared, and hence ranked, by their normalized dissimilarities. Specifically, we find that length-normalized motif dissimilarities still have intrinsic dependencies on the motif length, and that lowest dissimilarities are particularly affected by this dependency. Moreover, we find that such dependencies are generally non-linear and change with the considered data set and dissimilarity measure. Based on these findings, we propose a solution to rank those motifs and measure their significance. This solution relies on a compact but accurate model of the dissimilarity space, using a beta distribution with three parameters that depend on the motif length in a non-linear way. We believe the incomparability of variable-length dissimilarities could go beyond the field of time series, and that similar modeling strategies as the one used here could be of help in a more broad context.
cs.LG:Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the " ill-condition" of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability.
cs.LG:We resolve an open question from (Christiano, 2014b) posed in COLT'14 regarding the optimal dependency of the regret achievable for online local learning on the size of the label set. In this framework the algorithm is shown a pair of items at each step, chosen from a set of $n$ items. The learner then predicts a label for each item, from a label set of size $L$ and receives a real valued payoff. This is a natural framework which captures many interesting scenarios such as collaborative filtering, online gambling, and online max cut among others. (Christiano, 2014a) designed an efficient online learning algorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$ is the number of rounds. Information theoretically, one can achieve a regret of $O(\sqrt{n \log L T})$. One of the main open questions left in this framework concerns closing the above gap.   In this work, we provide a complete answer to the question above via two main results. We show, via a tighter analysis, that the semi-definite programming based algorithm of (Christiano, 2014a), in fact achieves a regret of $O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely, we show that a polynomial time algorithm for online local learning with lower regret would imply a polynomial time algorithm for the planted clique problem which is widely believed to be hard. We prove a similar hardness result under a related conjecture concerning planted dense subgraphs that we put forth. Unlike planted clique, the planted dense subgraph problem does not have any known quasi-polynomial time algorithms.   Computational lower bounds for online learning are relatively rare, and we hope that the ideas developed in this work will lead to lower bounds for other online learning scenarios as well.
cs.LG:Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.
cs.LG:The apsis toolkit presented in this paper provides a flexible framework for hyperparameter optimization and includes both random search and a bayesian optimizer. It is implemented in Python and its architecture features adaptability to any desired machine learning code. It can easily be used with common Python ML frameworks such as scikit-learn. Published under the MIT License other researchers are heavily encouraged to check out the code, contribute or raise any suggestions. The code can be found at github.com/FrederikDiehl/apsis.
cs.LG:Time-series classification is an important problem for the data mining community due to the wide range of application domains involving time-series data. A recent paradigm, called shapelets, represents patterns that are highly predictive for the target variable. Shapelets are discovered by measuring the prediction accuracy of a set of potential (shapelet) candidates. The candidates typically consist of all the segments of a dataset, therefore, the discovery of shapelets is computationally expensive. This paper proposes a novel method that avoids measuring the prediction accuracy of similar candidates in Euclidean distance space, through an online clustering pruning technique. In addition, our algorithm incorporates a supervised shapelet selection that filters out only those candidates that improve classification accuracy. Empirical evidence on 45 datasets from the UCR collection demonstrate that our method is 3-4 orders of magnitudes faster than the fastest existing shapelet-discovery method, while providing better prediction accuracy.
cs.LG:Utilizing the sample size of a dataset, the random cluster model is employed in order to derive an estimate of the mean number of K-Means clusters to form during classification of a dataset.
cs.LG:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.
cs.LG:In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD($\lambda$)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD($\lambda$), and GQ($\lambda$). Compared to these methods, our _emphatic TD($\lambda$)_ is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.
cs.LG:Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empiricallthat ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster). Second, these likely diverse representatives are then used to produce an extension of RF termed CLUB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, and mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 15 real datasets from the UCI repository prove the superiority of our proposed extension over the traditional RF. Most of our experiments achieved at least 95% or above pruning level while retaining or outperforming the RF accuracy.
cs.LG:Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-the-art shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible.   A method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm.
cs.LG:Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.
cs.LG:We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by back-propagation. The theorems provided here generalize recent work on the probabilistic interpretation of denoising auto-encoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). We study how GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.
cs.LG:We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.
cs.LG:In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, is chosen, and the decision maker receives a reward resulting from her strategy and also receives a side bonus resulting from that strategy for each arm's neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of \emph{zero regret} polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results.
cs.LG:We analyse optimum reject strategies for prototype-based classifiers and real-valued rejection measures, using the distance of a data point to the closest prototype or probabilistic counterparts. We compare reject schemes with global thresholds, and local thresholds for the Voronoi cells of the classifier. For the latter, we develop a polynomial-time algorithm to compute optimum thresholds based on a dynamic programming scheme, and we propose an intuitive linear time, memory efficient approximation thereof with competitive accuracy. Evaluating the performance in various benchmarks, we conclude that local reject options are beneficial in particular for simple prototype-based classifiers, while the improvement is less pronounced for advanced models. For the latter, an accuracy-reject curve which is comparable to support vector machine classifiers with state of the art reject options can be reached.
cs.LG:Predicting the Credit Defaulter is a perilous task of Financial Industries like Banks. Ascertaining non-payer before giving loan is a significant and conflict-ridden task of the Banker. Classification techniques are the better choice for predictive analysis like finding the claimant, whether he/she is an unpretentious customer or a cheat. Defining the outstanding classifier is a risky assignment for any industrialist like a banker. This allow computer science researchers to drill down efficient research works through evaluating different classifiers and finding out the best classifier for such predictive problems. This research work investigates the productivity of LADTree Classifier and REPTree Classifier for the credit risk prediction and compares their fitness through various measures. German credit dataset has been taken and used to predict the credit risk with a help of open source machine learning tool.
cs.LG:With the rapid increase in volume of time series medical data available through wearable devices, there is a need to employ automated algorithms to label data. Examples of labels include interventions, changes in activity (e.g. sleep) and changes in physiology (e.g. arrhythmias). However, automated algorithms tend to be unreliable resulting in lower quality care. Expert annotations are scarce, expensive, and prone to significant inter- and intra-observer variance. To address these problems, a Bayesian Continuous-valued Label Aggregator(BCLA) is proposed to provide a reliable estimation of label aggregation while accurately infer the precision and bias of each algorithm. The BCLA was applied to QT interval (pro-arrhythmic indicator) estimation from the electrocardiogram using labels from the 2006 PhysioNet/Computing in Cardiology Challenge database. It was compared to the mean, median, and a previously proposed Expectation Maximization (EM) label aggregation approaches. While accurately predicting each labelling algorithm's bias and precision, the root-mean-square error of the BCLA was 11.78$\pm$0.63ms, significantly outperforming the best Challenge entry (15.37$\pm$2.13ms) as well as the EM, mean, and median voting strategies (14.76$\pm$0.52ms, 17.61$\pm$0.55ms, and 14.43$\pm$0.57ms respectively with $p<0.0001$).
cs.LG:We give a probabilistic interpretation of sampling theory of graph signals. To do this, we first define a generative model for the data using a pairwise Gaussian random field (GRF) which depends on the graph. We show that, under certain conditions, reconstructing a graph signal from a subset of its samples by least squares is equivalent to performing MAP inference on an approximation of this GRF which has a low rank covariance matrix. We then show that a sampling set of given size with the largest associated cut-off frequency, which is optimal from a sampling theoretic point of view, minimizes the worst case predictive covariance of the MAP estimate on the GRF. This interpretation also gives an intuitive explanation for the superior performance of the sampling theoretic approach to active semi-supervised classification.
cs.LG:In this paper, we propose the problem of online cost-sensitive clas- sifier adaptation and the first algorithm to solve it. We assume we have a base classifier for a cost-sensitive classification problem, but it is trained with respect to a cost setting different to the desired one. Moreover, we also have some training data samples streaming to the algorithm one by one. The prob- lem is to adapt the given base classifier to the desired cost setting using the steaming training samples online. To solve this problem, we propose to learn a new classifier by adding an adaptation function to the base classifier, and update the adaptation function parameter according to the streaming data samples. Given a input data sample and the cost of misclassifying it, we up- date the adaptation function parameter by minimizing cost weighted hinge loss and respecting previous learned parameter simultaneously. The proposed algorithm is compared to both online and off-line cost-sensitive algorithms on two cost-sensitive classification problems, and the experiments show that it not only outperforms them one classification performances, but also requires significantly less running time.
cs.LG:Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality?   In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing $k$ principal components with relative error $\epsilon$ over $s$ workers has communication cost $\tilde{O}(s \rho k/\epsilon+s k^2/\epsilon^3)$ words, where $\rho$ is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets and showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches.
cs.LG:In supervised learning, simple baseline classifiers can be constructed by only looking at the class, i.e., ignoring any other information from the dataset. The single-label learning community frequently uses as a reference the one which always predicts the majority class. Although a classifier might perform worse than this simple baseline classifier, this behaviour requires a special explanation. Aiming to motivate the community to compare experimental results with the ones provided by a multi-label baseline classifier, calling the attention about the need of special explanations related to classifiers which perform worse than the baseline, in this work we propose the use of General_B, a multi-label baseline classifier. General_B was evaluated in contrast to results published in the literature which were carefully selected using a systematic review process. It was found that a considerable number of published results on 10 frequently used datasets are worse than or equal to the ones obtained by General_B, and for one dataset it reaches up to 43% of the dataset published results. Moreover, although a simple baseline classifier was not considered in these publications, it was observed that even for very poor results no special explanations were provided in most of them. We hope that the findings of this work would encourage the multi-label community to consider the idea of using a simple baseline classifier, such that further explanations are provided when a classifiers performs worse than a baseline.
cs.LG:Sample compression schemes were defined by Littlestone and Warmuth (1986) as an abstraction of the structure underlying many learning algorithms. Roughly speaking, a sample compression scheme of size $k$ means that given an arbitrary list of labeled examples, one can retain only $k$ of them in a way that allows to recover the labels of all other examples in the list. They showed that compression implies PAC learnability for binary-labeled classes, and asked whether the other direction holds. We answer their question and show that every concept class $C$ with VC dimension $d$ has a sample compression scheme of size exponential in $d$. The proof uses an approximate minimax phenomenon for binary matrices of low VC dimension, which may be of interest in the context of game theory.
cs.LG:Big Data concern large-volume, growing data sets that are complex and have multiple autonomous sources. Earlier technologies were not able to handle storage and processing of huge data thus Big Data concept comes into existence. This is a tedious job for users unstructured data. So, there should be some mechanism which classify unstructured data into organized form which helps user to easily access required data. Classification techniques over big transactional database provide required data to the users from large datasets more simple way. There are two main classification techniques, supervised and unsupervised. In this paper we focused on to study of different supervised classification techniques. Further this paper shows a advantages and limitations.
cs.LG:Automated learning of patients demographics can be seen as multi-label problem where a patient model is based on different race and gender groups. The resulting model can be further integrated into Privacy-Preserving Data Mining, where it can be used to assess risk of identification of different patient groups. Our project considers relations between diabetes and demographics of patients as a multi-labelled problem. Most research in this area has been done as binary classification, where the target class is finding if a person has diabetes or not. But very few, and maybe no work has been done in multi-labeled analysis of the demographics of patients who are likely to be diagnosed with diabetes. To identify such groups, we applied ensembles of several multi-label learning algorithms.
cs.LG:Quasi-Newton methods are widely used in practise for convex loss minimization problems. These methods exhibit good empirical performance on a wide variety of tasks and enjoy super-linear convergence to the optimal solution. For large-scale learning problems, stochastic Quasi-Newton methods have been recently proposed. However, these typically only achieve sub-linear convergence rates and have not been shown to consistently perform well in practice since noisy Hessian approximations can exacerbate the effect of high-variance stochastic gradient estimates. In this work we propose Vite, a novel stochastic Quasi-Newton algorithm that uses an existing first-order technique to reduce this variance. Without exploiting the specific form of the approximate Hessian, we show that Vite reaches the optimum at a geometric rate with a constant step-size when dealing with smooth strongly convex functions. Empirically, we demonstrate improvements over existing stochastic Quasi-Newton and variance reduced stochastic gradient methods.
cs.LG:Multi-armed bandits (MAB) model sequential decision making problems, in which a learner sequentially chooses arms with unknown reward distributions in order to maximize its cumulative reward. Most of the prior work on MAB assumes that the reward distributions of each arm are independent. But in a wide variety of decision problems -- from drug dosage to dynamic pricing -- the expected rewards of different arms are correlated, so that selecting one arm provides information about the expected rewards of other arms as well. We propose and analyze a class of models of such decision problems, which we call {\em global bandits}. In the case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a greedy policy that achieves {\em bounded regret}, with a bound that depends on the single true parameter of the problem. Hence, this policy selects suboptimal arms only finitely many times with probability one. For this case we also obtain a bound on regret that is {\em independent of the true parameter}; this bound is sub-linear, with an exponent that depends on the informativeness of the arms. We also propose a variant of the greedy policy that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ worst-case and $\mathcal{O}(1)$ parameter dependent regret. Finally, we perform experiments on dynamic pricing and show that the proposed algorithms achieve significant gains with respect to the well-known benchmarks.
cs.LG:Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The standard sketch based method, which we call the prototype model, produces relatively accurate approximations, but is inefficient on large square matrices. The Nystr\"om method is highly efficient, but can only achieve low accuracy. In this paper we propose a novel model that we call the {\it fast SPSD matrix approximation model}. The fast model is nearly as efficient as the Nystr\"om method and as accurate as the prototype model. We show that the fast model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size $n$ to achieve $1+\epsilon$ relative-error, whereas both the prototype model and the Nystr\"om method cost at least quadratic time to attain comparable error bound. Empirical comparisons among the prototype model, the Nystr\"om method, and our fast model demonstrate the superiority of the fast model. We also contribute new understandings of the Nystr\"om method. The Nystr\"om method is a special instance of our fast model and is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy.
cs.LG:Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.
cs.LG:Categorization axioms have been proposed to axiomatizing clustering results, which offers a hint of bridging the difference between human recognition system and machine learning through an intuitive observation: an object should be assigned to its most similar category. However, categorization axioms cannot be generalized into a general machine learning system as categorization axioms become trivial when the number of categories becomes one. In order to generalize categorization axioms into general cases, categorization input and categorization output are reinterpreted by inner and outer category representation. According to the categorization reinterpretation, two category representation axioms are presented. Category representation axioms and categorization axioms can be combined into a generalized categorization axiomatic framework, which accurately delimit the theoretical categorization constraints and overcome the shortcoming of categorization axioms. The proposed axiomatic framework not only discuses categorization test issue but also reinterprets many results in machine learning in a unified way, such as dimensionality reduction,density estimation, regression, clustering and classification.
cs.LG:The problem of feature selection has raised considerable interests in the past decade. Traditional unsupervised methods select the features which can faithfully preserve the intrinsic structures of data, where the intrinsic structures are estimated using all the input features of data. However, the estimated intrinsic structures are unreliable/inaccurate when the redundant and noisy features are not removed. Therefore, we face a dilemma here: one need the true structures of data to identify the informative features, and one need the informative features to accurately estimate the true structures of data. To address this, we propose a unified learning framework which performs structure learning and feature selection simultaneously. The structures are adaptively learned from the results of feature selection, and the informative features are reselected to preserve the refined structures of data. By leveraging the interactions between these two essential tasks, we are able to capture accurate structures and select more informative features. Experimental results on many benchmark data sets demonstrate that the proposed method outperforms many state of the art unsupervised feature selection methods.
cs.LG:Understanding the dynamic mechanisms that drive the high-impact scientific work (e.g., research papers, patents) is a long-debated research topic and has many important implications, ranging from personal career development and recruitment search, to the jurisdiction of research resources. Recent advances in characterizing and modeling scientific success have made it possible to forecast the long-term impact of scientific work, where data mining techniques, supervised learning in particular, play an essential role. Despite much progress, several key algorithmic challenges in relation to predicting long-term scientific impact have largely remained open. In this paper, we propose a joint predictive model to forecast the long-term scientific impact at the early stage, which simultaneously addresses a number of these open challenges, including the scholarly feature design, the non-linearity, the domain-heterogeneity and dynamics. In particular, we formulate it as a regularized optimization problem and propose effective and scalable algorithms to solve it. We perform extensive empirical evaluations on large, real scholarly data sets to validate the effectiveness and the efficiency of our method.
cs.LG:We propose a method for estimating channel parameters from RSSI measurements and the lost packet count, which can work in the presence of losses due to both interference and signal attenuation below the noise floor. This is especially important in the wireless networks, such as vehicular, where propagation model changes with the density of nodes. The method is based on Stochastic Expectation Maximization, where the received data is modeled as a mixture of distributions (no/low interference and strong interference), incomplete (censored) due to packet losses. The PDFs in the mixture are Gamma, according to the commonly accepted model for wireless signal and interference power. This approach leverages the loss count as additional information, hence outperforming maximum likelihood estimation, which does not use this information (ML-), for a small number of received RSSI samples. Hence, it allows inexpensive on-line channel estimation from ad-hoc collected data. The method also outperforms ML- on uncensored data mixtures, as ML- assumes that samples are from a single-mode PDF.
cs.LG:Stochastic Dual Coordinate Descent (SDCD) has become one of the most efficient ways to solve the family of $\ell_2$-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of SDCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first {\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.
cs.LG:The paper outlines a framework for autonomous control of a CRM (customer relationship management) system. First, it explores how a modified version of the widely accepted Recency-Frequency-Monetary Value system of metrics can be used to define the state space of clients or donors. Second, it describes a procedure to determine the optimal direct marketing action in discrete and continuous action space for the given individual, based on his position in the state space. The procedure involves the use of model-free Q-learning to train a deep neural network that relates a client's position in the state space to rewards associated with possible marketing actions. The estimated value function over the client state space can be interpreted as customer lifetime value, and thus allows for a quick plug-in estimation of CLV for a given client. Experimental results are presented, based on KDD Cup 1998 mailing dataset of donation solicitations.
cs.LG:The recruitment of new personnel is one of the most essential business processes which affect the quality of human capital within any company. It is highly essential for the companies to ensure the recruitment of right talent to maintain a competitive edge over the others in the market. However IT companies often face a problem while recruiting new people for their ongoing projects due to lack of a proper framework that defines a criteria for the selection process. In this paper we aim to develop a framework that would allow any project manager to take the right decision for selecting new talent by correlating performance parameters with the other domain-specific attributes of the candidates. Also, another important motivation behind this project is to check the validity of the selection procedure often followed by various big companies in both public and private sectors which focus only on academic scores, GPA/grades of students from colleges and other academic backgrounds. We test if such a decision will produce optimal results in the industry or is there a need for change that offers a more holistic approach to recruitment of new talent in the software companies. The scope of this work extends beyond the IT domain and a similar procedure can be adopted to develop a recruitment framework in other fields as well. Data-mining techniques provide useful information from the historical projects depending on which the hiring-manager can make decisions for recruiting high-quality workforce. This study aims to bridge this hiatus by developing a data-mining framework based on an ensemble-learning technique to refocus on the criteria for personnel selection. The results from this research clearly demonstrated that there is a need to refocus on the selection-criteria for quality objectives.
cs.LG:Representation learning is currently a very hot topic in modern machine learning, mostly due to the great success of the deep learning methods. In particular low-dimensional representation which discriminates classes can not only enhance the classification procedure, but also make it faster, while contrary to the high-dimensional embeddings can be efficiently used for visual based exploratory data analysis.   In this paper we propose Maximum Entropy Linear Manifold (MELM), a multidimensional generalization of Multithreshold Entropy Linear Classifier model which is able to find a low-dimensional linear data projection maximizing discriminativeness of projected classes. As a result we obtain a linear embedding which can be used for classification, class aware dimensionality reduction and data visualization. MELM provides highly discriminative 2D projections of the data which can be used as a method for constructing robust classifiers.   We provide both empirical evaluation as well as some interesting theoretical properties of our objective function such us scale and affine transformation invariance, connections with PCA and bounding of the expected balanced accuracy error.
cs.LG:Co-occurrence Data is a common and important information source in many areas, such as the word co-occurrence in the sentences, friends co-occurrence in social networks and products co-occurrence in commercial transaction data, etc, which contains rich correlation and clustering information about the items. In this paper, we study co-occurrence data using a general energy-based probabilistic model, and we analyze three different categories of energy-based model, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture different levels of dependency in the co-occurrence data. We also discuss how several typical existing models are related to these three types of energy models, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix Factorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted Boltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model (DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner. Furthermore, motivated by the observation that the partition function in the energy model is intractable and the fact that the major objective of modeling the co-occurrence data is to predict using the conditional probability, we apply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence, the developed model and its learning method naturally avoid the above difficulties and can be easily used to compute the conditional probability in prediction. Interestingly, our method is equivalent to learning a special structured deep neural network using back-propagation and a special sampling strategy, which makes it scalable on large-scale datasets. Finally, in the experiments, we show that the DEM can achieve comparable or better results than state-of-the-art methods on datasets across several application domains.
cs.LG:In this age of Big Data, machine learning based data mining methods are extensively used to inspect large scale data sets. Deriving applicable predictive modeling from these type of data sets is a challenging obstacle because of their high complexity. Opportunity with high data availability levels, automated classification of data sets has become a critical and complicated function. In this paper, the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build reliable predictive bag of classification models. Thus, (i) dataset ensembles are build; (ii) ELM algorithm is used to build weak classification models; and (iii) build a strong classification model from a set of weak classification models. This training model is applied to the publicly available knowledge discovery and data mining datasets.
cs.LG:Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem.We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.
cs.LG:Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.   We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate $\tilde{O}(1/t)$ to the global optimum, even for the top $k$ eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.
cs.LG:In this paper, we propose a maximum margin classifier that deals with uncertainty in data input. More specifically, we reformulate the SVM framework such that each training example can be modeled by a multi-dimensional Gaussian distribution described by its mean vector and its covariance matrix -- the latter modeling the uncertainty. We address the classification problem and define a cost function that is the expected value of the classical SVM cost when data samples are drawn from the multi-dimensional Gaussian distributions that form the set of the training examples. Our formulation approximates the classical SVM formulation when the training examples are isotropic Gaussians with variance tending to zero. We arrive at a convex optimization problem, which we solve efficiently in the primal form using a stochastic gradient descent approach. The resulting classifier, which we name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on synthetic data and five publicly available and popular datasets; namely, the MNIST, WDBC, DEAP, TV News Channel Commercial Detection, and TRECVID MED datasets. Experimental results verify the effectiveness of the proposed method.
cs.LG:This paper presents the Nataf-Beta Random Field Classifier, a discriminative approach that extends the applicability of the Beta conjugate prior to classification problems. The approach's key feature is to model the probability of a class conditional on attribute values as a random field whose marginals are Beta distributed, and where the parameters of marginals are themselves described by random fields. Although the classification accuracy of the approach proposed does not statistically outperform the best accuracies reported in the literature, it ranks among the top tier for the six benchmark datasets tested. The Nataf-Beta Random Field Classifier is suited as a general purpose classification approach for real-continuous and real-integer attribute value problems.
cs.LG:The nature of clinical data makes it difficult to quickly select, tune and apply machine learning algorithms to clinical prognosis. As a result, a lot of time is spent searching for the most appropriate machine learning algorithms applicable in clinical prognosis that contains either binary-valued or multi-valued attributes. The study set out to identify and evaluate the performance of machine learning classification schemes applied in clinical prognosis of post-operative life expectancy in the lung cancer patients. Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train and test models on Thoracic Surgery datasets obtained from the University of California Irvine machine learning repository. Stratified 10-fold cross-validation was used to evaluate baseline performance accuracy of the classifiers. The comparative analysis shows that multilayer perceptron performed best with classification accuracy of 82.3%, J48 came out second with classification accuracy of 81.8%, and Naive Bayes came out the worst with classification accuracy of 74.4%. The quality and outcome of the chosen machine learning algorithms depends on the ingenuity of the clinical miner.
cs.LG:We consider the following basic learning task: given independent draws from an unknown distribution over a discrete support, output an approximation of the distribution that is as accurate as possible in $\ell_1$ distance (i.e. total variation or statistical distance). Perhaps surprisingly, it is often possible to "de-noise" the empirical distribution of the samples to return an approximation of the true distribution that is significantly more accurate than the empirical distribution, without relying on any prior assumptions on the distribution. We present an instance optimal learning algorithm which optimally performs this de-noising for every distribution for which such a de-noising is possible. More formally, given $n$ independent draws from a distribution $p$, our algorithm returns a labelled vector whose expected distance from $p$ is equal to the minimum possible expected error that could be obtained by any algorithm that knows the true unlabeled vector of probabilities of distribution $p$ and simply needs to assign labels, up to an additive subconstant term that is independent of $p$ and goes to zero as $n$ gets large. One conceptual implication of this result is that for large samples, Bayesian assumptions on the "shape" or bounds on the tail probabilities of a distribution over discrete support are not helpful for the task of learning the distribution.   As a consequence of our techniques, we also show that given a set of $n$ samples from an arbitrary distribution, one can accurately estimate the expected number of distinct elements that will be observed in a sample of any size up to $n \log n$. This sort of extrapolation is practically relevant, particularly to domains such as genomics where it is important to understand how much more might be discovered given larger sample sizes, and we are optimistic that our approach is practically viable.
cs.LG:Feature selection and feature transformation, the two main ways to reduce dimensionality, are often presented separately. In this paper, a feature selection method is proposed by combining the popular transformation based dimensionality reduction method Linear Discriminant Analysis (LDA) and sparsity regularization. We impose row sparsity on the transformation matrix of LDA through ${\ell}_{2,1}$-norm regularization to achieve feature selection, and the resultant formulation optimizes for selecting the most discriminative features and removing the redundant ones simultaneously. The formulation is extended to the ${\ell}_{2,p}$-norm regularized case: which is more likely to offer better sparsity when $0<p<1$. Thus the formulation is a better approximation to the feature selection problem. An efficient algorithm is developed to solve the ${\ell}_{2,p}$-norm based optimization problem and it is proved that the algorithm converges when $0<p\le 2$. Systematical experiments are conducted to understand the work of the proposed method. Promising experimental results on various types of real-world data sets demonstrate the effectiveness of our algorithm.
cs.LG:We introduce a generalization of temporal-difference (TD) learning to networks of interrelated predictions. Rather than relating a single prediction to itself at a later time, as in conventional TD methods, a TD network relates each prediction in a set of predictions to other predictions in the set at a later time. TD networks can represent and apply TD learning to a much wider class of predictions than has previously been possible. Using a random-walk example, we show that these networks can be used to learn to predict by a fixed interval, which is not possible with conventional TD methods. Secondly, we show that if the inter-predictive relationships are made conditional on action, then the usual learning-efficiency advantage of TD methods over Monte Carlo (supervised learning) methods becomes particularly pronounced. Thirdly, we demonstrate that TD networks can learn predictive state representations that enable exact solution of a non-Markov problem. A very broad range of inter-predictive temporal relationships can be expressed in these networks. Overall we argue that TD networks represent a substantial extension of the abilities of TD methods and bring us closer to the goal of representing world knowledge in entirely predictive, grounded terms.
cs.LG:We present a Discriminative Switching Linear Dynamical System (DSLDS) applied to patient monitoring in Intensive Care Units (ICUs). Our approach is based on identifying the state-of-health of a patient given their observed vital signs using a discriminative classifier, and then inferring their underlying physiological values conditioned on this status. The work builds on the Factorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which has been previously used in a similar setting. The FSLDS is a generative model, whereas the DSLDS is a discriminative model. We demonstrate on two real-world datasets that the DSLDS is able to outperform the FSLDS in most cases of interest, and that an $\alpha$-mixture of the two models achieves higher performance than either of the two models separately.
cs.LG:Making use of predictions is a crucial, but under-explored, area of online algorithms. This paper studies a class of online optimization problems where we have external noisy predictions available. We propose a stochastic prediction error model that generalizes prior models in the learning and stochastic control communities, incorporates correlation among prediction errors, and captures the fact that predictions improve as time passes. We prove that achieving sublinear regret and constant competitive ratio for online algorithms requires the use of an unbounded prediction window in adversarial settings, but that under more realistic stochastic prediction error models it is possible to use Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear regret and constant competitive ratio in expectation using only a constant-sized prediction window. Furthermore, we show that the performance of AFHC is tightly concentrated around its mean.
cs.LG:To address the contextual bandit problem, we propose an online random forest algorithm. The analysis of the proposed algorithm is based on the sample complexity needed to find the optimal decision stump. Then, the decision stumps are assembled in a random collection of decision trees, Bandit Forest. We show that the proposed algorithm is optimal up to logarithmic factors. The dependence of the sample complexity upon the number of contextual variables is logarithmic. The computational cost of the proposed algorithm with respect to the time horizon is linear. These analytical results allow the proposed algorithm to be efficient in real applications, where the number of events to process is huge, and where we expect that some contextual variables, chosen from a large set, have potentially non- linear dependencies with the rewards. In the experiments done to illustrate the theoretical analysis, Bandit Forest obtain promising results in comparison with state-of-the-art algorithms.
cs.LG:In this paper, using a novel matrix factorization and simultaneous reduction to diagonal form approach (or in short simultaneous reduction approach), Accelerated Kernel Discriminant Analysis (AKDA) and Accelerated Kernel Subclass Discriminant Analysis (AKSDA) are proposed. Specifically, instead of performing the simultaneous reduction of the between- and within-class or subclass scatter matrices, the nonzero eigenpairs (NZEP) of the so-called core matrix, which is of relatively small dimensionality, and the Cholesky factorization of the kernel matrix are computed, achieving more than one order of magnitude speed up over kernel discriminant analysis (KDA). Moreover, consisting of a few elementary matrix operations and very stable numerical algorithms, AKDA and AKSDA offer improved classification accuracy. The experimental evaluation on various datasets confirms that the proposed approaches provide state-of-the-art performance in terms of both training time and classification accuracy.
cs.LG:We consider optimization of generalized performance metrics for binary classification by means of surrogate losses. We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_{\beta}$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\widehat{\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance metric. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\widehat{\theta}$) measured with respect to the target metric is upperbounded by the regret of $f$ measured with respect to the surrogate loss. We also extend our results to cover multilabel classification and provide regret bounds for micro- and macro-averaging measures. Our findings are further analyzed in a computational study on both synthetic and real data sets.
cs.LG:We present a machine learning algorithm for building classifiers that are comprised of a small number of disjunctions of conjunctions (or's of and's). An example of a classifier of this form is as follows: If X satisfies (x1 = 'blue' AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal and a conjunction of literals is called a pattern. Models of this form have the advantage of being interpretable to human experts, since they produce a set of conditions that concisely describe a specific class. We present two probabilistic models for forming a pattern set, one with a Beta-Binomial prior, and the other with Poisson priors. In both cases, there are prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide two scalable MAP inference approaches: a pattern level search, which involves association rule mining, and a literal level search. We show stronger priors reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict user behavior with respect to in-vehicle context-aware personalized recommender systems.
cs.LG:We analyze the problem of using Explore-Exploit techniques to improve precision in multi-result ranking systems such as web search, query autocompletion and news recommendation. Adopting an exploration policy directly online, without understanding its impact on the production system, may have unwanted consequences - the system may sustain large losses, create user dissatisfaction, or collect exploration data which does not help improve ranking quality. An offline framework is thus necessary to let us decide what policy and how we should apply in a production environment to ensure positive outcome. Here, we describe such an offline framework.   Using the framework, we study a popular exploration policy - Thompson sampling. We show that there are different ways of implementing it in multi-result ranking systems, each having different semantic interpretation and leading to different results in terms of sustained click-through-rate (CTR) loss and expected model improvement. In particular, we demonstrate that Thompson sampling can act as an online learner optimizing CTR, which in some cases can lead to an interesting outcome: lift in CTR during exploration. The observation is important for production systems as it suggests that one can get both valuable exploration data to improve ranking performance on the long run, and at the same time increase CTR while exploration lasts.
cs.LG:Music information retrieval faces a challenge in modeling contextualized musical concepts formulated by a set of co-occurring tags. In this paper, we investigate the suitability of our recently proposed approach based on a Siamese neural network in fighting off this challenge. By means of tag features and probabilistic topic models, the network captures contextualized semantics from tags via unsupervised learning. This leads to a distributed semantics space and a potential solution to the out of vocabulary problem which has yet to be sufficiently addressed. We explore the nature of the resultant music-based semantics and address computational needs. We conduct experiments on three public music tag collections -namely, CAL500, MagTag5K and Million Song Dataset- and compare our approach to a number of state-of-the-art semantics learning approaches. Comparative results suggest that this approach outperforms previous approaches in terms of semantic priming and music tag completion.
cs.LG:We observe that the standard log likelihood training objective for a Recurrent Neural Network (RNN) model of time series data is equivalent to a variational Bayesian training objective, given the proper choice of generative and inference models. This perspective may motivate extensions to both RNNs and variational Bayesian models. We propose one such extension, where multiple particles are used for the hidden state of an RNN, allowing a natural representation of uncertainty or multimodality.
cs.LG:Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$ denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.
cs.LG:Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks.
cs.LG:Is he/she my type or not? The answer to this question depends on the personal preferences of the one asking it. The individual process of obtaining a full answer may generally be difficult and time consuming, but often an approximate answer can be obtained simply by looking at a photo of the potential match. Such approximate answers based on visual cues can be produced in a fraction of a second, a phenomenon that has led to a series of recently successful dating apps in which users rate others positively or negatively using primarily a single photo. In this paper we explore using convolutional networks to create a model of an individual's personal preferences based on rated photos. This introduced task is difficult due to the large number of variations in profile pictures and the noise in attractiveness labels. Toward this task we collect a dataset comprised of $9364$ pictures and binary labels for each. We compare performance of convolutional models trained in three ways: first directly on the collected dataset, second with features transferred from a network trained to predict gender, and third with features transferred from a network trained on ImageNet. Our findings show that ImageNet features transfer best, producing a model that attains $68.1\%$ accuracy on the test set and is moderately successful at predicting matches.
cs.LG:The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them.   The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete.   We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.
cs.LG:In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.
cs.LG:Collectively, lung cancer, breast cancer and melanoma was diagnosed in over 535,340 people out of which, 209,400 deaths were reported [13]. It is estimated that over 600,000 people will be diagnosed with these forms of cancer in 2015. Most of the deaths from lung cancer, breast cancer and melanoma result due to late detection. All of these cancers, if detected early, are 100% curable. In this study, we develop and evaluate algorithms to diagnose Breast cancer, Melanoma, and Lung cancer. In the first part of the study, we employed a normalised Gradient Descent and an Artificial Neural Network to diagnose breast cancer with an overall accuracy of 91% and 95% respectively. In the second part of the study, an artificial neural network coupled with image processing and analysis algorithms was employed to achieve an overall accuracy of 93% A naive mobile based application that allowed people to take diagnostic tests on their phones was developed. Finally, a Support Vector Machine algorithm incorporating image processing and image analysis algorithms was developed to diagnose lung cancer with an accuracy of 94%. All of the aforementioned systems had very low false positive and false negative rates. We are developing an online network that incorporates all of these systems and allows people to collaborate globally.
cs.LG:In many naturally occurring optimization problems one needs to ensure that the definition of the optimization problem lends itself to solutions that are tractable to compute. In cases where exact solutions cannot be computed tractably, it is beneficial to have strong guarantees on the tractable approximate solutions. In order operate under these criterion most optimization problems are cast under the umbrella of convexity or submodularity. In this report we will study design and optimization over a common class of functions called submodular functions. Set functions, and specifically submodular set functions, characterize a wide variety of naturally occurring optimization problems, and the property of submodularity of set functions has deep theoretical consequences with wide ranging applications. Informally, the property of submodularity of set functions concerns the intuitive "principle of diminishing returns. This property states that adding an element to a smaller set has more value than adding it to a larger set. Common examples of submodular monotone functions are entropies, concave functions of cardinality, and matroid rank functions; non-monotone examples include graph cuts, network flows, and mutual information.   In this paper we will review the formal definition of submodularity; the optimization of submodular functions, both maximization and minimization; and finally discuss some applications in relation to learning and reasoning using submodular functions.
cs.LG:Many real world data mining applications involve obtaining predictive models using data sets with strongly imbalanced distributions of the target variable. Frequently, the least common values of this target variable are associated with events that are highly relevant for end users (e.g. fraud detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the events may have different costs and benefits, which when associated with the rarity of some of them on the available training data creates serious problems to predictive modelling techniques. This paper presents a survey of existing techniques for handling these important applications of predictive analytics. Although most of the existing work addresses classification tasks (nominal target variables), we also describe methods designed to handle similar problems within regression tasks (numeric target variables). In this survey we discuss the main challenges raised by imbalanced distributions, describe the main approaches to these problems, propose a taxonomy of these methods and refer to some related problems within predictive modelling.
cs.LG:Metric learning aims to embed one metric space into another to benefit tasks like classification and clustering. Although a greatly distorted metric space has a high degree of freedom to fit training data, it is prone to overfitting and numerical inaccuracy. This paper presents {\it bounded-distortion metric learning} (BDML), a new metric learning framework which amounts to finding an optimal Mahalanobis metric space with a bounded-distortion constraint. An efficient solver based on the multiplicative weights update method is proposed. Moreover, we generalize BDML to pseudo-metric learning and devise the semidefinite relaxation and a randomized algorithm to approximately solve it. We further provide theoretical analysis to show that distortion is a key ingredient for stability and generalization ability of our BDML algorithm. Extensive experiments on several benchmark datasets yield promising results.
cs.LG:Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule---that is based on the dual projection onto convex sets (DPC)---to quickly identify the inactive features---that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features---especially for high dimensional data---which leads to a speedup up to several orders of magnitude.
cs.LG:Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting.   The aim of this paper is to develop a concrete analysis concerning how to determine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways to select the shrinkage degree. The first one is to parameterize the shrinkage degree and the other one is to develope a data-driven approach of it. After rigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting learning, we compare the pros and cons of the proposed methods. We find that although these approaches can reach the same learning rates, the structure of the final estimate of the parameterized approach is better, which sometimes yields a better generalization capability when the number of sample is finite. With this, we recommend to parameterize the shrinkage degree of $L_2$-RBoosting. To this end, we present an adaptive parameter-selection strategy for shrinkage degree and verify its feasibility through both theoretical analysis and numerical verification.   The obtained results enhance the understanding of RBoosting and further give guidance on how to use $L_2$-RBoosting for regression tasks.
cs.LG:Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings.
cs.LG:We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates.
cs.LG:We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.
cs.LG:Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial set- ting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.
cs.LG:Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training.
cs.LG:Real-world machine learning applications may require functions that are fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function can be critical to user trust. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic look-up tables by solving a convex problem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms. Case studies with real-world problems with five to sixteen features and thousands to millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users.
cs.LG:This paper addresses an important issue, known as sensor drift that behaves a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly due to the frequent acquisition and labeling process for gases samples recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semi-supervised and unsupervised learning problems in single domain (i.e. source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework, referred to as Domain Adaptation Extreme Learning Machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gases recognition in E-nose systems, without loss of the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called DAELM-S and DAELM-T are proposed for the purpose of this paper, respectively. In order to percept the differences among ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected by E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift compensation methods without cumbersome measures, and also bring new perspectives for ELM.
cs.LG:This paper presents an algorithm for efficient training of sparse linear models with elastic net regularization. Extending previous work on delayed updates, the new algorithm applies stochastic gradient updates to non-zero features only, bringing weights current as needed with closed-form updates. Closed-form delayed updates for the $\ell_1$, $\ell_{\infty}$, and rarely used $\ell_2$ regularizers have been described previously. This paper provides closed-form updates for the popular squared norm $\ell^2_2$ and elastic net regularizers.   We provide dynamic programming algorithms that perform each delayed update in constant time. The new $\ell^2_2$ and elastic net methods handle both fixed and varying learning rates, and both standard {stochastic gradient descent} (SGD) and {forward backward splitting (FoBoS)}. Experimental results show that on a bag-of-words dataset with $260,941$ features, but only $88$ nonzero features on average per training example, the dynamic programming method trains a logistic regression classifier with elastic net regularization over $2000$ times faster than otherwise.
cs.LG:Online learning has been in the spotlight from the machine learning society for a long time. To handle massive data in Big Data era, one single learner could never efficiently finish this heavy task. Hence, in this paper, we propose a novel distributed online learning algorithm to solve the problem. Comparing to typical centralized online learner, the distributed learners optimize their own learning parameters based on local data sources and timely communicate with neighbors. However, communication may lead to a privacy breach. Thus, we use differential privacy to preserve the privacy of learners, and study the influence of guaranteeing differential privacy on the utility of the distributed online learning algorithm. Furthermore, by using the results from Kakade and Tewari (2009), we use the regret bounds of online learning to achieve fast convergence rates for offline learning algorithms in distributed scenarios, which provides tighter utility performance than the existing state-of-the-art results. In simulation, we demonstrate that the differentially private offline learning algorithm has high variance, but we can use mini-batch to improve the performance. Finally, the simulations show that the analytical results of our proposed theorems are right and our private distributed online learning algorithm is a general framework.
cs.LG:The ubiquity of professional sports and specifically the NFL have lead to an increase in popularity for Fantasy Football. Users have many tools at their disposal: statistics, predictions, rankings of experts and even recommendations of peers. There are issues with all of these, though. Especially since many people pay money to play, the prediction tools should be enhanced as they provide unbiased and easy-to-use assistance for users. This paper provides and discusses approaches to predict Fantasy Football scores of Quarterbacks with relatively limited data. In addition to that, it includes several suggestions on how the data could be enhanced to achieve better results. The dataset consists only of game data from the last six NFL seasons. I used two different methods to predict the Fantasy Football scores of NFL players: Support Vector Regression (SVR) and Neural Networks. The results of both are promising given the limited data that was used.
cs.LG:Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.
cs.LG:A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form $O(K \log T)$ but require restrictive assumptions, or offer bounds of the form $O(K^2 \log T)$ without requiring such assumptions. Our results offer the best of both worlds: $O(K \log T)$ bounds without restrictive assumptions.
cs.LG:The outputs of a trained neural network contain much richer information than just an one-hot classifier. For example, a neural network might give an image of a dog the probability of one in a million of being a cat but it is still much larger than the probability of being a car. To reveal the hidden structure in them, we apply two unsupervised learning algorithms, PCA and ICA, to the outputs of a deep Convolutional Neural Network trained on the ImageNet of 1000 classes. The PCA/ICA embedding of the object classes reveals their visual similarity and the PCA/ICA components can be interpreted as common visual features shared by similar object classes. For an application, we proposed a new zero-shot learning method, in which the visual features learned by PCA/ICA are employed. Our zero-shot learning method achieves the state-of-the-art results on the ImageNet of over 20000 classes.
cs.LG:As we aim at alleviating the curse of high-dimensionality, subspace learning is becoming more popular. Existing approaches use either information about global or local structure of the data, and few studies simultaneously focus on global and local structures as the both of them contain important information. In this paper, we propose a global and local structure preserving sparse subspace learning (GLoSS) model for unsupervised feature selection. The model can simultaneously realize feature selection and subspace learning. In addition, we develop a greedy algorithm to establish a generic combinatorial model, and an iterative strategy based on an accelerated block coordinate descent is used to solve the GLoSS problem. We also provide whole iterate sequence convergence analysis of the proposed iterative algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches.
cs.LG:Biclustering involves the simultaneous clustering of objects and their attributes, thus defining local two-way clustering models. Recently, efficient algorithms were conceived to enumerate all biclusters in real-valued datasets. In this case, the solution composes a complete set of maximal and non-redundant biclusters. However, the ability to enumerate biclusters revealed a challenging scenario: in noisy datasets, each true bicluster may become highly fragmented and with a high degree of overlapping. It prevents a direct analysis of the obtained results. To revert the fragmentation, we propose here two approaches for properly aggregating the whole set of enumerated biclusters: one based on single linkage and the other directly exploring the rate of overlapping. Both proposals were compared with each other and with the actual state-of-the-art in several experiments, and they not only significantly reduced the number of biclusters but also consistently increased the quality of the solution.
cs.LG:In this paper we propose the Structured Deep Neural Network (Structured DNN) as a structured and deep learning algorithm, learning to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structure rather than item by item.   When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learned utterance by utterance as a whole, rather than frame by frame.   Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning algorithm. It was shown to beat structured SVM in preliminary experiments on TIMIT.
cs.LG:Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades. In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features. Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time. To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection. Meanwhile, a sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix. In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros. To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteractively chase the optimal solution. Performance evaluation is extensively conducted over six benchmark data sets. From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches.
cs.LG:In machine learning contests such as the ImageNet Large Scale Visual Recognition Challenge and the KDD Cup, contestants can submit candidate solutions and receive from an oracle (typically the organizers of the competition) the accuracy of their guesses compared to the ground-truth labels. One of the most commonly used accuracy metrics for binary classification tasks is the Area Under the Receiver Operating Characteristics Curve (AUC). In this paper we provide proofs-of-concept of how knowledge of the AUC of a set of guesses can be used, in two different kinds of attacks, to improve the accuracy of those guesses. On the other hand, we also demonstrate the intractability of one kind of AUC exploit by proving that the number of possible binary labelings of $n$ examples for which a candidate solution obtains a AUC score of $c$ grows exponentially in $n$, for every $c\in (0,1)$.
cs.LG:In this paper, we address the problem of multi-label classification. We consider linear classifiers and propose to learn a prior over the space of labels to directly leverage the performance of such methods. This prior takes the form of a quadratic function of the labels and permits to encode both attractive and repulsive relations between labels. We cast this problem as a structured prediction one aiming at optimizing either the accuracies of the predictors or the F 1-score. This leads to an optimization problem closely related to the max-cut problem, which naturally leads to semidefinite and spectral relaxations. We show on standard datasets how such a general prior can improve the performances of multi-label techniques.
cs.LG:Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.
cs.LG:In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.
cs.LG:We first present a general risk bound for ensembles that depends on the Lp norm of the weighted combination of voters which can be selected from a continuous set. We then propose a boosting method, called QuadBoost, which is strongly supported by the general risk bound and has very simple rules for assigning the voters' weights. Moreover, QuadBoost exhibits a rate of decrease of its empirical error which is slightly faster than the one achieved by AdaBoost. The experimental results confirm the expectation of the theory that QuadBoost is a very efficient method for learning ensembles.
cs.LG:We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD($\lambda$) and ELSTD($\lambda$). We prove, under general off-policy conditions, the convergence in $L^1$ for ELSTD($\lambda$) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD($\lambda$) also converges under off-policy training for $\lambda$ sufficiently large.
cs.LG:In this paper, a novel framework of sparse kernel learning for Support Vector Data Description (SVDD) based anomaly detection is presented. In this work, optimal sparse feature selection for anomaly detection is first modeled as a Mixed Integer Programming (MIP) problem. Due to the prohibitively high computational complexity of the MIP, it is relaxed into a Quadratically Constrained Linear Programming (QCLP) problem. The QCLP problem can then be practically solved by using an iterative optimization method, in which multiple subsets of features are iteratively found as opposed to a single subset. The QCLP-based iterative optimization problem is solved in a finite space called the \emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space or \emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of the fact that the geometrical properties of the EKFS and the corresponding RKHS remain the same. Now, an explicit nonlinear exploitation of the data in a finite EKFS is achievable, which results in optimal feature ranking. Experimental results based on a hyperspectral image show that the proposed method can provide improved performance over the current state-of-the-art techniques.
cs.LG:We study the interpretability of conditional probability estimates for binary classification under the agnostic setting or scenario. Under the agnostic setting, conditional probability estimates do not necessarily reflect the true conditional probabilities. Instead, they have a certain calibration property: among all data points that the classifier has predicted P(Y = 1|X) = p, p portion of them actually have label Y = 1. For cost-sensitive decision problems, this calibration property provides adequate support for us to use Bayes Decision Theory. In this paper, we define a novel measure for the calibration property together with its empirical counterpart, and prove an uniform convergence result between them. This new measure enables us to formally justify the calibration property of conditional probability estimations, and provides new insights on the problem of estimating and calibrating conditional probabilities.
cs.LG:The outputs of non-linear feed-forward neural network are positive, which could be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in Feed-Forward Neural Network, our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the feed-forward neural network, a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement . The results illustrate that our method outperforms all the other baselines that are most popular clustering methods.
cs.LG:Margin-Based Principle has been proposed for a long time, it has been proved that this principle could reduce the structural risk and improve the performance in both theoretical and practical aspects. Meanwhile, feed-forward neural network is a traditional classifier, which is very hot at present with a deeper architecture. However, the training algorithm of feed-forward neural network is developed and generated from Widrow-Hoff Principle that means to minimize the squared error. In this paper, we propose a new training algorithm for feed-forward neural networks based on Margin-Based Principle, which could effectively promote the accuracy and generalization ability of neural network classifiers with less labelled samples and flexible network. We have conducted experiments on four UCI open datasets and achieved good results as expected. In conclusion, our model could handle more sparse labelled and more high-dimension dataset in a high accuracy while modification from old ANN method to our method is easy and almost free of work.
cs.LG:In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015) under the setting used in their experiments, which is also the best setting suggested by the authors that proposed this algorithm, is equivalent to the practical variant of DisDCA (Yang, NIPS, 2013).
cs.LG:This paper studies the generalization performance of multi-class classification algorithms, for which we obtain, for the first time, a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on $\ell_p$-norm regularization, where the parameter $p$ controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.
cs.LG:We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.
cs.LG:This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$ Gaussian components and $D$ dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to $\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets.
cs.LG:The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner.
cs.LG:We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation and missing data imputation. Our method outperforms many state-of-the-art competitors.
