cs.AI:Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.
cs.AI:Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.
cs.AI:We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.
cs.AI:As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.
cs.AI:To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime.
cs.AI:Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.
cs.AI:A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.
cs.AI:Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.
cs.AI:The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.
cs.AI:The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.
cs.AI:We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.
cs.AI:This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.
cs.AI:In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.
cs.AI:Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,...,N} that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.
cs.AI:Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.
cs.AI:This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.
cs.AI:This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.
cs.AI:The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.
cs.AI:This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.
cs.AI:For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.
cs.AI:Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k ``classes''). The definition is acquired by studying collections of training examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.
cs.AI:The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.
cs.AI:Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda &gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.
cs.AI:This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search.
cs.AI:Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.
cs.AI:Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n^2d^2) complexity (instead of O(n^3d^3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.
cs.AI:We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.
cs.AI:Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.
cs.AI:We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional ``basecase'' oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.
cs.AI:In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.
cs.AI:There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the "best" possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.
cs.AI:This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).
cs.AI:ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.
cs.AI:Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.
cs.AI:Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.
cs.AI:This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.
cs.AI:Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset.
cs.AI:Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a ``measure of goodness'' or ``membership value'' with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.
cs.AI:This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.
cs.AI:OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.
cs.AI:The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.
cs.AI:In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.
cs.AI:We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.
cs.AI:Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.
cs.AI:This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.
cs.AI:We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.
cs.AI:Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.
cs.AI:The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.
cs.AI:Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.
cs.AI:We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.
cs.AI:We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.
cs.AI:A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.
cs.AI:For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.
cs.AI:Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a ``difference matching'' procedure. The critic then proposes lemmas and generalizations which ``ripple'' these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.
cs.AI:Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.
cs.AI:Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.
cs.AI:This paper presents new experimental evidence against the utility of Occam's razor. A~systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.
cs.AI:The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.
cs.AI:This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
cs.AI:Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.
cs.AI:Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.
cs.AI:A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.
cs.AI:An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.
cs.AI:Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.
cs.AI:Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods. We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines.
cs.AI:Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Omega_1,Omega_2,..., with the following properties: first, Omega_1 is the class of all stratified knowledge bases; second, if a knowledge base Pi is in Omega_k, then Pi has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find the minimum k such that Pi belongs to Omega_k in time polynomial in the size of Pi; and, last, where K is the class of all knowledge bases, it is the case that union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some class in the hierarchy.
cs.AI:We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by UCPOP to select plans for refinement. The other is based on preferring ``zero commitment'' (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of UCPOP, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default UCPOP search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices.
cs.AI:Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.
cs.AI:This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol (UNP) is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals. Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one 'strict', the other 'tolerant', and analyze their affects on the stability and efficiency of negotiation outcomes.
cs.AI:First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization.
cs.AI:This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP.
cs.AI:A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as ``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.
cs.AI:Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a personUs name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains.
cs.AI:Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data.
cs.AI:Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.
cs.AI:Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken- language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.
cs.AI:Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic muALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualified number restrictions, muALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of muALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus.
cs.AI:We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the Toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment.
cs.AI:We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks (we show how they can be generated using clustering and conditioning algorithms). The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching.
cs.AI:An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the REGENT algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.
cs.AI:Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.
cs.AI:We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total.
cs.AI:The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions.
cs.AI:This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time.
cs.AI:Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.
cs.AI:Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.
cs.AI:SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.
cs.AI:Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.
cs.AI:Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.
cs.AI:The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback, while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.
cs.AI:Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms.
cs.AI:The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered.
cs.AI:Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.
cs.AI:An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.
cs.AI:Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.
cs.AI:This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.
cs.AI:In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.
cs.AI:In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.
cs.AI:This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.
cs.AI:One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.
cs.AI:We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the process of proving that certain planning problems are complete for NP^PP, we introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.
cs.AI:In this paper we describe SYNERGY, which is a highly parallelizable, linear planning system that is based on the genetic programming paradigm. Rather than reasoning about the world it is planning for, SYNERGY uses artificial selection, recombination and fitness measure to generate linear plans that solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase problem and a few variants of the robot navigation problem), and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by UCPOP. In order to facilitate the search reduction and to enhance the expressive power of SYNERGY, we also propose two major extensions to our planning system: a formalism for using hierarchical planning operators, and a framework for planning in dynamic environments.
cs.AI:We show that several constraint propagation algorithms (also called (local) consistency, consistency enforcing, Waltz, filtering or narrowing algorithms) are instances of algorithms that deal with chaotic iteration. To this end we propose a simple abstract framework that allows us to classify and compare these algorithms and to establish in a uniform way their basic properties.
cs.AI:This paper examines the phenomenon of daydreaming: spontaneously recalling or imagining personal or vicarious experiences in the past or future. The following important roles of daydreaming in human cognition are postulated: plan preparation and rehearsal, learning from failures and successes, support for processes of creativity, emotion regulation, and motivation.   A computational theory of daydreaming and its implementation as the program DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based on relaxed planning, 2) a dynamic episodic memory of experiences used by the scenario generator, 3) a collection of personal goals and control goals which guide the scenario generator, 4) an emotion component in which daydreams initiate, and are initiated by, emotional states arising from goal outcomes, and 5) domain knowledge of interpersonal relations and common everyday occurrences.   The role of emotions and control goals in daydreaming is discussed. Four control goals commonly used in guiding daydreaming are presented: rationalization, failure/success reversal, revenge, and preparation. The role of episodic memory in daydreaming is considered, including how daydreamed information is incorporated into memory and later used. An initial version of DAYDREAMER which produces several daydreams (in English) is currently running.
cs.AI:Real world combinatorial optimization problems such as scheduling are typically too complex to solve with exact methods. Additionally, the problems often have to observe vaguely specified constraints of different importance, the available data may be uncertain, and compromises between antagonistic criteria may be necessary. We present a combination of approximate reasoning based constraints and iterative optimization based heuristics that help to model and solve such problems in a framework of C++ software libraries called StarFLIP++. While initially developed to schedule continuous caster units in steel plants, we present in this paper results from reusing the library components in a shift scheduling system for the workforce of an industrial production plant.
cs.AI:The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman & Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.
cs.AI:How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) "iconic representations," which are analogs of the proximal sensory projections of distal objects and events, and (2) "categorical representations," which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) "symbolic representations," grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., "An X is a Y that is Z").
cs.AI:In tree search problem the best-first search algorithm needs too much of space . To remove such drawbacks of these algorithms the IDA* was developed which is both space and time cost efficient. But again IDA* can give an optimal solution for real valued problems like Flow shop scheduling, Travelling Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further modifications are done on it and the Iterative Deepening Branch and Bound Search Algorithms is developed which meets the requirements. We have tried using this algorithm for the Flow Shop Scheduling Problem and have found that it is quite effective.
cs.AI:Agents are small programs that autonomously take actions based on changes in their environment or ``state.'' Over the last few years, there have been an increasing number of efforts to build agents that can interact and/or collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top of legacy code. However, their framework assumes that agent states are completely determined, and there is no uncertainty in an agent's state. Thus, their framework allows an agent developer to specify how his agents will react when the agent is 100% sure about what is true/false in the world state. In this paper, we propose the concept of a \emph{probabilistic agent program} and show how, given an arbitrary program written in any imperative language, we may build a declarative ``probabilistic'' agent program on top of it which supports decision making in the presence of uncertainty. We provide two alternative semantics for probabilistic agent programs. We show that the second semantics, though more epistemically appealing, is more complex to compute. We provide sound and complete algorithms to compute the semantics of \emph{positive} agent programs.
cs.AI:The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.
cs.AI:We revisit the issue of connections between two leading formalisms in nonmonotonic reasoning: autoepistemic logic and default logic. For each logic we develop a comprehensive semantic framework based on the notion of a belief pair. The set of all belief pairs together with the so called knowledge ordering forms a complete lattice. For each logic, we introduce several semantics by means of fixpoints of operators on the lattice of belief pairs. Our results elucidate an underlying isomorphism of the respective semantic constructions. In particular, we show that the interpretation of defaults as modal formulas proposed by Konolige allows us to represent all semantics for default logic in terms of the corresponding semantics for autoepistemic logic. Thus, our results conclusively establish that default logic can indeed be viewed as a fragment of autoepistemic logic. However, as we also demonstrate, the semantics of Moore and Reiter are given by different operators and occupy different locations in their corresponding families of semantics. This result explains the source of the longstanding difficulty to formally relate these two semantics. In the paper, we also discuss approximating skeptical reasoning with autoepistemic and default logics and establish constructive principles behind such approximations.
cs.AI:Randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables. However, these algorithms are not complete. That is, they provide no guarantee that a satisfying assignment, if one exists, will be found. Thus, when studying randomized algorithms, there are two important characteristics that need to be considered: the running time and, even more importantly, the accuracy --- a measure of likelihood that a satisfying assignment will be found, provided one exists. In fact, we argue that without a reference to the accuracy, the notion of the running time for randomized algorithms is not well-defined. In this paper, we introduce a formal notion of accuracy. We use it to define a concept of the running time. We use both notions to study the random walk strategy GSAT algorithm. We investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments. We demonstrate that the running time of GSAT grows exponentially in the number of variables of the input formula for randomly generated 3-CNF formulas and for the formulas encoding 3- and 4-colorability of graphs.
cs.AI:Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentioned as a process that may have been used to obtain a primordial biological code necessary open-ended evolution.
cs.AI:This paper presents a method of computing a revision of a function-free normal logic program. If an added rule is inconsistent with a program, that is, if it leads to a situation such that no stable model exists for a new program, then deletion and addition of rules are performed to avoid inconsistency. We specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules. To compute such deletion and addition, we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule. We compute a minimally revised program, by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure.
cs.AI:This is a system description for the OSCAR defeasible reasoner.
cs.AI:Diagnostic reasoning has been characterized logically as consistency-based reasoning or abductive reasoning. Previous analyses in the literature have shown, on the one hand, that choosing the (in general more restrictive) abductive definition may be appropriate or not, depending on the content of the knowledge base [Console&Torasso91], and, on the other hand, that, depending on the choice of the definition the same knowledge should be expressed in different form [Poole94].   Since in Model-Based Diagnosis a major problem is finding the right way of abstracting the behavior of the system to be modeled, this paper discusses the relation between modeling, and in particular abstraction in the model, and the notion of diagnosis.
cs.AI:ACLP is a system which combines abductive reasoning and constraint solving by integrating the frameworks of Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). It forms a general high-level knowledge representation environment for abductive problems in Artificial Intelligence and other areas. In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems. The ACLP system is currently implemented on top of the CLP language of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver for finite domains. It has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the (lower level) constraint solving framework of CLP on which it is built. These experiments provide evidence that the abductive framework of ACLP does not compromise significantly the computational efficiency of the solutions. Other experiments show the natural ability of ACLP to accommodate easily and in a robust way new or changing requirements of the original problem.
cs.AI:We present a method for relevance sensitive non-monotonic inference from belief sequences which incorporates insights pertaining to prioritized inference and relevance sensitive, inconsistency tolerant belief revision.   Our model uses a finite, logically open sequence of propositional formulas as a representation for beliefs and defines a notion of inference from maxiconsistent subsets of formulas guided by two orderings: a temporal sequencing and an ordering based on relevance relations between the conclusion and formulas in the sequence. The relevance relations are ternary (using context as a parameter) as opposed to standard binary axiomatizations. The inference operation thus defined easily handles iterated revision by maintaining a revision history, blocks the derivation of inconsistent answers from a possibly inconsistent sequence and maintains the distinction between explicit and implicit beliefs. In doing so, it provides a finitely presented formalism and a plausible model of reasoning for automated agents.
cs.AI:We propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. In detail, we generalize the notions of Pearl's entailment in system Z, Lehmann's lexicographic entailment, and Geffner's conditional entailment to conditional constraints. We give some examples that show that the new notions of z-, lexicographic, and conditional entailment have similar properties like their classical counterparts. Moreover, we show that the new notions of z-, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints.
cs.AI:This paper describes a system, called PLP, for compiling ordered logic programs into standard logic programs under the answer set semantics. In an ordered logic program, rules are named by unique terms, and preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Since the result of the translation is an extended logic program, existing logic programming systems can be used as underlying reasoning engine. In particular, PLP is conceived as a front-end to the logic programming systems dlv and smodels.
cs.AI:The SLDNFA-system results from the LP+ project at the K.U.Leuven, which investigates logics and proof procedures for these logics for declarative knowledge representation. Within this project inductive definition logic (ID-logic) is used as representation logic. Different solvers are being developed for this logic and one of these is SLDNFA. A prototype of the system is available and used for investigating how to solve efficiently problems represented in ID-logic.
cs.AI:We describe an approach for compiling preferences into logic programs under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows both the specification of static orderings (as found in most previous work), in which preferences are external to a logic program, as well as orderings on sets of rules. In large part then, we are interested in describing a general methodology for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.
cs.AI:This paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base. The abductive inference processes described here depend on the semantic of the rule. We distinguish two classes of interpretation of a fuzzy rule, certainty generation rules and possible generation rules. In this paper we present the architecture of abductive inference in the first class of interpretation. We give two kinds of problem that we can resolve by using the proposed models of inference.
cs.AI:The goal of the LP+ project at the K.U.Leuven is to design an expressive logic, suitable for declarative knowledge representation, and to develop intelligent systems based on Logic Programming technology for solving computational problems using the declarative specifications. The ID-logic is an integration of typed classical logic and a definition logic. Different abductive solvers for this language are being developed. This paper is a report of the integration of high order aggregates into ID-logic and the consequences on the solver SLDNFA.
cs.AI:We propose a new approach to belief revision that provides a way to change knowledge bases with a minimum of effort. We call this way of revising belief states optimal belief revision. Our revision method gives special attention to the fact that most belief revision processes are directed to a specific informational objective. This approach to belief change is founded on notions such as optimal context and accessibility. For the sentential model of belief states we provide both a formal description of contexts as sub-theories determined by three parameters and a method to construct contexts. Next, we introduce an accessibility ordering for belief sets, which we then use for selecting the best (optimal) contexts with respect to the processing effort involved in the revision. Then, for finitely axiomatizable knowledge bases, we characterize a finite accessibility ranking from which the accessibility ordering for the entire base is generated and show how to determine the ranking of an arbitrary sentence in the language. Finally, we define the adjustment of the accessibility ranking of a revised base of a belief set.
cs.AI:High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like \congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting \ccgolog, a variant of \congolog which is based on the extended situation calculus. One benefit of \ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism.
cs.AI:The Smodels system implements the stable model semantics for normal logic programs. It handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints. On top of this core engine more involved systems can be built. As an example, we have implemented total and partial stable model computation for disjunctive logic programs. An interesting application method is based on answer set programming, i.e., encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules. Smodels has been applied to a number of areas including planning, model checking, reachability analysis, product configuration, dynamic constraint satisfaction, and feature interaction.
cs.AI:E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.
cs.AI:In this paper, we outline the prototype of an automated inference tool, called QUIP, which provides a uniform implementation for several nonmonotonic reasoning formalisms. The theoretical basis of QUIP is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae.
cs.AI:Over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information. One such language is the language of epistemic specifications. As is usual with logic programming languages, the problem of answering queries is intractable in the general case. For extended disjunctive logic programs, an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets. In this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications. Furthermore, an extension of the splitting set theorem will be presented. Also, a characterization of stratified epistemic specifications will be given in terms of splitting sets. This characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs.
cs.AI:The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.
cs.AI:We generalize a theorem by Francois Fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure. The study of this relationship is important in connection with the emergence of answer set programming. Whenever the two semantics are equivalent, answer sets can be computed by a satisfiability solver, and the use of answer set solvers such as smodels and dlv is unnecessary. A logic programming representation of the blocks world due to Ilkka Niemelae is discussed as an example.
cs.AI:We introduced decomposable negation normal form (DNNF) recently as a tractable form of propositional theories, and provided a number of powerful logical operations that can be performed on it in polynomial time. We also presented an algorithm for compiling any conjunctive normal form (CNF) into DNNF and provided a structure-based guarantee on its space and time complexity. We present in this paper a linear-time algorithm for converting an ordered binary decision diagram (OBDD) representation of a propositional theory into an equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the previous complexity guarantees on compiling DNNF continue to hold for this stricter subclass, which has stronger properties. In particular, we present a new operation on d-DNNF which allows us to count its models under the assertion, retraction and flipping of every literal by traversing the d-DNNF twice. That is, after such traversal, we can test in constant-time: the entailment of any literal by the d-DNNF, and the consistency of the d-DNNF under the retraction or flipping of any literal. We demonstrate the significance of these new operations by showing how they allow us to implement linear-time, complete truth maintenance systems and linear-time, complete belief revision systems for two important classes of propositional theories.
cs.AI:The paper reports on first preliminary results and insights gained in a project aiming at implementing the fluent calculus using methods and techniques based on binary decision diagrams. After reporting on an initial experiment showing promising results we discuss our findings concerning various techniques and heuristics used to speed up the reasoning process.
cs.AI:Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.
cs.AI:In an earlier work, we have presented operations of belief change which only affect the relevant part of a belief base. In this paper, we propose the application of the same strategy to the problem of model-based diangosis. We first isolate the subset of the system description which is relevant for a given observation and then solve the diagnosis problem for this subset.
cs.AI:We present a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such ``extension'' represents the revised state; alternately revision consists of the intersection of all such extensions.   The most general formulation of our approach is flexible enough to express other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: the choice revision operator gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A.
cs.AI:SATEN is an object-oriented web-based extraction and belief revision engine. It runs on any computer via a Java 1.1 enabled browser such as Netscape 4. SATEN performs belief revision based on the AGM approach. The extraction and belief revision reasoning engines operate on a user specified ranking of information. One of the features of SATEN is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm. We describe here an ASP system, DATALOG with constraints or DC, based on non-monotonic logic. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax, use and implementation of DC and provide experimental results.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm well attuned to search problems in AI, constraint satisfaction and combinatorics. Propositional logic is, arguably, the simplest ASP system with an intuitive semantics supporting direct modeling of problem constraints. However, for some applications, especially those requiring that transitive closure be computed, it requires additional variables and results in large theories. Consequently, it may not be a practical computational tool for such problems. On the other hand, ASP systems based on nonmonotonic logics, such as stable logic programming, can handle transitive closure computation efficiently and, in general, yield very concise theories as problem representations. Their semantics is, however, more complex. Searching for the middle ground, in this paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax and semantics of DC, and study its properties. We discuss an implementation of DC and present results of experimental study of the effectiveness of DC, comparing it with CSAT, a satisfiability checker and SMODELS implementation of stable logic programming. Our results show that DC is competitive with the other two approaches, in case of many search problems, often yielding much more efficient solutions.
cs.AI:We study here the well-known propagation rules for Boolean constraints. First we propose a simple notion of completeness for sets of such rules and establish a completeness result. Then we show an equivalence in an appropriate sense between Boolean constraint propagation and unit propagation, a form of resolution for propositional logic.   Subsequently we characterize one set of such rules by means of the notion of hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify the status of a similar, though different, set of rules introduced in (Simonis 1989a) and more fully in (Codognet and Diaz 1996).
cs.AI:A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.
cs.AI:In this paper we present a rule based formalism for filtering variables domains of constraints. This formalism is well adapted for solving dynamic CSP. We take diagnosis as an instance problem to illustrate the use of these rules. A diagnosis problem is seen like finding all the minimal sets of constraints to be relaxed in the constraint network that models the device to be diagnosed
cs.AI:Despite the effort of many researchers in the area of multi-agent systems (MAS) for designing and programming agents, a few years ago the research community began to take into account that common features among different MAS exists. Based on these common features, several tools have tackled the problem of agent development on specific application domains or specific types of agents. As a consequence, their scope is restricted to a subset of the huge application domain of MAS. In this paper we propose a generic infrastructure for programming agents whose name is Brainstorm/J. The infrastructure has been implemented as an object oriented framework. As a consequence, our approach supports a broader scope of MAS applications than previous efforts, being flexible and reusable.
cs.AI:In fuzzy propositional logic, to a proposition a partial truth in [0,1] is assigned. It is well known that under certain circumstances, fuzzy logic collapses to classical logic. In this paper, we will show that under dual conditions, fuzzy logic collapses to four-valued (relevance) logic, where propositions have truth-value true, false, unknown, or contradiction. As a consequence, fuzzy entailment may be considered as ``in between'' four-valued (relevance) entailment and classical entailment.
cs.AI:We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.
cs.AI:Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are systems based on stable model semantics, abductive systems, and first order logic model generators which compute solutions as models of some theory. This paper compares these different approaches from the point of view of knowledge representation (how declarative are the programs) and from the point of view of performance (how good are they at solving typical problems).
cs.AI:In this paper, we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery. This theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules. We show how to realize this theory in a learning system named CFRule. CFRule is a parallel weight-based model, but it departs from traditional neural computing in that its internal knowledge is comprehensible. Furthermore, when the model converges upon training, each channel converges to a target rule. The model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent. Since, however, gradient descent only guarantees local optimization, a multi-channel regression-based optimization strategy is developed to effectively deal with this problem. Formally, we prove that the CFRule model can explicitly and precisely encode any given rule set. Also, we prove a property related to asynchronous parallel convergence, which is a critical element of the multi-channel parallel adaptation theory for rule learning. Thanks to the quantizability nature of the CFRule model, rules can be extracted completely and soundly via a threshold-based mechanism. Finally, the practical application of the theory is demonstrated in DNA promoter recognition and hepatitis prognosis prediction.
cs.AI:We present an approach for modelling the structure and coarse content of legal documents with a view to providing automated support for the drafting of contracts and contract database retrieval. The approach is designed to be applicable where contract drafting is based on model-form contracts or on existing examples of a similar type. The main features of the approach are: (1) the representation addresses the structure and the interrelationships between the constituent parts of contracts, but not the text of the document itself; (2) the representation of documents is separated from the mechanisms that manipulate it; and (3) the drafting process is subject to a collection of explicitly stated constraints that govern the structure of the documents. We describe the representation of document instances and of 'generic documents', which are data structures used to drive the creation of new document instances, and we show extracts from a sample session to illustrate the features of a prototype system implemented in MacProlog.
cs.AI:One influential approach to assessing the "goodness" of arguments is offered by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an approach that originates in discourse analysis. In p-d terms an argument is good if it avoids committing a fallacy, whereas in RST terms an argument is good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for providing only a partially functional account of argument, and similar criticisms have been raised in the Natural Language Generation (NLG) community-particularly by Moore & Pollack (1992)- with regards to its account of intentionality in text in general. Mann and Thompson themselves note that although RST can be successfully applied to a wide range of texts from diverse domains, it fails to characterise some types of text, most notably legal contracts. There is ongoing research in the Artificial Intelligence and Law community exploring the potential for providing electronic support to contract negotiators, focusing on long-term, complex engineering agreements (see for example Daskalopulu & Sergot 1997). This paper provides a brief introduction to RST and illustrates its shortcomings with respect to contractual text. An alternative approach for modelling argument structure is presented which not only caters for contractual text, but also overcomes the aforementioned limitations of RST.
cs.AI:Information Integration is a young and exciting field with enormous research and commercial significance in the new world of the Information Society. It stands at the crossroad of Databases and Artificial Intelligence requiring novel techniques that bring together different methods from these fields. Information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible, transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program. The field although relatively young has already found many practical applications particularly for integrating information over the World Wide Web. This paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas. It attempts to evaluate the current and potential role of Computational Logic in this and suggests some of the problems where logic-based techniques could be used.
cs.AI:Constraint propagation is a general algorithmic approach for pruning the search space of a CSP. In a uniform way, K. R. Apt has defined a computation as an iteration of reduction functions over a domain. He has also demonstrated the need for integrating static properties of reduction functions (commutativity and semi-commutativity) to design specialized algorithms such as AC3 and DAC. We introduce here a set of operators for modeling compositions of reduction functions. Two of the major goals are to tackle parallel computations, and dynamic behaviours (such as slow convergence).
cs.AI:We consider an approach to update nonmonotonic knowledge bases represented as extended logic programs under answer set semantics. New information is incorporated into the current knowledge base subject to a causal rejection principle enforcing that, in case of conflicts, more recent rules are preferred and older rules are overridden. Such a rejection principle is also exploited in other approaches to update logic programs, e.g., in dynamic logic programming by Alferes et al. We give a thorough analysis of properties of our approach, to get a better understanding of the causal rejection principle. We review postulates for update and revision operators from the area of theory change and nonmonotonic reasoning, and some new properties are considered as well. We then consider refinements of our semantics which incorporate a notion of minimality of change. As well, we investigate the relationship to other approaches, showing that our approach is semantically equivalent to inheritance programs by Buccafurri et al. and that it coincides with certain classes of dynamic logic programs, for which we provide characterizations in terms of graph conditions. Therefore, most of our results about properties of causal rejection principle apply to these approaches as well. Finally, we deal with computational complexity of our approach, and outline how the update semantics and its refinements can be implemented on top of existing logic programming engines.
cs.AI:We introduce a learning method called ``gradient-based reinforcement planning'' (GREP). Unlike traditional DP methods that improve their policy backwards in time, GREP is a gradient-based method that plans ahead and improves its policy before it actually acts in the environment. We derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments.
cs.AI:Much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty. In this work we extend competitive analysis to the context of multi-agent systems. Unlike classical competitive analysis where the behavior of an agent's environment is taken to be arbitrary, we consider the case where an agent's environment consists of other agents. These agents will usually obey some (minimal) rationality constraints. This leads to the definition of rational competitive analysis. We introduce the concept of rational competitive analysis, and initiate the study of competitive analysis for multi-agent systems. We also discuss the application of rational competitive analysis to the context of bidding games, as well as to the classical one-way trading problem.
cs.AI:This article aims at clarifying the language and practice of scientific experiment, mainly by hooking observability on calculability.
cs.AI:Many systems that exhibit nonmonotonic behavior have been described and studied already in the literature. The general notion of nonmonotonic reasoning, though, has almost always been described only negatively, by the property it does not enjoy, i.e. monotonicity. We study here general patterns of nonmonotonic reasoning and try to isolate properties that could help us map the field of nonmonotonic reasoning by reference to positive properties. We concentrate on a number of families of nonmonotonic consequence relations, defined in the style of Gentzen. Both proof-theoretic and semantic points of view are developed in parallel. The former point of view was pioneered by D. Gabbay, while the latter has been advocated by Y. Shoham in. Five such families are defined and characterized by representation theorems, relating the two points of view. One of the families of interest, that of preferential relations, turns out to have been studied by E. Adams. The "preferential" models proposed here are a much stronger tool than Adams' probabilistic semantics. The basic language used in this paper is that of propositional logic. The extension of our results to first order predicate calculi and the study of the computational complexity of the decision problems described in this paper will be treated in another paper.
cs.AI:This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type "if ... then ...", represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, S. Kraus and the authors defined and studied "preferential" consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, "rational" relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a "ranked" preferential model, or by a (non-standard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.
cs.AI:It is shown that Darwiche and Pearl's postulates imply an interesting property, not noticed by the authors.
cs.AI:A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions.
cs.AI:We give a semantics to iterated update by a preference relation on possible developments. An iterated update is a sequence of formulas, giving (incomplete) information about successive states of the world. A development is a sequence of models, describing a possible trajectory through time. We assume a principle of inertia and prefer those developments, which are compatible with the information, and avoid unnecessary changes. The logical properties of the updates defined in this way are considered, and a representation result is proved.
cs.AI:A. Tarski proposed the study of infinitary consequence operations as the central topic of mathematical logic. He considered monotonicity to be a property of all such operations. In this paper, we weaken the monotonicity requirement and consider more general operations, inference operations. These operations describe the nonmonotonic logics both humans and machines seem to be using when infering defeasible information from incomplete knowledge. We single out a number of interesting families of inference operations. This study of infinitary inference operations is inspired by the results of Kraus, Lehmann and Magidor on finitary nonmonotonic operations, but this paper is self-contained.
cs.AI:The Expansion property considered by researchers in Social Choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the {\em pure}, i.e., not involving connectives, version of a previously known weak rationality condition. The assumption that the union of two definable sets of models is definable is needed for the soundness part of the result.
cs.AI:The lexicographic closure of any given finite set D of normal defaults is defined. A conditional assertion "if a then b" is in this lexicographic closure if, given the defaults D and the fact a, one would conclude b. The lexicographic closure is essentially a rational extension of D, and of its rational closure, defined in a previous paper. It provides a logic of normal defaults that is different from the one proposed by R. Reiter and that is rich enough not to require the consideration of non-normal defaults. A large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind Reiter's logic of defaults.
cs.AI:We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).
cs.AI:Stereotypical reasoning assumes that the situation at hand is one of a kind and that it enjoys the properties generally associated with that kind of situation. It is one of the most basic forms of nonmonotonic reasoning. A formal model for stereotypical reasoning is proposed and the logical properties of this form of reasoning are studied. Stereotypical reasoning is shown to be cumulative under weak assumptions.
cs.AI:We introduce a methodology and framework for expressing general preference information in logic programming under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of form s < t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed program correspond with the preferred answer sets of the original program. Our approach allows the specification of dynamic orderings, in which preferences can appear arbitrarily within a program. Static orderings (in which preferences are external to a logic program) are a trivial restriction of the general dynamic case. First, we develop a specific approach to reasoning with preferences, wherein the preference ordering specifies the order in which rules are to be applied. We then demonstrate the wide range of applicability of our framework by showing how other approaches, among them that of Brewka and Eiter, can be captured within our framework. Since the result of each of these transformations is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a publicly available compiler as a front-end for these programming systems.
cs.AI:Prioritized default reasoning has illustrated its rich expressiveness and flexibility in knowledge representation and reasoning. However, many important aspects of prioritized default reasoning have yet to be thoroughly explored. In this paper, we investigate two properties of prioritized logic programs in the context of answer set semantics. Specifically, we reveal a close relationship between mutual defeasibility and uniqueness of the answer set for a prioritized logic program. We then explore how the splitting technique for extended logic programs can be extended to prioritized logic programs. We prove splitting theorems that can be used to simplify the evaluation of a prioritized logic program under certain conditions.
cs.AI:The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula, but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed.
cs.AI:We study fixpoints of operators on lattices. To this end we introduce the notion of an approximation of an operator. We order approximations by means of a precision ordering. We show that each lattice operator O has a unique most precise or ultimate approximation. We demonstrate that fixpoints of this ultimate approximation provide useful insights into fixpoints of the operator O.   We apply our theory to logic programming and introduce the ultimate Kripke-Kleene, well-founded and stable semantics. We show that the ultimate Kripke-Kleene and well-founded semantics are more precise then their standard counterparts We argue that ultimate semantics for logic programming have attractive epistemological properties and that, while in general they are computationally more complex than the standard semantics, for many classes of theories, their complexity is no worse.
cs.AI:Representing defeasibility is an important issue in common sense reasoning. In reasoning about action and change, this issue becomes more difficult because domain and action related defeasible information may conflict with general inertia rules. Furthermore, different types of defeasible information may also interfere with each other during the reasoning. In this paper, we develop a prioritized logic programming approach to handle defeasibilities in reasoning about action. In particular, we propose three action languages {\cal AT}^{0}, {\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in action domains named defeasible constraints, defeasible observations and actions with defeasible and abnormal effects respectively. Each language with a higher superscript can be viewed as an extension of the language with a lower superscript. These action languages inherit the simple syntax of {\cal A} language but their semantics is developed in terms of transition systems where transition functions are defined based on prioritized logic programs. By illustrating various examples, we show that our approach eventually provides a powerful mechanism to handle various defeasibilities in temporal prediction and postdiction. We also investigate semantic properties of these three action languages and characterize classes of action domains that present more desirable solutions in reasoning about action within the underlying action languages.
cs.AI:An anticipatory system for guiding plot development in interactive narratives is described. The executable model is a finite automaton that provides the implemented system with a look-ahead. The identification of undesirable future states in the model is used to guide the player, in a transparent manner. In this way, too radical twists of the plot can be avoided. Since the player participates in the development of the plot, such guidance can have many forms, depending on the environment of the player, on the behavior of the other players, and on the means of player interaction. We present a design method for interactive narratives which produces designs suitable for the implementation of anticipatory mechanisms. Use of the method is illustrated by application to our interactive computer game Kaktus.
cs.AI:Open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics. There are obvious analogies between open predicates and abducible predicates. However, despite superficial similarities, there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa. Similarly, open programs cannot be immediately simulated with answer set programming (ASP). In this paper we start a thorough investigation of the relationships between open inference, abduction and ASP. We shall prove that open programs generalize the other two frameworks. The generalized framework suggests interesting extensions of abduction under the generalized stable model semantics. In some cases, we will be able to reduce open inference to abduction and ASP, thereby estimating its computational complexity. At the same time, the aforementioned reduction opens the way to new applications of abduction and ASP.
cs.AI:In this paper we consider three different kinds of domain-dependent control knowledge (temporal, procedural and HTN-based) that are useful in planning. Our approach is declarative and relies on the language of logic programming with answer set semantics (AnsProlog*). AnsProlog* is designed to plan without control knowledge. We show how temporal, procedural and HTN-based control knowledge can be incorporated into AnsProlog* by the modular addition of a small number of domain-dependent rules, without the need to modify the planner. We formally prove the correctness of our planner, both in the absence and presence of the control knowledge. Finally, we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length.
cs.AI:Dung's abstract framework for argumentation enables a study of the interactions between arguments based solely on an ``attack'' binary relation on the set of arguments. Various ways to solve conflicts between contradictory pieces of information have been proposed in the context of argumentation, nonmonotonic reasoning or logic programming, and can be captured by appropriate semantics within Dung's framework. A common feature of these semantics is that one can always maximize in some sense the set of acceptable arguments. We propose in this paper to extend Dung's framework in order to allow for the representation of what we call ``restricted'' arguments: these arguments should only be used if absolutely necessary, that is, in order to support other arguments that would otherwise be defeated. We modify Dung's preferred semantics accordingly: a set of arguments becomes acceptable only if it contains a minimum of restricted arguments, for a maximum of unrestricted arguments.
cs.AI:We address a general representation problem for belief change, and describe two interrelated representations for iterative non-prioritized change: a logical representation in terms of persistent epistemic states, and a constructive representation in terms of flocks of bases.
cs.AI:An extension of an abstract argumentation framework, called collective argumentation, is introduced in which the attack relation is defined directly among sets of arguments. The extension turns out to be suitable, in particular, for representing semantics of disjunctive logic programs. Two special kinds of collective argumentation are considered in which the opponents can share their arguments.
cs.AI:Logic programs with ordered disjunction (LPODs) combine ideas underlying Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming. Logic programming under answer set semantics is extended with a new connective called ordered disjunction. The new connective allows us to represent alternative, ranked options for problem solutions in the heads of rules: A \times B intuitively means: if possible A, but if A is not possible then at least B. The semantics of logic programs with ordered disjunction is based on a preference relation on answer sets. LPODs are useful for applications in design and configuration and can serve as a basis for qualitative decision making.
cs.AI:In this paper, we investigate the extent to which knowledge compilation can be used to improve inference from propositional weighted bases. We present a general notion of compilation of a weighted base that is parametrized by any equivalence--preserving compilation function. Both negative and positive results are presented. On the one hand, complexity results are identified, showing that the inference problem from a compiled weighted base is as difficult as in the general case, when the prime implicates, Horn cover or renamable Horn cover classes are targeted. On the other hand, we show that the inference problem becomes tractable whenever DNNF-compilations are used and clausal queries are considered. Moreover, we show that the set of all preferred models of a DNNF-compilation of a weighted base can be computed in time polynomial in the output size. Finally, we sketch how our results can be used in model-based diagnosis in order to compute the most probable diagnoses of a system.
cs.AI:This paper studies the problem of modeling complex domains of actions and change within high-level action description languages. We investigate two main issues of concern: (a) can we represent complex domains that capture together different problems such as ramifications, non-determinism and concurrency of actions, at a high-level, close to the given natural ontology of the problem domain and (b) what features of such a representation can affect, and how, its computational behaviour. The paper describes the main problems faced in this representation task and presents the results of an empirical study, carried out through a series of controlled experiments, to analyze the computational performance of reasoning in these representations. The experiments compare different representations obtained, for example, by changing the basic ontology of the domain or by varying the degree of use of indirect effect laws through domain constraints. This study has helped to expose the main sources of computational difficulty in the reasoning and suggest some methodological guidelines for representing complex domains. Although our work has been carried out within one particular high-level description language, we believe that the results, especially those that relate to the problems of representation, are independent of the specific modeling language.
cs.AI:This paper introduces the notion of value-based argumentation frameworks, an extension of the standard argumentation frameworks proposed by Dung, which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote.
cs.AI:We analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory. We start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics. We then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs. The suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics. In particular, our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics. Finally, we indicate how an existing preferred well-founded semantics can be captured within our semantical framework.
cs.AI:In this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems. This transformation allows to characterize all notions of Reiter's default logic in the framework of argumentation systems. As a consequence, computing extensions, or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic. The transformation proposed is linear in the number of defaults.
cs.AI:In the present paper, the existence and multiplicity problems of extensions are addressed. The focus is on extension of the stable type. The main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification, a close cousin of the notion of admissibility. The characterization is given in the context of the particular logic for dialectical argumentation DEFLOG. The results are of direct relevance for several well-established models of defeasible reasoning (like default logic, logic programming and argumentation frameworks), since elsewhere dialectical argumentation has been shown to have close formal connections with these models.
cs.AI:Recently, it has been shown that probabilistic entailment under coherence is weaker than model-theoretic probabilistic entailment. Moreover, probabilistic entailment under coherence is a generalization of default entailment in System P. In this paper, we continue this line of research by presenting probabilistic generalizations of more sophisticated notions of classical default entailment that lie between model-theoretic probabilistic entailment and probabilistic entailment under coherence. That is, the new formalisms properly generalize their counterparts in classical default reasoning, they are weaker than model-theoretic probabilistic entailment, and they are stronger than probabilistic entailment under coherence. The new formalisms are useful especially for handling probabilistic inconsistencies related to conditioning on zero events. They can also be applied for probabilistic belief revision. More generally, in the same spirit as a similar previous paper, this paper sheds light on exciting new formalisms for probabilistic reasoning beyond the well-known standard ones.
cs.AI:We seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic. Rather than stipulating that the conclusion of an inference be true in all models in which the premises are true, we require that the conclusion of a nonmonotonic inference be true in ``almost all'' models of a certain sort in which the premises are true. This ``certain sort'' specification picks out the models that are relevant to the inference, taking into account factors such as specificity and vagueness, and previous inferences. The frequencies characterizing the relevant models reflect known frequencies in our actual world. The criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension. We show that this avoids the implausibilities that might otherwise result from the chaining of default inferences. The model proportions, when construed in terms of frequencies, provide a verifiable grounding of default rules, and can become the basis for generating default rules from statistics.
cs.AI:About ten years ago, various notions of preferential entailment have been introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM), one of the main competitor being a more general version defined by Makinson (MAK). These two versions have already been compared, but it is time to revisit these comparisons. Here are our three main results: (1) These two notions are equivalent, provided that we restrict our attention, as done in KLM, to the cases where the entailment respects logical equivalence (on the left and on the right). (2) A serious simplification of the description of the fundamental cases in which MAK is equivalent to KLM, including a natural passage in both ways. (3) The two previous results are given for preferential entailments more general than considered in some of the original texts, but they apply also to the original definitions and, for this particular case also, the models can be simplified.
cs.AI:This work analyses main features that should be present in knowledge representation. It suggests a model for representation and a way to implement this model in software. Representation takes care of both low-level sensor information and high-level concepts.
cs.AI:We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.
cs.AI:Recently, several approaches to updating knowledge bases modeled as extended logic programs have been introduced, ranging from basic methods to incorporate (sequences of) sets of rules into a logic program, to more elaborate methods which use an update policy for specifying how updates must be incorporated. In this paper, we introduce a framework for reasoning about evolving knowledge bases, which are represented as extended logic programs and maintained by an update policy. We first describe a formal model which captures various update approaches, and we define a logical language for expressing properties of evolving knowledge bases. We then investigate semantical and computational properties of our framework, where we focus on properties of knowledge states with respect to the canonical reasoning task of whether a given formula holds on a given evolving knowledge base. In particular, we present finitary characterizations of the evolution for certain classes of framework instances, which can be exploited for obtaining decidability results. In more detail, we characterize the complexity of reasoning for some meaningful classes of evolving knowledge bases, ranging from polynomial to double exponential space complexity.
cs.AI:In this thesis I present a virtual laboratory which implements five different models for controlling animats: a rule-based system, a behaviour-based system, a concept-based system, a neural network, and a Braitenberg architecture. Through different experiments, I compare the performance of the models and conclude that there is no "best" model, since different models are better for different things in different contexts.   The models I chose, although quite simple, represent different approaches for studying cognition. Using the results as an empirical philosophical aid,   I note that there is no "best" approach for studying cognition, since different approaches have all advantages and disadvantages, because they study different aspects of cognition from different contexts. This has implications for current debates on "proper" approaches for cognition: all approaches are a bit proper, but none will be "proper enough". I draw remarks on the notion of cognition abstracting from all the approaches used to study it, and propose a simple classification for different types of cognition.
cs.AI:This paper deals with the revision of partially ordered beliefs. It proposes a semantic representation of epistemic states by partial pre-orders on interpretations and a syntactic representation by partially ordered belief bases. Two revision operations, the revision stemming from the history of observations and the possibilistic revision, defined when the epistemic state is represented by a total pre-order, are generalized, at a semantic level, to the case of a partial pre-order on interpretations, and at a syntactic level, to the case of a partially ordered belief base. The equivalence between the two representations is shown for the two revision operations.
cs.AI:This is the first in a series of connected papers discussing the problem of a dynamically reconfigurable universal learning neurocomputer that could serve as a computational model for the whole human brain. The whole series is entitled "The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal Learning Neurocomputer." (For more information visit the website www.brain0.com.) This introductory paper is concerned with general methodology. Its main goal is to explain why it is critically important for both neural modeling and cognitive modeling to pay much attention to the basic requirements of the whole brain as a complex computing system. The author argues that it can be easier to develop an adequate computational model for the whole "unprogrammed" (untrained) human brain than to find adequate formal representations of some nontrivial parts of brain's performance. (In the same way as, for example, it is easier to describe the behavior of a complex analytical function than the behavior of its real and/or imaginary part.) The "curse of dimensionality" that plagues purely phenomenological ("brainless") cognitive theories is a natural penalty for an attempt to represent insufficiently large parts of brain's performance in a state space of insufficiently high dimensionality. A "partial" modeler encounters "Catch 22." An attempt to simplify a cognitive problem by artificially reducing its dimensionality makes the problem more difficult.
cs.AI:As a part of our effort for studying the evolution and development of cognition, we present results derived from synthetic experimentations in a virtual laboratory where animats develop koncepts adaptively and ground their meaning through action. We introduce the term "koncept" to avoid confusions and ambiguity derived from the wide use of the word "concept". We present the models which our animats use for abstracting koncepts from perceptions, plastically adapt koncepts, and associate koncepts with actions. On a more philosophical vein, we suggest that knowledge is a property of a cognitive system, not an element, and therefore observer-dependent.
cs.AI:This paper presents a model for dynamic adjustment of the motivation degree, using a reinforcement learning approach, in an action selection mechanism previously developed by the authors. The learning takes place in the modification of a parameter of the model of combination of internal and external stimuli. Experiments that show the claimed properties are presented, using a VR simulation developed for such purposes. The importance of adaptation by learning in action selection is also discussed.
cs.AI:This article analyses the properties of the Internal Behaviour network, an action selection mechanism previously proposed by the authors, with the aid of a simulation developed for such ends. A brief review of the Internal Behaviour network is followed by the explanation of the implementation of the simulation. Then, experiments are presented and discussed analysing the properties of the action selection in the proposed model.
cs.AI:This paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent, based in an action selection mechanism previously proposed by the authors. This combination model includes additive and multiplicative elements, which allows to incorporate new properties, which enhance the action selection. A given parameter a, which is part of the proposed model, allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity.
cs.AI:Reinforcement learning (RL) involves sequential decision making in uncertain environments. The aim of the decision-making agent is to maximize the benefit of acting in its environment over an extended period of time. Finding an optimal policy in RL may be very slow. To speed up learning, one often used solution is the integration of planning, for example, Sutton's Dyna algorithm, or various other methods using macro-actions.   Here we suggest to separate plannable, i.e., close to deterministic parts of the world, and focus planning efforts in this domain. A novel reinforcement learning method called plannable RL (pRL) is proposed here. pRL builds a simple model, which is used to search for macro actions. The simplicity of the model makes planning computationally inexpensive. It is shown that pRL finds an optimal policy, and that plannable macro actions found by pRL are near-optimal. In turn, it is unnecessary to try large numbers of macro actions, which enables fast learning. The utility of pRL is demonstrated by computer simulations.
cs.AI:Optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length. For interacting agents in time-critical applications, learning of the possibility of scheduling of subtasks (events) or the full task is an additional relevant issue. Besides, there exist highly stochastic problems where the actual trajectories show great variety from episode to episode, but completing the task takes almost the same amount of time. The identification of sub-problems of this nature may promote e.g., planning, scheduling and segmenting Markov decision processes. In this work, formulae for the average duration as well as the standard deviation of the duration of events are derived. The emerging Bellman-type equation is a simple extension of Sobel's work (1982). Methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension. Computer demonstration on a toy problem serve to highlight the principle.
cs.AI:Much work has been done on extending the well-founded semantics to general disjunctive logic programs and various approaches have been proposed. However, these semantics are different from each other and no consensus is reached about which semantics is the most intended. In this paper we look at disjunctive well-founded reasoning from different angles. We show that there is an intuitive form of the well-founded reasoning in disjunctive logic programming which can be characterized by slightly modifying some exisitng approaches to defining disjunctive well-founded semantics, including program transformations, argumentation, unfounded sets (and resolution-like procedure). We also provide a bottom-up procedure for this semantics. The significance of our work is not only in clarifying the relationship among different approaches, but also shed some light on what is an intended well-founded semantics for disjunctive logic programs.
cs.AI:We provide a semantic framework for preference handling in answer set programming. To this end, we introduce preference preserving consequence operators. The resulting fixpoint characterizations provide us with a uniform semantic framework for characterizing preference handling in existing approaches. Although our approach is extensible to other semantics by means of an alternating fixpoint theory, we focus here on the elaboration of preferences under answer set semantics. Alternatively, we show how these approaches can be characterized by the concept of order preservation. These uniform semantic characterizations provide us with new insights about interrelationships and moreover about ways of implementation.
cs.AI:The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions.   In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis.   The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents.
cs.AI:Cooperative constraint solving is an area of constraint programming that studies the interaction between constraint solvers with the aim of discovering the interaction patterns that amplify the positive qualities of individual solvers. Automatisation and formalisation of such studies is an important issue of cooperative constraint solving.   In this paper we present a constraint-based analysis of composite solvers that integrates reasoning about the individual solvers and the processed data. The idea is to approximate this reasoning by resolution of set constraints on the finite sets representing the predicates that express all the necessary properties. We illustrate application of our analysis to two important cooperation patterns: deterministic choice and loop.
cs.AI:There is a growing interest in using Kalman-filter models for brain modelling. In turn, it is of considerable importance to represent Kalman-filter in connectionist forms with local Hebbian learning rules. To our best knowledge, Kalman-filter has not been given such local representation. It seems that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a connectionist representation is presented, which is derived by means of the recursive prediction error method. We show that this method gives rise to attractive local learning rules and can adapt the Kalman-gain.
cs.AI:We discuss philosophical issues concerning the notion of cognition basing ourselves in experimental results in cognitive sciences, especially in computer simulations of cognitive systems. There have been debates on the "proper" approach for studying cognition, but we have realized that all approaches can be in theory equivalent. Different approaches model different properties of cognitive systems from different perspectives, so we can only learn from all of them. We also integrate ideas from several perspectives for enhancing the notion of cognition, such that it can contain other definitions of cognition as special cases. This allows us to propose a simple classification of different types of cognition.
cs.AI:The paper studies an implementation methodology for partial and disjunctive stable models where partiality and disjunctions are unfolded from a logic program so that an implementation of stable models for normal (disjunction-free) programs can be used as the core inference engine. The unfolding is done in two separate steps. Firstly, it is shown that partial stable models can be captured by total stable models using a simple linear and modular program transformation. Hence, reasoning tasks concerning partial stable models can be solved using an implementation of total stable models. Disjunctive partial stable models have been lacking implementations which now become available as the translation handles also the disjunctive case. Secondly, it is shown how total stable models of disjunctive programs can be determined by computing stable models for normal programs. Hence, an implementation of stable models of normal programs can be used as a core engine for implementing disjunctive programs. The feasibility of the approach is demonstrated by constructing a system for computing stable models of disjunctive programs using the smodels system as the core engine. The performance of the resulting system is compared to that of dlv which is a state-of-the-art special purpose system for disjunctive programs.
cs.AI:When tracking a large number of targets, it is often computationally expensive to represent the full joint distribution over target states. In cases where the targets move independently, each target can instead be tracked with a separate filter. However, this leads to a model-data association problem. Another approach to solve the problem with computational complexity is to track only the first moment of the joint distribution, the probability hypothesis density (PHD). The integral of this distribution over any area S is the expected number of targets within S. Since no record of object identity is kept, the model-data association problem is avoided.   The contribution of this paper is a particle filter implementation of the PHD filter mentioned above. This PHD particle filter is applied to tracking of multiple vehicles in terrain, a non-linear tracking problem. Experiments show that the filter can track a changing number of vehicles robustly, achieving near-real-time performance.
cs.AI:Search in cyclic AND/OR graphs was traditionally known to be an unsolved problem. In the recent past several important studies have been reported in this domain. In this paper, we have taken a fresh look at the problem. First, a new and comprehensive theoretical framework for cyclic AND/OR graphs has been presented, which was found missing in the recent literature. Based on this framework, two best-first search algorithms, S1 and S2, have been developed. S1 does uninformed search and is a simple modification of the Bottom-up algorithm by Martelli and Montanari. S2 performs a heuristically guided search and replicates the modification in Bottom-up's successors, namely HS and AO*. Both S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles. We then present a detailed analysis for the correctness and complexity results of S1 and S2, using the proposed framework. We have observed through experiments that S1 and S2 output correct results in all cases.
cs.AI:Thomas M. Strat has developed a decision-theoretic apparatus for Dempster-Shafer theory (Decision analysis using belief functions, Intern. J. Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility intervals are constructed for different choices. The choice with the highest expected utility is preferable to others. However, to find the preferred choice when the expected utility interval of one choice is included in that of another, it is necessary to interpolate a discerning point in the intervals. This is done by the parameter rho, defined as the probability that the ambiguity about the utility of every nonsingleton focal element will turn out as favorable as possible. If there are several different decision makers, we might sometimes be more interested in having the highest expected utility among the decision makers rather than only trying to maximize our own expected utility regardless of choices made by other decision makers. The preference of each choice is then determined by the probability of yielding the highest expected utility. This probability is equal to the maximal interval length of rho under which an alternative is preferred. We must here take into account not only the choices already made by other decision makers but also the rational choices we can assume to be made by later decision makers. In Strats apparatus, an assumption, unwarranted by the evidence at hand, has to be made about the value of rho. We demonstrate that no such assumption is necessary. It is sufficient to assume a uniform probability distribution for rho to be able to discern the most preferable choice. We discuss when this approach is justifiable.
cs.AI:Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete. This is a fundamental problem in general, and of particular interest for Bayesian networks. Recently, Grunwald and Halpern have shown that commonly used updating strategies fail in this case, except under very special assumptions. In this paper we propose a new method for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no assumptions about the so-called incompleteness mechanism that associates complete with incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we use only coherence arguments to turn prior into posterior probabilities. In general, this new approach to updating produces lower and upper posterior probabilities and expectations, as well as partially determinate decisions. This is a logical consequence of the existing ignorance about the incompleteness mechanism. We apply the new approach to the problem of classification of new evidence in probabilistic expert systems, where it leads to a new, so-called conservative updating rule. In the special case of Bayesian networks constructed using expert knowledge, we provide an exact algorithm for classification based on our updating rule, which has linear-time complexity for a class of networks wider than polytrees. This result is then extended to the more general framework of credal networks, where computations are often much harder than with Bayesian nets. Using an example, we show that our rule appears to provide a solid basis for reliable updating with incomplete observations, when no strong assumptions about the incompleteness mechanism are justified.
cs.AI:As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a ``naive space'', which does not take into account the protocol used, can often lead to counterintuitive results. Here we examine why. A criterion known as CAR (``coarsening at random'') in the statistical literature characterizes when ``naive'' conditioning in a naive space works. We show that the CAR condition holds rather infrequently, and we provide a procedural characterization of it, by giving a randomized algorithm that generates all and only distributions for which CAR holds. This substantially extends previous characterizations of CAR. We also consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE). We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, and show that there exist some very simple settings in which MRE essentially never gives the right results. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.
cs.AI:Configuring consists in simulating the realization of a complex product from a catalog of component parts, using known relations between types, and picking values for object attributes. This highly combinatorial problem in the field of constraint programming has been addressed with a variety of approaches since the foundation system R1(McDermott82). An inherent difficulty in solving configuration problems is the existence of many isomorphisms among interpretations. We describe a formalism independent approach to improve the detection of isomorphisms by configurators, which does not require to adapt the problem model. To achieve this, we exploit the properties of a characteristic subset of configuration problems, called the structural sub-problem, which canonical solutions can be produced or tested at a limited cost. In this paper we present an algorithm for testing the canonicity of configurations, that can be added as a symmetry breaking constraint to any configurator. The cost and efficiency of this canonicity test are given.
cs.AI:This article introduces the idea that probabilistic reasoning (PR) may be understood as "information compression by multiple alignment, unification and search" (ICMAUS). In this context, multiple alignment has a meaning which is similar to but distinct from its meaning in bio-informatics, while unification means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic.   A software model, SP61, has been developed for the discovery and formation of 'good' multiple alignments, evaluated in terms of information compression. The model is described in outline.   Using examples from the SP61 model, this article describes in outline how the ICMAUS framework can model various kinds of PR including: PR in best-match pattern recognition and information retrieval; one-step 'deductive' and 'abductive' PR; inheritance of attributes in a class hierarchy; chains of reasoning (probabilistic decision networks and decision trees, and PR with 'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with default values; modelling the function of a Bayesian network.
cs.AI:This article presents an overview of the idea that "information compression by multiple alignment, unification and search" (ICMAUS) may serve as a unifying principle in computing (including mathematics and logic) and in such aspects of human cognition as the analysis and production of natural language, fuzzy pattern recognition and best-match information retrieval, concept hierarchies with inheritance of attributes, probabilistic reasoning, and unsupervised inductive learning. The ICMAUS concepts are described together with an outline of the SP61 software model in which the ICMAUS concepts are currently realised. A range of examples is presented, illustrated with output from the SP61 model.
cs.AI:We propose a calculus integrating two calculi well-known in Qualitative Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus, and a coarser version of Freksa's relative orientation calculus. An original constraint propagation procedure is presented, which implements the interaction between the two integrated calculi. The importance of taking into account the interaction is shown with a real example providing an inconsistent knowledge base, whose inconsistency (a) cannot be detected by reasoning separately about each of the two components of the knowledge, just because, taken separately, each is consistent, but (b) is detected by the proposed algorithm, thanks to the interaction knowledge propagated from each of the two compnents to the other.
cs.AI:We define a ternary Relation Algebra (RA) of relative position relations on two-dimensional directed lines (d-lines for short). A d-line has two degrees of freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The representation of the RDF of a d-line will be handled by an RA of 2D orientations, CYC_t, known in the literature. A second algebra, TA_t, which will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and TA_t, will constitute, respectively, the translational and the rotational components of the RA, PA_t, of relative position relations on d-lines: the PA_t atoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that are compatible. We present in detail the RA PA_t, with its converse table, its rotation table and its composition tables. We show that a (polynomial) constraint propagation algorithm, known in the literature, is complete for a subset of PA_t relations including almost all of the atomic relations. We will discuss the application scope of the RA, which includes incidence geometry, GIS (Geographic Information Systems), shape representation, localisation in (multi-)robot navigation, and the representation of motion prepositions in NLP (Natural Language Processing). We then compare the RA to existing ones, such as an algebra for reasoning about rectangles parallel to the axes of an (orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval algebra, and an algebra for reasoning about 2D segments.
cs.AI:An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, it will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases, that can include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly {from} the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other non-deductive reasoning systems.
cs.AI:This paper describes an approach to the representation and processing of ontologies in the Semantic Web, based on the ICMAUS theory of computation and AI. This approach has strengths that complement those of languages based on the Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main benefits of the ICMAUS approach are simplicity and comprehensibility in the representation of ontologies, an ability to cope with errors and uncertainties in knowledge, and a versatile reasoning system with capabilities in the kinds of probabilistic reasoning that seem to be required in the Semantic Web.
cs.AI:Interactions are patterns between several attributes in data that cannot be inferred from any subset of these attributes. While mutual information is a well-established approach to evaluating the interactions between two attributes, we surveyed its generalizations as to quantify interactions between several attributes. We have chosen McGill's interaction information, which has been independently rediscovered a number of times under various names in various disciplines, because of its many intuitively appealing properties. We apply interaction information to visually present the most important interactions of the data. Visualization of interactions has provided insight into the structure of data on a number of domains, identifying redundant attributes and opportunities for constructing new features, discovering unexpected regularities in data, and have helped during construction of predictive models; we illustrate the methods on numerous examples. A machine learning method that disregards interactions may get caught in two traps: myopia is caused by learning algorithms assuming independence in spite of interactions, whereas fragmentation arises from assuming an interaction in spite of independence.
cs.AI:In this paper we develop an evidential force aggregation method intended for classification of evidential intelligence into recognized force structures. We assume that the intelligence has already been partitioned into clusters and use the classification method individually in each cluster. The classification is based on a measure of fitness between template and fused intelligence that makes it possible to handle intelligence reports with multiple nonspecific and uncertain propositions. With this measure we can aggregate on a level-by-level basis, starting from general intelligence to achieve a complete force structure with recognized units on all hierarchical levels.
cs.AI:Article discusses the application of Kullback-Leibler divergence to the recognition of speech signals and suggests three algorithms implementing this divergence criterion: correlation algorithm, spectral algorithm and filter algorithm. Discussion covers an approach to the problem of speech variability and is illustrated with the results of experimental modeling of speech signals. The article gives a number of recommendations on the choice of appropriate model parameters and provides a comparison to some other methods of speech recognition.
cs.AI:Richard Cox [1] set the axiomatic foundations of probable inference and the algebra of propositions. He showed that consistency within these axioms requires certain rules for updating belief. In this paper we use the analogy between probability and utility introduced in [2] to propose an axiomatic foundation for utility inference and the algebra of preferences. We show that consistency within these axioms requires certain rules for updating preference. We discuss a class of utility functions that stems from the axioms of utility inference and show that this class is the basic building block for any general multiattribute utility function. We use this class of utility functions together with the algebra of preferences to construct utility functions represented by logical operations on the attributes.
cs.AI:Recent literature in the last Maximum Entropy workshop introduced an analogy between cumulative probability distributions and normalized utility functions. Based on this analogy, a utility density function can de defined as the derivative of a normalized utility function. A utility density function is non-negative and integrates to unity. These two properties form the basis of a correspondence between utility and probability. A natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values. Maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment, and helps assign utility values based on partial preference information. This paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility.
cs.AI:Abduction, first proposed in the setting of classical logics, has been studied with growing interest in the logic programming area during the last years.   In this paper we study abduction with penalization in the logic programming framework. This form of abductive reasoning, which has not been previously analyzed in logic programming, turns out to represent several relevant problems, including optimization problems, very naturally. We define a formal model for abduction with penalization over logic programs, which extends the abductive framework proposed by Kakas and Mancarella. We address knowledge representation issues, encoding a number of problems in our abductive framework. In particular, we consider some relevant problems, taken from different domains, ranging from optimization theory to diagnosis and planning; their encodings turn out to be simple and elegant in our formalism. We thoroughly analyze the computational complexity of the main problems arising in the context of abduction with penalization from logic programs. Finally, we implement a system supporting the proposed abductive framework on top of the DLV engine. To this end, we design a translation from abduction problems with penalties into logic programs with weak constraints. We prove that this approach is sound and complete.
cs.AI:We study local-search satisfiability solvers for propositional logic extended with cardinality atoms, that is, expressions that provide explicit ways to model constraints on cardinalities of sets. Adding cardinality atoms to the language of propositional logic facilitates modeling search problems and often results in concise encodings. We propose two ``native'' local-search solvers for theories in the extended language. We also describe techniques to reduce the problem to standard propositional satisfiability and allow us to use off-the-shelf SAT solvers. We study these methods experimentally. Our general finding is that native solvers designed specifically for the extended language perform better than indirect methods relying on SAT solvers.
cs.AI:We describe WSAT(cc), a local-search solver for computing models of theories in the language of propositional logic extended by cardinality atoms. WSAT(cc) is a processing back-end for the logic PS+, a recently proposed formalism for answer-set programming.
cs.AI:This paper presents duality between probability distributions and utility functions.
cs.AI:Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge Representation and Reasoning (KRR). \DLP is very expressive in a precise mathematical sense: it allows to express every property of finite structures that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$). Importantly, the \DLP encodings are often simple and natural.   In this paper, we single out some limitations of \DLP for KRR, which cannot naturally express problems where the size of the disjunction is not known ``a priori'' (like N-Coloring), but it is part of the input. To overcome these limitations, we further enhance the knowledge modelling abilities of \DLP, by extending this language by {\em Parametric Connectives (OR and AND)}. These connectives allow us to represent compactly the disjunction/conjunction of a set of atoms having a given property. We formally define the semantics of the new language, named $DLP^{\bigvee,\bigwedge}$ and we show the usefulness of the new constructs on relevant knowledge-based problems. We address implementation issues and discuss related works.
cs.AI:The research field of Agent-Oriented Software Engineering (AOSE) aims to find abstractions, languages, methodologies and toolkits for modeling, verifying, validating and prototyping complex applications conceptualized as Multiagent Systems (MASs). A very lively research sub-field studies how formal methods can be used for AOSE. This paper presents a detailed survey of six logic-based executable agent specification languages that have been chosen for their potential to be integrated in our ARPEGGIO project, an open framework for specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each executable language, the logic foundations are described and an example of use is shown. A comparison of the six languages and a survey of similar approaches complete the paper, together with considerations of the advantages of using logic-based languages in MAS modeling and prototyping.
cs.AI:We propose a generalization of expected utility that we call generalized EU (GEU), where a decision maker's beliefs are represented by plausibility measures, and the decision maker's tastes are represented by general (i.e.,not necessarily real-valued) utility functions. We show that every agent, ``rational'' or not, can be modeled as a GEU maximizer. We then show that we can customize GEU by selectively imposing just the constraints we want. In particular, we show how each of Savage's postulates corresponds to constraints on GEU.
cs.AI:Many different rules for decision making have been introduced in the literature. We show that a notion of generalized expected utility proposed in Part I of this paper is a universal decision rule, in the sense that it can represent essentially all other decision rules.
cs.AI:This paper describes a novel approach to grammar induction that has been developed within a framework designed to integrate learning with other aspects of computing, AI, mathematics and logic. This framework, called "information compression by multiple alignment, unification and search" (ICMAUS), is founded on principles of Minimum Length Encoding pioneered by Solomonoff and others. Most of the paper describes SP70, a computer model of the ICMAUS framework that incorporates processes for unsupervised learning of grammars. An example is presented to show how the model can infer a plausible grammar from appropriate input. Limitations of the current model and how they may be overcome are briefly discussed.
cs.AI:We consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations, well-known in QSR. The more general, integrating language we consider is based on convex constraints of the qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$, with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning of the quantitative constraint, in particular, is that point $x$ belongs to the (convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and $\beta$. The general form of a constraint is a disjunction of the form $[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha _{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha _i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above: the meaning of such a general constraint is that, for some $i=1... n_1$, $r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A conjunction of such general constraints is a $\tcsp$-like CSP, which we will refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective solution search algorithm for an $\scsp$ will be described, which uses (1) constraint propagation, based on a composition operation to be defined, as the filtering method during the search, and (2) the Simplex algorithm, guaranteeing completeness, at the leaves of the search tree. The approach is particularly suited for large-scale high-level vision, such as, e.g., satellite-like surveillance of a geographic area.
cs.AI:Object oriented constraint programs (OOCPs) emerge as a leading evolution of constraint programming and artificial intelligence, first applied to a range of industrial applications called configuration problems. The rich variety of technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP, Terminological systems, constraint programs with set variables ...) is a source of difficulty. No universally accepted formal language exists for communicating about OOCPs, which makes the comparison of systems difficult. We present here a Z based specification of OOCPs which avoids the falltrap of hidden object semantics. The object system is part of the specification, and captures all of the most advanced notions from the object oriented modeling standard UML. The paper illustrates these issues and the conciseness and precision of Z by the specification of a working OOCP that solves an historical AI problem : parsing a context free grammar. Being written in Z, an OOCP specification also supports formal proofs. The whole builds the foundation of an adaptative and evolving framework for communicating about constrained object models and programs.
cs.AI:In this paper we suggest an architecture for a software agent which operates a physical device and is capable of making observations and of testing and repairing the device's components. We present simplified definitions of the notions of symptom, candidate diagnosis, and diagnosis which are based on the theory of action language ${\cal AL}$. The definitions allow one to give a simple account of the agent's behavior in which many of the agent's tasks are reduced to computing stable models of logic programs.
cs.AI:We compare two recent extensions of the answer set (stable model) semantics of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the bodies and heads of rules to contain nested expressions. The other, due to Niemela and Simons, uses weight constraints. We show that there is a simple, modular translation from the language of weight constraints into the language of nested expressions that preserves the program's answer sets. Nested expressions can be eliminated from the result of this translation in favor of additional atoms. The translation makes it possible to compute answer sets for some programs with weight constraints using satisfiability solvers, and to prove the strong equivalence of programs with weight constraints using the logic of here-and there.
cs.AI:(We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend Schlipf's result to prove that SLP solves all search problems in the class $\mathit{NP}$. Moreover, we do this in a uniform way as defined in \cite{mt99}. Specifically, we show that there is a single $\mathrm{DATALOG}^{\neg}$ program $P_{\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$ with non-negative integer coefficients and any input $\sigma$ of size $n$ over a fixed alphabet $\Sigma$, there is an extensional database $\mathit{edb}_{M,p,\sigma}$ such that there is a one-to-one correspondence between the stable models of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ and the accepting computations of the machine $M$ that reach the final state in at most $p(n)$ steps. Moreover, $\mathit{edb}_{M,p,\sigma}$ can be computed in polynomial time from $p$, $\sigma$ and the description of $M$ and the decoding of such accepting computations from its corresponding stable model of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ can be computed in linear time. A similar statement holds for Default Logic with respect to $\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result involves additional technical complications and will be a subject of another publication.}.
cs.AI:This book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as "information compression by multiple alignment, unification and search". This "SP theory", which has been under development since 1987, provides a unified view of such things as the workings of a universal Turing machine, the nature of 'knowledge', the interpretation and production of natural language, pattern recognition and best-match information retrieval, several kinds of probabilistic reasoning, planning and problem solving, unsupervised learning, and a range of concepts in mathematics and logic. The theory also provides a basis for the design of an 'SP' computer with several potential advantages compared with traditional digital computers.
cs.AI:In rule-based systems, goal-oriented computations correspond naturally to the possible ways that an observation may be explained. In some applications, we need to compute explanations for a series of observations with the same domain. The question whether previously computed answers can be recycled arises. A yes answer could result in substantial savings of repeated computations. For systems based on classic logic, the answer is YES. For nonmonotonic systems however, one tends to believe that the answer should be NO, since recycling is a form of adding information. In this paper, we show that computed answers can always be recycled, in a nontrivial way, for the class of rewrite procedures that we proposed earlier for logic programs with negation. We present some experimental results on an encoding of the logistics domain.
cs.AI:Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. This framework can be successfully applied to the investigation of the performance of Artificial Intelligence (AI) inference and cognitive systems. In this paper, we delineate a categorical formalisation of memory as a control structure driving performance in inference systems. Abstracting away control mechanisms from three widely used representations of memory in cognitive systems (scripts, production rules and clusters) we explain how categorical triples capture the interaction between learning and problem-solving.
cs.AI:The field of machine learning (ML) is concerned with the question of how to construct algorithms that automatically improve with experience. In recent years many successful ML applications have been developed, such as datamining programs, information-filtering systems, etc. Although ML algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems, most of these algorithms are based on quantitative reasoning, as they rely on training data in order to infer so-called target functions.   In the last years defeasible argumentation has proven to be a sound setting to formalize common-sense qualitative reasoning. This approach can be combined with other inference techniques, such as those provided by machine learning theory.   In this paper we outline different alternatives for combining defeasible argumentation and machine learning techniques. We suggest how different aspects of a generic argument-based framework can be integrated with other ML-based approaches.
cs.AI:Stable model semantics has become a very popular approach for the management of negation in logic programming. This approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called Gelfond-Lifschitz transformation.   The primary goal of this work is to present an alternative and epistemic-based characterization of stable model semantics, to the Gelfond-Lifschitz transformation. In particular, we show that stable model semantics can be defined entirely as an extension of the Kripke-Kleene semantics. Indeed, we show that the closed world assumption can be seen as an additional source of `falsehood' to be added cumulatively to the Kripke-Kleene semantics. Our approach is purely algebraic and can abstract from the particular formalism of choice as it is based on monotone operators (under the knowledge order) over bilattices only.
cs.AI:We address the problem of the development of representations and their relationship to the environment. We study a software agent which develops in a network a representation of its simple environment which captures and integrates the relationships between agent and environment through a closure mechanism. The inclusion of a variable behavior modifier allows better representation development. This can be confirmed with an internal description of the closure mechanism, and with an external description of the properties of the representation network.
cs.AI:This document describes syntax, semantics and implementation guidelines in order to enrich the DLV system with the possibility to make external C function calls. This feature is realized by the introduction of parametric external predicates, whose extension is not specified through a logic program but implicitly computed through external code.
cs.AI:This document describes the functions as they are treated in the DLV system. We give first the language, then specify the main implementation issues.
cs.AI:We introduce Ak, an extension of the action description language A (Gelfond and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. We also present a translation of Ak domain descriptions into epistemic logic programs.
cs.AI:In this paper, we examine the performance of four fuzzy rule generation methods on Wisconsin breast cancer data. The first method generates fuzzy if then rules using the mean and the standard deviation of attribute values. The second approach generates fuzzy if then rules using the histogram of attributes values. The third procedure generates fuzzy if then rules with certainty of each attribute into homogeneous fuzzy sets. In the fourth approach, only overlapping areas are partitioned. The first two approaches generate a single fuzzy if then rule for each class by specifying the membership function of each antecedent fuzzy set using the information about attribute values of training patterns. The other two approaches are based on fuzzy grids with homogeneous fuzzy partitions of each attribute. The performance of each approach is evaluated on breast cancer data sets. Simulation results show that the Modified grid approach has a high classification rate of 99.73 %.
cs.AI:The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.
cs.AI:Neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems. This paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on Box--Jenkins autoregressive integrated moving average (ARIMA) model to predict electricity demand in the State of Victoria, Australia. The soft computing methods considered are an evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN) trained using scaled conjugate gradient algorithm (CGA) and backpropagation (BP) algorithm. The forecast accuracy is compared with the forecasts used by Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models. Test results show that the neuro-fuzzy system performed better than neural networks, ARIMA model and the VPX forecasts.
cs.AI:Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model.
q-bio.BM:We consider the regime in which the bands of the torsional acoustic (TA) and the hydrogen-bond-stretch (HBS) modes of the DNA interpenetrate each other. Within the framework of a model that accommodates the structure of the double helix, we find the three-wave interaction between the TA- and the HBS-modes, and show that microwave radiation could bring about torsional vibrations that could serve as a pump mode for maintaining the HBS-one. Rayleigh's threshold condition for the parametric resonance provides an estimate for the power density of the mw-field necessary for generating the HBS-mode.
q-bio.BM:Identifying the driving forces and the mechanism of association of huntingtin-exon1, a close marker for the progress of Huntington's disease, is an important prerequisite towards finding potential drug targets, and ultimately a cure. We introduce here a modelling framework based on a key analogy of the physico-chemical properties of the exon1 fragment to block copolymers. We use a systematic mesoscale methodology, based on Dissipative Particle Dynamics, which is capable of overcoming kinetic barriers, thus capturing the dynamics of significantly larger systems over longer times than considered before. Our results reveal that the relative hydrophobicity of the poly-glutamine block as compared to the rest of the (proline-based) exon1 fragment, ignored to date, constitutes a major factor in the initiation of the self-assembly process. We find that the assembly is governed by both the concentration of exon1 and the length of the poly-glutamine stretch, with a low length threshold for association even at the lowest volume fractions we considered. Moreover, this self-association occurs irrespective of whether the glutamine stretch is in random coil or hairpin configuration, leading to spherical or cylindrical assemblies, respectively. We discuss the implications of these results for reinterpretation of existing research within this context, including that the routes towards aggregation of exon1 may be distinct to those of the widely studied homopolymeric poly-glutamine peptides.
q-bio.BM:The molecular mechanism of the solvent motion that is required to instigate the protein structural relaxation above a critical hydration level or transition temperature has yet to be determined. In this work we use quasi-elastic neutron scattering (QENS) and molecular dynamics simulation to investigate hydration water dynamics near a greatly simplified protein surface. We consider the hydration water dynamics near the completely deuterated N-acetyl-leucine-methylamide (NALMA) solute, a hydrophobic amino acid side chain attached to a polar blocked polypeptide backbone, as a function of concentration between 0.5M-2.0M, under ambient conditions. In this Communication, we focus our results of hydration dynamics near a model protein surface on the issue of how enzymatic activity is restored once a critical hydration level is reached, and provide a hypothesis for the molecular mechanism of the solvent motion that is required to trigger protein structural relaxation when above the hydration transition.
q-bio.BM:We analyze the dependence of thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, N. Using lattice Go models we show that DeltaT/T_F ~ N^-1, where T_F is the folding transition temperature and DeltaT is the folding transition width. This finding is consistent with finite size effects expected for the systems undergoing a phase transition from a disordered to an ordered phase. The dependence of the folding rates k_F on N for lattice models and the dataset of 57 proteins and peptides shows that k_F = k_F^0 exp(-CN^beta) provides a good fit, if 0 < beta <= 2/3 and C is a constant. We find that k_F = k_F^0 exp(-1.1N^0.5) with k_F^0 =(0.4x10^-6 s)^-1 can estimate optimal protein folding rates to within an order of magnitude in most cases. By using this fit for a set of proteins with beta-sheet topology we find that k_F^0 is approximately equal to k_U^0, the prefactor for unfolding rates. The maximum ratio of k_U^0/k_F^0 is 10 for this class of proteins.
q-bio.BM:The asymmetry in the shapes of folded and unfolded states are probed using two parameters, one being a measure of the sphericity and the other that describes the shape. For the folded states, whose interiors are densely packed, the radii of gyration (Rg) and these two parameters are calculated using the coordinates of the experimentally determined structures. Although Rg scales as expected for maximally compact structures, the distributions of the shape parameters show that there is considerable asymmetry in the shapes of folded structures. The degree of asymmetry is greater for proteins that form oligomers. Analysis of the two- and three-body contacts in the native structures shows that the presence of near equal number of contacts between backbone and side-chains and between side-chains gives rise to dense packing. We suggest that proteins with relatively large values of shape parameters can tolerate volume mutations without greatly affecting the network of contacts or their stability. To probe shape characteristics of denatured states we have developed a model of a WW-like domain. The shape parameters, which are calculated using Langevin simulations, change dramatically in the course of coil to globule transition. Comparison of the values of shape parameters between the globular state and the folded state of WW domain shows that both energetic (especially dispersion in the hydrophobic interactions) and steric effects are important in determining packing in proteins.
q-bio.BM:Instead of conformation states of single residues, refined conformation states of quintuplets are proposed to reflect conformation correlation. Simple hidden Markov models combining with sliding window scores are used for predicting secondary structure of a protein from its amino acid sequence. Since the length of protein conformation segments varies in a narrow range, we ignore the duration effect of the length distribution. The window scores for residues are a window version of the Chou-Fasman propensities estimated under an approximation of conditional independency. Different window widths are examined, and the optimal width is found to be 17. A high accuracy about 70% is achieved.
q-bio.BM:Is protein secondary structure primarily determined by local interactions between residues closely spaced along the amino acid backbone, or by non-local tertiary interactions? To answer this question we have measured the entropy densities of primary structure and secondary structure sequences, and the local inter-sequence mutual information density. We find that the important inter-sequence interactions are short ranged, that correlations between neighboring amino acids are essentially uninformative, and that only 1/4 of the total information needed to determine the secondary structure is available from local inter-sequence correlations. Since the remaining information must come from non-local interactions, this observation supports the view that the majority of most proteins fold via a cooperative process where secondary and tertiary structure form concurrently. To provide a more direct comparison to existing secondary structure prediction methods, we construct a simple hidden Markov model (HMM) of the sequences. This HMM achieves a prediction accuracy comparable to other single sequence secondary structure prediction algorithms, and can extract almost all of the inter-sequence mutual information. This suggests that these algorithms are almost optimal, and that we should not expect a dramatic improvement in prediction accuracy. However, local correlations between secondary and primary structure are probably of under-appreciated importance in many tertiary structure prediction methods, such as threading.
q-bio.BM:The determination of the folding mechanisms of proteins is critical to understand the topological change that can propagate Alzheimer and Creutzfeld-Jakobs diseases, among others. The computational community has paid considerable attention to this problem; however, the associated time scale, typically on the order of milliseconds or more, represents a formidable challenge. Ab initio protein folding from long molecular dynamics (MD) simulations or ensemble dynamics is not feasible with ordinary computing facilities and new techniques must be introduced. Here we present a detailed study of the folding of a 16-residue beta-hairpin, described by a generic energy model and using the activation-relaxation technique. From a total of 90 trajectories at 300 K, three folding pathways emerge. All involve a simultaneous optimization of the complete hydrophobic and hydrogen bonding interactions. The first two follow closely those observed by previous theoretical studies. The third pathway, never observed by previous all-atom folding, unfolding and equilibrium simulations, can be described as a reptation move of one strand of the beta-sheet with respect to the other. This reptation move indicates that non-native interactions can play a dominant role in the folding of secondary structures. These results point to a more complex folding picture than expected for a simple beta-hairpin.
q-bio.BM:Analytic estimates for the forces and free energy generated by bilayer deformation reveal a compelling and intuitive model for MscL channel gating analogous to the nucleation of a second phase. We argue that the competition between hydrophobic mismatch and tension results in a surprisingly rich story which can provide both a quantitative comparison to measurements of opening tension for MscL when reconstituted in bilayers of different thickness and qualitative insights into the function of the MscL channel and other transmembrane proteins.
q-bio.BM:It is important to understand how protein folding and evolution influences each other. Several studies based on entropy calculation correlating experimental measurement of residue participation in folding nucleus and sequence conservation have reached different conclusions. Here we report analysis of conservation of folding nucleus using an evolutionary model alternative to entropy based approaches. We employ a continuous time Markov model of codon substitution to distinguish mutation fixed by evolution and mutation fixed by chance. This model takes into account bias in codon frequency, bias favoring transition over transversion, as well as explicit phylogenetic information. We measure selection pressure using the ratio $\omega$ of synonymous vs. non-synonymous substitution at individual residue site. The $\omega$-values are estimated using the {\sc Paml} method, a maximum-likelihood estimator. Our results show that there is little correlation between the extent of kinetic participation in protein folding nucleus as measured by experimental $\phi$-value and selection pressure as measured by $\omega$-value. In addition, two randomization tests failed to show that folding nucleus residues are significantly more conserved than the whole protein. These results suggest that at the level of codon substitution, there is no indication that folding nucleus residues are significantly more conserved than other residues. We further reconstruct candidate ancestral residues of the folding nucleus and suggest possible test tube mutation studies of ancient folding nucleus.
q-bio.BM:Many signalling functions in molecular biology require proteins bind to substrates such as DNA in response to environmental signals such as the simultaneous binding to a small molecule. Examples are repressor proteins which may transmit information via a conformational change in response to the ligand binding. An alternative entropic mechanism of ``allostery'' suggests that the inducer ligand changes the intramolecular vibrational entropy not just the static structure. We present a quantitative, coarse-grained model of entropic allostery that suggests design rules for internal cohesive potentials in proteins employing this effect. It also addresses the issue of how the signal information to bind or unbind is transmitted through the protein. The model may be applicable to a wide range of repressors and also to signalling in transmembrane proteins.
q-bio.BM:How DNA repair enzymes find the relatively rare sites of damage is not known in great detail. Recent experiments and molecular data suggest that the individual repair enzymes do not work independently of each other, but rather interact with each other through currents exchanged along DNA. A damaged site in DNA hinders this exchange and this makes it possible to quickly free up resources from error free stretches of DNA. Here the size of the speedup gained from this current exchange mechanism is calculated and the characteristic length and time scales are identified. In particular for Escherichia coli we estimate the speedup to be 50000/N, where N is the number of repair enzymes participating in the current exchange mechanism. Even though N is not exactly known a speedup of order 10 is not entirely unreasonable. Furthermore upon over expression of repair enzymes the detection time only varies as one over the squareroot of N and not as 1/N. This behavior is of interest in assessing the impact of stress full and radioactive environments on individual cell mutation rates.
q-bio.BM:We study DNA adsorption and renaturation in a water-phenol two-phase system, with or without shaking. In very dilute solutions, single-stranded DNA is adsorbed at the interface in a salt-dependent manner. At high salt concentrations the adsorption is irreversible. The adsorption of the single-stranded DNA is specific to phenol and relies on stacking and hydrogen bonding. We establish the interfacial nature of a DNA renaturation at a high salt concentration. In the absence of shaking, this reaction involves an efficient surface diffusion of the single-stranded DNA chains. In the presence of a vigorous shaking, the bimolecular rate of the reaction exceeds the Smoluchowski limit for a three-dimensional diffusion-controlled reaction. DNA renaturation in these conditions is known as the Phenol Emulsion Reassociation Technique or PERT. Our results establish the interfacial nature of PERT. A comparison of this interfacial reaction with other approaches shows that PERT is the most efficient technique and reveals similarities between PERT and the renaturation performed by single-stranded nucleic acid binding proteins. Our results lead to a better understanding of the partitioning of nucleic acids in two-phase systems, and should help design improved extraction procedures for damaged nucleic acids. We present arguments in favor of a role of phenol and water-phenol interface in prebiotic chemistry. The most efficient renaturation reactions (in the presence of condensing agents or with PERT) occur in heterogeneous systems. This reveals the limitations of homogeneous approaches to the biochemistry of nucleic acids. We propose a heterogeneous approach to overcome the limitations of the homogeneous viewpoint.
q-bio.BM:Hydrophobicity is thought to be one of the primary forces driving the folding of proteins. On average, hydrophobic residues occur preferentially in the core, whereas polar residues tends to occur at the surface of a folded protein. By analyzing the known protein structures, we quantify the degree to which the hydrophobicity sequence of a protein correlates with its pattern of surface exposure. We have assessed the statistical significance of this correlation for several hydrophobicity scales in the literature, and find that the computed correlations are significant but far from optimal. We show that this less than optimal correlation arises primarily from the large degree of mutations that naturally occurring proteins can tolerate. Lesser effects are due in part to forces other than hydrophobicity and we quantify this by analyzing the surface exposure distributions of all amino acids. Lastly we show that our database findings are consistent with those found from an off-lattice hydrophobic-polar model of protein folding.
q-bio.BM:The approach for the description of the DNA conformational transformations on the mesoscopic scales in the frame of the double helix is presented. Due to consideration of the joint motions of DNA structural elements along the conformational pathways the models for different transformations may be constructed in the unifying two-component form. One component of the model is the degree of freedom of the elastic rod and another component -- the effective coordinate of the conformational transformation. The internal and external model components are interrelated, as it is characteristic for the DNA structure organization. It is shown that the kinetic energy of the conformational transformation of heterogeneous DNA may be put in homogeneous form. In the frame of the developed approach the static excitations of the DNA structure under the transitions between the stable states are found for internal and external components. The comparison of the data obtained with the experiment on intrinsic DNA deformability shows good qualitative agreement. The conclusion is made that the found excitations in the DNA structure may be classificated as the static conformational solitons.
q-bio.BM:Molecular combing is a powerful and simple method for aligning DNA molecules onto a surface. Using this technique combined with fluorescence microscopy, we observed that the length of lambda-DNA molecules was extended to about 1.6 times their contour length (unextended length, 16.2 micrometers) by the combing method on hydrophobic polymethylmetacrylate (PMMA) coated surfaces. The effects of sodium and magnesium ions and pH of the DNA solution were investigated. Interestingly, we observed force-induced melting of single DNA molecules.
q-bio.BM:Using a Brownian dynamics simulation, we numerically studied the interaction of DNA with histone and proposed an octamer-rotation model to describe the process of nucleosome formation. Nucleosome disruption under stretching was also simulated. The theoretical curves of extension versus time as well as of force versus extension are consistent with previous experimental results.
q-bio.BM:We propose a two-dimensional model for a complete description of the dynamics of molecular motors, including both the processive movement along track filaments and the dissociation from the filaments. The theoretical results on the distributions of the run length and dwell time at a given ATP concentration, the dependences of mean run length, mean dwell time and mean velocity on ATP concentration and load are in good agreement with the previous experimental results.
q-bio.BM:Kinesin motors have been studied extensively both experimentally and theoretically. However, the microscopic mechanism of the processive movement of kinesin is still an open question. In this paper, we propose a hand-over-hand model for the processivity of kinesin, which is based on chemical, mechanical, and electrical couplings. In the model the processive movement does not need to rely on the two heads' coordination in their ATP hydrolysis and mechanical cycles. Rather, the ATP hydrolyses at the two heads are independent. The much higher ATPase rate at the trailing head than the leading head makes the motor walk processively in a natural way, with one ATP being hydrolyzed per step. The model is consistent with the structural study of kinesin and the measured pathway of the kinesin ATPase. Using the model the estimated driving force of ~ 5.8 pN is in agreements with the experimental results (5~7.5 pN). The prediction of the moving time in one step (~10 microseconds) is also consistent with the measured values of 0~50 microseconds. The previous observation of substeps within the 8-nm step is explained. The shapes of velocity-load (both positive and negative) curves show resemblance to previous experimental results.
q-bio.BM:Myosin V and myosin VI are two classes of two-headed molecular motors of the myosin superfamily that move processively along helical actin filaments in opposite directions. Here we present a hand-over-hand model for their processive movements. In the model, the moving direction of a dimeric molecular motor is automatically determined by the relative orientation between its two heads at free state and its head's binding orientation on track filament. This determines that myosin V moves toward the barbed end and myosin VI moves toward the pointed end of actin. During the moving period in one step, one head remains bound to actin for myosin V whereas two heads are detached for myosin VI: The moving manner is determined by the length of neck domain. This naturally explains the similar dynamic behaviors but opposite moving directions of myosin VI and mutant myosin V (the neck of which is truncated to only one-sixth of the native length). Because of different moving manners, myosin VI and mutant myosin V exhibit significantly broader step-size distribution than native myosin V. However, all three motors give the same mean step size of 36 nm (the pseudo-repeat of actin helix). Using the model we study the dynamics of myosin V quantitatively, with theoretical results in agreement with previous experimental ones.
q-bio.BM:We describe a faster and more accurate algorithm for computing the statistical mechanics of DNA denaturation according to the Poland-Scheraga type. Nearest neighbor thermodynamics is included in a complete and general way. The algorithm represents an optimization with respect to algorithmic complexity of the partition function algorithm of Yeramian et al.: We reduce the computation time for a base-pairing probability profile from O(N2) to O(N). This speed-up comes in addition to the speed-up due to a multiexponential approximation of the loop entropy factor as introduced by Fixman and Freire. The speed-up, however, is independent of the multiexponential approximation and reduces time from O(N3) to O(N2) in the exact case. In addition to calculating the standard base-pairing probability profiles, we propose to use the algorithm to calculate various other probabilities (loops, helices, tails) for a more direct view of the melting regions and their positions and sizes.
q-bio.BM:A joint experimental / theoretical investigation of the elastin-like octapeptide GVG(VPGVG) was carried out. In this paper a comprehensive molecular dynamics study of the temperature dependent folding and unfolding of the octapeptide is presented. The current study, as well as its experimental counterpart find that this peptide undergoes an "inverse temperature transition", ITT, leading to a folding at about 310-330 K. In addition, an unfolding transition is identified at unusually high temperatures approaching the boiling point of water. Due to the small size of the system two broad temperature regimes are found: the "ITT regime" (at about 280-320 K) and the "unfolding regime" at about T > 330 K, where the peptide has a maximum probability of being folded at approximately 330 K. A detailed molecular picture involving a thermodynamic order parameter, or reaction coordinate, for this process is presented along with a time-correlation function analysis of the hydrogen bond dynamics within the peptide as well as between the peptide and solvating water molecules. Correlation with experimental evidence and ramifications on the properties of elastin are discussed.
q-bio.BM:A simplified model for the closed circular DNA (ccDNA) is proposed to describe some specific features of the helix-coil transition in such molecule. The Hamiltonian of ccDNA is related to the one introduced earlier for the linear DNA. The basic assumption is that the reduced energy of the hydrogen bond is not constant through the transition process but depends effectively on the fraction of already broken bonds. A transformation formula is obtained which relates the temperature of ccDNA at a given degree of helicity during the transition to the temperature of the corresponding linear chain at the same degree of helicity. The formula provides a simple method to calculate the melting curve for the ccDNA from the experimental melting curve of the linear DNA with the same nucleotide sequence.
q-bio.BM:We develop a simple but rigorous model of protein-protein association kinetics based on diffusional association on free energy landscapes obtained by sampling configurations within and surrounding the native complex binding funnels. Guided by results obtained on exactly solvable model problems, we transform the problem of diffusion in a potential into free diffusion in the presence of an absorbing zone spanning the entrance to the binding funnel. The free diffusion problem is solved using a recently derived analytic expression for the rate of association of asymmetrically oriented molecules. Despite the required high steric specificity and the absence of long-range attractive interactions, the computed rates are typically on the order of 10^4-10^6 M-1 s-1, several orders of magnitude higher than rates obtained using a purely probabilistic model in which the association rate for free diffusion of uniformly reactive molecules is multiplied by the probability of a correct alignment of the two partners in a random collision. As the association rates of many protein-protein complexes are also in the 10^5-10^6 M-1 s-1, our results suggest that free energy barriers arising from desolvation and/or side-chain freezing during complex formation or increased ruggedness within the binding funnel, which are completely neglected in our simple diffusional model, do not contribute significantly to the dynamics of protein-protein association. The transparent physical interpretation of our approach that computes association rates directly from the size and geometry of protein-protein binding funnels makes it a useful complement to Brownian dynamics simulations.
q-bio.BM:Functional proteins must fold with some minimal stability to a structure that can perform a biochemical task. Here we use a simple model to investigate the relationship between the stability requirement and the capacity of a protein to evolve the function of binding to a ligand. Although our model contains no built-in tradeoff between stability and function, proteins evolved function more efficiently when the stability requirement was relaxed. Proteins with both high stability and high function evolved more efficiently when the stability requirement was gradually increased than when there was constant selection for high stability. These results show that in our model, the evolution of function is enhanced by allowing proteins to explore sequences corresponding to marginally stable structures, and that it is easier to improve stability while maintaining high function than to improve function while maintaining high stability. Our model also demonstrates that even in the absence of a fundamental biophysical tradeoff between stability and function, the speed with which function can evolve is limited by the stability requirement imposed on the protein.
q-bio.BM:Using the model for the processive movement of a dimeric kinesin we proposed before, we study the dynamics of a number of mutant homodimeric and heterodimeric kinesins that were constructed by Kaseda et al. (Kaseda, K., Higuchi, H. and Hirose, K. PNAS 99, 16058 (2002)). The theoretical results of ATPase rate per head, moving velocity, and stall force of the motors show good agreement with the experimental results by Kaseda et al.: The puzzling dynamic behaviors of heterodimeric kinesin that consists of two distinct heads compared with its parent homodimers can be easily explained by using independent ATPase rates of the two heads in our model. We also study the collective kinetic behaviors of kinesins in MT-gliding motility. The results explains well that the average MT-gliding velocity is independent of the number of bound motors and is equal to the moving velocity of a single kinesin relative to MT.
q-bio.BM:The simplest approximation of interaction potential between amino-acids in proteins is the contact potential, which defines the effective free energy of a protein conformation by a set of amino acid contacts formed in this conformation. Finding a contact potential capable of predicting free energies of protein states across a variety of protein families will aid protein folding and engineering in silico on a computationally tractable time-scale. We test the ability of contact potentials to accurately and transferably (across various protein families) predict stability changes of proteins upon mutations. We develop a new methodology to determine the contact potentials in proteins from experimental measurements of changes in protein thermodynamic stabilities (ddG) upon mutations. We apply our methodology to derive sets of contact interaction parameters for a hierarchy of interaction models including solvation and multi-body contact parameters. We test how well our models reproduce experimental measurements by statistical tests. We evaluate the maximum accuracy of predictions obtained by using contact potentials and the correlation between parameters derived from different data-sets of experimental ddG values. We argue that it is impossible to reach experimental accuracy and derive fully transferable contact parameters using the contact models of potentials. However, contact parameters can yield reliable predictions of ddG for datasets of mutations confined to specific amino-acid positions in the sequence of a single protein.
q-bio.BM:We first review how to determine the rate of vibrational energy relaxation (VER) using perturbation theory. We then apply those theoretical results to the problem of VER of a CD stretching mode in the protein cytochrome c. We model cytochrome c in vacuum as a normal mode system with the lowest-order anharmonic coupling elements. We find that, for the ``lifetime'' width parameter $\gamma=3 \sim 30$ cm$^{-1}$, the VER time is $0.2 \sim 0.3$ ps, which agrees rather well with the previous classical calculation using the quantum correction factor method, and is consistent with spectroscopic experiments by Romesberg's group. We decompose the VER rate into separate contributions from two modes, and find that the most significant contribution, which depends on the ``lifetime'' width parameter, comes from those modes most resonant with the CD vibrational mode.
q-bio.BM:The three-dimensional structures of two common repeat motifs Val$^1$-Pro$^2$-Gly$^3$-Val$^4$-Gly$^5$ and Val$^1$-Gly$^2$-Val$^3$-Pro$^4$-Gly$^5$-Val$^6$-Gly$^7$-Val$^8$-Pro$^9$ of tropoelastin are investigated by using the multicanonical simulation procedure. By minimizing the energy structures along the trajectory the thermodynamically most stable low-energy microstates of the molecule are determined. The structural predictions are in good agreement with X-ray diffraction experiments.
q-bio.BM:We address the controversial hot question concerning the validity of the loose-coupling versus the lever-arm models in the actomyosin dynamics by re-interpreting and extending the washboard potential model proposed by some of us in a previous paper. In the new theory, a loose-coupling mechanism co-exists with the deterministic lever-arm model. The synergetic action of a random component, originating from the harnessed thermal energy, and of the power-stroke generated by the lever-arm classical mechanism is seen to yield an excellent fit of the set of data obtained in T. Yanagida's laboratory on the sliding of Myosin II heads on actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of theory is tested via different combination of parameters and potential profiles.
q-bio.BM:In simple models side chains are often represented implicitly (e.g., by spin-states) or simplified as one atom. We study side chain effects using square lattice and tetrahedral lattice models, with explicitly side chains of two atoms. We distinguish effects due to chirality and effects due to side chain flexibilities, since residues in proteins are L-residues, and their side chains adopt different rotameric states. Short chains are enumerated exhaustively. For long chains, we sample effectively rare events (eg, compact conformations) and obtain complete pictures of ensemble properties of these models at all compactness region. We find that both chirality and reduced side chain flexibility lower the folding entropy significantly for globally compact conformations, suggesting that they are important properties of residues to ensure fast folding and stable native structure. This corresponds well with our finding that natural amino acid residues have reduced effective flexibility, as evidenced by analysis of rotamer libraries and side chain rotatable bonds. We further develop a method calculating the exact side-chain entropy for a given back bone structure. We show that simple rotamer counting often underestimates side chain entropy significantly, and side chain entropy does not always correlate well with main chain packing. Among compact backbones with maximum side chain entropy, helical structures emerges as the dominating configurations. Our results suggest that side chain entropy may be an important factor contributing to the formation of alpha helices for compact conformations.
q-bio.BM:We show that the contact map of the native structure of globular proteins can be reconstructed starting from the sole knowledge of the contact map's principal eigenvector, and present an exact algorithm for this purpose. Our algorithm yields a unique contact map for all 221 globular structures of PDBselect25 of length $N \le 120$. We also show that the reconstructed contact maps allow in turn for the accurate reconstruction of the three-dimensional structure. These results indicate that the reduced vectorial representation provided by the principal eigenvector of the contact map is equivalent to the protein structure itself. This representation is expected to provide a useful tool in bioinformatics algorithms for protein structure comparison and alignment, as well as a promising intermediate step towards protein structure prediction.
q-bio.BM:Function of proteins or a network of interacting proteins often involves communication between residues that are well separated in sequence. The classic example is the participation of distant residues in allosteric regulation. Bioinformatic and structural analysis methods have been introduced to infer residues that are correlated. Recently, increasing attention has been paid to obtain the sequence properties that determine the tendency of disease related proteins (Abeta peptides, prion proteins, transthyretin etc.) to aggregate and form fibrils. Motivated in part by the need to identify sequence characteristics that indicate a tendency to aggregate, we introduce a general method that probes covariations in charged residues along the sequence in a given protein family. The method, which involves computing the Sequence Correlation Entropy (SCE) using the quenched probability Psk(i,j) of finding a residue pair at a given sequence separation sk, allows us to classify protein families in terms of their SCE. Our general approach may be a useful way in obtaining evolutionary covariations of amino acid residues on a genome wide level.
q-bio.BM:We present an analysis of the effects of global topology on the structural stability of folded proteins in thermal equilibrium with a heat bath. For a large class of single domain proteins, we computed the harmonic spectrum within the Gaussian Network Model (GNM) and determined the spectral dimension, a parameter describing the low frequency behaviour of the density of modes. We find a surprisingly strong correlation between the spectral dimension and the number of amino acids of the protein. Considering that larger spectral dimension value relate to more topologically compact folded state, our results indicate that for a given temperature and length of the protein, the folded structure corresponds to the less compact folding compatible with thermodynamic stability.
q-bio.BM:We present a simple physical model which demonstrates that the native state folds of proteins can emerge on the basis of considerations of geometry and symmetry. We show that the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and hydrophobicity are sufficient to yield a free energy landscape with broad minima even for a homopolymer. These minima correspond to marginally compact structures comprising the menu of folds that proteins choose from to house their native-states in. Our results provide a general framework for understanding the common characteristics of globular proteins.
q-bio.BM:With the aim to study the relationship between protein sequences and their native structures, we adopt vectorial representations for both sequence and structure. The structural representation is based on the Principal Eigenvector of the fold's contact matrix (PE). As recently shown, the latter encodes sufficient information for reconstructing the whole contact matrix. The sequence is represented through a Hydrophobicity Profile (HP), using a generalized hydrophobicity scale that we obtain from the principal eigenvector of a residue-residue interaction matrix and denote it as interactivity scale. Using this novel scale, we define the optimal HP of a protein fold, and predict, by means of stability arguments, that it is strongly correlated with the PE of the fold's contact matrix. This prediction is confirmed through an evolutionary analysis, which shows that the PE correlates with the HP of each individual sequence adopting the same fold and, even more strongly, with the average HP of this set of sequences. Thus, protein sequences evolve in such a way that their average HP is close to the optimal one, implying that neutral evolution can be viewed as a kind of motion in sequence space around the optimal HP. Our results indicate that the correlation coefficient between N-dimensional vectors constitutes a natural metric in the vectorial space in which we represent both protein sequences and protein structures, which we call Vectorial Protein Space. In this way, we define a unified framework for sequence to sequence, sequence to structure, and structure to structure alignments. We show that the interactivity scale is nearly optimal both for the comparison of sequences with sequences and sequences with structures.
q-bio.BM:We fit the Fourier transforms of solvent accessibility and hydrophobicity profiles of a representative set of proteins to a joint multi-variable Gaussian. This allows us to separate the intrinsic tendencies of sequence and structure profiles from the interactions that correlate them; for example, the $\alpha$-helix periodicity in sequence hydrophobicity is dictated by the solvent accessibility of structures. The distinct intrinsic tendencies of sequence and structure profiles are most pronounced at long periods, where sequence hydrophobicity fluctuates more, while solvent accessibility fluctuations are less than average. Interestingly, correlations between the two profiles can be interpreted as the Boltzmann weight of the solvation energy at room temperature.
q-bio.BM:In this paper, we examine the mechanical role of the lipid bilayer in ion channel conformation and function with specific reference to the case of the mechanosensitive channel of large conductance (MscL). In a recent paper (Wiggins and Phillips, 2004), we argued that mechanotransduction very naturally arises from lipid-protein interactions by invoking a simple analytic model of the MscL channel and the surrounding lipid bilayer. In this paper, we focus on improving and expanding this analytic framework for studying lipid-protein interactions with special attention to MscL. Our goal is to generate simple scaling relations which can be used to provide qualitative understanding of the role of membrane mechanics in protein function and to quantitatively interpret experimental results. For the MscL channel, we find that the free energies induced by lipid-protein interaction are of the same order as the free energy differences between conductance states measured by Sukharev et al. (1999). We therefore conclude that the mechanics of the bilayer plays an essential role in determining the conformation and function of the channel. Finally, we compare the predictions of our model to experimental results from the recent investigations of the MscL channel by Perozo et al. (2002), Powl et al. (2003), Yoshimura et al. (2004), and others and suggest a suite of new experiments.
q-bio.BM:The conjunction of insights from structural biology, solution biochemistry, genetics and single molecule biophysics has provided a renewed impetus for the construction of quantitative models of biological processes. One area that has been a beneficiary of these experimental techniques is the study of viruses. In this paper we describe how the insights obtained from such experiments can be utilized to construct physical models of processes in the viral life cycle. We focus on dsDNA bacteriophages and show that the bending elasticity of DNA and its electrostatics in solution can be combined to determine the forces experienced during packaging and ejection of the viral genome. Furthermore, we quantitatively analyze the effect of fluid viscosity and capsid expansion on the forces experienced during packaging. Finally, we present a model for DNA ejection from bacteriophages based on the hypothesis that the energy stored in the tightly packed genome within the capsid leads to its forceful ejection. The predictions of our model can be tested through experiments in vitro where DNA ejection is inhibited by the application of external osmotic pressure.
q-bio.BM:Amyloid fibers are aggregates of proteins. They are built out of a peptide called $\beta$--amyloid (A$\beta$) containing between 41 and 43 residues, produced by the action of an enzyme which cleaves a much larger protein known as the Amyloid Precursor Protein (APP). X-ray diffraction experiments have shown that these fibrils are rich in $\beta$--structures, whereas the shape of the peptide displays an $\alpha$--helix structure within the APP in its biologically active conformation. A realistic model of fibril formation is developed based on the seventeen residues A$\beta$12--28 amyloid peptide, which has been shown to form fibrils structurally similar to those of the whole A$\beta$ peptide. With the help of physical arguments and in keeping with experimental findings, the A$\beta$12--28 monomer is assumed to be in four possible states (i.e., native helix conformation, $\beta$--hairpin, globular low--energy state and unfolded state). Making use of these monomeric states, oligomers (dimers, tertramers and octamers) were constructed. With the help of short, detailed Molecular Dynamics (MD) calculations of the three monomers and of a variety of oligomers, energies for these structures were obtained. Making use of these results within the framework of a simple yet realistic model to describe the entropic terms associated with the variety of amyloid conformations, a phase diagram can be calculated of the whole many--body system, leading to a thermodynamical picture in overall agreement with the experimental findings. In particular, the existence of micellar metastable states seem to be a key issue to determine the thermodynamical properties of the system.
q-bio.BM:The possibility of deriving the contact potentials between amino acids from their frequencies of occurence in proteins is discussed in evolutionary terms. This approach allows the use of traditional thermodynamics to describe such frequencies and, consequently, to develop a strategy to include in the calculations correlations due to the spatial proximity of the amino acids and to their overall tendency of being conserved in proteins. Making use of a lattice model to describe protein chains and defining a "true" potential, we test these strategies by selecting a database of folding model sequences, deriving the contact potentials from such sequences and comparing them with the "true" potential. Taking into account correlations allows for a markedly better prediction of the interaction potentials.
q-bio.BM:While all the information required for the folding of a protein is contained in its amino acid sequence, one has not yet learned how to extract this information to predict the three--dimensional, biologically active, native conformation of a protein whose sequence is known. Using insight obtained from simple model simulations of the folding of proteins, in particular of the fact that this phenomenon is essentially controlled by conserved (native) contacts among (few) strongly interacting ("hot"), as a rule hydrophobic, amino acids, which also stabilize local elementary structures (LES, hidden, incipient secondary structures like $\alpha$--helices and $\beta$--sheets) formed early in the folding process and leading to the postcritical folding nucleus (i.e., the minimum set of native contacts which bring the system pass beyond the highest free--energy barrier found in the whole folding process) it is possible to work out a succesful strategy for reading the native structure of designed proteins from the knowledge of only their amino acid sequence and of the contact energies among the amino acids. Because LES have undergone millions of years of evolution to selectively dock to their complementary structures, small peptides made out of the same amino acids as the LES are expected to selectively attach to the newly expressed (unfolded) protein and inhibit its folding, or to the native (fluctuating) native conformation and denaturate it. These peptides, or their mimetic molecules, can thus be used as effective non--conventional drugs to those already existing (and directed at neutralizing the active site of enzymes), displaying the advantage of not suffering from the uprise of resistance.
q-bio.BM:Methods for alignment of protein sequences typically measure similarity by using substitution matrix with scores for all possible exchanges of one amino acid with another. Although widely used, the matrices derived from homologous sequence segments, such as Dayhoff's PAM matrices and Henikoff's BLOSUM matrices, are not specific for protein conformation identification. Using a different approach, we got many amino acid segment blocks. For each of them, the protein secondary structure is identical. Based on these blocks, we have derived new amino acid substitution matrices. The application of these matrices led to marked improvements in conformation segment search and homologues detection in twilight zone.
q-bio.BM:The advent of new experimental genomic technologies and the massive increase of DNA sequence information is helping researchers better understand how our genes work. Recently, experiments on mRNA abundance (gene expression) have revealed that gene expression shows a stationary organization described by a power-law distribution (scale-free organization) (i.e., gene expression $k$ decays as $k^{-\gamma}$), which is highly conserved in all the major five kingdoms of life, from Bacteria to Human. An underlying gene expression dynamics "rich-travel-more" was suggested to recover that evolutional conservation of transcriptional organization. Here we propose a constructive approach to gene expression dynamics with larger scope. Our gene expression construction restores the stationary state, predicts the power-law exponent for different organisms with natural explanation for small correction at high and low expression levels, describes the intermediate state dynamics (time finite) and elucidates the gene expression stability. This approach requires only one assumption: Markov property.
q-bio.BM:Proteins are minimally frustrated polymers. However, for realistic protein models non-native interactions must be taken into account. In this paper we analyze the effect of non-native interactions on the folding rate and on the folding free energy barrier. We present an analytic theory to account for the modification on the free energy landscape upon introduction of non-native contacts, added as a perturbation to the strong native interactions driving folding. Our theory predicts a rate-enhancement regime at fixed temperature, under the introduction of weak, non-native interactions. We have thoroughly tested this theoretical prediction with simulations of a coarse-grained protein model, by employing an off-lattice $C_\alpha$ model of the src-SH3 domain. The strong agreement between results from simulations and theory confirm the non trivial result that a relatively small amount of non-native interaction energy can actually assist the folding to the native structure.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. For simplified models of proteins where coordinates of only $C_\alpha$ atoms need to be specified, an accurate potential function is important. Such a simplified model is essential for efficient search of conformational space. In this work, we present a formulation of potential function for simplified representations of protein structures. It is based on the combination of descriptors derived from residue-residue contact and sequence-dependent local geometry. The optimal weight coefficients for contact and local geometry is obtained through optimization by maximizing margins among native and decoy structures. The latter are generated by chain growth and by gapless threading. The performance of the potential function in blind test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. This potential function have comparable or better performance than several residue-based potential functions that require in addition coordinates of side chain centers or coordinates of all side chain atoms.
q-bio.BM:Circular permutation connects the N and C termini of a protein and concurrently cleaves elsewhere in the chain, providing an important mechanism for generating novel protein fold and functions. However, their in genomes is unknown because current detection methods can miss many occurances, mistaking random repeats as circular permutation. Here we develop a method for detecting circularly permuted proteins from structural comparison. Sequence order independent alignment of protein structures can be regarded as a special case of the maximum-weight independent set problem, which is known to be computationally hard. We develop an efficient approximation algorithm by repeatedly solving relaxations of an appropriate intermediate integer programming formulation, we show that the approximation ratio is much better then the theoretical worst case ratio of $r = 1/4$. Circularly permuted proteins reported in literature can be identified rapidly with our method, while they escape the detection by publicly available servers for structural alignment.
q-bio.BM:Motivation. Protein design aims to identify sequences compatible with a given protein fold but incompatible to any alternative folds. To select the correct sequences and to guide the search process, a design scoring function is critically important. Such a scoring function should be able to characterize the global fitness landscape of many proteins simultaneously.   Results. To find optimal design scoring functions, we introduce two geometric views and propose a formulation using mixture of nonlinear Gaussian kernel functions. We aim to solve a simplified protein sequence design problem. Our goal is to distinguish each native sequence for a major portion of representative protein structures from a large number of alternative decoy sequences, each a fragment from proteins of different fold. Our scoring function discriminate perfectly a set of 440 native proteins from 14 million sequence decoys. We show that no linear scoring function can succeed in this task. In a blind test of unrelated proteins, our scoring function misclassfies only 13 native proteins out of 194. This compares favorably with about 3-4 times more misclassifications when optimal linear functions reported in literature are used. We also discuss how to develop protein folding scoring function.
q-bio.BM:Being HIV-1-PR an essential enzyme in the viral life cycle, its inhibition can control AIDS. Because the folding of single domain proteins, like HIV-1-PR is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved amino acids) which have evolved over myriads of generations to recognize and strongly attract each other so as to make the protein fold fast, we suggest a novel type of HIV-1-PR inhibitors which interfere with the folding of the protein: short peptides displaying the same amino acid sequence of that of LES. Theoretical and experimental evidence for the specificity and efficiency of such inhibitors are presented.
q-bio.BM:Vibrational energy relaxation (VER) of a selected mode in cytochrome c (hemeprotein) in vacuum is studied using two theoretical approaches: One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes coupled with nonlinear coupling elements. Both methods result in estimates of VER time (sub ps) for a CD stretching mode in the protein at room temperature, that are in accord with the experimental data of Romesberg's group. The applicability of the two methods is examined through a discussion of the validity of Fermi's golden rule on which the two methods are based.
q-bio.BM:The 16-22 amino acid fragment of the beta-amyloid peptide associated with the Alzheimer's disease, Abeta, is capable of forming amyloid fibrils. Here we study the aggregation mechanism of Abeta(16-22) peptides by unbiased thermodynamic simulations at the atomic level for systems of one, three and six Abeta(16-22) peptides. We find that the isolated Abeta(16-22) peptide is mainly a random coil in the sense that both the alpha-helix and beta-strand contents are low, whereas the three- and six-chain systems form aggregated structures with a high beta-sheet content. Furthermore, in agreement with experiments on Abeta(16-22) fibrils, we find that large parallel beta-sheets are unlikely to form. For the six-chain system, the aggregated structures can have many different shapes, but certain particularly stable shapes can be identified.
q-bio.BM:We identified latent periodicity in catalytic domains of approximately 85% of serine/threonine and tyrosine protein kinases. Similar results were obtained for other 22 protein domains. We also designed the method of noise decomposition, which is aimed to distinguish between different periodicity types of the same period length. The method is to be used in conjunction with the cyclic profile alignment, and this combination is able to reveal structure-related or function-related patterns of latent periodicity. Possible origins of the periodic structure of protein kinase active sites are discussed. Summarizing, we presume that latent periodicity is the common property of many catalytic protein domains.
q-bio.BM:We found latent periodicity of 150 protein families now. We suppose that latent periodicity can determine a spectrum of resonance oscillations in proteins.
q-bio.BM:Proteins have regular tertiary structures but irregular amino acid sequences. This made it very difficult to decode the structural information in the protein sequences. Here we demonstrate that many small alpha protein domains have hidden sequence symmetries characteristic of their pseudo-symmetric tertiary structures. We also present a modified method of recurrent plot to reveal this kind of the hidden sequence symmetry. The results may enable us understand parts of the relations between protein sequences and their tertiary structures, i.e, how the primary sequence of a protein determines its tertiary structure.
q-bio.BM:Summary: The F2CS server provides access to the software, F2CS2.00, that implements an automated prediction method of SCOP and CATH classifications of proteins, based on their FSSP Z-scores (Getz et al., 2002), Availability: Free, at http://www.weizmann.ac.il/physics/complex/compphys/f2cs/. Contact: eytan.domany@weizmann.ac.il Supplementary information: The site contains links to additional figures and tables.
q-bio.BM:Activated processes such as protein unfolding are highly sensitive to heterogeneity in the environment. We study a highly simplified model of a protein in a random heterogeneous environment, a model of the in vivo environment. It is found that if the heterogeneity is sufficiently large the total rate of the process is essentially a random variable; this may be the cause of the species-to-species variability in the rate of prion protein conversion found by Deleault et al. [Nature, 425 (2003) 717].
q-bio.BM:Water molecules and molecular chaperones efficiently help the protein folding process. Here we describe their action in the context of the energy and topological networks of proteins. In energy terms water and chaperones were suggested to decrease the activation energy between various local energy minima smoothing the energy landscape, rescuing misfolded proteins from conformational traps and stabilizing their native structure. In kinetic terms water and chaperones may make the punctuated equilibrium of conformational changes less punctuated and help protein relaxation. Finally, water and chaperones may help the convergence of multiple energy landscapes during protein-macromolecule interactions. We also discuss the possibility of the introduction of protein games to narrow the multitude of the energy landscapes when a protein binds to another macromolecule. Both water and chaperones provide a diffuse set of rapidly fluctuating weak links (low affinity and low probability interactions), which allow the generalization of all these statements to a multitude of networks.
q-bio.BM:Nucleosomes organize the folding of DNA into chromatin and significantly influence transcription, replication, regulation and repair. All atom molecular dynamics simulations of a nucleosome and of its 146 basepairs of DNA free in solution have been conducted. DNA helical parameters are extracted from each trajectory to compare the conformation, effective force constants, persistence length measures, and fluctuations of nucleosomal DNA to free DNA. A method for disassembling and reconstructing the conformation and dynamics of the nucleosome using Fourier analysis is presented. Results indicate that the superhelical path of DNA in the nucleosome is irregular. Long length variations in the conformation of nucleosomal DNA are identified other than those associated with helix repeat. These variations are required to create a proposed tetrasome conformation or to qualitatively reconstruct the 1.75 turns of the nuclesomal superhelix. Free DNA achieves enough bend and shear in solution to create an ideal nucleosome superhelix, but these deformations are not organized so the conformation is essentially linear. Reconstruction of free DNA using selected long wavelength variations in conformation can produce either a left-handed or a right-handed superhelix. DNA is less flexible in the nucleosome than when free in solution, however such measures are length scale dependent.
q-bio.BM:Gene expression analysis by means of microarrays is based on the sequence specific binding of mRNA to DNA oligonucleotide probes and its measurement using fluorescent labels. The binding of RNA fragments involving other sequences than the intended target is problematic because it adds a "chemical background" to the signal, which is not related to the expression degree of the target gene. The paper presents a molecular signature of specific and non specific hybridization with potential consequences for gene expression analysis. We analyzed the signal intensities of perfect match (PM) and mismatch (MM) probes of GeneChip microarrays to specify the effect of specific and non specific hybridization. We found that these events give rise to different relations between the PM and MM intensities as function of the middle base of the PMs, namely a triplet- (C>G=T>A>0) and a duplet-like (C=T>0>G=A) pattern of the PM-MM log-intensity difference upon binding of specific and non specific RNA fragments, respectively. The systematic behaviour of the intensity difference can be rationalized on the level of base pairings of DNA/RNA oligonucleotide duplexes in the middle of the probe sequence. Non-specific binding is characterized by the reversal of the central Watson Crick (WC) pairing for each PM/MM probe pair, whereas specific binding refers to the combination of a WC and a self complementary (SC) pairing in PM and MM probes, respectively. The intensity of complementary MM introduces a systematic source of variation which decreases the precision of expression measures based on the MM intensities.
q-bio.BM:We implement the replica exchange molecular dynamics algorithm to study the interactions of a model peptide (WALP-16) with an explicitly represented DPPC membrane bilayer. We observe the spontaneous, unbiased insertion of WALP-16 into the DPPC bilayer and its folding into an a-helix with a trans-bilayer orientation. We observe that the insertion of the peptide into the DPPC bilayer precedes secondary structure formation. Although the peptide has some propensity to form a partially helical structure in the interfacial region of the DPPC/water system, this state is not a productive intermediate but rather an off-pathway trap for WALP-16 insertion. Equilibrium simulations show that the observed insertion/folding pathway mirrors the potential of mean force (PMF). Calculation of the enthalpic and entropic contributions to this PMF show that the surface bound conformation of WALP-16 is significantly lower in energy than other conformations, and that the insertion of WALP-16 into the bilayer without regular secondary structure is enthalpically unfavorable by 5-10 kcal/mol/residue. The observed insertion/folding pathway disagrees with the dominant conceptual model, which is that a surface bound helix is an obligatory intermediate for the insertion of a-helical peptides into lipid bilayers. In our simulations, the observed insertion/folding pathway is favored because of a large (> 100 kcal/mol) increase in system entropy that occurs when the unstructured WALP-16 peptide enters the lipid bilayer interior. The insertion/folding pathway that is lowest in free energy depends sensitively on the near cancellation of large enthalpic and entropic terms. This suggests that intrinsic membrane peptides may have a diversity of insertion/folding behaviors depending on the exact system of peptide and lipid under consideration.
q-bio.BM:Analysis of data from an Affymetrix Latin Square spike-in experiment indicates that measured fluorescence intensities of features on an oligonucleotide microarray are related to spike-in RNA target concentrations via a hyperbolic response function, generally identified as a Langmuir adsorption isotherm. Furthermore the asymptotic signal at high spike-in concentrations is almost invariably lower for a mismatch feature than for its partner perfect match feature. We survey a number of theoretical adsorption models of hybridization at the microarray surface and find that in general they are unable to explain the differing saturation responses of perfect and mismatch features. On the other hand, we find that a simple and consistent explanation can be found in a model in which equilibrium hybridization followed by partial dissociation of duplexes during the post-hybridization washing phase.
q-bio.BM:A model for the processive movement of dynein is presented based on experimental observations available. In the model, the change from strong microtubule-binding to weak binding of dynein is determined naturally by the variation of the relative orientation between the two interacting surfaces of the stalk tip and the microtubule as the stalk rotates from the ADP.Vi-state orientation to the apo-state orientation. This means that the puzzling communication from the ATP binding site in the globular head to the MT-binding site in the tip of the stalk, which is prerequisite in the conventional model, is not required. Using the present model, the previous experimental results, such as (i) the step size of a dynein being an integer times of the period of the MT lattice, (ii) the dependence of the step size on load, i.e., the step size decreasing with the increase of load, and (iii) the stall force being proportional to [ATP] at low [ATP] and becoming saturated at high [ATP], are well explained.
q-bio.BM:We address the controversial hot question concerning the validity of the loose coupling versus the lever-arm theories in the actomyosin dynamics by re-interpreting and extending the phenomenological washboard potential model proposed by some of us in a previous paper. In this new model a Brownian motion harnessing thermal energy is assumed to co-exist with the deterministic swing of the lever-arm, to yield an excellent fit of the set of data obtained by some of us on the sliding of Myosin II heads on immobilized actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of the model is tested via different choices of parameters and potential profiles.
q-bio.BM:In this paper we investigate the role of native geometry on the kinetics of protein folding based on simple lattice models and Monte Carlo simulations. Results obtained within the scope of the Miyazawa-Jernigan indicate the existence of two dynamical folding regimes depending on the protein chain length. For chains larger than 80 amino acids the folding performance is sensitive to the native state's conformation. Smaller chains, with less than 80 amino acids, fold via two-state kinetics and exhibit a significant correlation between the contact order parameter and the logarithmic folding times. In particular, chains with N=48 amino acids were found to belong to two broad classes of folding, characterized by different cooperativity, depending on the contact order parameter. Preliminary results based on the G\={o} model show that the effect of long range contact interaction strength in the folding kinetics is largely dependent on the native state's geometry.
q-bio.BM:Monte Carlo simulations show that long-range interactions play a major role in determining the folding rates of 48-mer three-dimensional lattice polymers modelled by the Go potential. For three target structures with different native geometries we found a sharp increase in the folding time when the relative contribution of the long-range interactions to the native state's energy is decreased from ~50% towards zero. However, the dispersion of the simulated folding times depends strongly on the native geometry and Go polymers folding to one of the target structures exhibit folding times spanning three orders of magnitude. We have also found that, depending on the target geometry, a strong geometric coupling may exist between local and long-range contacts meaning that, when this coupling exists, the formation of long-range contacts is forced by the previous formation of local contacts. The absence of a strong geometric coupling leads to kinetics that are more sensitive to the interaction energy parameters; in this case the formation of local contacts is not sufficient to promote the establishment of long-range ones when these are strongly penalized energetically, leading to longer folding times.
q-bio.BM:A simplified interaction potential for protein folding studies at the atomic level is discussed and tested on a set of peptides with about 20 residues each. The test set contains both alpha-helical (Trp cage, Fs) and beta-sheet (GB1p, GB1m2, GB1m3, Betanova, LLM) peptides. The model, which is entirely sequence-based, is able to fold these different peptides for one and the same choice of model parameters. Furthermore, the melting behavior of the peptides is in good quantitative agreement with experimental data. Apparent folded populations obtained using different observables are compared, and are found to be very different for some of the peptides (e.g., Betanova). In other cases (in particular, GB1m2 and GB1m3), the different estimates agree reasonably well, indicating a more two-state-like melting behavior.
q-bio.BM:Single molecule FRET (fluorescence resonance energy transfer) is a powerful technique for detecting real-time conformational changes and molecular interactions during biological reactions. In this review, we examine different techniques of extending observation times via immobilization and illustrate how useful biological information can be obtained from single molecule FRET time trajectories with or without absolute distance information.
q-bio.BM:An ensemble of directed macromolecules on a lattice is considered, where the constituting molecules are chosen as a random sequence of N different types. The same type of molecules experiences a hard-core (exclusion) interaction. We study the robustness of the macromolecules with respect to breaking and substituting individual molecules, using a 1/N expansion. The properties depend strongly on the density of macromolecules. In particular, the macromolecules are robust against breaking and substituting at high densities.
q-bio.BM:The vibrational dynamics of a DNA molecule with counterions neutralizing the charged phosphate groups have been studied. With the help of elaborated model the conformational vibrations of the DNA double helix with alkaline metal ions have been described both qualitatively and quantitatively. For the complexes of DNA with counterions Li+, Na+, K+, Rb+ and Cs+ the normal modes have been found, and a mode characterized by the most notable ion displacements with respect to the DNA backbone has been determined. The frequency of counterion vibrations has been established to decrease as the ion mass increases. The results of theoretical calculation have been showed to be in good agreement with the experimental data of Raman spectroscopy.
q-bio.BM:To understand the mechanism of TATA-box conformational transformations we model structure mobility and find the types of conformational excitations of DNA macromolecule in heteronomous conformation. We have constructed the two-component model for describing DNA conformational transformation with simultaneous transitions in the furanos rings of the monomer link. Internal component describes the change of the base pair position in the double helix. External component describes the displacement of mass center of the monomer link. Nonlinearity of the system is accounted with a form of potential energy describing C3'-C2' and C2'-C3' sugars transitions in monomer link, and interrelation between monomer conformational transition and macromolecule deformation. The comparison of our results with experimental data allows to confirm that the localized conformational excitations may realise in DNA TATA-box. These excitations cause the deformation of the macromolecule fragment.
q-bio.BM:A simple approach is proposed to investigate the protein structure. Using a low complexity model, a simple pairwise interaction and the concept of global optimization, we are able to calculate ground states of proteins, which are in agreement with experimental data. All possible model structures of small proteins are available below a certain energy threshold. The exact lowenergy landscapes for the trp cage protein (1L2Y) is presented showing the connectivity of all states and energy barriers.
q-bio.BM:The effective DNA-DNA interaction force is calculated by computer simulations with explicit tetravalent counterions and monovalent salt. For overcharged DNA molecules, the interaction force shows a double-minimum structure. The positions and depths of these minima are regulated by the counterion density in the bulk. Using two-dimensional lattice sum and free energy perturbation theories, the coexisting phases for DNA bundles are calculated. A DNA-condensation and redissolution transition and a stable mesocrystal with an intermediate lattice constant for high counterion concentration are obtained.
q-bio.BM:By using a mixture model for the density distribution of the three pseudobond angles formed by $C_\alpha$ atoms of four consecutive residues, the local structural states are discretized as 17 conformational letters of a protein structural alphabet. This coarse-graining procedure converts a 3D structure to a 1D code sequence. A substitution matrix between these letters is constructed based on the structural alignments of the FSSP database.
q-bio.BM:An overview of theories related to vibrational energy relaxation (VER) in proteins is presented. VER of a selected mode in cytochrome c is studied using two theoretical approaches. One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes interacting through nonlinear coupling elements. Both methods result in estimates of the VER time (sub ps) for a CD stretching mode in the protein at room temperature. The theoretical predictions are in accord with the experimental data of Romesberg's group. A perspective on future directions for the detailed study of time scales and mechanisms for VER in proteins is presented.
q-bio.BM:Protein one-dimensional (1D) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional (3D) structure of a protein is encoded in the amino acid sequence. However, it has not been clear whether a given set of 1D structures contains sufficient information for recovering the underlying 3D structure. Here we show that the 3D structure of a protein can be recovered from a set of three types of 1D structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. Using simulated annealing molecular dynamics simulations, the structures satisfying the given native 1D structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. By selecting the structures best satisfying the restraints, all the proteins showed a coordinate RMS deviation of less than 4\AA{} from the native structure, and for most of them, the deviation was even less than 2\AA{}. The present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship.
q-bio.BM:The lack of specificity in microarray experiments due to non-specific hybridization raises a serious problem for the analysis of microarray data because the residual chemical background intensity is not related to the expression degree of the gene of interest. We analyzed the concentration dependence of the signal intensity of perfect match (PM) and mismatch (MM) probes in terms using a microscopic binding model using a combination of mean hybridization isotherms and single base related affinity terms. The signal intensities of the PM and MM probes and their difference are assessed with regard to their sensitivity, specificity and resolution for gene expression measures. The presented theory implies the refinement of existing algorithms of probe level analysis to correct microarray data for non-specific background intensities.
q-bio.BM:Residue-wise contact order (RWCO) is a new kind of one-dimensional protein structures which represents the extent of long-range contacts. We have recently shown that a set of three types of one-dimensional structures (secondary structure, contact number, and RWCO) contains sufficient information for reconstructing the three-dimensional structure of proteins. Currently, there exist prediction methods for secondary structure and contact number from amino acid sequence, but none exists for RWCO. Also, the properties of amino acids that affect RWCO is not clearly understood. Here, we present a linear regression-based method to predict RWCO from amino acid sequence, and analyze the regression parameters to identify the properties that correlates with the RWCO. The present method achieves the significant correlation of 0.59 between the native and predicted RWCOs on average. An unusual feature of the RWCO prediction is the remarkably large optimal half window size of 26 residues. The regression parameters for the central and near-central residues of the local sequence segment highly correlate with those of the contact number prediction, and hence with hydrophobicity.
q-bio.BM:In this paper the heat signaling in microtubules (MT) is investigated. It is argued that for the description of the heat signaling phenomena in MT, the hyperbolic heat transport (HHT) equation must be used. It is shown that HHT is the Klein-Gordon (K-G) equation. The general solution for the K-G equation for MT is obtained. For the undistorted signal propagation in MT the Heisenberg uncertainty principle is formulated and discussed.   Key words: Microtubules; Heat signaling; Klein-Gordon equation; Heisenberg principle.
q-bio.BM:In the last years, tens of thousands gene expression profiles for cells of several organisms have been monitored. Gene expression is a complex transcriptional process where mRNA molecules are translated into proteins, which control most of the cell functions. In this process, the correlation among genes is crucial to determine the specific functions of genes. Here, we propose a novel multi-dimensional stochastic approach to deal with the gene correlation phenomena. Interestingly, our stochastic framework suggests that the study of the gene correlation requires only one theoretical assumption -Markov property- and the experimental transition probability, which characterizes the gene correlation system. Finally, a gene expression experiment is proposed for future applications of the model.
q-bio.BM:The importance of understanding the mechanism of protein aggregation into insoluble amyloid fibrils relies not only on its medical consequences, but also on its more basic properties of self--organization. The discovery that a large number of uncorrelated proteins can form, under proper conditions, structurally similar fibrils has suggested that the underlying mechanism is a general feature of polypeptide chains. In the present work, we address the early events preceeding amyloid fibril formation in solutions of zinc--free human insulin incubated at low pH and high temperature. Aside from being a easy--to--handle model for protein fibrillation, subcutaneous aggregation of insulin after injection is a nuisance which affects patients with diabetes. Here, we show by time--lapse atomic force microscopy (AFM) that a steady-state distribution of protein oligomers with an exponential tail is reached within few minutes after heating. This metastable phase lasts for few hours until aggregation into fibrils suddenly occurs. A theoretical explanation of the oligomer pre--fibrillar distribution is given in terms of a simple coagulation--evaporation kinetic model, in which concentration plays the role of a critical parameter. Due to high resolution and sensitivity of AFM technique, the observation of a long-lasting latency time should be considered an actual feature of the aggregation process, and not simply ascribed to instrumental inefficency. These experimental facts, along with the kinetic model used, claim for a critical role of thermal concentration fluctuations in the process of fibril nucleation.
q-bio.BM:Identical objects, regularly assembled, form a helix, which is the principal motif of nucleic acids, proteins, and viral capsids.
q-bio.BM:The ambitious and ultimate research purpose in Systems Biology is the understanding and modelling of the cell's system. Although a vast number of models have been developed in order to extract biological knowledge from complex systems composed of basic elements as proteins, genes and chemical compounds, a need remains for improving our understanding of dynamical features of the systems (i.e., temporal-dependence).   In this article, we analyze the gene expression dynamics (i.e., how the genes expression fluctuates in time) by using a new constructive approach. This approach is based on only two fundamental ingredients: symmetry and the Markov property of dynamics. First, by using experimental data of human and yeast gene expression time series, we found a symmetry in short-time transition probability from time $t$ to time $t+1$. We call it self-similarity symmetry (i.e., surprisingly, the gene expression short-time fluctuations contain a repeating pattern of smaller and smaller parts that are like the whole, but different in size). Secondly, the Markov property of dynamics reflects that the short-time fluctuation governs the full-time behaviour of the system. Here, we succeed in reconstructing naturally the global behavior of the observed distribution of gene expression (i.e., scaling-law) and the local behaviour of the power-law tail of this distribution, by using only these two ingredients: symmetry and the Markov property of dynamics. This approach may represent a step forward toward an integrated image of the basic elements of the whole cell.
q-bio.BM:In this work, the dynamics of fluctuations in gene expression time series is investigated. By using collected data of gene expression from yeast and human organisms, we found that the fluctuations of gene expression level and its average value over time are strongly correlated and obey a scaling law. As this feature is found in yeast and human organisms, it suggests that probably this coupling is a common dynamical organizing property of all living systems. To understand these observations, we propose a stochastic model which can explain these collective fluctuations, and predict the scaling exponent. Interestingly, our results indicate that the observed scaling law emerges from the self-similarity symmetry embedded in gene expression fluctuations.
q-bio.BM:We study the mechanism underlying the attraction between nucleosomes, the fundamental packaging units of DNA inside the chromatin complex. We introduce a simple model of the nucleosome, the eight-tail colloid, consisting of a charged sphere with eight oppositely charged, flexible, grafted chains that represent the terminal histone tails. We demonstrate that our complexes are attracted via the formation of chain bridges and that this attraction can be tuned by changing the fraction of charged monomers on the tails. This suggests a physical mechanism of chromatin compaction where the degree of DNA condensation can be controlled via biochemical means, namely the acetylation and deacetylation of lysines in the histone tails.
q-bio.BM:The effects of monovalent (Na+, K+) and divalent (Mg2+, Ca2+, Mn2+) ions on the interaction between DNA and histone are studied using the molecular combing technique. Lamda-DNA molecules and DNA-histone complexes incubated with metal cations (Na+, K+, Mg2+, Ca2+, Mn2+) are stretched on hydrophobic surfaces, and directly observed by fluorescence microscopy. The results indicate that when these cations are added into the DNA solution, the fluorescence intensities of the stained DNA are reduced differently. The monovalent cations (Na+, K+) inhibit binding of histone to DNA. The divalent cations (Mg2+, Ca2+, Mn2+) enhance significantly the binding of histone to DNA and the binding of the DNA-histone complex to the hydrophobic surface. Mn2+ also induces condensation and aggregation of the DNA-histone complex.
q-bio.BM:PDZ (Post-synaptic density-95/discs large/zonula occludens-1) domains are relatively small (80 to 120 residues) protein binding modules central in the organization of receptor clusters and in the association of cellular proteins. Their main function is to bind C-terminals of selected proteins that are recognized through specific amino-acids in their carboxyl end. Binding is associated with a deformation of the PDZ native structure and is responsible for dynamical changes in regions not in direct contact with the target. We investigate how this deformation is related to the harmonic dynamics of the PDZ structure and show that one low-frequency collective normal mode, characterized by the concerted movements of different secondary structures, is involved in the binding process. Our results suggest that even minimal structural changes are responsible of communication between distant regions of the protein, in agreement with recent Nuclear Magnetic Resonance (NMR) experiments. Thus PDZ domains are a very clear example of how collective normal modes are able to characterize the relation between function and dynamics of proteins, and to provide indications on the precursors of binding/unbonding events.
q-bio.BM:We introduce a new measure of antigenic distance between influenza A vaccine and circulating strains. The measure correlates well with efficacies of the H3N2 influenza A component of the annual vaccine between 1971 and 2004, as do results of a theory of the immune response to influenza following vaccination. This new measure of antigenic distance is correlated with vaccine efficacy to a greater degree than are current state-of-the-art phylogenetic sequence analyzes or ferret antisera inhibition assays. We suggest that this new measure of antigenic distance be used in the design of the annual influenza vaccine and in the interpretation of vaccine efficacy monitoring.
q-bio.BM:Prediction of one-dimensional protein structures such as secondary structures and contact numbers is useful for the three-dimensional structure prediction and important for the understanding of sequence-structure relationship. Here we present a new machine-learning method, critical random networks (CRNs), for predicting one-dimensional structures, and apply it, with position-specific scoring matrices, to the prediction of secondary structures (SS), contact numbers (CN), and residue-wise contact orders (RWCO). The present method achieves, on average, $Q_3$ accuracy of 77.8% for SS, correlation coefficients of 0.726 and 0.601 for CN and RWCO, respectively. The accuracy of the SS prediction is comparable to other state-of-the-art methods, and that of the CN prediction is a significant improvement over previous methods. We give a detailed formulation of critical random networks-based prediction scheme, and examine the context-dependence of prediction accuracies. In order to study the nonlinear and multi-body effects, we compare the CRNs-based method with a purely linear method based on position-specific scoring matrices. Although not superior to the CRNs-based method, the surprisingly good accuracy achieved by the linear method highlights the difficulty in extracting structural features of higher order from amino acid sequence beyond that provided by the position-specific scoring matrices.
q-bio.BM:We describe the results obtained from an improved model for protein folding. We find that a good agreement with the native structure of a 46 residue long, five-letter protein segment is obtained by carefully tuning the parameters of the self-avoiding energy. In particular we find an improved free-energy profile. We also compare the efficiency of the multidimensional replica exchange method with the widely used parallel tempering.
q-bio.BM:We use single-particle tracking to study the elastic properties of single microtubules grafted to a substrate. Thermal fluctuations of the free microtubule's end are recorded, in order to measure position distribution functions from which we calculate the persistence length of microtubules with contour lengths between 2.6 and 48 micrometers. We find the persistence length to vary by more than a factor of 20 over the total range of contour lengths. Our results support the hypothesis that shearing between protofilaments contributes significantly to the mechanics of microtubules.
q-bio.BM:Optimal structure of proteins is described by linear stochastic differential equation with mean decrease of free energy and volatility. Structure determining strategy is given by a twin of stochastic variables for which empirical conditions are not postulated. Optimal structure determination will be deformed to be adoptive to trading strategy employing martingale property where stochastic integral w.r.t. analytical solution of stochastic differential equation will be employed.
q-bio.BM:One of the main problems of drug design is that of optimizing the drug--target interaction. In the case in which the target is a viral protein displaying a high mutation rate, a second problem arises, namely the eventual development of resistance. We wish to suggest a scheme for the design of non--conventional drugs which do not face any of these problems and apply it to the case of HIV--1 protease. It is based on the knowledge that the folding of single--domain proteins, like e.g. each of the monomers forming the HIV--1--PR homodimer, is controlled by local elementary structures (LES), stabilized by local contacts among hydrophobic, strongly interacting and highly conserved amino acids which play a central role in the folding process. Because LES have evolved over myriads of generations to recognize and strongly interact with each other so as to make the protein fold fast as well as to avoid aggregation with other proteins, highly specific (and thus little toxic) as well as effective folding--inhibitor drugs suggest themselves: short peptides (or eventually their mimetic molecules), displaying the same amino acid sequence of that of LES (p--LES). Aside from being specific and efficient, these inhibitors are expected not to induce resistance: in fact, mutations which successfully avoid their action imply the destabilization of one or more LES and thus should lead to protein denaturation. Making use of Monte Carlo simulations within the framework of a simple although not oversimplified model, which is able to reproduce the main thermodynamic as well as dynamic properties of monoglobular proteins, we first identify the LES of the HIV--1--PR and then show that the corresponding p--LES peptides act as effective inhibitors of the folding of the protease which do not create resistance.
q-bio.BM:Protein structure is generally conceptualized as the global arrangement or of smaller, local motifs of helices, sheets, and loops. These regular, recurring secondary structural elements have well-understood and standardized definitions in terms of amino acid backbone geometry and the manner in which hydrogen bonding requirements are satisfied. Recently, "tube" models have been proposed to explain protein secondary structure in terms of the geometrically optimal packing of a featureless cylinder. However, atomically detailed simulations demonstrate that such packing considerations alone are insufficient for defining secondary structure; both excluded volume and hydrogen bonding must be explicitly modeled for helix formation. These results have fundamental implications for the construction and interpretation of realistic and meaningful biomacromolecular models.
q-bio.BM:Structure predictions of helical membrane proteins have been designed to take advantage of the structural autonomy of secondary structure elements, as postulated by the two-stage model of Engelman and Popot. In this context, we investigate structure calculation strategies for two membrane proteins with different functions, sizes, aminoacid compositions, and topologies: the glycophorin A homodimer (a paradigm for close inter-helical packing in membrane proteins) and aquaporin (a channel protein). Our structure calculations are based on two alternative folding schemes: a one-step simulated annealing from an extended chain conformation, and a two-step procedure inspired by the grid-search methods traditionally used in membrane protein predictions. In this framework, we investigate rationales for the utilization of sparse NMR data such as distance-based restraints and residual dipolar couplings in structure calculations of helical membrane proteins.
q-bio.BM:Proteins created by combinatorial methods in vitro are an important source of information for understanding sequence-structure-function relationships. Alignments of folded proteins from combinatorial libraries can be analyzed using methods developed for naturally occurring proteins, but this neglects the information contained in the unfolded sequences of the library. We introduce two algorithms, logistic regression and excess information analysis, that use both the folded and unfolded sequences and compare them against contingency table and statistical coupling analysis, which only use the former. The test set for this benchmark study is a library of fictitious proteins that fold according to a hypothetical energy model. Of the four methods studied, only logistic regression is able to correctly recapitulate the energy model from the sequence alignment. The other algorithms predict spurious interactions between alignment positions with strong but individual influences on protein stability. When present in the same protein, stabilizing amino acids tend to lower the energy below the threshold needed for folding. As a result, their frequencies in the alignment can be correlated even if the positions do not interact. We believe any algorithm that neglects the nonlinear relationship between folding and energy is susceptible to this error.
q-bio.BM:In order to extend the results obtained with minimal lattice models to more realistic systems, we study a model where proteins are described as a chain of 20 kinds of structureless amino acids moving in a continuum space and interacting through a contact potential controlled by a 20x20 quenched random matrix. The goal of the present work is to design and characterize amino acid sequences folding to the SH3 conformation, a 60-residues recognition domain common to many regulatory proteins. We show that a number of sequences can fold, starting from a random conformation, to within a distance root mean square deviation (dRMSD) of 2.6A from the native state. Good folders are those sequences displaying in the native conformation an energy lower than a sequence--independent threshold energy.
q-bio.BM:We recently introduced a physical model [Hoang et al., P. Natl. Acad. Sci. USA (2004), Banavar et al., Phys. Rev. E (2004)] for proteins which incorporates, in an approximate manner, several key features such as the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and the role played by hydrophobicity. Within this framework, marginally compact conformations resembling the native state folds of proteins emerge as broad competing minima in the free energy landscape even for a homopolymer. Here we show how the introduction of sequence heterogeneity using a simple scheme of just two types of amino acids, hydrophobic (H) and polar (P), and sequence design allows a selected putative native fold to become the free energy minimum at low temperature. The folding transition exhibits thermodynamic cooperativity, if one neglects the degeneracy between two different low energy conformations sharing the same fold topology.
q-bio.BM:In eukaryote nucleosome, DNA wraps around a histone octamer in a left-handed way. We study the process of chirality formation of nucleosome with Brownian dynamics simulation. We model the histone octamer with a quantitatively adjustable chirality: left-handed, right-handed or non-chiral, and simulate the dynamical wrapping process of a DNA molecule on it. We find that the chirality of a nucleosome formed is strongly dependent on that of the histone octamer, and different chiralities of the histone octamer induce its different rotation directions in the wrapping process of DNA. In addition, a very weak chirality of the histone octamer is quite enough for sustaining the correct chirality of the nucleosome formed. We also show that the chirality of a nucleosome may be broken at elevated temperature.
q-bio.BM:Trypsin and chymotrypsin are both serine proteases with high sequence and structural similarities, but with different substrate specificity. Previous experiments have demonstrated the critical role of the two loops outside the binding pocket in controlling the specificity of the two enzymes. To understand the mechanism of such a control of specificity by distant loops, we have used the Gaussian Network Model to study the dynamic properties of trypsin and chymotrypsin and the roles played by the two loops. A clustering method was introduced to analyze the correlated motions of residues. We have found that trypsin and chymotrypsin have distinct dynamic signatures in the two loop regions which are in turn highly correlated with motions of certain residues in the binding pockets. Interestingly, replacing the two loops of trypsin with those of chymotrypsin changes the motion style of trypsin to chymotrypsin-like, whereas the same experimental replacement was shown necessary to make trypsin have chymotrypsin's enzyme specificity and activity. These results suggest that the cooperative motions of the two loops and the substrate-binding sites contribute to the activity and substrate specificity of trypsin and chymotrypsin.
q-bio.BM:The emergence and spreading of chirality on the early Earth is considered by studying a set of reaction-diffusion equations based on a polymerization model. It is found that effective mixing of the early oceans is necessary to reach the present homochiral state. The possibility of introducing mass extinctions and modifying the emergence rate of life is discussed.
q-bio.BM:The differences between uni-directional and bi-directional polymerization are considered. The uni-directional case is discussed in the framework of the RNA world. Similar to earlier models of this type, where polymerization was assumed to proceed in a bi-directional fashion (presumed to be relevant to peptide nucleic acids), left-handed and right-handed monomers are produced via an autocatalysis from an achiral substrate. The details of the bifurcation from a racemic solution to a homochiral state of either handedness is shown to be remarkably independent of whether the polymerization in uni-directional or bi-directional. Slightly larger differences are seen when dissociation is allowed and the dissociation fragments are being recycled into the achiral substrate.
q-bio.BM:A variety of viruses tightly pack their genetic material into protein capsids that are barely large enough to enclose the genome. In particular, in bacteriophages, forces as high as 60 pN are encountered during packaging and ejection, produced by DNA bending elasticity and self-interactions. The high forces are believed to be important for the ejection process, though the extent of their involvement is not yet clear. As a result, there is a need for quantitative models and experiments that reveal the nature of the forces relevant to DNA ejection. Here we report measurements of the ejection forces for two different mutants of bacteriophage lambda, lambda b221cI26 and lambda cI60, which differ in genome length by ~30%. As expected for a force-driven ejection mechanism, the osmotic pressure at which DNA release is completely inhibited varies with the genome length: we find inhibition pressures of 15 atm and 25 atm, respectively, values that are in agreement with our theoretical calculations.
q-bio.BM:The correlations of primary and secondary structures were analyzed using proteins with known structure from Protein Data Bank. The correlation values of amino acid type and the eight secondary structure types at distant position were calculated for distances between -25 and 25. Shapes of the diagrams indicate that amino acids polarity and capability for hydrogen bonding have influence on the secondary structure at some distances. Clear preference of most of the amino acids towards certain secondary structure type classifies amino acids into four groups: alpha-helix admirers, strand admirers, turn and bend admirers and the others. Group four consists of His and Cis, the amino acids that do not show clear preference for any secondary structure. Amino acids from a group have similar physicochemical properties, and the same structural characteristics. The results suggest that amino acid preference for secondary structure type is based on the structural characteristics at Cb and Cg atoms of amino acid. alpha-helix admirers do not have polar heteroatoms on Cb and Cg atoms, nor branching or aromatic group on Cb atom. Amino acids that have aromatic groups or branching on Cb atom are strand admirers. Turn and bend admirers have polar heteroatom on Cb or Cg atoms or do not have Cb atom at all. Our results indicate that polarity and capability for hydrogen bonding have influence on the secondary structure at some distance, and that amino acid preference for secondary structure is caused by structural properties at Cb or Cg atoms.
q-bio.BM:The functionality of proteins is related to their structure in the native state. Protein structures are made up of emergent building blocks of helices and almost planar sheets. A simple coarse-grained geometrical model of a flexible tube barely subject to compaction provides a unified framework for understanding the common character of globular proteins.We argue that a recent critique of the tube idea is not well founded.
q-bio.BM:To gain a deeper insight into cellular processes such as transcription and translation, one needs to uncover the mechanisms controlling the configurational changes of nucleic acids. As a step toward this aim, we present here a novel mesoscopic-level computational model that provides a new window into nucleic acid dynamics. We model a single-stranded nucleic as a polymer chain whose monomers are the nucleosides. Each monomer comprises a bead representing the sugar molecule and a pin representing the base. The bead-pin complex can rotate about the backbone of the chain. We consider pairwise stacking and hydrogen-bonding interactions. We use a modified Monte Carlo dynamics that splits the dynamics into translational bead motion and rotational pin motion. By performing a number of tests we first show that our model is physically sound. We then focus on the study of a the kinetics of a DNA hairpin--a single-stranded molecule comprising two complementary segments joined by a non-complementary loop--studied experimentally. We find that results from our simulations agree with experimental observations, demonstrating that our model is a suitable tool for the investigation of the hybridization of single strands.
q-bio.BM:We investigate the folding behavior of protein sequences by numerically studying all sequences with maximally compact lattice model through exhaustive enumeration. We get the prion-like behavior of protein folding. Individual proteins remaining stable in the isolated native state may change their conformations when they aggregate. We observe the folding properties as the interfacial interaction strength changes, and find that the strength must be strong enough before the propagation of the most stable structures happens.
q-bio.BM:We review some of our recent results obtained within the scope of simple lattice models and Monte Carlo simulations that illustrate the role of native geometry in the folding kinetics of two state folders.
q-bio.BM:Ion channels are proteins with a hole down the middle embedded in cell membranes. Membranes form insulating structures and the channels through them allow and control the movement of charged particles, spherical ions, mostly Na+, K+, Ca++, and Cl-. Membranes contain hundreds or thousands of types of channels, fluctuating between open conducting, and closed insulating states. Channels control an enormous range of biological function by opening and closing in response to specific stimuli using mechanisms that are not yet understood in physical language. Open channels conduct current of charged particles following laws of Brownian movement of charged spheres rather like the laws of electrodiffusion of quasi-particles in semiconductors. Open channels select between similar ions using a combination of electrostatic and 'crowded charge' (Lennard-Jones) forces. The specific location of atoms and the exact atomic structure of the channel protein seems much less important than certain properties of the structure, namely the volume accessible to ions and the effective density of fixed and polarization charge. There is no sign of other chemical effects like delocalization of electron orbitals between ions and the channel protein. Channels play a role in biology as important as transistors in computers, and they use rather similar physics to perform part of that role. Understanding their fluctuations awaits physical insight into the source of the variance and mathematical analysis of the coupling of the fluctuations to the other components and forces of the system.
q-bio.BM:We propose a criterion for optimal parameter selection in coarse-grained models of proteins, and develop a refined elastic network model (ENM) of bovine trypsinogen. The unimodal density-of-states distribution of the trypsinogen ENM disagrees with the bimodal distribution obtained from an all-atom model; however, the bimodal distribution is recovered by strengthening interactions between atoms that are backbone neighbors. We use the backbone-enhanced model to analyze allosteric mechanisms of trypsinogen, and find relatively strong communication between the regulatory and active sites.
q-bio.BM:The protonation of N2 bound to the active center of nitrogenase has been investigated using state-of-the-art DFT calculations. Dinitrogen in the bridging mode is activated by forming two bonds to Fe sites, which results in a reduction of the energy for the first hydrogen transfer by 123 kJ/mol. The axial binding mode with open sulfur bridge is less reactive by 30 kJ/mol and the energetic ordering of the axial and bridged binding mode is reversed in favor of the bridging dinitrogen during the first protonation. Protonation of the central ligand is thermodynamically favorable but kinetically hindered. If the central ligand is protonated, the proton is transferred to dinitrogen following the second protonation. Protonation of dinitrogen at the Mo site does not lead to low-energy intermediates.
q-bio.BM:The ejection of DNA from a bacterial virus (``phage'') into its host cell is a biologically important example of the translocation of a macromolecular chain along its length through a membrane. The simplest mechanism for this motion is diffusion, but in the case of phage ejection a significant driving force derives from the high degree of stress to which the DNA is subjected in the viral capsid. The translocation is further sped up by the ratcheting and entropic forces associated with proteins that bind to the viral DNA in the host cell cytoplasm. We formulate a generalized diffusion equation that includes these various pushing and pulling effects and make estimates of the corresponding speed-ups in the overall translocation process. Stress in the capsid is the dominant factor throughout early ejection, with the pull due to binding particles taking over at later stages. Confinement effects are also investigated, in the case where the phage injects its DNA into a volume comparable to the capsid size. Our results suggest a series of in vitro experiments involving the ejection of DNA into vesicles filled with varying amounts of binding proteins from phage whose state of stress is controlled by ambient salt conditions or by tuning genome length.
q-bio.BM:We present a base-pairing model of oligonuleotide duplex formation and show in detail its equivalence to the Nearest-Neighbour dimer methods from fits to free energy of duplex formation data for short DNA-DNA and DNA-RNA hybrids containing only Watson Crick pairs. In this approach the connection between rank-deficient polymer and rank-determinant oligonucleotide parameter, sets for DNA duplexes is transparent. The method is generalised to include RNA/DNA hybrids where the rank-deficient model with 11 dimer parameters in fact provides marginally improved predictions relative to the standard method with 16 independent dimer parameters ($\Delta G$ mean errors of 4.5 and 5.4 % respectively).
q-bio.BM:A formalism is developed which allows to determine the locations of all local symmetry axes of three-dimensional particles with overall icosahedral symmetry. It relies on the fact that the root system of the non-crystallographic Coxeter group H_3 encodes the locations of the planes of reflection that generate the discrete rotational symmetries of the particles. Via an appropriate extension of the root system, new planes of reflection are introduced which determine local axes of rotational symmetry. An easy-to-implement formalism is derived that allows to compute the surface structure of any three-dimensional icosahedral particle with local symmetries. It can be used also for particles with overall octahedral and tetrahedral symmetry in conjunction with the root systems of the corresponding reflection groups.   Applications to viruses are discussed explicitly. It is shown that the concept of quasi-equivalence in Caspar-Klug Theory corresponds to the special case of local six-fold symmetry axes contained in the theory developed here, and the corresponding geometries can hence be obtained with this formalism based on the root system of H_3.   Moreover, as a by-product, the theory answers the long-standing open question why only certain types of capsomeres, i.e. clusters of protein subunits, are observed in the surface structures of viruses. Since the types of the capsomeres are determined by the orders of the local symmetry axes on which they are located, the possible types of capsomeres are restricted by the spectrum of local symmetry axes allowed by the theory. Based on this we determine the spectrum of all capsomere types that may occur in viral capsids and give explicit examples for the lower-order cases.
q-bio.BM:The tethered-particle method is a single-molecule technique that has been used to explore the dynamics of a variety of macromolecules of biological interest. We give a theoretical analysis of the particle motions in such experiments. Our analysis reveals that the proximity of the tethered bead to a nearby surface (the microscope slide) gives rise to a volume-exclusion effect, resulting in an entropic force on the molecule. This force stretches the molecule, changing its statistical properties. In particular, the proximity of bead and surface brings about intriguing scaling relations between key observables (statistical moments of the bead) and parameters such as the bead size and contour length of the molecule. We present both approximate analytic solutions and numerical results for these effects in both flexible and semiflexible tethers. Finally, our results give a precise, experimentally-testable prediction for the probability distribution of the distance between the polymer attachment point and the center of the mobile bead.
q-bio.BM:The distribution of inequivalent geometries occurring during self-assembly of the major capsid protein in thermodynamic equilibrium is determined based on a master equation approach. These results are implemented to characterize the assembly of SV40 virus and to obtain information on the putative pathways controlling the progressive build-up of the SV40 capsid. The experimental testability of the predictions is assessed and an analysis of the geometries of the assembly intermediates on the dominant pathways is used to identify targets for antiviral drug design.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Assembly models are developed for viral capsids built from protein building blocks that can assume different local bonding structures in the capsid. This situation occurs, for example, for viruses in the family of Papovaviridae, which are linked to cancer and are hence of particular interest for the health sector. More specifically, the viral capsids of the (pseudo-) T=7 particles in this family consist of pentamers that exhibit two different types of bonding structures. While this scenario cannot be described mathematically in terms of Caspar-Klug Theory (Caspar and Klug 1962), it can be modelled via tiling theory (Twarock 2004). The latter is used to encode the local bonding environment of the building blocks in a combinatorial structure, called the assembly tree, which is a basic ingredient in the derivation of assembly models for Papovaviridae along the lines of the equilibrium approach of Zlotnick (Zlotnick 1994). A phase space formalism is introduced to characterize the changes in the assembly pathways and intermediates triggered by the variations in the association energies characterizing the bonds between the building blocks in the capsid. Furthermore, the assembly pathways and concentrations of the statistically dominant assembly intermediates are determined. The example of Simian Virus 40 is discussed in detail.
q-bio.BM:We calculate the equation of state of DNA under tension for the case that the DNA features loops. Such loops occur transiently during DNA condensation in the presence of multivalent ions or sliding cationic protein linkers. The force-extension relation of such looped DNA modelled as a wormlike chain is calculated via path integration in the semiclassical limit. This allows us to determine rigorously the high stretching asymptotics. Notably the functional form of the force-extension curve resembles that of straight DNA, yet with a strongly renormalized apparent persistence length. That means that the experimentally extracted single molecule elasticity does not necessarily reflect the bare DNA stiffness only, but can also contain additional contributions that depend on the overall chain conformation and length.
q-bio.BM:A generalized computational method for folding proteins with a fully transferable potential and geometrically realistic all-atom model is presented and tested on seven different helix bundle proteins. The protocol, which includes graph-theoretical analysis of the ensemble of resulting folded conformations, was systematically applied and consistently produced structure predictions of approximately 3 Angstroms without any knowledge of the native state. To measure and understand the significance of the results, extensive control simulations were conducted. Graph theoretic analysis provides a means for systematically identifying the native fold and provides physical insight, conceptually linking the results to modern theoretical views of protein folding. In addition to presenting a method for prediction of structure and folding mechanism, our model suggests that a accurate all-atom amino acid representation coupled with a physically reasonable atomic interaction potential (that does not require optimization to the test set) and hydrogen bonding are essential features for a realistic protein model.
q-bio.BM:Being the HIV-1 Protease (HIV-1-PR) an essential enzyme in the viral life cycle, its inhibition can control AIDS. The folding of single domain proteins, like each of the monomers forming the HIV-1-PR homodimer, is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved, as a rule hydrophobic, amino acids). These LES have evolved over myriad of generations to recognize and strongly attract each other, so as to make the protein fold fast and be stable in its native conformation. Consequently, peptides displaying a sequence identical to those segments of the monomers associated with LES are expected to act as competitive inhibitors and thus destabilize the native structure of the enzyme. These inhibitors are unlikely to lead to escape mutants as they bind to the protease monomers through highly conserved amino acids which play an essential role in the folding process. The properties of one of the most promising inhibitors of the folding of the HIV-1-PR monomers found among these peptides is demonstrated with the help of spectrophotometric assays and CD spectroscopy.
q-bio.BM:It is shown that a small subset of modes which are likely to be involved in protein functional motions of large amplitude can be determined by retaining the most robust normal modes obtained using different protein models. This result should prove helpful in the context of several applications proposed recently, like for solving difficult molecular replacement problems or for fitting atomic structures into low-resolution electron density maps. Moreover, it may also pave the way for the development of methods allowing to predict such motions accurately.
q-bio.BM:A new formalism for calculation of the partition function of single stranded nucleic acids is presented. Secondary structures and the topology of structure elements are the level of resolution that is used. The folding model deals with matches, mismatches, symmetric and asymmetric interior loops, stacked pairs in loop and dangling end regions, multi-branched loops, bulges and single base stacking that might exist at duplex ends or at the ends of helices. Calculations on short and long sequences show, that for short oligonucleotides, a duplex formation often displays a two-state transition. However, for longer oligonucleotides, the thermodynamic properties of the single self-folding transition affects the transition nature of the duplex formation, resulting in a population of intermediate hairpin species in the solution. The role of intermediate hairpin species is analyzed in the case when a short oligonucleotides (molecular beacons) have to reliably identify and hybridize to accessible nucleotides within their targeted mRNA sequences. It is shown that the enhanced specificity of the molecular beacons is a result of their constrained conformational flexibility and the all-or-none mechanism of their hybridization to the target sequence.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Viral capsids are usually spherical, and for a significant number of viruses exhibit overall icosahedral symmetry. The corresponding surface lattices, that encode the locations of the capsid proteins and intersubunit bonds, can be modelled by Viral Tiling Theory.   It has been shown in vitro that under a variation of the experimental boundary conditions, such as the pH value and salt concentration, tubular particles may appear instead of, or in addition to, spherical ones. In order to develop models that describe the simultaneous assembly of both spherical and tubular variants, and hence study the possibility of triggering tubular malformations as a means of interference with the replication mechanism, Viral Tiling Theory has to be extended to include tubular lattices with end caps. This is done here for the case of Papovaviridae, which play a distinguished role from the viral structural point of view as they correspond to all pentamer lattices, i.e. lattices formed from clusters of five protein subunits throughout. These results pave the way for a generalisation of recently developed assembly models.
q-bio.BM:Experimental investigations of the biosynthesis of a number of proteins have pointed out that part of the native structure can be acquired already during translation. We carried out a comprehensive statistical analysis of some average structural properties of proteins that have been put forward as possible signatures of this progressive buildup process. Contrary to a widespread belief, it is found that there is no major propensity of the amino acids to form contacts with residues that are closer to the N terminus. Moreover, it is found that the C terminus is significantly more compact and locally-organized than the N one. Also this bias, though, is unlikely to be related to vectorial effects, since it correlates with subtle differences in the primary sequence. These findings indicate that even if proteins aquire their structure vectorially no signature of this seems to be detectable in their average structural properties.
q-bio.BM:The aim of this article is to present a developed method that decomposes the autofluorescence spectrum into the spectra of naturally occurring biochemical components of biotissue. It requires knowledge of detailed spectrum behaviour of different endogenous fluorophores. We have studied the main bio-markers in human tissue and proposed a simple modelling algorithm for their spectra shapes. The empirical method was tested theoretically by quantum-mechanical calculations of the spectra in the unharmonic Morse potential approach.
q-bio.BM:Experimental evidence suggests that the folding and aggregation of the amyloid $\beta$-protein (A$\beta$) into oligomers is a key pathogenetic event in Alzheimer's disease (AD). Inhibiting the pathologic folding and oligomerization of A$\beta$ could be effective in the prevention and treatment of AD. Here, using all-atom molecular dynamics simulations in explicit solvent, we probe the initial stages of folding of a decapeptide segment of A$\beta$, A$\beta_{21-30}$, shown experimentally to nucleate the folding process. In addition, we examine the folding of a homologous decapeptide containing an amino acid substitution linked to hereditary cerebral hemorrhage with amyloidosis--Dutch type, [Gln22]A$\beta_{21-30}$. We find that: (i) when the decapeptide is in water, hydrophobic interactions and transient salt bridges between Lys28 and either Glu22 or Asp23 are important in the formation of a loop in the Val24--Lys28 region of the wild type decapeptide; (ii) in the presence of salt ions, salt bridges play a more prominent role in the stabilization of the loop; (iii) in water with a reduced density, the decapeptide forms a helix, indicating the sensitivity of folding to different aqueous environments; (iv) the ``Dutch'' peptide in water, in contrast to the wild type peptide, fails to form a long-lived Val24--Lys28 loop, suggesting that loop stability is a critical factor in determining whether A$\beta$ folds into pathologic structures. Our results are relevant to understand the mechanism of A$\beta$ peptide folding in different environments, such as intra- and extracellular milieus or cell membranes, and how amino acid substitutions linked to familial forms of amyloidosis cause disease.
q-bio.BM:This paper was withdrawn by the authors.
q-bio.BM:We develop a class of models with which we simulate the assembly of particles into T1 capsid-like objects using Newtonian dynamics. By simulating assembly for many different values of system parameters, we vary the forces that drive assembly. For some ranges of parameters, assembly is facile, while for others, assembly is dynamically frustrated by kinetic traps corresponding to malformed or incompletely formed capsids. Our simulations sample many independent trajectories at various capsomer concentrations, allowing for statistically meaningful conclusions. Depending on subunit (i.e., capsomer) geometries, successful assembly proceeds by several mechanisms involving binding of intermediates of various sizes. We discuss the relationship between these mechanisms and experimental evaluations of capsid assembly processes.
q-bio.BM:In this paper the heat transport in microtubules (MT) is investigated. When the dimension of the structure is of the order of the de Broglie wave length the transport phenomena must be analyzed within quantum mechanics. In this paper we developed the Dirac type thermal equation for MT .The solution of the equation-the temperature fields for electrons can be wave type or diffusion type depending on the dynamics of the scattering. Key words: Microtubules ultrashort laser pulses, Dirac thermal equation, temperature fields.
q-bio.BM:Mouse prion protein PrP106-126 is a peptide corresponding to the residues 107-127 of human prion protein. It has been shown that PrP106-126 can reproduce the main neuropathological features of prionrelated transmissible spongiform encephalopathies and can form amyloid-like fibrils in vitro. The conformational characteristics of PrP106-126 fibril have been investigated by electron microscopy, CD spectroscopy, NMR and molecular dynamics simulations. Recent researches have found out that PrP106-126 in water assumes a stable structure consisting of two parallel beta-sheets that are tightly packed against each other. In this work we perform molecular dynamics simulation to reveal the elongation mechanism of PrP106-126 fibril. Influenced by the edge strands of the fibril which already adopt beta-sheets conformation, single PrP106-126 peptide forms beta-structure and becomes a new element of the fibril. Under acidic condition, single PrP106-126 peptide adopts a much larger variety of conformations than it does under neural condition, which makes a peptide easier to be influenced by the edge strands of the fibril. However, acidic condition dose not largely affect the stability of PrP106-126 peptide fibril. Thus, the speed of fibril elongation can be dramatically increased by lowering the pH value of the solution. The pH value was adjusted by either changing the protonation state of the residues or adding hydronium ions (acidic solution) or hydroxyl ions (alkaline solution). The differences between these two approaches are analyzed here.
q-bio.BM:The thermodynamics of the small SH3 protein domain is studied by means of a simplified model where each bead-like amino acid interacts with the others through a contact potential controlled by a 20x20 random matrix. Good folding sequences, characterized by a low native energy, display three main thermodynamical phases, namely a coil-like phase, an unfolded globule and a folded phase (plus other two phases, namely frozen and random coil, populated only at extremes temperatures). Interestingly, the unfolded globule has some regions already structured. Poorly designed sequences, on the other hand, display a wide transition from the random coil to a frozen state. The comparison with the analytic theory of heteropolymers is discussed.
q-bio.BM:In a seminal paper Caspar and Klug established a theory that provides a family of polyhedra as blueprints for the structural organisation of viral capsids. In particular, they encode the locations of the proteins in the shells that encapsulate, and hence provide protection for, the viral genome. Despite of its huge success and numerous applications in virology experimental results have provided evidence for the fact that the theory is too restrictive to describe all known viruses. Especially, the family of Papovaviridae, which contains cancer-causing viruses, falls out of the scope of this theory.   In a recent paper we have shown that certain members of the family of Papovaviridae can be described via tilings. In this paper, we develop a comprehensive mathematical framework for the derivation of all surface structures of viral particles in this family. We show that this formalism fixes the structure and relative sizes of all particles collectively so that there exists only one scaling factor that relates the sizes of all particles with their biological counterparts.   The series of polyhedra derived here complements the Caspar-Klug family of polyhedra. It is the first mathematical result that provides a common organisational principle for different types of viral particles in the family of Papovaviridae and paves the way for an understanding of Papovaviridae polymorphism. Moreover, it provides crucial input for the construction of assembly models.
q-bio.BM:The amino acid sequences of proteins provide rich information for inferring distant phylogenetic relationships and for predicting protein functions. Estimating the rate matrix of residue substitutions from amino acid sequences is also important because the rate matrix can be used to develop scoring matrices for sequence alignment. Here we use a continuous time Markov process to model the substitution rates of residues and develop a Bayesian Markov chain Monte Carlo method for rate estimation. We validate our method using simulated artificial protein sequences. Because different local regions such as binding surfaces and the protein interior core experience different selection pressures due to functional or stability constraints, we use our method to estimate the substitution rates of local regions. Our results show that the substitution rates are very different for residues in the buried core and residues on the solvent exposed surfaces. In addition, the rest of the proteins on the binding surfaces also have very different substitution rates from residues. Based on these findings, we further develop a method for protein function prediction by surface matching using scoring matrices derived from estimated substitution rates for residues located on the binding surfaces. We show with examples that our method is effective in identifying functionally related proteins that have overall low sequence identity, a task known to be very challenging.
q-bio.BM:This chapter discusses geometric models of biomolecules and geometric constructs, including the union of ball model, the weigthed Voronoi diagram, the weighted Delaunay triangulation, and the alpha shapes. These geometric constructs enable fast and analytical computaton of shapes of biomoleculres (including features such as voids and pockets) and metric properties (such as area and volume). The algorithms of Delaunay triangulation, computation of voids and pockets, as well volume/area computation are also described. In addition, applications in packing analysis of protein structures and protein function prediction are also discussed.
q-bio.BM:This chapter discusses theoretical framework and methods for developing knowledge-based potential functions essential for protein structure prediction, protein-protein interaction, and protein sequence design. We discuss in some details about the Miyazawa-Jernigan contact statistical potential, distance-dependent statistical potentials, as well as geometric statistical potentials. We also describe a geometric model for developing both linear and non-linear potential functions by optimization. Applications of knowledge-based potential functions in protein-decoy discrimination, in protein-protein interactions, and in protein design are then described. Several issues of knowledge-based potential functions are finally discussed.
q-bio.BM:$\beta$-barrel membrane proteins are found in the outer membrane of gram-negative bacteria, mitochondria, and chloroplasts. We have developed probabilistic models to quantify propensities of residues for different spatial locations and for interstrand pairwise contact interactions involving strong H-bonds, side-chain interactions, and weak H-bonds. The propensity values and p-values measuring statistical significance are calculated exactly by analytical formulae we have developed. Contrary to the ``positive-inside'' rule for helical membrane proteins, $\beta$-barrel membrane proteins follow a significant albeit weaker ``positive-outside'' rule, in that the basic residues Arg and Lys are disproportionately favored in the extracellular cap region and disfavored in the periplasmic cap region. Different residue pairs prefer strong backbone H-bonded interstrand pairings (e.g. Gly-Aromatic) or non-H-bonded pairings (e.g. Aromatic-Aromatic). In addition, Tyr and Phe participate in aromatic rescue by shielding Gly from polar environments. These propensities can be used to predict the registration of strand pairs, an important task for the structure prediction of $\beta$-barrel membrane proteins. Our accuracy of 44% is considerably better than random (7%) and other studies. Our results imply several experiments that can help to elucidate the mechanisms of in vitro and in vivo folding of $\beta$-barrel membrane proteins. See supplementary material after the bibliography for detailed techniques.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. Simplified protein models such as those requiring only $C_\alpha$ or backbone atoms are attractive because they enable efficient search of the conformational space. We show residue specific reduced discrete state models can represent the backbone conformations of proteins with small RMSD values. However, no potential functions exist that are designed for such simplified protein models. In this study, we develop optimal potential functions by combining contact interaction descriptors and local sequence-structure descriptors. The form of the potential function is a weighted linear sum of all descriptors, and the optimal weight coefficients are obtained through optimization using both native and decoy structures. The performance of the potential function in test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. Our potential function requiring only backbone atoms or $C_\alpha$ atoms have comparable or better performance than several residue-based potential functions that require additional coordinates of side chain centers or coordinates of all side chain atoms. By reducing the residue alphabets down to size 5 for local structure-sequence relationship, the performance of the potential function can be further improved. Our results also suggest that local sequence-structure correlation may play important role in reducing the entropic cost of protein folding.
q-bio.BM:Without invoking the Markov approximation, we derive formulas for vibrational energy relaxation (VER) and dephasing for an anharmonic system oscillator using a time-dependent perturbation theory. The system-bath Hamiltonian contains more than the third order coupling terms since we take a normal mode picture as a zeroth order approximation. When we invoke the Markov approximation, our theory reduces to the Maradudin-Fein formula which is used to describe VER properties of glass and proteins. When the system anharmonicity and the renormalization effect due to the environment vanishes, our formulas reduce to those derived by Mikami and Okazaki invoking the path-integral influence functional method [J. Chem. Phys. 121 (2004) 10052]. We apply our formulas to VER of the amide I mode of a small amino-acide like molecule, N-methylacetamide, in heavy water.
q-bio.BM:Simplified Go models, where only native contacts interact favorably, have proven useful to characterize some aspects of the folding of small proteins. The success of these models is limited by the fact that all residues interact in the same way, so that the folding features of a protein are determined only by the geometry of its native conformation. We present an extended version of a C-alpha based Go model where different residues interact with different energies. The model is used to calculate the thermodynamics of three small proteins (Protein G, SrcSH3 and CI2) and the effect of mutations on the wildtype sequence. The model allows to investigate some of the most controversial areas in protein folding such as its earliest stages, a subject which has lately received particular attention. The picture which emerges for the three proteins under study is that of a hierarchical process, where local elementary structures (LES) (not necessarily coincident with elements of secondary structure) are formed at the early stages of the folding and drive the protein, through the transition state and the postcritical folding nucleus (FN), resulting from the docking of the LES, to the native conformation.
q-bio.BM:The chiral nature of DNA plays a crucial role in cellular processes. Here we use magnetic tweezers to explore one of the signatures of this chirality, the coupling between stretch and twist deformations. We show that the extension of a stretched DNA molecule increases linearly by 0.42 nm per excess turn applied to the double helix. This result contradicts the intuition that DNA should lengthen as it is unwound and get shorter with overwinding. We then present numerical results of energy minimizations of torsionally restrained DNA that display a behaviour similar to the experimental data and shed light on the molecular details of this surprising effect.
q-bio.BM:PCR (Polymerase Chain Reaction), a method which replicates a selected sequence of DNA, has revolutionized the study of genomic material, but mathematical study of the process has been limited to simple deterministic models or descriptions relying on stochastic processes. In this paper we develop a suite of deterministic models for the reactions of quantitative PCR (Polymerase Chain Reaction) based on the law of mass action. Maps are created from DNA copy number in one cycle to the next, with ordinary differential equations describing the evolution of difference molecular species during each cycle. Qualitative analysis is preformed at each stage and parameters are estimated by fitting each model to data from Roche LightCycler (TM) runs.
q-bio.BM:Kinetics of folding of a protein held in a force-clamp are compared to an unconstrained folding. The comparison is made within a simple topology-based dynamical model of ubiquitin. We demonstrate that the experimentally observed variations in the end-to-end distance reflect microscopic events during folding. However, the folding scenarios in and out of the force-clamp are distinct.
q-bio.BM:In the template-assistance model, normal prion protein (PrPC), the pathogenic cause of prion diseases such as Creutzfeldt-Jakob (CJD) in human, Bovine Spongiform Encephalopathy (BSE) in cow, and scrapie in sheep, converts to infectious prion (PrPSc) through an autocatalytic process triggered by a transient interaction between PrPC and PrPSc. Conventional studies suggest the S1-H1-S2 region in PrPC to be the template of S1-S2 $\beta$-sheet in PrPSc, and the conformational conversion of PrPC into PrPSc may involve an unfolding of H1 in PrPC and its refolding into the $\beta$-sheet in PrPSc. Here we conduct a series of simulation experiments to test the idea of transient interaction of the template-assistance model. We find that the integrity of H1 in PrPC is vulnerable to a transient interaction that alters the native dihedral angles at residue Asn$^{143}$, which connects the S1 flank to H1, but not to interactions that alter the internal structure of the S1 flank, nor to those that alter the relative orientation between H1 and the S2 flank.
q-bio.BM:The Yakushevich model of DNA torsion dynamics supports soliton solutions, which are supposed to be of special interest for DNA transcription. In the discussion of the model, one usually adopts the approximation $\ell_0 \to 0$, where $\ell_0$ is a parameter related to the equilibrium distance between bases in a Watson-Crick pair. Here we analyze the Yakushevich model without $\ell_0 \to 0$. The model still supports soliton solutions indexed by two winding numbers $(n,m)$; we discuss in detail the fundamental solitons, corresponding to winding numbers (1,0) and (0,1) respectively.
q-bio.BM:The Yakushevich (Y) model provides a very simple pictures of DNA torsion dynamics, yet yields remarkably correct predictions on certain physical characteristics of the dynamics. In the standard Y model, the interaction between bases of a pair is modelled by a harmonic potential, which becomes anharmonic when described in terms of the rotation angles; here we substitute to this different types of improved potentials, providing a more physical description of the H-bond mediated interactions between the bases. We focus in particular on soliton solutions; the Y model predicts the correct size of the nonlinear excitations supposed to model the ``transcription bubbles'', and this is essentially unchanged with the improved potential. Other features of soliton dynamics, in particular curvature of soliton field configurations and the Peierls-Nabarro barrier, are instead significantly changed.
q-bio.BM:Simple coarse-grained models, such as the Gaussian Network Model, have been shown to capture some of the features of equilibrium protein dynamics. We extend this model by using atomic contacts to define residue interactions and introducing more than one interaction parameter between residues. We use B-factors from 98 ultra-high resolution X-ray crystal structures to optimize the interaction parameters. The average correlation between GNM fluctuation predictions and the B-factors is 0.64 for the data set, consistent with a previous large-scale study. By separating residue interactions into covalent and noncovalent, we achieve an average correlation of 0.74, and addition of ligands and cofactors further improves the correlation to 0.75. However, further separating the noncovalent interactions into nonpolar, polar, and mixed yields no significant improvement. The addition of simple chemical information results in better prediction quality without increasing the size of the coarse-grained model.
q-bio.BM:Background: One-dimensional protein structures such as secondary structures or contact numbers are useful for three-dimensional structure prediction and helpful for intuitive understanding of the sequence-structure relationship. Accurate prediction methods will serve as a basis for these and other purposes. Results: We implemented a program CRNPRED which predicts secondary structures, contact numbers and residue-wise contact orders. This program is based on a novel machine learning scheme called critical random networks. Unlike most conventional one-dimensional structure prediction methods which are based on local windows of an amino acid sequence, CRNPRED takes into account the whole sequence. CRNPRED achieves, on average per chain, Q3 = 81% for secondary structure prediction, and correlation coefficients of 0.75 and 0.61 for contact number and residue-wise contact order predictions, respectively. Conclusion: CRNPRED will be a useful tool for computational as well as experimental biologists who need accurate one-dimensional protein structure predictions.
q-bio.BM:To confer high specificity and affinity in binding, contacts at interfaces between two interacting macromolecules are expected to exhibit pair preferences for types of atoms or residues. Here we quantify these preferences by measuring the mutual information of contacts for 895 protein-protein interfaces. The information content is significant and is highest at the atomic resolution. A simple phenomenological theory reveals a connection between information at interfaces and the free energy spectrum of association. The connection is presented in the form of a relation between mutual information and the energy gap of the native bound state to off-target bound states. Measurement of information content in designed lattice interfaces show the predicted scaling behavior to the energy gap. Our theory also suggests that mutual information in contacts emerges by a selection mechanism, and that strong selection, or high conservation, of residues should lead to correspondingly high mutual information. Amino acids which contribute more heavily to information content are then expected to be more conserved. We verify this by showing a statistically significant correlation between the conservation of each of the twenty amino acids and their individual contribution to the information content at protein-protein interfaces
q-bio.BM:It was first suggested by Englander et al to model the nonlinear dynamics of DNA relevant to the transcription process in terms of a chain of coupled pendulums. In a related paper [q-bio.BM/0604014] we argued for the advantages of an extension of this approach based on considering a chain of double pendulums with certain characteristics. Here we study a simplified model of this kind, focusing on its general features and nonlinear travelling wave excitations; in particular, we show that some of the degrees of freedom are actually slaved to others, allowing for an effective reduction of the relevant equations.
q-bio.BM:The Fast Fourier Transform (FFT) correlation approach to protein-protein docking can evaluate the energies of billions of docked conformations on a grid if the energy is described in the form of a correlation function. Here, this restriction is removed, and the approach is efficiently used with pairwise interactions potentials that substantially improve the docking results. The basic idea is approximating the interaction matrix by its eigenvectors corresponding to the few dominant eigenvalues, resulting in an energy expression written as the sum of a few correlation functions, and solving the problem by repeated FFT calculations. In addition to describing how the method is implemented, we present a novel class of structure based pairwise intermolecular potentials. The DARS (Decoys As the Reference State) potentials are extracted from structures of protein-protein complexes and use large sets of docked conformations as decoys to derive atom pair distributions in the reference state. The current version of the DARS potential works well for enzyme-inhibitor complexes. With the new FFT-based program, DARS provides much better docking results than the earlier approaches, in many cases generating 50\% more near-native docked conformations. Although the potential is far from optimal for antibody-antigen pairs, the results are still slightly better than those given by an earlier FFT method. The docking program PIPER is freely available for non-commercial applications.
q-bio.BM:We investigated the structural relaxation of myosin motor domain from the pre-power stroke state to the near-rigor state using molecular dynamics simulation of a coarse-grained protein model. To describe the structural change, we propose a "dual Go-model," a variant of the Go-like model that has two reference structures. The nucleotide dissociation process is also studied by introducing a coarse-grained nucleotide in the simulation. We found that the myosin structural relaxation toward the near-rigor conformation cannot be completed before the nucleotide dissociation. Moreover, the relaxation and the dissociation occurred cooperatively when the nucleotide was tightly bound to the myosin head. The result suggested that the primary role of the nucleotide is to suppress the structural relaxation.
q-bio.BM:Self-similar properties of the ribosome in terms of the mass fractal dimension are investigated. We find that both the 30S subunit and the 16S rRNA have fractal dimensions of 2.58 and 2.82, respectively; while the 50S subunit as well as the 23S rRNA has the mass fractal dimension close to 3, implying a compact three dimensional macromolecule. This finding supports the dynamic and active role of the 30S subunit in the protein synthesis, in contrast to the pass role of the 50S subunit.
q-bio.BM:We present an extremely simplified model of multiple-domains polymer stretching in an atomic force microscopy experiment. We portray each module as a binary set of contacts and decompose the system energy into a harmonic term (the cantilever) and long-range interactions terms inside each domain. Exact equilibrium computations and Monte Carlo simulations qualitatively reproduce the experimental saw-tooth pattern of force-extension profiles, corresponding (in our model) to first-order phase transitions. We study the influence of the coupling induced by the cantilever and the pulling speed on the relative heights of the force peaks. The results suggest that the increasing height of the critical force for subsequent unfolding events is an out-of-equilibrium effect due to a finite pulling speed. The dependence of the average unfolding force on the pulling speed is shown to reproduce the experimental logarithmic law.
q-bio.BM:Phi-values are experimental measures of the effects of mutations on the folding kinetics of a protein. A central question is which structural information Phi-values contain about the transition state of folding. Traditionally, a Phi-value is interpreted as the 'nativeness' of a mutated residue in the transition state. However, this interpretation is often problematic because it assumes a linear relation between the nativeness of the residue and its free-energy contribution. We present here a better structural interpretation of Phi-values for mutations within a given helix. Our interpretation is based on a simple physical model that distinguishes between secondary and tertiary free-energy contributions of helical residues. From a linear fit of our model to the experimental data, we obtain two structural parameters: the extent of helix formation in the transition state, and the nativeness of tertiary interactions in the transition state. We apply our model to all proteins with well-characterized helices for which more than 10 Phi-values are available: protein A, CI2, and protein L. The model captures nonclassical Phi-values <0 or >1 in these helices, and explains how different mutations at a given site can lead to different Phi-values.
q-bio.BM:A model for the unidirectional movement of dynein is presented based on structural observations and biochemical experimental results available. In this model, the binding affinity of dynein for microtubule is independent of its nucleotide state and the change between strong and weak microtubule-binding is determined naturally by the variation of relative orientation between the stalk and microtubule as the stalk rotates following nucleotide-state transition. Thus the enigmatic communication from the ATP binding site in the globular domain to the far MT-binding site in the tip of the stalk, which is prerequisite in conventional models, is not required. Using the present model, the previous experimental results such as the effect of ATP and ADP bindings on dissociation of dynein from microtubule, the processive movement of single-headed axonemal dyneins at saturating ATP concentration, the load dependence of step size for the processive movement of two-headed cytoplasmic dyneins and the dependence of stall force on ATP concentration can be well explained.
q-bio.BM:Over the last 10-15 years a general understanding of the chemical reaction of protein folding has emerged from statistical mechanics. The lessons learned from protein folding kinetics based on energy landscape ideas have benefited protein structure prediction, in particular the development of coarse grained models. We survey results from blind structure prediction. We explore how second generation prediction energy functions can be developed by introducing information from an ensemble of previously simulated structures. This procedure relies on the assumption of a funnelled energy landscape keeping with the principle of minimal frustration. First generation simulated structures provide an improved input for associative memory energy functions in comparison to the experimental protein structures chosen on the basis of sequence alignment.
q-bio.BM:The precise details of how myosin-V coordinates the biochemical reactions and mechanical motions of its two head elements to engineer effective processive molecular motion along actin filaments remain unresolved. We compare a quantitative kinetic model of the myosin-V walk, consisting of five basic states augmented by two further states to allow for futile hydrolysis and detachments, with experimental results for run lengths, velocities, and dwell times and their dependence on bulk nucleotide concentrations and external loads in both directions. The model reveals how myosin-V can use the internal strain in the molecule to synchronise the motion of the head elements. Estimates for the rate constants in the reaction cycle and the internal strain energy are obtained by a computational comparison scheme involving an extensive exploration of the large parameter space. This scheme exploits the fact that we have obtained analytic results for our reaction network, e.g. for the velocity but also the run length, diffusion constant and fraction of backward steps. The agreement with experiment is often reasonable but some open problems are highlighted, in particular the inability of such a general model to reproduce the reported dependence of run length on ADP. The novel way that our approach explores parameter space means that any confirmed discrepancies should give new insights into the reaction network model.
q-bio.BM:The prion protein (PrP) binds Cu2+ ions in the octarepeat domain of the N-terminal tail up to full occupancy at pH=7.4. Recent experiments show that the HGGG octarepeat subdomain is responsible for holding the metal bound in a square planar coordination. By using first principle ab initio molecular dynamics simulations of the Car-Parrinello type, the Cu coordination mode to the binding sites of the PrP octarepeat region is investigated. Simulations are carried out for a number of structured binding sites. Results for the complexes Cu(HGGGW)+(wat), Cu(HGGG) and the 2[Cu(HGGG)] dimer are presented. While the presence of a Trp residue and a H2O molecule does not seem to affect the nature of the Cu coordination, high stability of the bond between Cu and the amide Nitrogens of deprotonated Gly's is confirmed in the case of the Cu(HGGG) system. For the more interesting 2[Cu(HGGG)] dimer a dynamically entangled arrangement of the two monomers, with intertwined N-Cu bonds, emerges. This observation is consistent with the highly packed structure seen in experiments at full Cu occupancy.
q-bio.BM:We formulate a simple solvation potential based on a coarsed-grain representation of amino acids with two spheres modeling the $C_\alpha$ atom and an effective side-chain centroid. The potential relies on a new method for estimating the buried area of residues, based on counting the effective number of burying neighbours in a suitable way. This latter quantity shows a good correlation with the buried area of residues computed from all atom crystallographic structures. We check the discriminatory power of the solvation potential alone to identify the native fold of a protein from a set of decoys and show the potential to be considerably selective.
q-bio.BM:The aim of this work is to elucidate how physical principles of protein design are reflected in natural sequences that evolved in response to the thermal conditions of the environment. Using an exactly solvable lattice model, we design sequences with selected thermal properties. Compositional analysis of designed model sequences and natural proteomes reveals a specific trend in amino acid compositions in response to the requirement of stability at elevated environmental temperature, i.e. the increase of fractions of hydrophobic and charged amino acid residues at the expense of polar ones. We show that this from both ends of hydrophobicity scale trend is due to positive (to stabilize the native state) and negative (to destabilize misfolded states) components of protein design. Negative design strengthens specific repulsive nonnative interactions that appear in misfolded structures. A pressure to preserve specific repulsive interactions in non-native conformations may result in correlated mutations between amino acids which are far apart in the native state but may be in contact in misfolded conformations. Such correlated mutations are indeed found in TIM barrel and other proteins.
q-bio.BM:F-actin bundles constitute principal components of a multitude of cytoskeletal processes including stereocilia, filopodia, microvilli, neurosensory bristles, cytoskeletal stress fibers, and the sperm acrosome. The bending, buckling, and stretching behaviors of these processes play key roles in cellular functions ranging from locomotion to mechanotransduction and fertilization. Despite their central importance to cellular function, F-actin bundle mechanics remain poorly understood. Here, we demonstrate that bundle bending stiffness is a state-dependent quantity with three distinct regimes that are mediated by bundle dimensions in addition to crosslink properties. We calculate the complete state-dependence of the bending stiffness and elucidate the mechanical origin of each. A generic set of design parameters delineating the regimes in state-space is derived and used to predict the bending stiffness of a variety of F-actin bundles found in cells. Finally, the broad and direct implications that the isolated state-dependence of F-actin bundle stiffness has on the interpretation of the bending, buckling, and stretching behavior of cytoskeletal bundles is addressed.
q-bio.BM:In this work we develop a theory of interaction of randomly patterned surfaces as a generic prototype model of protein-protein interactions. The theory predicts that pairs of randomly superimposed identical (homodimeric) random patterns have always twice as large magnitude of the energy fluctuations with respect to their mutual orientation, as compared with pairs of different (heterodimeric) random patterns. The amplitude of the energy fluctuations is proportional to the square of the average pattern density, to the square of the amplitude of the potential and its characteristic length, and scales linearly with the area of surfaces. The greater dispersion of interaction energies in the ensemble of homodimers implies that strongly attractive complexes of random surfaces are much more likely to be homodimers, rather than heterodimers. Our findings suggest a plausible physical reason for the anomalously high fraction of homodimers observed in real protein interaction networks.
q-bio.BM:We extend our previously developed general approach (1) to study a phenomenological model in which the simulated packing of hard, attractive spheres on a prolate spheroid surface with convexity constraints produces structures identical to those of prolate virus capsid structures. Our simulation approach combines the traditional Monte Carlo method with the method of random sampling on an ellipsoidal surface and a convex hull searching algorithm. Using this approach we study the assembly and structural origin of non-icosahedral, elongated virus capsids, such as two aberrant flock house virus (FHV) particles and the prolate prohead of bacteriophage phi29, and discuss the implication of our simulation results in the context of recent experimental findings.
q-bio.BM:The need to understand the assembly kinetics of fibril formation has become urgent because of the realization that soluble oligomers of amyloidogenic peptides may be even more neurotoxic than the end product, namely, the amyloid fibrils. In order to fully understand the routes to fibril formation one has to characterize the major species in the assembly pathways. The characterization of the energetics and dynamics of oligomers (dimers, trimers etc) is difficult using experiments alone because they undergo large conformational fluctuations. In this context, carefully planned molecular dynamics simulation studies, computations using coarse-grained models, and bioinformatic analysis have given considerable insights into the early events in the route to fibril formation. Here, we describe progress along this direction using examples taken largely from our own work. In this chapter, we focus on aspects of protein aggregation using Abeta-peptides and prion proteins as examples.
q-bio.BM:The native three dimensional structure of a single protein is determined by the physico chemical nature of its constituent amino acids. The twenty different types of amino acids, depending on their physico chemical properties, can be grouped into three major classes - hydrophobic, hydrophilic and charged. We have studied the anatomy of the weighted and unweighted networks of hydrophobic, hydrophilic and charged residues separately for a large number of proteins. Our results show that the average degree of the hydrophobic networks has significantly larger value than that of hydrophilic and charged networks. The average degree of the hydrophilic networks is slightly higher than that of charged networks. The average strength of the nodes of hydrophobic networks is nearly equal to that of the charged network; whereas that of hydrophilic networks has smaller value than that of hydrophobic and charged networks. The average strength for each of the three types of networks varies with its degree. The average strength of a node in charged networks increases more sharply than that of the hydrophobic and hydrophilic networks. Each of the three types of networks exhibits the 'small-world' property. Our results further indicate that the all amino acids' networks and hydrophobic networks are of assortative type. While maximum of the hydrophilic and charged networks are of assortative type, few others have the characteristics of disassortative mixing of the nodes. We have further observed that all amino acids' networks and hydrophobic networks bear the signature of hierarchy; whereas the hydrophilic and charged networks do not have any hierarchical signature.
q-bio.BM:We study statistical properties of interacting protein-like surfaces and predict two strong, related effects: (i) statistically enhanced self-attraction of proteins; (ii) statistically enhanced attraction of proteins with similar structures. The effects originate in the fact that the probability to find a pattern self-match between two identical, even randomly organized interacting protein surfaces is always higher compared with the probability for a pattern match between two different, promiscuous protein surfaces. This theoretical finding explains statistical prevalence of homodimers in protein-protein interaction networks reported earlier. Further, our findings are confirmed by the analysis of curated database of protein complexes that showed highly statistically significant overrepresentation of dimers formed by structurally similar proteins with highly divergent sequences (superfamily heterodimers). We predict that significant fraction of heterodimers evolved from homodimers with the negative design evolutionary pressure applied against promiscuous homodimer formation. This is achieved through the formation of highly specific contacts formed by charged residues as demonstrated both in model and real superfamily heterodimers
q-bio.BM:Stretching of a protein by a fluid flow is compared to that in a force-clamp apparatus. The comparison is made within a simple topology-based dynamical model of a protein in which the effects of the flow are implemented using Langevin dynamics. We demonstrate that unfolding induced by a uniform flow shows a richer behavior than that in the force clamp. The dynamics of unfolding is found to depend strongly on the selection of the amino acid, usually one of the termini, which is anchored. These features offer potentially wider diagnostic tools to investigate structure of proteins compared to experiments based on the atomic force microscopy.
q-bio.BM:Secretion and role of autotaxin and lysophosphatidic acid in adipose tissue In obesity, adipocyte hypertrophy is often associated with recrutement of new fat cells (adipogenesis) under the control of circulating and local regulatory factors. Among the different lipids released in the extracellular compartment of adipocytes, our group found the presence of lysophosphatidic acid (LPA). LPA is a bioactive phospholipid able to regulate several cell responses via the activation of specific G-protein coupled membrane receptors. Our group found that LPA increases preadipocyte proliferation and inhibits adipogenesis via the activation of LPA1 receptor subtype. Extracellular LPA-synthesis is catalyzed by a lysophospholipase D secreted by adipocytes : autotaxin (ATX). Adipocyte ATX expression strongly increases with adipogenesis as well as in individuals exhibiting type 2 diabetes associated with massive obesity. A possible contribution of ATX and LPA as paracrine regulators of adipogenesis and obesity associated diabetes is proposed.
q-bio.BM:A recently proposed model of non-autocatalytic reactions in dipeptide reactions leading to spontaneous symmetry breaking and homochirality is examined. The model is governed by activation, polymerization, epimerization and depolymerization of amino acids. Symmetry breaking is primarily a consequence of the fact that the rates of reactions involving homodimers and heterodimers are different, i.e., stereoselective, and on the fact that epimerization can only occur on the N-terminal residue and not on the Cterminal residue. This corresponds to an auto-inductive cyclic process that works only in one sense. It is argued that epimerization mimics both autocatalytic behavior as well as mutual antagonism - both of which were known to be crucial for producing full homochirality.
q-bio.BM:The structural organisation of the viral genome within its protein container, called the viral capsid, is an important aspect of virus architecture. Many single-stranded (ss) RNA viruses organise a significant part of their genome in a dodecahedral cage as a RNA duplex structure that mirrors the symmetry of the capsid. Bruinsma and Rudnick have suggested a model for the structural organisation of the RNA in these cages. It is the purpose of this paper to further develop their approach based on results from the areas of graph theory and DNA network engineering. We start by suggesting a scenario for pariacoto virus, a representative of this class of viruses, that is energetically more favorable than those derived previously. We then show that it is a representative of a whole family of cage structures that abide to the same construction principle, and then derive the energetically optimal configuration for a second family of cage structures along similar lines. Finally, we give reasons for the conjecture that these two families are more likely to occur in nature than other scenarios.
q-bio.BM:The sequence-dependent elasticity of double-helical DNA on a nm length scale can be captured by the rigid base-pair model, whose strains are the relative position and orientation of adjacent base-pairs. Corresponding elastic potentials have been obtained from all-atom MD simulation and from high-resolution structural data. On the scale of a hundred nm, DNA is successfully described by a continuous worm-like chain model with homogeneous elastic properties characterized by a set of four elastic constants, which have been directly measured in single-molecule experiments. We present here a theory that links these experiments on different scales, by systematically coarse-graining the rigid base-pair model for random sequence DNA to an effective worm-like chain description. The average helical geometry of the molecule is exactly taken into account in our approach. We find that the available microscopic parameters sets predict qualitatively similar mesoscopic parameters. The thermal bending and twisting persistence lengths computed from MD data are 42 and 48 nm, respectively. The static persistence lengths are generally much higher, in agreement with cyclization experiments. All microscopic parameter sets predict negative twist-stretch coupling. The variability and anisotropy of bending stiffness in short random chains lead to non-Gaussian bend angle distributions, but become unimportant after two helical turns.
q-bio.BM:Processive molecular motors take more-or-less uniformly sized steps, along spatially periodic tracks, mostly forwards but increasingly backwards under loads. Experimentally, the major steps can be resolved clearly within the noise but one knows biochemically that one or more mechanochemical substeps remain hidden in each enzymatic cycle. In order to properly interpret experimental data for back/forward step ratios, mean conditional step-to-step dwell times, etc., a first-passage analysis has been developed that takes account of hidden substeps in $N$-state sequential models. The explicit, general results differ significantly from previous treatments that identify the observed steps with complete mechanochemical cycles; e.g., the mean dwell times $\tau_+$ and $\tau_-$ prior to forward and back steps, respectively, are normally {\it unequal} although the dwell times $\tau_{++}$ and $\tau_{--}$ between {\it successive} forward and back steps are equal. Illustrative (N=2)-state examples display a wide range of behavior. The formulation extends to the case of two or more detectable transitions in a multistate cycle with hidden substeps.
q-bio.BM:For the vast majority of naturally occurring, small, single domain proteins folding is often described as a two-state process that lacks detectable intermediates. This observation has often been rationalized on the basis of a nucleation mechanism for protein folding whose basic premise is the idea that after completion of a specific set of contacts forming the so-called folding nucleus the native state is achieved promptly. Here we propose a methodology to identify folding nuclei in small lattice polymers and apply it to the study of protein molecules with chain length N=48. To investigate the extent to which protein topology is a robust determinant of the nucleation mechanism we compare the nucleation scenario of a native-centric model with that of a sequence specific model sharing the same native fold. To evaluate the impact of the sequence's finner details in the nucleation mechanism we consider the folding of two non- homologous sequences. We conclude that in a sequence-specific model the folding nucleus is, to some extent, formed by the most stable contacts in the protein and that the less stable linkages in the folding nucleus are solely determined by the fold's topology. We have also found that independently of protein sequence the folding nucleus performs the same `topological' function. This unifying feature of the nucleation mechanism results from the residues forming the folding nucleus being distributed along the protein chain in a similar and well-defined manner that is determined by the fold's topological features.
q-bio.BM:The folding of naturally occurring, single domain proteins is usually well-described as a simple, single exponential process lacking significant trapped states. Here we further explore the hypothesis that the smooth energy landscape this implies, and the rapid kinetics it engenders, arises due to the extraordinary thermodynamic cooperativity of protein folding. Studying Miyazawa-Jernigan lattice polymers we find that, even under conditions where the folding energy landscape is relatively optimized (designed sequences folding at their temperature of maximum folding rate), the folding of protein-like heteropolymers is accelerated when their thermodynamic cooperativity enhanced by enhancing the non-additivity of their energy potentials. At lower temperatures, where kinetic traps presumably play a more significant role in defining folding rates, we observe still greater cooperativity-induced acceleration. Consistent with these observations, we find that the folding kinetics of our computational models more closely approximate single-exponential behavior as their cooperativity approaches optimal levels. These observations suggest that the rapid folding of naturally occurring proteins is, at least in part, consequences of their remarkably cooperative folding.
q-bio.BM:An increasing number of proteins are being discovered with a remarkable and somewhat surprising feature, a knot in their native structures. How the polypeptide chain is able to knot itself during the folding process to form these highly intricate protein topologies is not known. Here, we perform a computational study on the 160-amino acid homodimeric protein YibK which, like other proteins in the SpoU family of MTases, contains a deep trefoil knot in its C-terminal region. In this study, we use a coarse-grained C-alpha-chain representation and Langevin dynamics to study folding kinetics. We find that specific, attractive nonnative interactions are critical for knot formation. In the absence of these interactions, i.e. in an energetics driven entirely by native interactions, knot formation is exceedingly unlikely. Further, we find, in concert with recent experimental data on YibK, two parallel folding pathways which we attribute to an early and a late formation of the trefoil knot, respectively. For both pathways, knot formation occurs before dimerization. A bioinformatics analysis of the SpoU family of proteins reveals further that the critical nonnative interactions may originate from evolutionary conserved hydrophobic segments around the knotted region.
q-bio.BM:The structure of the self-cleaving hairpin ribozyme is well characterized, and its folding has been examined in bulk and by single-molecule fluorescence, establishing the importance of cations, especially magnesium in the stability of the native fold. Here we describe the first all-atom folding simulations of the hairpin ribozyme, using a version of a Go potential with separate secondary and tertiary structure energetic contributions. The ratio of tertiary/secondary interaction energies serves as a proxy for non-specific cation binding: a high ratio corresponds to a high concentration, while a low one mimics low concentration. By studying the unfolding behavior of the RNA over a range of temperature and tertiary/secondary energies, a three-state phase diagram emerges, with folded, unfolded (coil) and transient folding/unfolding tertiary structure species. The thermodynamics were verified by paired folding simulations in each region of the phase diagram. The three phase behaviors correspond with experimentally observed states, so this simple model captures the essential aspect of thermodynamics in RNA folding.
q-bio.BM:The refolding from stretched initial conformations of ubiquitin (PDB ID: 1ubq) under the quenched force is studied using the Go model and the Langevin dynamics. It is shown that the refolding decouples the collapse and folding kinetics. The force quench refolding times scale as tau_F ~ exp(f_q*x_F/k_B*T), where f_q is the quench force and x_F = 0.96 nm is the location of the average transition state along the reaction coordinate given by the end-to-end distance. This value is close to x_F = 0.8 nm obtained from the force-clamp experiments. The mechanical and thermal unfolding pathways are studied and compared with the experimental and all-atom simulation results in detail. The sequencing of thermal unfolding was found to be markedly different from the mechanical one. It is found that fixing the N-terminus of ubiquitin changes its mechanical unfolding pathways much more drastically compared to the case when the C-end is anchored. We obtained the distance between the native state and the transition state x_UF=0.24 nm which is in reasonable agreement with the experimental data.
q-bio.BM:Natural proteins fold to a unique, thermodynamically dominant state. Modeling of the folding process and prediction of the native fold of proteins are two major unsolved problems in biophysics. Here, we show successful all-atom ab initio folding of a representative diverse set of proteins, using a minimalist transferable energy model that consists of two-body atom-atom interactions, hydrogen-bonding, and a local sequence energy term that models sequence-specific chain stiffness. Starting from a random coil, the native-like structure was observed during replica exchange Monte Carlo (REMC) simulation for most proteins regardless of their structural classes; the lowest energy structure was close to native- in the range of 2-6 A root-mean-square deviation (RMSD). Our results demonstrate that the successful all-atom folding of a protein chain to its native state is governed by only a few crucial energetic terms.
q-bio.BM:Protein-DNA interactions are vital for many processes in living cells, especially transcriptional regulation and DNA modification. To further our understanding of these important processes on the microscopic level, it is necessary that theoretical models describe the macromolecular interaction energetics accurately. While several methods have been proposed, there has not been a careful comparison of how well the different methods are able to predict biologically important quantities such as the correct DNA binding sequence, total binding free energy, and free energy changes caused by DNA mutation. In addition to carrying out the comparison, we present two important theoretical models developed initially in protein folding that have not yet been tried on protein-DNA interactions. In the process, we find that the results of these knowledge-based potentials show a strong dependence on the interaction distance and the derivation method. Finally, we present a knowledge-based potential that gives comparable or superior results to the best of the other methods, including the molecular mechanics force field AMBER99.
q-bio.BM:The folding of the alpha-helice domain hbSBD of the mammalian mitochondrial branched-chain alpha-ketoacid dehydrogenase (BCKD) complex is studied by the circular dichroism technique in absence of urea. Thermal denaturation is used to evaluate various thermodynamic parameters defining the equilibrium unfolding, which is well described by the two-state model with the folding temperature T_f = 317.8 K and the enthalpy change Delta H_g = 19.67 kcal/mol. The folding is also studied numerically using the off-lattice coarse-grained Go model and the Langevin dynamics. The obtained results, including the population of the native basin, the free energy landscape as a function of the number of native contacts and the folding kinetics, also suggest that the hbSBD domain is a two-state folder. These results are consistent with the biological function of hbSBD in BCKD.
q-bio.BM:We analyze the dependence of cooperativity of the thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, $N$, using lattice models with side chains,off-lattice Go models and the available experimental data. A dimensionless measure of cooperativity, $\Omega_c$ ($0 < \Omega_c < \infty$), scales as $\Omega_c \sim N^{\zeta}$. The results of simulations and the analysis of experimental data further confirm the earlier prediction that $\zeta$ is universal with $\zeta = 1 +\gamma$, where exponent $\gamma$ characterizes the susceptibility of a self-avoiding walk. This finding suggests that the structural characteristics in the denaturated state are manifested in the folding cooperativity at the transition temperature. The folding rates $k_F$ for the Go models and a dataset of 69 proteins can be fit using $k_F = k_F^0 \exp(-cN^\beta)$. Both $\beta = 1/2$ and 2/3 provide a good fit of the data. We find that $k_F = k_F^0 \exp(-cN^{{1/2}})$, with the average (over the dataset of proteins) $k_F^0 \approx (0.2\mu s)^{-1}$ and $c \approx 1.1$, can be used to estimate folding rates to within an order of magnitude in most cases. The minimal models give identical $N$ dependence with $c \approx 1$. The prefactor for off-lattice Go models is nearly four orders of magnitude larger than the experimental value.
q-bio.BM:The didemnins represent a versatile class of depsipeptides of marine origin and hold a great deal of potential for biomedical application. The biological and geographical origins of the didemnins are reviewed in addition to the chemical structures of the major didemnins. The biological mechanisms behind the antiviral and anticancer effects of selected didemnins are summarized and the special case of dehydrodidemnin B (Aplidin) is expounded upon including structural characteristics, synthesis, pharmacological mechanism and a discussion of its current clinical trials as an anticancer agent.
q-bio.BM:TThe paper had many errors.
q-bio.BM:Pathological folding and oligomer formation of the amyloid beta-protein (Abeta) are widely perceived as central to Alzheimer's disease (AD). Experimental approaches to study Abeta self-assembly are problematic, because most relevant aggregates are quasi-stable and inhomogeneous. We apply a discrete molecular dynamics (DMD) approach combined with a four-bead protein model to study oligomer formation of the amyloid beta-protein (Abeta). We address the differences between the two most common Abeta alloforms, Abeta40 and Abeta42, which oligomerize differently in vitro. We study how the presence of electrostatic interactions (EIs) between pairs of charged amino acids affects Abeta40 and Abeta42 oligomer formation. Our results indicate that EIs promote formation of larger oligomers in both Abeta40 and Abeta42. The Abeta40 size distribution remains unimodal, whereas the Abeta42 distribution is trimodal, as observed experimentally. Abeta42 folded structure is characterized by a turn in the C-terminus that is not present in Abeta40. We show that the same C-terminal region is also responsible for the strongest intermolecular contacts in Abeta42 pentamers and larger oligomers. Our results suggest that this C-terminal region plays a key role in the formation of Abeta42 oligomers and the relative importance of this region increases in the presence of EIs. These results suggest that inhibitors targeting the C-terminal region of Abeta42 oligomers may be able to prevent oligomer formation or structurally modify the assemblies to reduce their toxicity.
q-bio.BM:The results of Brownian dynamics simulations of a single DNA molecule in shear flow are presented taking into account the effect of internal viscosity. The dissipative mechanism of internal viscosity is proved necessary in the research of DNA dynamics. A stochastic model is derived on the basis of the balance equation for forces acting on the chain. The Euler method is applied to the solution of the model. The extensions of DNA molecules for different Weissenberg numbers are analyzed. Comparison with the experimental results available in the literature is carried out to estimate the contribution of the effect of internal viscosity.
q-bio.BM:The network paradigm is increasingly used to describe the topology and dynamics of complex systems. Here we review the results of the topological analysis of protein structures as molecular networks describing their small-world character, and the role of hubs and central network elements in governing enzyme activity, allosteric regulation, protein motor function, signal transduction and protein stability. We summarize available data how central network elements are enriched in active centers and ligand binding sites directing the dynamics of the entire protein. We assess the feasibility of conformational and energy networks to simplify the vast complexity of rugged energy landscapes and to predict protein folding and dynamics. Finally, we suggest that modular analysis, novel centrality measures, hierarchical representation of networks and the analysis of network dynamics will soon lead to an expansion of this field.
q-bio.BM:Annealed importance sampling is a means to assign equilibrium weights to a nonequilibrium sample that was generated by a simulated annealing protocol. The weights may then be used to calculate equilibrium averages, and also serve as an ``adiabatic signature'' of the chosen cooling schedule. In this paper we demonstrate the method on the 50-atom dileucine peptide, showing that equilibrium distributions are attained for manageable cooling schedules. For this system, as naively implemented here, the method is modestly more efficient than constant temperature simulation. However, the method is worth considering whenever any simulated heating or cooling is performed (as is often done at the beginning of a simulation project, or during an NMR structure calculation), as it is simple to implement and requires minimal additional CPU expense. Furthermore, the naive implementation presented here can be improved.
q-bio.BM:Conformational transitions in macromolecular complexes often involve the reorientation of lever-like structures. Using a simple theoretical model, we show that the rate of such transitions is drastically enhanced if the lever is bendable, e.g. at a localized "hinge''. Surprisingly, the transition is fastest with an intermediate flexibility of the hinge. In this intermediate regime, the transition rate is also least sensitive to the amount of "cargo'' attached to the lever arm, which could be exploited by molecular motors. To explain this effect, we generalize the Kramers-Langer theory for multi-dimensional barrier crossing to configuration dependent mobility matrices.
q-bio.BM:We report 10 successfully folding events of trpzip2 by molecular dynamics simulation. It is found that the trizip2 can fold into its native state through different zipper pathways, depending on the ways of forming hydrophobic core. We also find a very fast non-zipper pathway. This indicates that there may be no inconsistencies in the current pictures of beta-hairpin folding mechanisms. These pathways occur with different probabilities. zip-out is the most probable one. This may explain the recent experiment that the turn formation is the rate-limiting step for beta-hairpin folding.
q-bio.BM:Structural fluctuations in the thermal equilibrium of the kinesin motor domain are studied using a lattice protein model with Go interactions. By means of the multi-self-overlap ensemble (MSOE) Monte Carlo method and the principal component analysis (PCA), the free-energy landscape is obtained. It is shown that kinesins have two subdomains that exhibit partial folding/unfolding at functionally important regions: one is located around the nucleotide binding site and the other includes the main microtubule binding site. These subdomains are consistent with structural variability that was reported recently based on experimentally-obtained structures. On the other hand, such large structural fluctuations have not been captured by B-factor or normal mode analyses. Thus, they are beyond the elastic regime, and it is essential to take into account chain connectivity for studying the function of kinesins.
q-bio.BM:We introduce a topology-based nonlinear network model of protein dynamics with the aim of investigating the interplay of spatial disorder and nonlinearity. We show that spontaneous localization of energy occurs generically and is a site-dependent process. Localized modes of nonlinear origin form spontaneously in the stiffest parts of the structure and display site-dependent activation energies. Our results provide a straightforward way for understanding the recently discovered link between protein local stiffness and enzymatic activity. They strongly suggest that nonlinear phenomena may play an important role in enzyme function, allowing for energy storage during the catalytic process.
q-bio.BM:We incorporate hydrodynamic interactions in a structure-based model of ubiquitin and demonstrate that the hydrodynamic coupling may reduce the peak force when stretching the protein at constant speed, especially at larger speeds. Hydrodynamic interactions are also shown to facilitate unfolding at constant force and inhibit stretching by fluid flows.
q-bio.BM:We demonstrate a new algorithm for finding protein conformations that minimize a non-bonded energy function. The new algorithm, called the difference map, seeks to find an atomic configuration that is simultaneously in two constraint spaces. The first constraint space is the space of atomic configurations that have a valid peptide geometry, while the second is the space of configurations that have a non-bonded energy below a given target. These two constraint spaces are used to define a deterministic dynamical system, whose fixed points produce atomic configurations in the intersection of the two constraint spaces. The rate at which the difference map produces low energy protein conformations is compared with that of a contemporary search algorithm, parallel tempering. The results indicate the difference map finds low energy protein conformations at a significantly higher rate then parallel tempering.
q-bio.BM:Vibrational energy transfer of the amide I mode of N-methylacetamide (NMA) is studied theoretically using the vibrational configuration interaction method. A quartic force field of NMA is constructed at the B3LYP/6-31G+(d) level of theory and its accuarcy is checked by comparing the resulting anharmonic frequencies with available theoretical and experimental values. Quantum dynamics calculations for the amide I mode excitation clarify the dominant energy transfer pathways, which sensitively depend on the anharmonic couplings among vibrational modes. A ratio of the anharmonic coupling to the frequency mismatch is employed to predict and interpret the dominant energy flow pathways.
q-bio.BM:It previously has been discovered that visible light irradiation of crystalline substrates can lead to enhancement of subsequent enzymatic reaction rates as sharply peaked oscillatory functions of irradiation time. The particular activating irradiation times can vary with source of a given enzyme and thus, presumably, its molecular structure. The experiments reported here demonstrate that the potential for this anomalous enzyme reaction rate enhancement can be transferred from one bacterial species to another coincident with transfer of the genetic determinant for the relevant enzyme. In particular, the effect of crystal-irradiated chloramphenicol on growth of bacterial strains in which a transferable R-factor DNA plasmid coding for chloramphenicol resistance was or was not present (S. panama R+, E. coli R+, and E. coli R-) was determined. Chloramphenicol samples irradiated 10, 35 and 60 sec produced increased growth rates (diminished inhibition) for the resistant S. panama and E. coli strains, while having no such effect on growth rate of the sensitive E. coli strain. Consistent with past findings, chloramphenicol samples irradiated 5, 30 and 55 sec produced decreased growth rates (increased inhibition) for all three strains.
q-bio.BM:Inherent structure theory is used to discover strong connections between simple characteristics of protein structure and the energy landscape of a Go model. The potential energies and vibrational free energies of inherent structures are highly correlated, and both reflect simple measures of networks of native contacts. These connections have important consequences for models of protein dynamics and thermodynamics.
q-bio.BM:The free-energy landscape of the alpha-helix of protein G is studied by means of metadynamics coupled with a solute tempering algorithm. Metadynamics allows to overcome large energy barriers, whereas solute tempering improves the sampling with an affordable computational effort. From the sampled free-energy surface we are able to reproduce a number of experimental observations, such as the fact that the lowest minimum corresponds to a globular conformation displaying some degree of beta-structure, that the helical state is metastable and involves only 65% of the chain. The calculations also show that the system populates consistently a pi-helix state and that the hydrophobic staple motif is present only in the free-energy minimum associated with the helices, and contributes to their stabilization. The use of metadynamics coupled with solute tempering results then particularly suitable to provide the thermodynamics of a short peptide, and its computational efficiency is promising to deal with larger proteins.
q-bio.BM:Using a time-dependent perturbation theory, vibrational energy relaxation (VER) of isotopically labeled amide I modes in cytochrome c solvated with water is investigated. Contributions to the VER are decomposed into two contributions from the protein and water. The VER pathways are visualized using radial and angular excitation functions for resonant normal modes. Key differences of VER among different amide I modes are demonstrated, leading to a detailed picture of the spatial anisotropy of the VER. The results support the experimental observation that amide I modes in proteins relax with sub picosecond timescales, while the relaxation mechanism turns out to be sensitive to the environment of the amide I mode.
q-bio.BM:Local minima and the saddle points separating them in the energy landscape are known to dominate the dynamics of biopolymer folding. Here we introduce a notion of a "folding funnel" that is concisely defined in terms of energy minima and saddle points, while at the same time conforming to a notion of a "folding funnel" as it is discussed in the protein folding literature.
q-bio.BM:Using magnetic tweezers to investigate the mechanical response of single chromatin fibers, we show that fibers submitted to large positive torsion transiently trap positive turns, at a rate of one turn per nucleosome. A comparison with the response of fibers of tetrasomes (the (H3-H4)2 tetramer bound with ~50 bp of DNA) obtained by depletion of H2A-H2B dimers, suggests that the trapping reflects a nucleosome chiral transition to a metastable form built on the previously documented righthanded tetrasome. In view of its low energy, <8 kT, we propose this transition is physiologically relevant and serves to break the docking of the dimers on the tetramer which in the absence of other factors exerts a strong block against elongation of transcription by the main RNA polymerase.
q-bio.BM:We perform extensive Monte Carlo simulations of a lattice model and the Go potential to investigate the existence of folding pathways at the level of contact cluster formation for two native structures with markedly different geometries. Our analysis of folding pathways revealed a common underlying folding mechanism, based on nucleation phenomena, for both protein models. However, folding to the more complex geometry (i.e. that with more non-local contacts) is driven by a folding nucleus whose geometric traits more closely resemble those of the native fold. For this geometry folding is clearly a more cooperative process.
q-bio.BM:In this study we evaluate, at full atomic detail, the folding processes of two small helical proteins, the B domain of protein A and the Villin headpiece. Folding kinetics are studied by performing a large number of ab initio Monte Carlo folding simulations using a single transferable all-atom potential. Using these trajectories, we examine the relaxation behavior, secondary structure formation, and transition-state ensembles (TSEs) of the two proteins and compare our results with experimental data and previous computational studies. To obtain a detailed structural information on the folding dynamics viewed as an ensemble process, we perform a clustering analysis procedure based on graph theory. Moreover, rigorous pfold analysis is used to obtain representative samples of the TSEs and a good quantitative agreement between experimental and simulated Fi-values is obtained for protein A. Fi-values for Villin are also obtained and left as predictions to be tested by future experiments. Our analysis shows that two-helix hairpin is a common partially stable structural motif that gets formed prior to entering the TSE in the studied proteins. These results together with our earlier study of Engrailed Homeodomain and recent experimental studies provide a comprehensive, atomic-level picture of folding mechanics of three-helix bundle proteins.
q-bio.BM:Strong experimental and theoretical evidence shows that transcription factors and other specific DNA-binding proteins find their sites using a two-mode search: alternating between 3D diffusion through the cell and 1D sliding along the DNA. We consider the role spatial effects in the mechanism on two different scales. First, we reconcile recent experimental findings by showing that the 3D diffusion of the transcription factor is often local, i.e. the transcription factor lands quite near its dissociation site. Second, we discriminate between two types of searches: global searches and local searches. We show that these searches differ significantly in average search time and the variability of search time. Using experimentally measured parameter values, we also show that 1D and 3D search is not optimally balanced, leading to much larger estimates of search time. Together, these results lead to a number of biological implications including suggestions of how prokaryotes and eukaryotes achieve rapid gene regulation and the relationship between the search mechanism and noise in gene expression.
q-bio.BM:Thermal shape fluctuations of grafted microtubules were studied using high resolution particle tracking of attached fluorescent beads. First mode relaxation times were extracted from the mean square displacement in the transverse coordinate. For microtubules shorter than 10 um, the relaxation times were found to follow an L^2 dependence instead of L^4 as expected from the standard wormlike chain model. This length dependence is shown to result from a complex length dependence of the bending stiffness which can be understood as a result of the molecular architecture of microtubules. For microtubules shorter than 5 um, high drag coefficients indicate contributions from internal friction to the fluctuation dynamics.
q-bio.BM:E. Coli. dihydrofolate reductase (DHFR) undergoes conformational transitions between the closed (CS) and occluded (OS) states which, respectively, describe whether the active site is closed or occluded by the Met20 loop. A sequence-based approach is used to identify a network of residues that represents the allostery wiring diagram. We also use a self-organized polymer model to monitor the kinetics of the CS->OS and the reverse transitions. a sliding motion of Met20 loop is observed. The residues that facilitate the Met20 loop motion are part of the network of residues that transmit allosteric signals during the CS->OS transition.
q-bio.BM:A model is presented to describe the nucleotide and repeat addition processivity by the telomerase. In the model, the processive nucleotide addition is implemented on the basis of two requirements: One is that stem IV loop stimulates the chemical reaction of nucleotide incorporation, and the other one is the existence of an ssRNA-binding site adjacent to the polymerase site that has a high affinity for the unpaired base of the template. The unpairing of DNA:RNA hybrid after the incorporation of the nucleotide paired with the last base on the template, which is the prerequisite for repeat addition processivity, is caused by a force acting on the primer. The force is resulted from the unfolding of stem III pseudoknot that is induced by the swinging of stem IV loop towards the nucleotide-bound polymerase site. Based on the model, the dynamics of processive nucleotide and repeat additions by Tetrahymena telomerase are quantitatively studied, which give good explanations to the previous experimental results. Moreover, some predictions are presented. In particular, it is predicted that the repeat addition processivity is mainly determined by the difference between the free energy required to disrupt the DNA:RNA hybrid and that required to unfold the stem III pseudoknot, with the large difference corresponding to a low repeat addition processivity while the small one corresponding to a high repeat addition processivity.
q-bio.BM:The Caspar-Klug classification of viruses whose protein shell, called viral capsid, exhibits icosahedral symmetry, has recently been extended to incorporate viruses whose capsid proteins are exclusively organised in pentamers. The approach, named `Viral Tiling Theory', is inspired by the theory of quasicrystals, where aperiodic Penrose tilings enjoy 5-fold and 10-fold local symmetries. This paper analyzes the extent to which this classification approach informs dynamical properties of the viral capsids, in particular the pattern of Raman active modes of vibrations, which can be observed experimentally.
q-bio.BM:Position-specific scoring matrices (PSSMs) are useful for detecting weak homology in protein sequence analysis, and they are thought to contain some essential signatures of the protein families. In order to elucidate what kind of ingredients constitute such family-specific signatures, we apply singular value decomposition to a set of PSSMs and examine the properties of dominant right and left singular vectors. The first right singular vectors were correlated with various amino acid indices including relative mutability, amino acid composition in protein interior, hydropathy, or turn propensity, depending on proteins. A significant correlation between the first left singular vector and a measure of site conservation was observed. It is shown that the contribution of the first singular component to the PSSMs act to disfavor potentially but falsely functionally important residues at conserved sites. The second right singular vectors were highly correlated with hydrophobicity scales, and the corresponding left singular vectors with contact numbers of protein structures. It is suggested that sequence alignment with a PSSM is essentially equivalent to threading supplemented with functional information. The presented method may be used to separate functionally important sites from structurally important ones, and thus it may be a useful tool for predicting protein functions.
q-bio.BM:A method to search for local structural similarities in proteins at atomic resolution is presented. It is demonstrated that a huge amount of structural data can be handled within a reasonable CPU time by using a conventional relational database management system with appropriate indexing of geometric data. This method, which we call geometric indexing, can enumerate ligand binding sites that are structurally similar to sub-structures of a query protein among more than 160,000 possible candidates within a few hours of CPU time on an ordinary desktop computer. After detecting a set of high scoring ligand binding sites by the geometric indexing search, structural alignments at atomic resolution are constructed by iteratively applying the Hungarian algorithm, and the statistical significance of the final score is estimated from an empirical model based on a gamma distribution. Applications of this method to several protein structures clearly shows that significant similarities can be detected between local structures of non-homologous as well as homologous proteins.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum stack-length. We show that the numbers of $k$-noncrossing structures without isolated base pairs are significantly smaller than the number of all $k$-noncrossing structures. In particular we prove that the number of 3- and 4-noncrossing RNA structures with stack-length $\ge 2$ is for large $n$ given by $311.2470 \frac{4!}{n(n-1)...(n-4)}2.5881^n$ and $1.217\cdot 10^{7} n^{-{21/2}} 3.0382^n$, respectively. We furthermore show that for $k$-noncrossing RNA structures the drop in exponential growth rates between the number of all structures and the number of all structures with stack-size $\ge 2$ increases significantly. Our results are of importance for prediction algorithms for pseudoknot-RNA and provide evidence that there exist neutral networks of RNA pseudoknot structures.
q-bio.BM:Network science is already making an impact on the study of complex systems and offers a promising variety of tools to understand their formation and evolution (1-4) in many disparate fields from large communication networks (5,6), transportation infrastructures (7) and social communities (8,9) to biological systems (1,10,11). Even though new highthroughput technologies have rapidly been generating large amounts of genomic data, drug design has not followed the same development, and it is still complicated and expensive to develop new single-target drugs. Nevertheless, recent approaches suggest that multi-target drug design combined with a network-dependent approach and large-scale systems-oriented strategies (12-14) create a promising framework to combat complex multigenetic disorders like cancer or diabetes. Here, we investigate the human network corresponding to the interactions between all US approved drugs and human therapies, defined by known drug-therapy relationships. Our results show that the key paths in this network are shorter than three steps, indicating that distant therapies are separated by a surprisingly low number of chemical compounds. We also identify a sub-network composed by drugs with high centrality measures (15), which represent the structural back-bone of the drug-therapy system and act as hubs routing information between distant parts of the network. These findings provide for the first time a global map of the largescale organization of all known drugs and associated therapies, bringing new insights on possible strategies for future drug development. Special attention should be given to drugs which combine the two properties of (a) having a high centrality value and (b) acting on multiple targets.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum arc- and stack-length. That is, we study the numbers of RNA pseudoknot structures with arc-length $\ge 3$, stack-length $\ge \sigma$ and in which there are at most $k-1$ mutually crossing bonds, denoted by ${\sf T}_{k,\sigma}^{[3]}(n)$. In particular we prove that the numbers of 3, 4 and 5-noncrossing RNA structures with arc-length $\ge 3$ and stack-length $\ge 2$ satisfy ${\sf T}_{3,2}^{[3]}(n)^{}\sim K_3 n^{-5} 2.5723^n$, ${\sf T}^{[3]}_{4,2}(n)\sim K_4 n^{-{21/2}} 3.0306^n$, and ${\sf T}^{[3]}_{5,2}(n)\sim K_5 n^{-18} 3.4092^n$, respectively, where $K_3,K_4,K_5$ are constants. Our results are of importance for prediction algorithms for RNA pseudoknot structures.
q-bio.BM:We have developed a new extended replica exchange method to study thermodynamics of a system in the presence of external force. Our idea is based on the exchange between different force replicas to accelerate the equilibrium process. We have shown that the refolding pathways of single ubiquitin depend on which terminus is fixed. If the N-end is fixed then the folding pathways are different compared to the case when both termini are free, but fixing the C-terminal does not change them. Surprisingly, we have found that the anchoring terminal does not affect the pathways of individual secondary structures of three-domain ubiquitin, indicating the important role of the multi-domain construction. Therefore, force-clamp experiments, in which one end of a protein is kept fixed, can probe the refolding pathways of a single free-end ubiquitin if one uses either the poly-ubiquitin or a single domain with the C-terminus anchored. However, it is shown that anchoring one end does not affect refolding pathways of the titin domain I27, and the force-clamp spectroscopy is always capable to predict folding sequencing of this protein. We have obtained the reasonable estimate for unfolding barrier of ubiqutin. The linkage between residue Lys48 and the C-terminal of ubiquitin is found to have the dramatic effect on the location of the transition state along the end-to-end distance reaction coordinate, but the multi-domain construction leaves the transition state almost unchanged. We have found that the maximum force in the force-extension profile from constant velocity force pulling simulations depends on temperature nonlinearly. However, for some narrow temperature interval this dependence becomes linear, as have been observed in recent experiments.
q-bio.BM:A construction method for duplex cage structures with icosahedral sym- metry made out of single-stranded DNA molecules is presented and applied to an icosidodecahedral cage. It is shown via a mixture of analytic and computer techniques that there exist realisations of this graph in terms of two circular DNA molecules. These blueprints for the organisation of a cage structure with a noncrystallographic symmetry may assist in the design of containers made from DNA for applications in nanotechnology.
q-bio.BM:We analyzed folding routes predicted by a variational model in terms of a generalized formalism of the capillarity scaling theory for 28 two-state proteins. The scaling exponent ranged from 0.2 to 0.45 with an average of 0.33. This average value corresponds to packing of rigid objects.That is, on average the folded core of the nucleus is found to be relatively diffuse. We also studied the growth of the folding nucleus and interface along the folding route in terms of the density or packing fraction. The evolution of the folded core and interface regions can be classified into three patterns of growth depending on how the growth of the folded core is balanced by changes in density of the interface. Finally, we quantified the diffuse versus polarized structure of the critical nucleus through direct calculation of the packing fraction of the folded core and interface regions. Our results support the general picture of describing protein folding as the capillarity-like growth of folding nuclei.
q-bio.BM:Biomolecular structures are assemblies of emergent anisotropic building modules such as uniaxial helices or biaxial strands. We provide an approach to understanding a marginally compact phase of matter that is occupied by proteins and DNA. This phase, which is in some respects analogous to the liquid crystal phase for chain molecules, stabilizes a range of shapes that can be obtained by sequence-independent interactions occurring intra- and intermolecularly between polymeric molecules. We present a singularityfree self-interaction for a tube in the continuum limit and show that this results in the tube being positioned in the marginally compact phase. Our work provides a unified framework for understanding the building blocks of biomolecules.
q-bio.BM:The mean time required by a transcription factor (TF) or an enzyme to find a target in the nucleus is of prime importance for the initialization of transcription, gene activation or the start of DNA repair. We obtain new estimates for the mean search time when the TF or enzyme, confined to the cell nucleus, can switch from a one dimensional motion along the DNA and a free Brownian regime inside the crowded nucleus. We give analytical expressions for the mean time the particle stays bound to the DNA, $\tau_{DNA}$, and the mean time it diffuses freely, $\tau_{free}$. Contrary to previous results but in agreement with experimental data, we find a factor $\tau_{DNA} \approx 3.7 \tau_{free}$ for the Lac-I TF. The formula obtained for the time required to bind to a target site is found to be coherent with observed data. We also conclude that a higher DNA density leads to a more efficient search process.
q-bio.BM:We develop coarse-grained models that describe the dynamic encapsidation of functionalized nanoparticles by viral capsid proteins. We find that some forms of cooperative interactions between protein subunits and nanoparticles can dramatically enhance rates and robustness of assembly, as compared to the spontaneous assembly of subunits into empty capsids. For large core-subunit interactions, subunits adsorb onto core surfaces en masse in a disordered manner, and then undergo a cooperative rearrangement into an ordered capsid structure. These assembly pathways are unlike any identified for empty capsid formation. Our models can be directly applied to recent experiments in which viral capsid proteins assemble around the functionalized inorganic nanoparticles [Sun et al., Proc. Natl. Acad. Sci (2007) 104, 1354]. In addition, we discuss broader implications for understanding the dynamic encapsidation of single-stranded genomic molecules during viral replication and for developing multicomponent nanostructured materials.
q-bio.BM:The total conformational energy is assumed to consist of pairwise interaction energies between atoms or residues, each of which is expressed as a product of a conformation-dependent function (an element of a contact matrix, C-matrix) and a sequence-dependent energy parameter (an element of a contact energy matrix, E-matrix). Such pairwise interactions in proteins force native C-matrices to be in a relationship as if the interactions are a Go-like potential [N. Go, Annu. Rev. Biophys. Bioeng. 12. 183 (1983)] for the native C-matrix, because the lowest bound of the total energy function is equal to the total energy of the native conformation interacting in a Go-like pairwise potential. This relationship between C- and E-matrices corresponds to (a) a parallel relationship between the eigenvectors of the C- and E-matrices and a linear relationship between their eigenvalues, and (b) a parallel relationship between a contact number vector and the principal eigenvectors of the C- and E-matrices; the E-matrix is expanded in a series of eigenspaces with an additional constant term, which corresponds to a threshold of contact energy that approximately separates native contacts from non-native ones. These relationships are confirmed in 182 representatives from each family of the SCOP database by examining inner products between the principal eigenvector of the C-matrix, that of the E-matrix evaluated with a statistical contact potential, and a contact number vector. In addition, the spectral representation of C- and E-matrices reveals that pairwise residue-residue interactions, which depends only on the types of interacting amino acids but not on other residues in a protein, are insufficient and other interactions including residue connectivities and steric hindrance are needed to make native structures the unique lowest energy conformations.
q-bio.BM:We consider an elastic rod model for twisted DNA in the plectonemic regime. The molecule is treated as an impenetrable tube with an effective, adjustable radius. The model is solved analytically and we derive formulas for the contact pressure, twisting moment and geometrical parameters of the supercoiled region. We apply our model to magnetic tweezer experiments of a DNA molecule subjected to a tensile force and a torque, and extract mechanical and geometrical quantities from the linear part of the experimental response curve. These reconstructed values are derived in a self-contained manner, and are found to be consistent with those available in the literature.
q-bio.BM:Nicodemi and Prisco recently proposed a model for X-chromosome inactivation in mammals, explaining this phenomenon in terms of a spontaneous symmetry-breaking mechanism [{\it Phys. Rev. Lett.} 99 (2007), 108104]. Here we provide a mean-field version of their model.
q-bio.BM:We present a theoretical investigation on possible selection of olfactory receptors (ORs) as sensing components of nanobiosensors. Accordingly, we generate the impedance spectra of the rat OR I7 in the native and activated state and analyze their differences. In this way, we connect the protein morphological transformation, caused by the sensing action, with its change of electrical impedance. The results are compared with those obtained by studying the best known protein of the GPCR family: bovine rhodopsin. Our investigations indicate that a change in morphology goes with a change in impedance spectrum mostly associated with a decrease of the static impedance up to about 60 % of the initial value, in qualitative agreement with existing experiments on rat OR I7. The predictiveness of the model is tested successfully for the case of recent experiments on bacteriorhodopsin. The present results point to a promising development of a new class of nanobiosensors based on the electrical properties of GPCR and other sensing proteins.
q-bio.BM:How molecular motors like Kinesin regulates the affinity to the rail protein in the process of ATP hydrolysis remains to be uncovered. To understand the regulation mechanism, we investigate the structural fluctuation of KIF1A in different nucleotide states that are realized in the ATP hydrolysis process by molecular dynamics simulations of Go-like model. We found that "alpha4 helix", which is a part of the microtubule (MT) binding site, changes its fluctuation systematically according to the nucleotide states. In particular, the frequency of large fluctuations of alpha4 strongly correlates with the affinity of KIF1A for microtubule. We also show how the strength of the thermal fluctuation and the interaction with the nucleotide affect the dynamics of microtubule binding site. These results suggest that KIF1A regulates the affinity to MT by changing the flexibility of alpha4 helix according to the nucleotide states.
q-bio.BM:A theoretical framework is developed to study the dynamics of protein folding. The key insight is that the search for the native protein conformation is influenced by the rate r at which external parameters, such as temperature, chemical denaturant or pH, are adjusted to induce folding. A theory based on this insight predicts that (1) proteins with non-funneled energy landscapes can fold reliably to their native state, (2) reliable folding can occur as an equilibrium or out-of-equilibrium process, and (3) reliable folding only occurs when the rate r is below a limiting value, which can be calculated from measurements of the free energy. We test these predictions against numerical simulations of model proteins with a single energy scale.
q-bio.BM:Despite the spontaneity of some in vitro protein folding reactions, native folding in vivo often requires the participation of barrel-shaped multimeric complexes known as chaperonins. Although it has long been known that chaperonin substrates fold upon sequestration inside the chaperonin barrel, the precise mechanism by which confinement within this space facilitates folding remains unknown. In this study, we examine the possibility that the chaperonin mediates a favorable reorganization of the solvent for the folding reaction. We begin by discussing the effect of electrostatic charge on solvent-mediated hydrophobic forces in an aqueous environment. Based on these initial physical arguments, we construct a simple, phenomenological theory for the thermodynamics of density and hydrogen bond order fluctuations in liquid water. Within the framework of this model, we investigate the effect of confinement within a chaperonin-like cavity on the configurational free energy of water by calculating solvent free energies for cavities corresponding to the different conformational states in the ATP- driven catalytic cycle of the prokaryotic chaperonin GroEL. Our findings suggest that one function of chaperonins may be to trap unfolded proteins and subsequently expose them to a micro-environment in which the hydrophobic effect, a crucial thermodynamic driving force for folding, is enhanced.
q-bio.BM:We present a top-down approach to the study of the dynamics of icosahedral virus capsids, in which each protein is approximated by a point mass. Although this represents a rather crude coarse-graining, we argue that it highlights several generic features of vibrational spectra which have been overlooked so far. We furthermore discuss the consequences of approximate inversion symmetry as well as the role played by Viral Tiling Theory in the study of virus capsid vibrations.
q-bio.BM:We study the coupled dynamics of primary and secondary structure formation (i.e. slow genetic sequence selection and fast folding) in the context of a solvable microscopic model that includes both short-range steric forces and and long-range polarity-driven forces. Our solution is based on the diagonalization of replicated transfer matrices, and leads in the thermodynamic limit to explicit predictions regarding phase transitions and phase diagrams at genetic equilibrium. The predicted phenomenology allows for natural physical interpretations, and finds satisfactory support in numerical simulations.
q-bio.BM:We analyze the thermodynamic properties of a simplified model for folded RNA molecules recently studied by G. Vernizzi, H. Orland, A. Zee (in {\it Phys. Rev. Lett.} {\bf 94} (2005) 168103). The model consists of a chain of one-flavor base molecules with a flexible backbone and all possible pairing interactions equally allowed. The spatial pseudoknot structure of the model can be efficiently studied by introducing a $N \times N$ hermitian random matrix model at each chain site, and associating Feynman diagrams of these models to spatial configurations of the molecules. We obtain an exact expression for the topological expansion of the partition function of the system. We calculate exact and asymptotic expressions for the free energy, specific heat, entanglement and chemical potential and study their behavior as a function of temperature. Our results are consistent with the interpretation of $1/N$ as being a measure of the concentration of $\rm{Mg}^{++}$ in solution.
q-bio.BM:Blueprints of polyhedral cages with icosahedral symmetry made of circular DNA molecules are provided. The basic rule is that every edge of the cage is met twice in opposite directions by the DNA strand, and vertex junctions are realised by a set of admissible junction types. As nanocontainers for cargo storage and delivery, the icosidodecahedral cages are of special interest as they have the largest volume per surface ratio of all cages discussed here.
q-bio.BM:Group theoretical arguments combined with normal mode analysis techniques are applied to a coarse-grained approximation of icosahedral viral capsids which incorporates areas of variable flexibility. This highlights a remarkable structure of the low-frequency spectrum in this approximation, namely the existence of a plateau of 24 near zero-modes with universal group theory content.
q-bio.BM:The sequence-dependent structural variability and conformational dynamics of DNA play pivotal roles in many biological milieus, such as in the site-specific binding of transcription factors to target regulatory elements. To better understand DNA structure, function, and dynamics in general, and protein-DNA recognition in the 'kB' family of genetic regulatory elements in particular, we performed molecular dynamics simulations of a 20-base pair DNA encompassing a cognate kB site recognized by the proto-oncogenic 'c-Rel' subfamily of NF-kB transcription factors. Simulations of the kB DNA in explicit water were extended to microsecond duration, providing a broad, atomically-detailed glimpse into the structural and dynamical behavior of double helical DNA over many timescales. Of particular note, novel (and structurally plausible) conformations of DNA developed only at the long times sampled in this simulation -- including a peculiar state arising at ~ 0.7 us and characterized by cross-strand intercalative stacking of nucleotides within a longitudinally-sheared base pair, followed (at ~ 1 us) by spontaneous base flipping of a neighboring thymine within the A-rich duplex. Results and predictions from the us-scale simulation include implications for a dynamical NF-kB recognition motif, and are amenable to testing and further exploration via specific experimental approaches that are suggested herein.
q-bio.BM:The interaction cutoff contribution to the ruggedness of protein-protein energy landscape (the artificial ruggedness) is studied in terms of relative energy fluctuations for 1/r^n potentials based on a simplistic model of a protein complex. Contradicting the principle of minimal frustration, the artificial ruggedness exists for short cutoffs and gradually disappears with the cutoff increase. The critical values of the cutoff were calculated for each of eleven popular power-type potentials with n=0-9, 12 and for two thresholds of 5% and 10%. The artificial ruggedness decreases to tolerable thresholds for cutoffs longer than the critical ones. The results showed that for both thresholds the critical cutoff is a non-monotonic function of the potential power n. The functions reach the maximum at n=3-4 and then decrease with the increase of the potential power. The difference between two cutoffs for 5% and 10% artificial ruggedness becomes negligible for potentials decreasing faster than 1/r^12. The results suggest that cutoffs longer than critical ones can be recommended for protein-protein potentials.
q-bio.BM:Mechanical characterization of protein molecules has played a role on gaining insight into the biological functions of proteins, since some proteins perform the mechanical function. Here, we present the mesoscopic model of biological protein materials composed of protein crystals prescribed by Go potential for characterization of elastic behavior of protein materials. Specifically, we consider the representative volume element (RVE) containing the protein crystals represented by alpha-carbon atoms, prescribed by Go potential, with application of constant normal strain to RVE. The stress-strain relationship computed from virial stress theory provides the nonlinear elastic behavior of protein materials and their mechanical properties such as Young's modulus, quantitatively and/or qualitatively comparable to mechanical properties of biological protein materials obtained from experiments and/or atomistic simulations. Further, we discuss the role of native topology on the mechanical properties of protein crystals. It is shown that parallel strands (hydrogen bonds in parallel) enhances the mechanical resilience of protein materials.
q-bio.BM:Unfolded proteins may contain native or non-native residual structure, which has important implications for the thermodynamics and kinetics of folding as well as for misfolding and aggregation diseases. However, it has been universally accepted that residual structure should not affect the global size scaling of the denatured chain, which obeys the statistics of random coil polymers. Here we use a single-molecule optical technique, fluorescence correlation spectroscopy, to probe the denatured state of set of repeat proteins containing an increasing number of identical domains, from two to twenty. The availability of this set allows us to obtain the scaling law for the unfolded state of these proteins, which turns out to be unusually compact, strongly deviating from random-coil statistics. The origin of this unexpected behavior is traced to the presence of extensive non-native polyproline II helical structure, which we localize to specific segments of the polypeptide chain. We show that the experimentally observed effects of PPII on the size scaling of the denatured state can be well-described by simple polymer models. Our findings suggest an hitherto unforeseen potential of non-native structure to induce significant compaction of denatured proteins, affecting significantly folding pathways and kinetics.
q-bio.BM:Renaturation and hybridization reactions lead to the pairing of complementary single-stranded nucleic acids. We present here a theoretical investigation of the mechanism of these reactions in vitro under thermal conditions (dilute solutions of single-stranded chains, in the presence of molar concentrations of monovalent salts and at elevated temperatures). The mechanism follows a Kramers' process, whereby the complementary chains overcome a potential barrier through Brownian motion. The barrier originates from a single rate-limiting nucleation event in which the first complementary base pairs are formed. The reaction then proceeds through a fast growth of the double helix. For the DNA of bacteriophages T7, T4 and $\phi$X174 as well as for Escherichia coli DNA, the bimolecular rate $k_2$ of the reaction increases as a power law of the average degree of polymerization $<N>$ of the reacting single- strands: $k_2 \prop <N>^\alpha$. This relationship holds for $100 \leq <N> \leq 50 000$ with an experimentally determined exponent $\alpha = 0.51 \pm 0.01$. The length dependence results from a thermodynamic excluded-volume effect. The reacting single-stranded chains are predicted to be in universal good solvent conditions, and the scaling law is determined by the relevant equilibrium monomer contact probability. The value theoretically predicted for the exponent is $\alpha = 1-\nu \theta_2$, where $\nu$ is Flory's swelling exponent ($nu approx 0.588$) and $\theta_2$ is a critical exponent introduced by des Cloizeaux ($\theta_2 \approx 0.82$), yielding $\alpha = 0.52 \pm 0.01$, in agreement with the experimental results.
q-bio.BM:This article is interested in the origin of the genetic code, it puts forward a scenario of a simultaneous selection of the bases and amino acids and setting up of a correlation between them. Each amino acid is associated with a pair of its own kind, called the binding pair and each binding pair is associated with the codon(s) corresponding to the same amino acid. An explanation is also proposed about the origin of the start and stop codons.
q-bio.BM:We develop equilibrium and kinetic theories that describe the assembly of viral capsid proteins on a charged central core, as seen in recent experiments in which brome mosaic virus (BMV) capsids assemble around nanoparticles functionalized with polyelectrolyte. We model interactions between capsid proteins and nanoparticle surfaces as the interaction of polyelectrolyte brushes with opposite charge, using the nonlinear Poisson Boltzmann equation. The models predict that there is a threshold density of functionalized charge, above which capsids efficiently assemble around nanoparticles, and that light scatter intensity increases rapidly at early times, without the lag phase characteristic of empty capsid assembly. These predictions are consistent with, and enable interpretation of, preliminary experimental data. However, the models predict a stronger dependence of nanoparticle incorporation efficiency on functionalized charge density than measured in experiments, and do not completely capture a logarithmic growth phase seen in experimental light scatter. These discrepancies may suggest the presence of metastable disordered states in the experimental system. In addition to discussing future experiments for nanoparticle-capsid systems, we discuss broader implications for understanding assembly around charged cores such as nucleic acids.
q-bio.BM:The binding of a ligand molecule to a protein is often accompanied by conformational changes of the protein. A central question is whether the ligand induces the conformational change (induced-fit), or rather selects and stabilizes a complementary conformation from a pre-existing equilibrium of ground and excited states of the protein (selected-fit). We consider here the binding kinetics in a simple four-state model of ligand-protein binding. In this model, the protein has two conformations, which can both bind the ligand. The first conformation is the ground state of the protein when the ligand is off, and the second conformation is the ground state when the ligand is bound. The induced-fit mechanism corresponds to ligand binding in the unbound ground state, and the selected-fit mechanism to ligand binding in the excited state. We find a simple, characteristic difference between the on- and off-rates in the two mechanisms if the conformational relaxation into the ground states is fast. In the case of selected-fit binding, the on-rate depends on the conformational equilibrium constant, while the off-rate is independent. In the case of induced-fit binding, in contrast, the off-rate depends on the conformational equilibrium, while the on-rate is independent. Whether a protein binds a ligand via selected-fit or induced-fit thus may be revealed by mutations far from the protein's binding pocket, or other "perturbations" that only affect the conformational equilibrium. In the case of selected-fit, such mutations will only change the on-rate, and in the case of induced-fit, only the off-rate.
q-bio.BM:We study a matrix model of RNA in which an external perturbation acts on n nucleotides of the polymer chain. The effect of the perturbation appears in the exponential generating function of the partition function as a factor $(1-\frac{n\alpha}{L})$ [where $\alpha$ is the ratio of strengths of the original to the perturbed term and L is length of the chain]. The asymptotic behaviour of the genus distribution functions for the extended matrix model are analyzed numerically when (i) $n=L$ and (ii) $n=1$. In these matrix models of RNA, as $n\alpha/L$ is increased from 0 to 1, it is found that the universality of the number of diagrams $a_{L, g}$ at a fixed length L and genus g changes from $3^{L}$ to $(3-\frac{n\alpha}{L})^{L}$ ($2^{L}$ when $n\alpha/L=1$) and the asymptotic expression of the total number of diagrams $\cal N$ at a fixed length L but independent of genus g, changes in the factor $\exp^{\sqrt{L}}$ to $\exp^{(1-\frac{n\alpha}{L})\sqrt{L}}$ ($exp^{0}=1$ when $n\alpha/L=1$)
q-bio.BM:In protein folding the term plasticity refers to the number of alternative folding pathways encountered in response to free energy perturbations such as those induced by mutation. Here we explore the relation between folding plasticity and a gross, generic feature of the native geometry, namely, the relative number of local and non-local native contacts. The results from our study, which is based on Monte Carlo simulations of simple lattice proteins, show that folding to a structure that is rich in local contacts is considerably more plastic than folding to a native geometry characterized by having a very large number of long-range contacts (i.e., contacts between amino acids that are separated by more than 12 units of backbone distance). The smaller folding plasticity of `non-local' native geometries is probably a direct consequence of their higher folding cooperativity that renders the folding reaction more robust against single- and multiple-point mutations.
q-bio.BM:Significant overweight represents a major health problem in industrialized countries. Besides its known metabolic origins, this condition may also have an infectious cause, as recently postulated. Here, it is surmised that the potentially causative adenovirus 36 contributes to such disorder by inactivating the retinoblastoma tumor suppressor protein (RB) in a manner reminiscent of a mechanism employed by both another pathogenic adenoviral agent and insulin. The present insight additionally suggests novel modes of interfering with obesity-associated pathology.
q-bio.BM:Biological forces govern essential cellular and molecular processes in all living organisms. Many cellular forces, e.g. those generated in cyclic conformational changes of biological machines, have repetitive components. However, little is known about how proteins process repetitive mechanical stresses. To obtain first insights into dynamic protein mechanics, we probed the mechanical stability of single and multimeric ubiquitins perturbed by periodic forces. Using coarse-grained molecular dynamics simulations, we were able to model repetitive forces with periods about two orders of magnitude longer than the relaxation time of folded ubiquitins. We found that even a small periodic force weakened the protein and shifted its unfolding pathways in a frequency- and amplitude-dependent manner. Our results also showed that the dynamic response of even a small protein can be complex with transient refolding of secondary structures and an increasing importance of local interactions in asymmetric protein stability. These observations were qualitatively and quantitatively explained using an energy landscape model and discussed in the light of dynamic single-molecule measurements and physiological forces. We believe that our approach and results provide first steps towards a framework to better understand dynamic protein biomechanics and biological force generation.
q-bio.BM:It is a standard exercise in mechanical engineering to infer the external forces and torques on a body from its static shape and known elastic properties. Here we apply this kind of analysis to distorted double-helical DNA in complexes with proteins. We extract the local mean forces and torques acting on each base-pair of bound DNA from high-resolution complex structures. Our method relies on known elastic potentials and a careful choice of coordinates of the well-established rigid base-pair model of DNA. The results are robust with respect to parameter and conformation uncertainty. They reveal the complex nano-mechanical patterns of interaction between proteins and DNA. Being non-trivially and non-locally related to observed DNA conformations, base-pair forces and torques provide a new view on DNA-protein binding that complements structural analysis.
q-bio.BM:Molecular dynamics studies within a coarse-grained structure based model were used on two similar proteins belonging to the transcarbamylase family to probe the effects in the native structure of a knot. The first protein, N-acetylornithine transcarbamylase, contains no knot whereas human ormithine transcarbamylase contains a trefoil knot located deep within the sequence. In addition, we also analyzed a modified transferase with the knot removed by the appropriate change of a knot-making crossing of the protein chain. The studies of thermally- and mechanically-induced unfolding processes suggest a larger intrinsic stability of the protein with the knot.
q-bio.BM:Comprehensive knowledge of protein-ligand interactions should provide a useful basis for annotating protein functions, studying protein evolution, engineering enzymatic activity, and designing drugs. To investigate the diversity and universality of ligand binding sites in protein structures, we conducted the all-against-all atomic-level structural comparison of over 180,000 ligand binding sites found in all the known structures in the Protein Data Bank by using a recently developed database search and alignment algorithm. By applying a hybrid top-down-bottom-up clustering analysis to the comparison results, we determined approximately 3000 well-defined structural motifs of ligand binding sites. Apart from a handful of exceptions, most structural motifs were found to be confined within single families or superfamilies, and to be associated with particular ligands. Furthermore, we analyzed the components of the similarity network and enumerated more than 4000 pairs of ligand binding sites that were shared across different protein folds.
q-bio.BM:Protein electrostatic states have been demonstrated to play crucial roles in catalysis, ligand binding, protein stability, and in the modulation of allosteric effects. Electrostatic states are demonstrated to appear conserved among DEAD-box motifs and evidence is presented that the structural changes that occur to DEAD box proteins upon ligand binding alter the DEAD-box motif electrostatics in a way the facilitates the catalytic role of the DEAD-box glutatmate.
q-bio.BM:ESPSim is an open source JAVA program that enables the comparisons of protein electrostatic potential maps via the computation of an electrostatic similarity measure. This program has been utilized to demonstrate a high degree of electrostatic similarity among the potential maps of lysozyme proteins, suggesting that protein electrostatic states are conserved within lysozyme proteins. ESPSim is freely available under the AGPL License from http://www.bioinformatics.org/project/?group_id=830
q-bio.BM:Protein electrostatics have been demonstrated to play a vital role in protein functionality, with many functionally important amino acid residues exhibiting an electrostatic state that is altered from that of a normal amino acid residue. Residues with altered electrostatic states can be identified by the presence of a pKa value that is perturbed by 2 or more pK units, and such residues have been demonstrated to play critical roles in catalysis, ligand binding, and protein stability. Within the HCV helicase and polymerase, as well as the HIV reverse transcriptase, highly conserved regions were demonstrated to possess a greater number and magnitude of perturbations than lesser conserved regions, suggesting that there is an interrelationship present between protein electrostatics and evolution.
q-bio.BM:Protein dynamics in cells may be different from that in dilute solutions in vitro since the environment in cells is highly concentrated with other macromolecules. This volume exclusion due to macromolecular crowding is predicted to affect both equilibrium and kinetic processes involving protein conformational changes. To quantify macromolecular crowding effects on protein folding mechanisms, here we have investigated the folding energy landscape of an alpha/beta protein, apoflavodoxin, in the presence of inert macromolecular crowding agents using in silico and in vitro approaches. By coarse-grained molecular simulations and topology-based potential interactions, we probed the effects of increased volume fraction of crowding agents (phi_c) as well as of crowding agent geometry (sphere or spherocylinder) at high phi_c. Parallel kinetic folding experiments with purified Desulfovibro desulfuricans apoflavodoxin in vitro were performed in the presence of Ficoll (sphere) and Dextran (spherocylinder) synthetic crowding agents. In conclusion, we have identified in silico crowding conditions that best enhance protein stability and discovered that upon manipulation of the crowding conditions, folding routes experiencing topological frustrations can be either enhanced or relieved. The test-tube experiments confirmed that apoflavodoxin's time-resolved folding path is modulated by crowding agent geometry. We propose that macromolecular crowding effects may be a tool for manipulation of protein folding and function in living cells.
q-bio.BM:The force generated between actin and myosin acts predominantly along the direction of the actin filament, resulting in relative sliding of the thick and thin filaments in muscle or transport of myosin cargos along actin tracks. Previous studies have also detected lateral forces or torques that are generated between actin and myosin, but the origin and biological role of these sideways forces is not known. Here we adapt an actin gliding filament assay in order to measure the rotation of an actin filament about its axis (twirling) as it is translocated by myosin. We quantify the rotation by determining the orientation of sparsely incorporated rhodamine-labeled actin monomers, using polarized total internal reflection (polTIRF) microscopy. In order to determine the handedness of the filament rotation, linear incident polarizations in between the standard s- and p-polarizations were generated, decreasing the ambiguity of our probe orientation measurement four-fold. We found that whole myosin II and myosin V both twirl actin with a relatively long (micron), left-handed pitch that is insensitive to myosin concentration, filament length and filament velocity.
q-bio.BM:Most of the theoretical models describing the translocation of a polymer chain through a nanopore use the hypothesis that the polymer is always relaxed during the complete process. In other words, models generally assume that the characteristic relaxation time of the chain is small enough compared to the translocation time that non-equilibrium molecular conformations can be ignored. In this paper, we use Molecular Dynamics simulations to directly test this hypothesis by looking at the escape time of unbiased polymer chains starting with different initial conditions. We find that the translocation process is not quite in equilibrium for the systems studied, even though the translocation time tau is about 10 times larger than the relaxation time tau_r. Our most striking result is the observation that the last half of the chain escapes in less than ~12% of the total escape time, which implies that there is a large acceleration of the chain at the end of its escape from the channel.
physics.optics:A numerical study of the properties of Gaussian pulses propagating in planar waveguide under the combined effect of positive Kerr-type nonlinearity, diffraction in planar waveguides and anomalous or normal dispersion, is presented. It is demonstrated how the relative strength of dispersion and diffraction, the strength of nonlinearity and the initial spatial and temporal pulse chirps effect on the parameters of pulse compression, such as the maximal compression factor and the distance to the point of maximal compression.
physics.optics:We report on a theoretical and numerical investigation of the switching of power in new hybrid models of nonlinear coherent couplers consisting of optical slab waveguides with various orders of nonlinearity. The first model consists of two guides with second-order instead of the usual third-order susceptibilities as typified by the Jensen coupler. This second-order system is shown to have a power self-trapping transition at a critical power greater than the third-order susceptibility coupler. Next, we consider a mixed coupler composed of a second-order guide coupled to a third-order guide and show that, although it does not display a rigorous self-trapping transition, for a particular choice of parameters it does show a fairly abrupt trapping of power at a lower power than in the third-order coupler. By coupling this mixed nonlinear pair to a third, purely linear guide, the power trapping can be brought to even lower levels and in this way a satisfactory switching profile can be achieved at less than one sixth the input power needed in the Jensen coupler.
physics.optics:It is noted that the Jones-matrix formalism for polarization optics is a six-parameter two-by-two representation of the Lorentz group. It is shown that the four independent Stokes parameters form a Minkowskian four-vector, just like the energy-momentum four-vector in special relativity. The optical filters are represented by four-by-four Lorentz-transformation matrices. This four-by-four formalism can deal with partial coherence described by the Stokes parameters. A four-by-four matrix formulation is given for decoherence effects on the Stokes parameters, and a possible experiment is proposed. It is shown also that this Lorentz-group formalism leads to optical filters with a symmetry property corresponding to that of two-dimensional Euclidean transformations.
physics.optics:We study the electromagnetic scattering by multilayered biperiodic aggregates of dielectric layers and gratings of conducting plates. We show that the characteristic lengths of such structures provide a good control of absorption bands. The influence of the physical parameters of the problem (sizes, impedances) is discussed.
physics.optics:The propagation of an electromagnetic pulse in a plasma is studied for pulse durations that are comparable to the plasma period. When the carrier frequency of the incident pulse is much higher than the plasma frequency, the pulse propagates without distortion at its group speed. When the carrier frequency is comparable to the plasma frequency, the pulse is distorted and leaves behind it an electromagnetic wake.
physics.optics:We present scattering from many body systems in a new light. In place of the usual van Hove treatment, (applicable to a wide range of scattering processes using both photons and massive particles) based on plane waves, we calculate the scattering amplitude as a space-time integral over the scattering sample for an incident wave characterized by its correlation function which results from the shaping of the wave field by the apparatus. Instrument resolution effects - seen as due to the loss of correlation caused by the path differences in the different arms of the instrument are automatically included and analytic forms of the resolution function for different instruments are obtained. The intersection of the moving correlation volumes (those regions where the correlation functions are significant) associated with the different elements of the apparatus determines the maximum correlation lengths (times) that can be observed in a sample, and hence, the momentum (energy) resolution of the measurement. This geometrical picture of moving correlation volumes derived by our technique shows how the interaction of the scatterer with the wave field shaped by the apparatus proceeds in space and time. Matching of the correlation volumes so as to maximize the intersection region yields a transparent, graphical method of instrument design. PACS: 03.65.Nk, 3.80 +r, 03.75, 61.12.B
physics.optics:In this paper we extend for the case of Maxwell equations the "X-shaped" solutions previously found in the case of scalar (e.g., acoustic) wave equations. Such solutions are localized in theory, i.e., diffraction-free and particle-like (wavelets), in that they maintain their shape as they propagate. In the electromagnetic case they are particularly interesting, since they are expected to be Superluminal. We address also the problem of their practical, approximate production by finite (dynamic) radiators. Finally, we discuss the appearance of the X-shaped solutions from the purely geometric point of view of the Special Relativity theory.   [PACS nos.: 03.50.De; 1.20.Jb; 03.30.+p; 03.40.Kf; 14.80.-j.   Keywords: X-shaped waves; localized solutions to Maxwell equations; Superluminal waves; Bessel beams; Limited-dispersion beams; electromagnetic wavelets; Special Relativity; Extended Relativity].
physics.optics:This paper has been withdrawn by the authors until some changes are made.
physics.optics:The effect of dispersion or diffraction on zero-velocity solitons is studied for the generalized massive Thirring model describing a nonlinear optical fiber with grating or parallel-coupled planar waveguides with misaligned axes. The Thirring solitons existing at zero dispersion/diffraction are shown numerically to be separated by a finite gap from three isolated soliton branches. Inside the gap, there is an infinity of multi-soliton branches. Thus, the Thirring solitons are structurally unstable. In another parameter region (far from the Thirring limit), solitons exist everywhere.
physics.optics:We theoretically study reflection of light by a phase-conjugating mirror preceded by a partially reflecting normal mirror. The presence of a suitably chosen normal mirror in front of the phase conjugator is found to greatly enhance the total phase-conjugate reflected power, even up to an order of magnitude. Required conditions are that the phase-conjugating mirror itself amplifies upon reflection and that constructive interference of light in the region between the mirrors takes place. We show that the phase-conjugate reflected power then exhibits a maximum as a function of the transmittance of the normal mirror.
physics.optics:Reliable control of the deposition process of optical films and coatings frequently requires monitoring of the refractive index profile throughout the layer. In the present work a simple in situ approach is proposed which uses a WKBJ matrix representation of the optical transfer function of a single thin film on a substrate. Mathematical expressions are developed which represent the minima and maxima envelopes of the curves transmittance-vs-time and reflectance-vs-time. The refractive index and extinction coefficient depth profiles of different films are calculated from simulated spectra as well as from experimental data obtained during PECVD of silicon-compound films. Variation of the deposition rate with time is also evaluated from the position of the spectra extrema as a function of time. The physical and mathematical limitations of the method are discussed.
physics.optics:A new definition for the electromagnetic field velocity is proposed. The velocity depends on the physical fields.
physics.optics:We have fabricated light emitting diodes (LEDs) with Schottky contacts on Si-nanocrystals formed by simple techniques as used for standard Si devices. Orange electroluminescence (EL) from these LEDs could be seen with the naked eye at room temperature when a reverse bias voltage was applied. The EL spectrum has a major peak with a photon energy of 1.9 eV and a minor peak with a photon energy of 2.2 eV. Since the electrons and holes are injected into the radiative recombination centers related to nanocrystals through avalanche breakdown, the voltage needed for a visible light emission is reduced to 4.0 - 4.5 V, which is low enough to be applied by a standard Si transistor.
physics.optics:A general model is presented for coupling of high-$Q$ whispering-gallery modes in optical microsphere resonators with coupler devices possessing discrete and continuous spectrum of propagating modes. By contrast to conventional high-Q optical cavities, in microspheres independence of high intrinsic quality-factor and controllable parameters of coupling via evanescent field offer variety of regimes earlier available in RF devices. The theory is applied to the earlier-reported data on different types of couplers to microsphere resonators and complemented by experimental demonstration of enhanced coupling efficiency (about 80%) and variable loading regimes with Q>10^8 fused silica microspheres.
physics.optics:The mechanism of DC-Electric-Field-Induced Second-Harmonic (EFISH) generation at weakly nonlinear buried Si(001)-SiO$_2$ interfaces is studied experimentally in planar Si(001)-SiO$_2$-Cr MOS structures by optical second-harmonic generation (SHG) spectroscopy with a tunable Ti:sapphire femtosecond laser. The spectral dependence of the EFISH contribution near the direct two-photon $E_1$ transition of silicon is extracted. A systematic phenomenological model of the EFISH phenomenon, including a detailed description of the space charge region (SCR) at the semiconductor-dielectric interface in accumulation, depletion, and inversion regimes, has been developed. The influence of surface quantization effects, interface states, charge traps in the oxide layer, doping concentration and oxide thickness on nonlocal screening of the DC-electric field and on breaking of inversion symmetry in the SCR is considered. The model describes EFISH generation in the SCR using a Green function formalism which takes into account all retardation and absorption effects of the fundamental and second harmonic (SH) waves, optical interference between field-dependent and field-independent contributions to the SH field and multiple reflection interference in the SiO$_2$ layer. Good agreement between the phenomenological model and our recent and new EFISH spectroscopic results is demonstrated. Finally, low-frequency electromodulated EFISH is demonstrated as a useful differential spectroscopic technique for studies of the Si-SiO$_2$ interface in silicon-based MOS structures.
physics.optics:The new mechanism for obtaining a nonlinear phase shift has been proposed and the schemes are described for its implementation. As it is shown, the interference of two waves with intensity-dependent amplitude ratio coming from the second harmonic generation should produce the nonlinear phase shift. The sign and amount of nonlinear distortion of a beam wavefront is dependent of the relative phase of the waves that is introduced by the phase element. Calculated value of $n_2^{eff}$ exceeds that connected with cascaded quadratic nonlinearity, at the same conditions.
physics.optics:We analyze the guiding problem in a realistic photonic crystal fiber using a novel full-vector modal technique, a biorthogonal modal method based on the nonselfadjoint character of the electromagnetic propagation in a fiber. Dispersion curves of guided modes for different fiber structural parameters are calculated along with the 2D transverse intensity distribution of the fundamental mode. Our results match those achieved in recent experiments, where the feasibility of this type of fiber was shown.
physics.optics:A new method for investigation of x-ray propagation in a rough narrow dielectric waveguide is proposed on the basis of the numerical integration of the quazioptical equation. In calculations a model rough surface was used with the given statistical properties. It was shown that the losses in the narrow waveguides strongly depend on the wall roughness and on the input angle. The losses are not zero even at zero input angle if the width of the waveguide is smaller or about 1 mkm. The effect is accounted for as the influence of diffraction. The angular spread of the transmitted X-ray radiation is much more narrow than the Fresnel angle of the total external reflection.
physics.optics:We study a generalized notion of two-mode squeezing for the Stokes and anti-Stokes fields in a model of a cavity Raman laser, which leads to a significant reduction in decoherence or quantum noise. The model comprises a loss-less cavity with classical pump, unsaturated medium and arbitrary homogeneous broadening and dispersion. Allowing for arbitrary linear combinations of the two modes in the definition of quadrature variables, we find that there always exists a combination of the two output modes which exhibits quadrature squeezing with noise reduction below the vacuum level. The number of noise photons for this combination mode is proportional to the square root of the number of Stokes noise photons.
physics.optics:We study the effects of higher order transversal modes in a model of a singly-resonant OPO, using both numerical solutions and mode expansions including up to two radial modes. The numerical and two-mode solutions predict lower threshold and higher conversion than the single-mode solution at negative dispersion. Relative power in the zero order radial mode ranges from about 88% at positive and small negative dispersion to 48% at larger negative dispersion, with most of the higher mode content in the first mode, and less than 2% in higher modes.
physics.optics:This paper presents a detailed numerical study of the effect of focusing on the conversion efficiency of low-loss singly-resonant parametric oscillators with collinear focusing of pump and signal. Results are given for the maximal pump depletion and for pump power levels required for various amounts of depletion, as functions of pump and signal confocal parameters, for kI/kP=0.33 and 0.50. It is found that the ratio of pump depletion to maximal depletion as a function of the ratio of pump power to threshold power agrees with the plane-wave prediction to within 5%, for a wide range of focusing conditions. The observed trends are explained as resulting from intensity and phase dependent mechanisms.
physics.optics:Via solution of appropriate variational problem it is shown that light beams with Gaussian spatial profile and sufficiently short duration provide maximal destruction of global coherence under nonlinear self-modulation.
physics.optics:The dynamics of Fabry-Perot cavity with suspended mirrors is described. The suspended mirrors are nonlinear oscillators interacting with each other through the laser circulating in the cavity. The degrees of freedom decouple in normal coordinates, which are the position of the center of mass and the length of the cavity. We introduce two parameters and study how the dynamics changes with respect to these parameters. The first parameter specifies how strong the radiation pressure is. It determines whether the cavity is multistable or not. The second parameter is the control parameter, which determines location of the cavity equilibrium states. The equilibrium state shows hysteresis if the control parameter varies within a wide range. We analyze stability of the equilibrium states and identify the instability region. The instability is explained in terms of the effective potential: the stable states correspond to local minima of the effective potential and unstable states correspond to local maxima. The minima of the effective potential defines the resonant frequencies for the oscillations of the cavity length. We find the frequencies, and analyze how to tune them. Multistability of the cavity with a feedback control system is analyzed in terms of the servo potential. The results obtained in this paper are general and apply to all Fabry-Perot cavities with suspended mirrors.
physics.optics:We show that an azimuthally-periodically-modulated bright ring "necklace" beam can self-trap in self-focusing Kerr media and can exhibit stable propagation for very large distances. These are the first bright (2+1) D beams to exhibit stable self-trapping in a system described by the cubic (2+1) D Nonlinear Schrodinger Equation (NLSE).
physics.optics:We present a new class of micro lasers based on nanoporous molecular sieve host-guest systems. Organic dye guest molecules of 1-Ethyl-4-(4-(p-Dimethylaminophenyl)-1,3-butadienyl)-pyridinium Perchlorat were inserted into the 0.73-nm-wide channel pores of a zeolite AlPO$_4$-5 host. The zeolitic micro crystal compounds where hydrothermally synthesized according to a particular host-guest chemical process. The dye molecules are found not only to be aligned along the host channel axis, but to be oriented as well. Single mode laser emission at 687 nm was obtained from a whispering gallery mode oscillating in a 8-$\mu$m-diameter monolithic micro resonator, in which the field is confined by total internal reflection at the natural hexagonal boundaries inside the zeolitic microcrystals.
physics.optics:We report a quantum ring-like toroidal cavity naturally formed in a vertical-cavity-like active microdisk plane due to Rayleigh's band of whispering gallery modes. The $\sqrt{T}$-dependent redshift and a square-law property of microampere-range threshold currents down to 2 $\mu$A are consistent with a photonic quantum wire view, due to whispering gallery mode-induced dimensional reduction.
physics.optics:The effect of capture of X-ray beam into narrow submicron capillary was investigated with account for diffraction and decay of coherency by roughness scattering in transitional boundary layer. In contrast to well-known Andronov-Leontovich approach the losses do not vanish at zero gliding angle and scale proportional to the first power of roughness amplitude for small gliding angles. It was shown that for small correlation radius of roughness the scattering decay of coherency can be made of the same order as absorption decay of lower channeling modes to produce angular collimation of X-ray beams. Estimates were given for optimum capillary length at different roughness amplitudes for angular sensitivity of X-ray transmission and chenneling effects that can be usefull for designing of detector systems.
physics.optics:We report the measurement of the photons flux produced in parametric down-conversion, performed in photon counting regime with actively quenched silicon avalanche photodiodes as single photon detectors. Measurements are done with the detector in a well defined geometrical and spectral situation. By comparison of the experimental data with the theory, a value for the second order susceptibilities of the non linear crystal can be inferred.
physics.optics:In a frame of quasi-crystal approximation the dispersion equations are obtained for the wave vector of a coherent electromagnetic wave propagating in a media which contains a random set of parallel dielectric cylinders with possible overlapping. The results are compared with that for the case when a regularity at the cylinder placement exists.
physics.optics:Accurate calculation of internal and surface scattering losses in fused silica microspheres is done. We show that in microspheres internal scattering is partly inhibited as compared to losses in the bulk material. We pay attention on the effect of frozen thermodynamical capillary waves on surface roughness. We calculate also the value of mode splitting due to backscattering and other effects of this backscattering.
physics.optics:We analytically compute a localization criterion in double scattering approximation for a set of dielectric spheres or perfectly conducting disks uniformly distributed in a spatial volume which can be either spherical or layered. For every disordered medium, we numerically investigate a localization criterion, and examine the influence of the system parameters on the wavelength localization domains.
physics.optics:The use of specific symmetry properties of the optical second-harmonic generation (the s,s-exclusion rule) has allowed us to observe high-contrast hyper-Rayleigh interference patterns in a completely diffuse light - an effect having no analog in case of linear (Rayleigh) scattering.
physics.optics:We report detailed measurements of the pump-current dependency of the self-pulsating frequency of semiconductor CD lasers. A distinct kink in this dependence is found and explained using rate-equation model. The kink denotes a transition between a region where the self-pulsations are weakly sustained relaxation oscillations and a region where Q-switching takes place. Simulations show that spontaneous emission noise plays a crucial role for the cross-over.
physics.optics:A new method is proposed to produce population inversion on transitions involving the ground state of atoms. The method is realized experimentally with sodium atoms. Lasing at the frequency corresponding to the sodium D_2 line is achieved in the presence of pump radiation resonant to the D_1 line with helium as a buffer gas.
physics.optics:A moving dielectric appears to light as an effective gravitational field. At low flow velocities the dielectric acts on light in the same way as a magnetic field acts on a charged matter wave. We develop in detail the geometrical optics of moving dispersionless media. We derive a Hamiltonian and a Lagrangian to describe ray propagation. We elucidate how the gravitational and the magnetic model of light propagation are related to each other. Finally, we study light propagation around a vortex flow. The vortex shows an optical Aharonov--Bohm effect at large distances from the core, and, at shorter ranges, the vortex may resemble an optical black hole.
physics.optics:We report the first observation of a nonlinear mode in a cylindrical nonlinear Fabry-Perot cavity. The field enhancement from cavity buildup, as well as the large chi3 optical nonlinearity due to resonantly-excited Rb-85 vapor, allows the nonlinear mode to form at low incident optical powers of less than a milliwatt. The mode is observed to occur for both the self-focusing and self-defocusing nonlinearity.
physics.optics:Optical second harmonic generation (SHG) is used as a noninvasive probe of two-dimensional (2D) ferroelectricity in Langmuir-Blodgett (LB) films of copolymer vinylidene fluoride with trifluorethylene. The surface 2D ferroelectric-paraelectric phase transition in the topmost layer of LB films and a thickness independent (almost 2D) transition in the bulk of these films are observed in temperature studies of SHG.
physics.optics:An all optical set-reset flip flop is presented that is based on two coupled identical laser diodes. The lasers are coupled so that when one of the lasers lases it quenches lasing in the other laser. The state of the flip flop is determined by which laser is currently lasing. Rate equations are used to model the flip flop and obtain steady state characteristics. The flip flop is experimentally demonstrated by use of antireflection coated laser diodes and free space optics.
physics.optics:We report on the fabrication of what we believe is the first example of a two dimensional nonlinear periodic crystal\cite{berger}, where the refractive index is constant but in which the 2nd order nonlinear susceptibility is spatially periodic. Such a crystal allows for efficient quasi-phase matched 2nd harmonic generation using multiple reciprocal lattice vectors of the crystal lattice. External 2nd harmonic conversion efficiencies > 60% were measured with picosecond pulses. The 2nd harmonic light can be simultaneously phase matched by multiple reciprocal lattice vectors, resulting in the generation of multiple coherent beams. The fabrication technique is extremely versatile and allows for the fabrication of a broad range of 2-D crystals including quasi-crystals.
physics.optics:The influence of linearly and circularly polarized laser fields on the dynamics of fast electron-impact excitation in atomic helium is discussed. A detailed analysis is made in the excitation of 2^1S, 3^1S and 3^1D dressed states of helium target.
physics.optics:The nonlinear dynamics of dissipative quantum systems in incoherent laser fields is studied in the framework of master equation with random model describing the laser noise and Markovian approximation for dealing with the system-bath couplings.
physics.optics:We present a theoretical study of strong laser-atom interactions, when the laser field parameters are subjected to random processes. The atom is modelled by a two-level and three-level systems, while the statistical fluctuations of the laser field are described by a pre-Gaussian model.
physics.optics:We consider the effect of spatial correlations on sources of polarized electromagnetic radiation. The sources, assumed to be monochromatic, are constructed out of dipoles aligned along a line such that their orientation is correlated with their position. In one representative example, the dipole orientations are prescribed by a generalized form of the standard von Mises distribution for angular variables such that the azimuthal angle of dipoles is correlated with their position. In another example the tip of the dipole vector traces a helix around the symmetry axis of the source, thereby modelling the DNA molecule. We study the polarization properties of the radiation emitted from such sources in the radiation zone. For certain ranges of the parameters we find a rather striking angular dependence of polarization. This may find useful applications in certain biological systems as well as in astrophysical sources.
physics.optics:We study the dynamics of the reduced density matrix(RDM) of the field in the micromaser. The resonator is pumped by N-atomic clusters of two-level atoms. At each given instant there is only one cluster in the cavity. We find the conditions of the independent evolution of the matrix elements of RDM belonging to a (sub)diagonal of the RDM, i.e. conditions of the diagonal invariance for the case of pumping by N-atomic clusters. We analyze the spectrum of the evolution operator of the RDM and discover the existence of the quasitrapped states of the field mode. These states exist for a wide range of number of atoms in the cluster as well as for a broad range of relaxation rates. We discuss the hierarchy of dynamical processes in the micromaser and find an important property of the field states corresponding to the quasi-equilibrium: these states are close to either Fock states or to a superposition of the Fock states. A possibility to tune the distribution function of photon numbers is discussed.
physics.optics:Temporal and angular correlations in atom-mediated photon-photon scattering are measured. Good agreement is found with the theory presented in Part~I.
physics.optics:The mediated photon-photon interaction due to the resonant Kerr nonlinearity in an inhomogeneously broadened atomic vapor is considered. The time-scale for photon-photon scattering is computed and found to be determined by the inhomogeneous broadening and the magnitude of the momentum transfer. This time can be shorter than the atomic relaxation time. Effects of atom statistics are included and the special case of small-angle scattering is considered. In the latter case the time-scale of the nonlinear response remains fast, even though the linear response slows as the inverse of the momentum transfer.
physics.optics:A simple variation of the traditional Young's double slit experiment can demonstrate several subtleties of interference with polarized light, including Berry and Pancharatnam's phase. Since the position of the fringes depends on the polarization state of the light at the input, the apparatus can also be used to measure the light's polarization without a quarter-wave plate or an accurate measurement of the light's intensity. In principle this technique can be used for any wavelength of photon as long as one can effectively polarize the incoming radiation.
physics.optics:The combination of charge separation induced by the formation of a single photorefractive screening soliton and an applied external bias field in a paraelectric is shown to lead to a family of useful electro-optic guiding patterns and properties.
physics.optics:The nonlinear pulse propagation in an optical fibers with varying parameters is investigated. The capture of moving in the frequency domain femtosecond colored soliton by a dispersive trap formed in an amplifying fiber makes it possible to accumulate an additional energy and to reduce significantly the soliton pulse duration. Nonlinear dynamics of the chirped soliton pulses in the dispersion managed systems is also investigated. The methodology developed does provide a systematic way to generate infinite ``ocean'' of the chirped soliton solutions of the nonlinear Schr\"odinger equation (NSE) with varying coefficients.
physics.optics:The Jaynes-Cummings model describing the interaction of a single linearly- polarized mode of the quantized electromagnetic field with an isolated two- level atom is generalized to the case of atomic levels degenerate in the projections of the angular momenta on the quantization axis, which is a usual case in the experiments. This generalization, like the original model, obtains the explicit solution. The model is applied to calculate the dependence of the atomic level populations on the angle between the polarization of cavity field mode and that of the laser excitation pulse in the experiment with one-atom micromaser.
physics.optics:The second-harmonic interferometric spectroscopy (SHIS) which combines both amplitude (intensity) and phase spectra of the second-harmonic (SH) radiation is proposed as a new spectroscopic technique being sensitive to the type of critical points (CP's) of combined density of states at semiconductor surfaces. The increased sensitivity of SHIS technique is demonstrated for the buried Si(111)-SiO$_2$ interface for SH photon energies from 3.6 eV to 5 eV and allows to separate the resonant contributions from $E^\prime_0/E_1$, $E_2$ and $E^\prime_1$ CP's of silicon.
physics.optics:The frequency of a 700mW monolithic non-planar Nd:YAG ring laser (NPRO) depends with a large coupling coefficient (some MHz/mW) on the power of its laser-diode pump source. Using this effect we demonstrate the frequency stabilization of an NPRO to a frequency reference by feeding back to the current of its pump diodes. We achieved an error point frequency noise smaller than 1mHz/sqrt(Hz), and simultaneously a reduction of the power noise of the NPRO by 10dB without an additional power stabilization feed-back system.
physics.optics:We deduce the emissivity of radiation from a metallic surface as a function of angle and polarization. This effect has found application in the calibration of detectors for cosmic microwave background radiation.
physics.optics:We present the first experimental observation of modulation instability of partially spatially incoherent light beams in non-instantaneous nonlinear media. We show that even in such a nonlinear partially coherent system (of weakly-correlated particles) patterns can form spontaneously. Incoherent MI occurs above a specific threshold that depends on the beams' coherence properties (correlation distance), and leads to a periodic train of one-dimensional (1D) filaments. At a higher value of nonlinearity, incoherent MI displays a two-dimensional (2D) instability and leads to self-ordered arrays of light spots.
physics.optics:We study the polarization of light emitted by spatially correlated sources. We show that in general polarization acquires nontrivial spectral dependence due to spatial correlations. The spectral dependence is found to be absent only for a special class of sources where the correlation length scales as the wavelength of light. We further study the cross correlations between two spatially distinct points that are generated due to propagation. It is found that such cross correlation leads to sufficiently strong spectral dependence of polarization which can be measured experimentally.
physics.optics:We demonstrate experimentally that in a centrosymmetric paraelectric non-stationary boundary conditions can dynamically halt the intrinsic instability of quasi-steady-state photorefractive self-trapping, driving beam evolution into a stable oscillating two-soliton-state configuration.
physics.optics:Mugnai et al. have reported an experiment in which microwave packets appear to travel in air with a speed substantially greater than c. They calculate the group velocity of their packets and find that it agrees with their experimental result. That calculation is incorrect. A correct calculation gives a group velocity less than c. The reported experimental result cannot be reconciled with the Maxwell equations.
physics.optics:Scalar Bessel beams are derived both via the wave equation and via diffraction theory. While such beams have a group velocity that exceeds the speed of light, this is a manifestation of the "scissors paradox" of special relativty. The signal velocity of a modulated Bessel beam is less than the speed of light. Forms of Bessel beams that satisfy Maxwell's equations are also given.
physics.optics:Various algebraic structures of degenerate four-wave mixing equations of optical phase conjugation are analyzed. Two approaches (the spinorial and the Lax-pair based), complementary to each other, are utilized for a systematic derivation of conserved quantities. Symmetry groups of both the equations and the conserved quantities are determined, and the corresponding generators are written down explicitly. Relation between these two symmetry groups is found. Conserved quantities enable the introduction of new methods for integration of the equations in the cases when the coupling $\Gamma$ is either purely real or purely imaginary. These methods allow for both geometries of the process, namely the transmission and the reflection, to be treated on an equal basis. One approach to introduction of Hamiltonian and Lagrangian structures for the 4WM systems is explored, and the obstacles in successful implementation of that programe are identified. In case of real coupling these obstacles are removable, and full Hamiltonian and Lagrangian formulations of the initial system are possible.
physics.optics:Instead of using frequency dependent refractive index, we propose to use the extinction theorem to describe reflection and transmission of an ultrashort pulse passing through the boundary. When the duration of the pulse is comparable with the relaxation time, the results differ significantly from those given by the traditional method, especially if the carrier frequency is close to an absorbtion line. We compare the two approaches using the data of GaAs in the infrared domain.
physics.optics:The Classification of Polarization elements, the polarization affecting optical devices which have a Jones matrix representation, according to the types of eigenvectors they possess, is given a new visit through the Group-theoretical connection of polarization elements. The diattenuators and retarders are recognized as the elements corresponding to boosts and rotations respectively. The structure of homogeneous elements other than diattenuators and retarders are identified by giving the quaternion corresponding to these elements. The set of degenerate polarization elements is identified with the so called `null' elements of the Lorentz Group. Singular polarization elements are examined in their more illustrative Mueller matrix representation and finally the eigenstructure of a special class of singular Mueller matrices is studied.
physics.optics:Complex photonic band structures (CPBS) of transmission metallic gratings with rectangular slits are shown to exhibit strong discontinuities that are not evidenced in the usual energetic band structures. These discontinuities are located on Wood's anomalies and reveal unambiguously two different types of resonances, which are identified as horizontal and vertical surface-plasmon resonances. Spectral position and width of peaks in the transmission spectrum can be directly extracted from CPBS for both kinds of resonances.
physics.optics:We present a brief classical discussion of a process to reduce the group velocity of an electromagnetic pulse by many orders of magnitude.
physics.optics:The properties of pulse propagation in a nonlinear fiber including linear damped term added in the usual nonlinear Schr\"odinger equation is analyzed analytically. We apply variational modified approach based on the lagrangian that describe the dynamic of system and with a trial function we obtain a solution which is more accuracy when compared with a pertubative solution. As a result, the problem of pulse propagation in a fiber with loss can be described in good agreement with exact results.
physics.optics:The group velocity for pulses in an optical medium can be negative at frequencies between those of a pair of laser-pumped spectral lines. The gain medium then can amplify the leading edge of a pulse resulting in a time advance of the pulse when it exits the medium, as has been recently demonstrated in the laboratory. This effect has been called superluminal, but, as a classical analysis shows, it cannot result in signal propgation at speeds greater than that of light in vacuum.
physics.optics:In two models it is shown that a light pulse propagates from a vacuum into certain media with velocity greater than that of a light in a vacuum (c). By numerical calculation the propagating properties of such a light are given.
physics.optics:The results of the study of ultra-short pulse generation in continuous-wave Kerr-lens mode-locked (KLM) solid-state lasers with semiconductor saturable absorbers are presented. The issues of extremely short pulse generation are addressed in the frames of the theory that accounts for the coherent nature of the absorber-pulse interaction. We developed an analytical model that bases on the coupled generalized Landau-Ginzburg laser equation and Bloch equations for a coherent absorber. We showed, that in the absence of KLM semiconductor absorber produces 2pi - non-sech-pulses of self-induced transparency, while the KLM provides an extremely short sech-shaped pulse generation. 2pi- and pi-sech-shaped solutions and variable-area chirped pulses have been found. It was shown, that the presence of KLM removes the limitation on the minimal modulation depth in absorber. An automudulational stability and self-starting ability were analyzed, too.
physics.optics:Based on self - consistent field theory we study a soliton generation in cw solid-state lasers with semiconductor saturable absorber. Various soliton destabilizations, i.e. the switch from femtosecond to picosecond generation (''picosecond collapse''), an automodulation regime, breakdown of soliton generation and hysteresis behavior, are predicted.
physics.optics:The effect of transmission of x-ray beams through submicron capillaries was investigated with account for diffraction and roughness scattering. Possible explanation of anomalous energy dependence of transmission through thin Cr/C/Cr channeles was given due to effect of periodic deformations.
physics.optics:Nonstationary pulse regimes associated with self modulation of a Kerr-lens modelocked Ti:sapphire laser have been studied experimentally and theoretically. Such laser regimes occur at an intracavity group delay dispersion that is smaller or larger than what is required for stable modelocking and exhibit modulation in pulse amplitude and spectra at frequencies of several hundred kHz. Stabilization of such modulations, leading to an increase in the pulse peak power by a factor of ten, were accomplished by weakly modulating the pump laser with the self-modulation frequency. The main experimental observations can be explained with a round trip model of the fs laser taking into account gain saturation, Kerr lensing, and second- and third-order dispersion.
physics.optics:The theoretical calculation for nonlinear refractive index in Cr: ZnSe - active medium predicts the strong defocusing cascaded second-order nonlinearity within 2000 - 3000 nm spectral range. On the basis of this result the optimal cavity configuration for Kerr-lens mode locking is proposed that allows to achieve a sub-100 fs pulse duration. The numerical simulations testify about strong destabilizing processes in the laser resulting from a strong self-phase modulation. The stabilization of the ultrashort pulse generation is possible due to spectral filtering that increases the pulse duration up to 300 fs.
physics.optics:The influence of nonlinear properties of semiconductor saturable absorbers on ultrashort pulse generation was investigated. It was shown, that linewidth enhancement, quadratic and linear ac Stark effect contribute essentially to the mode locking in cw solid-state lasers, that can increase the pulse stability, decrease pulse duration and reduce the mode locking threshold
physics.optics:We demonstrate that the shift of the stop band position with increasing oblique angle in periodic structures results in a wide transverse exponential field distribution corresponding to strong angular confinement of the radiation. The beam expansion follows an effective diffusive equation depending only upon the spectral mode width. In the presence of gain, the beam cross section is limited only by the size of the gain area. As an example of an active periodic photonic medium, we calculate and measure laser emission from a dye-doped cholesteric liquid crystal film.
physics.optics:The frequency of the Calcium ^3P_1--^1S_0 intercombination line at 657 nm is phase-coherently measured in terms of the output of a primary cesium frequency standard using an optical frequency comb generator comprising a sub-10 fs Kerr-lens mode-locked Ti:Sapphire laser and an external microstructure fiber for self-phase-modulation. The measured frequency of \nu_Ca = 455 986 240 494 276 Hz agrees within its relative uncertainty of 4 10^-13 with the values previously measured with a conceptually different harmonic frequency chain and with the value recommended for the realization of the SI unit of length.
physics.optics:Accurate phase-locked 3:1 division of an optical frequency was achieved, by using a continuous-wave (cw) doubly resonant optical parametric oscillator. A fractional frequency stability of 2*10^(-17) of the division process has been achieved for 100s integration time. The technique developed in this work can be generalized to the accurate phase and frequency control of any cw optical parametric oscillator.
physics.optics:It was usually assumed that the resonator based on a waveguide has the eigen oscillations that are formed by interference of two waves which propagate in different directions and have equal amplitudes. These patterns are usually called standing waves. We have shown that the eigen oscillations of a resonator which is filled by a layered dielectric can be base on the evanescent (non-propagating) waves. In some cases we need only one eigen wave to compose the eigen oscillation of a closed cavity.
physics.optics:The plane-wave dynamics of 3*omega => (2*omega, omega) subharmonic optical parametric oscillators containing a second harmonic generator of the idler wave omega is analyzed analytically by using the meanfield approximation and numerically by taking into account the field propagation inside the media. The resonant Chi(2):Chi(2) cascaded second-order nonlinearities induce a mutual injection-locking of the signal and idler waves that leads to coherent self phase-locking of the pump and subharmonic waves, freezing the phase diffusion noise. In case of signal-and-idler resonant devices, largely detuned sub-threshold states occur due to a subcritical bifurcation, broadening out the self-locking frequency range to a few cavity linewidths.
physics.optics:For the first time an all optical flip-flop is demonstrated based on two coupled Mach-Zehnder interferometers which contain semiconductor optical amplifiers in their arms. The flip-flop operation is discussed and it is demonstrated using commercially available fiber pigtailed devices. Being based on Mach-Zehnder interferometers, the flip-flop has potential for very high speed operation.
physics.optics:The strong asymmetry in charge distribution supporting a single non-interacting spatial needle soliton in a paraelectric photorefractive is directly observed by means of electroholographic readout. Whereas in trapping conditions a quasi-circular wave is supported, the underlying double-dipolar structure can be made to support two distinct propagation modes.
physics.optics:I present a theoretical treatment of parametric scattering in strong coupling semiconductor microcavities to model experiments in which parametric oscillator behaviour has been observed. The model consists of a non-linear excitonic oscillator coupled to a cavity mode which is driven by the external fields, and predicts the output power, below threshold gain and spectral blue shifts of the parametric oscillator. The predictions are found to be in excellent agreement with the experimental data.
physics.optics:A number of factors that influence spectral position of the femtosecond pulse in a Kerr-lens modelocked Cr:LiSGaF laser have been identified: high-order dispersion, gain saturation, reabsorption from the ground state, and stimulated Raman scattering. Using the one-dimensional numerical model for the simulation of the laser cavity, the relative contributions of different factors have been compared. The Raman effect provides the largest self-frequency shift from the gain peak (up to 60 nm), followed by the gain saturation (25 nm), while the high-order dispersion contribution is insignificant (5 nm). Comparison with the experimental data confirm that the stimulated Raman scattering is a main cause of the ultrashort pulse self-frequency shift observed in Cr:LiSGaF and Cr:LiSAF lasers
physics.optics:Simultaneous measurements of the intensity and phase of a probe wave reflected from an interface between silica and elemental alpha-gallium reveal its very strong optical nonlinearity, affecting both these parameters of the reflected wave. The data corroborate with a non-thermal mechanism of optical response which assumes appearance of a homogeneous highly metallic layer, only a few nanometer thick, between the silica and bulk alpha-gallium.
physics.optics:We consider pulse propagation in a linear anomalously dispersive medium where the group velocity exceeds the speed of light in vacuum (c) or even becomes negative. A signal velocity is defined operationally based on the optical signal-to-noise ratio, and is computed for cases appropriate to the recent experiment where such a negative group velocity was observed. It is found that quantum fluctuations limit the signal velocity to values less than c.
physics.optics:Polarization dynamics of femtosecond light pulses propagating in air is studied by computer simulation. A rich variety of dynamics is found that depends on the initial polarization state and power of the pulse. Effects of polarization on the plasma and supercontinuum generation are also discussed.
physics.optics:We report measurements of thermal self-locking of a Fabry-Perot cavity containing a potassium niobate (KNbO3) crystal. We develop a method to determine linear and nonlinear optical absorption coefficients in intracavity crystals by detailed analysis of the transmission lineshapes. These lineshapes are typical of optical bistability in thermally loaded cavities. For our crystal, we determine the one-photon absorption coefficient at 846 nm to be (0.0034 \pm 0.0022) per m and the two-photon absorption coefficient at 846 nm to be (3.2 \pm 0.5) \times 10^{-11} m/W and the one-photon absorption coefficient at 423 nm to be (13 \pm 2) per m. We also address the issue of blue-light-induced-infrared-absorption (BLIIRA), and determine a coefficient for this excited state absorption process. Our method is particularly well suited to bulk absorption measurements where absorption is small compared to scattering. We also report new measurements of the temperature dependence of the index of refraction at 846 nm, and compare to values in the literature.
physics.optics:We review and extend the analogies between Gaussian pulse propagation and Gaussian beam diffraction. In addition to the well-known parallels between pulse dispersion in optical fiber and CW beam diffraction in free space, we review temporal lenses as a way to describe nonlinearities in the propagation equations, and then introduce further concepts that permit the description of pulse evolution in more complicated systems. These include the temporal equivalent of a spherical dielectric interface, which is used by way of example to derive design parameters used in a recent dispersion-mapped soliton transmission experiment. Our formalism offers a quick, concise and powerful approach to analyzing a variety of linear and nonlinear pulse propagation phenomena in optical fibers.
physics.optics:We have measured the frequency of the $6s^2S_{1/2} - 5d^2D_{3/2}$ electric-quadrupole transition of $^{171}$Yb$^+$ with a relative uncertainty of $1\times 10^{-14}$, $\nu_{Yb}$ = 688 358 979 309 312 Hz $\pm$ 6 Hz. A femtosecond frequency comb generator was used to phase-coherently link the optical frequency derived from a single trapped ion to a cesium fountain controlled hydrogen maser. This measurement is one of the most accurate measurements of optical frequencies ever reported, and it represents a contribution to the development of optical clocks based on an $^{171}$Yb$^+$ ion standard.
physics.optics:The time behaviour of microwaves undergoing partial reflection by photonic barriers was measured in the time and in the frequency domain. It was observed that unlike the duration of partial reflection by dielectric layers, the measured reflection duration of barriers is independent of their length. The experimental results point to a nonlocal behaviour of evanescent modes at least over a distance of some ten wavelengths. Evanescent modes correspond to photonic tunnelling in quantum mechanics.
physics.optics:We study the conditions for soliton-like wave propagation in the Photorefractive (PR) and electro-optic (i.e., Pockels) material, by using Nonlinear Schrodinger (NLS) equation. The complete NLS equation is solved analytically and numerically by transforming it into the phase space. Our results clearly show the existence of both the dark and bright solitary solutions for the PR medium. Interestingly, however, we find only one bright solitary solution in the Pockels case and there is no evidence of any dark solitary solution.
physics.optics:In the theory of optical gap solitons, slowly-moving finite-amplitude Lorentzian solutions are found to mediate the transition from bright to coexistent dark-antidark solitary wave pairs when the laser frequency is detuned out of the proper edge of a dynamical photonic bandgap. Catastrophe theory is applied to give a geometrical description of this strongly asymmetrical 'morphing' process.
physics.optics:Harmonic and Intermodulation distortions occur when a physical system is excited with a single or several frequencies and when the relationship between the input and output is non-linear. Working with non-linearities in the Frequency domain is not straightforward specially when the relationship between the input and output is not trivial. We outline the complete derivation of the Harmonic and Intermodulation distortions from basic principles to a general physical system. For illustration, the procedure is applied to the Single Mode laser diode where the relationship of input to output is non-trivial. The distortions terms are extracted directly from the Laser Diode rate equations and the method is tested by comparison to many results cited in the literature. This methodology is general enough to be applied to the extraction of distortion terms to any desired order in many physical systems in a general and systematic way.
physics.optics:We reelaborate on the basic properties of lossless multilayers. We show that the transfer matrices for these multilayers have essentially the same algebraic properties as the Lorentz group SO(2,1) in a (2+1)-dimensional spacetime, as well as the group SL(2,R) underlying the structure of the ABCD law in geometrical optics. By resorting to the Iwasawa decomposition, we represent the action of any multilayer as the product of three matrices of simple interpretation. This group-theoretical structure allows us to introduce bilinear transformations in the complex plane. The concept of multilayer transfer function naturally emerges and its corresponding properties in the unit disc are studied. We show that the Iwasawa decomposition reflects at this geometrical level in three simple actions that can be considered the basic pieces for a deeper undestanding of the multilayer behavior. We use the method to analyze in detail a simple practical example.
physics.optics:A method is presented to investigate diffraction of an electromagnetic plane wave by an infinitely thin infinitely conducting circular cylinder with longitudinal slots. It is based on the use of the combined boundary conditions method that consists on expressing the continuity of the tangential components of both the electric and the magnetic fields in a single equation. This method proves to be very efficient for this kind of problems and leads to fast numerical codes.
physics.optics:We present a reliable, narrow linewidth (100 kHz) continuous-wave optical parametric oscillator (OPO) suitable for high-resolution spectroscopy applications. The OPO is based on a periodically-poled lithium-niobate crystal and features a specially designed intracavity etalon which permits its continuous tuning and stable operation at any desired wavelength in a wide operation range. We demonstrate Doppler-free spectroscopy on a rovibrational transition of methane at 3.39 um.
physics.optics:We describe the action of a plane interface between two semi-infinite media in terms of a transfer matrix. We find a remarkably simple factorization of this matrix, which enables us to express the Fresnel coefficients as a hyperbolic rotation.
physics.optics:We report on a methodology for the evaluation of the DC characteristics, small-signal frequency response and large-signal dynamic response of carrier and photon density responses in semiconductor laser diodes. A single mode laser is considered and described with a pair of rate equations containing a novel non-linear gain compensation term depending on a single parameter that can be chosen arbitrarily. This approach can be applied to any type of solid-state laser as long as it is described by a set of rate equations.
physics.optics:We report the discovery of a "dark area theorem," a new quantum optical relation for propagation of unmatched pulses in thick three-level $\Lambda$-type media. We define dark area and derive the dark area theorem for a coherently prepared and inhomogeneously broadened lambda medium. We also obtain the first equation for the spatial evolution of the dark state amplitude prior to pulse-matching.
physics.optics:We propose experimentally simplified schemes of an optically dispersive interface region between two coupled free electron lasers (FELs), aimed at achieving a much broader gain bandwidth than in a conventional FEL or a conventional optical klystron composed of two separated FELs. The proposed schemes can {\it universally} enhance the gain of FELs, regardless of their design when operated in the short pulsed regime.
physics.optics:As a light beam is produced by an amplification of modes of the zero point field in its source, this field cannot be distinguished; consequently a nonlinear optical effect is a function of the total field. However, we generally prefer to use a conventional field which excludes the zero point field; for a low conventional field, the total field may be developed to the first order, so that the effect appears linear.   This nearly trivial remark allows a correct computation of the signal of a photocell used for photon counting and shows that the "impulsive stimulated Raman scattering" (ISRS), a nonlinear, without threshold effect, which shifts the frequencies, becomes linear at low light levels, so that the shifted spectra are not distorted.
physics.optics:The effect of thermal fluctuations in the resonance fluorescence of a three-level system is studied. The damped three-level system is driven by two strong incident classical fields near resonances frequencies. The simulation of a thermal bath is obtained with a large system of harmonic oscillators that represent the normal modes of the thermal radiation field. The time evolution of the fluorescent light intensities are obtained solving by a iterative method the Heisenberg equations of motion in the integral form. The results show that the time development of the intensity of the fluorescence light is strongly affected by the interaction of the system with the thermal bath.
physics.optics:The dynamical response of a relativistic bunch of electrons injected in a planar magnetic undulator and interacting with a counterpropagating electromagnetic wave is studied. We demonstrate a resonance condition for which the free electron laser (FEL) dynamics is strongly influenced by the presence of the external field. It opens up the possibility of control of short wavelength FEL emission characteristics by changing the parameters of the microwave field without requiring change in the undulator's geometry or configuration. Numerical examples, assuming realistic parameter values analogous to those of the TTF-FEL, currently under development at DESY, are given for possible control of the amplitude or the polarization of the emitted radiation.
physics.optics:We have operated a CW triply resonant OPO using a PPLN crystal pumped by a Nd:YAG laser at 1.06 micron and generating signal and idler modes in the 2-2.3 micron range. The OPO was operated stably in single mode operation over large periods of time with a pump threshold as low as 500 microwatts.
physics.optics:We extend a modal theory of diffraction by a set of parallel fibers to deal with the case of a hard boundary: that is a structure made for instance of air-holes inside a dielectric matrix. Numerical examples are given concerning some resonant phenomena.
physics.optics:We report observation of lasing in the scarred modes in an asymmetrically deformed microcavity made of liquid jet. The observed scarred modes correspond to morphology-dependent resonance of radial mode order 3 with their Q values in the range of 10^6. Emission directionality is also observed, corresponding to a hexagonal unstable periodic orbit.
physics.optics:We have demonstrated an ultrahigh-Q whispering-gallery-mode (WGM) microsphere laser based on the evanescent-wave-coupled gain. Dye molecules outside the sphere near the equator were excited, resulting in WGM lasing in the lowest radial mode order. The loaded quality factor of the lasing WGM was 8(2)\times 10^9, the highest ever achieved in the microlaser.
physics.optics:An extended cavity diode laser operating in the Littrow configuration emitting near 657 nm is stabilized via its injection current to a reference cavity with a finesse of more than 10^5 and a corresponding resonance linewidth of 14 kHz. The laser linewidth is reduced from a few MHz to a value below 30 Hz. The compact and robust setup appears ideal for a portable optical frequency standard using the Calcium intercombination line.
physics.optics:We introduce a novel concept for optical frequency measurement and division which employs a Kerr-lens mode-locked laser as a transfer oscillator whose noise properties do not enter the measurement process. We experimentally demonstrate, that this method opens up the route to phase-link signals with arbitrary frequencies in the optical or microwave range while their frequency stability is preserved.
physics.optics:The mixed crystal of a para-dibromobenzene with a para-chloronitrobenzene is investigated at concentration of components from 0% up to 60% of a para-chloronitrobenzene by the method of Low-Frequency Raman spectroscopy. It is shown, that in range of concentrations from 25% up to 50% of a para-chloronitrobenzene the spectrum of the mixed crystal would consist of the sum of spectrums a and b phases which relation of intensities depends on concentration of components. It is also found, that the single crystal in this range has rod frame.
physics.optics:The stability of polarization, areas, and number of self-induced transparency (SIT)-solitons at the output from the LaF_3:Pr^{3+} crystal is theoretically studied versus the polarization direction and the area of the input linearly polarized laser pulse. For this purpose the Vector Area Theorem is rederived and two-dimensional Vector Area Theorem map is obtained. The map is governed by the crystal symmetry and takes into account directions of the dipole matrix element vectors of the different site subgroups of optically excited ions. The Vector Area Theorem mapping of the time evolution of the laser pulse allows one to highlight soliton polarization properties.
physics.optics:The dynamics of light in Fabry-Perot cavities with varying length and input laser frequency are analyzed and the exact condition for resonance is derived. This dynamic resonance depends on the light transit time in the cavity and the Doppler effect due to the mirror motions. The response of the cavity to length variations is very different from its response to laser frequency variations. If the frequency of these variations is equal to multiples of the cavity free spectral range, the response to length is maximized while the response to the laser frequency is zero. Implications of these results for the detection of gravitational waves using kilometer-scale Fabry-Perot cavities are discussed.
physics.optics:Numerical simulation of the National Ignition Facility (NIF) laser performance and automated control of the laser setup process are crucial to the project's success. These functions will be performed by two closely coupled computer code: the virtual beamline (VBL) and the laser performance operations model (LPOM).
physics.optics:Since its birth, the laser has been extraordinarily effective in the study and applications of laser-matter interaction at the atomic and molecular level and in the nonlinear optics of the bound electron. In its early life, the laser was associated with the physics of electron volts and of the chemical bond. Over the past fifteen years, however, we have seen a surge in our ability to produce high intensities, five to six orders of magnitude higher than was possible before. At these intensities, particles, electrons and protons, acquire kinetic energy in the mega-electron-volt range through interaction with intense laser fields. This opens a new age for the laser, the age of nonlinear relativistic optics coupling even with nuclear physics. We suggest a path to reach an extremely high-intensity level $10^{26-28} $W/cm$^2$ in the coming decade, much beyond the current and near future intensity regime $10^{23} $W/cm$^2$, taking advantage of the megajoule laser facilities. Such a laser at extreme high intensity could accelerate particles to frontiers of high energy, tera-electron-volt and peta-electron-volt, and would become a tool of fundamental physics encompassing particle physics, gravitational physics, nonlinear field theory, ultrahigh-pressure physics, astrophysics, and cosmology. We focus our attention on high-energy applications in particular and the possibility of merged reinforcement of high-energy physics and ultraintense laser.
physics.optics:A simple and intuitive geometical method to analyze Fresnel formulas is presented. It applies to transparent media and is valid for perpendicular and parallel polarizations. The approach gives a graphical characterization particularly simple of the critical and Brewster angles. It also provides an interpretation of the relation between the reflection coefficients for both basic polarizations as a symmetry in the plane.
physics.optics:The tunnel effect is considered here within the framework of electromagnetic propagation. The classical problem of a plane gap of dielectric, surrounded on both sides by a medium with larger refraction index, is studied in the case in which an electromagnetic plane wave impinges into the gap with an incidence angle larger than the critical angle. In this condition (total reflection), the gap acts as a classically forbidden region and behaves like a tunnel. The field inside the forbidden gap consists of two evanescent waves, each one having its wavefronts normal to the interface. In the present paper we study the total field derived as a superposition of two such evanescent waves, its wavefronts, and the directions of propagation of both phase and energy.
physics.optics:The motion of an electromagnetic wave, through a classically-forbidden region, has recently attracted renewed interest because of its implication with regard to the theoretical and experimental problems of superluminality. From an experimental point of view, many papers provide an evidence of superluminality in different physical systems. Theoretically, the problem of a passage through a forbidden gap has been treated by considering plane waves at oblique incidence into a plane parallel layer of a medium with a refractive index smaller than the index of the surrounding medium, and also confined (Gaussian) beams, still at oblique incidence. In the present paper the case of a Bessel beam is examined, at normal incidence into the layer (Secs. II and III), in the scalar approximation (Sec. IV) and by developing also a vectorial treatment (Sec. V). Conclusions are reported in Sic. VI.
physics.optics:The tunneling time is here investigated by means of an electromagnetic model, for a system where a gap, between two parallel planes, acts as a classically-forbidden region for an impinging pulse with incidence angle larger than the critical angle. In all cases of frustrated total reflection we obtain a superluminal behavior both for phase and group delays.
physics.optics:We report an injection-locked cw titanium:sapphire ring laser at 846 nm. It produces 1.00 W in a single frequency when pumped with 5.5 W. Single frequency operation requires only a few milliwatts of injected power.
physics.optics:Accurate knowledge of absorption coefficient of a sample is a prerequisite for measuring the third order optical nonlinearity of materials, which could become a serious limitation for unknown samples. We introduce a new method, which measures both the absorption coefficient and the third order optical nonlinearity of materials with high sensitivity in a single experimental setup. We use a dual-beam pump-probe experiment under different conditions to achieve this goal. We also demonstrate a counterintuitive coupling of the non-interacting probe-beam with the pump-beam in pump-probe z-scan experiment.
physics.optics:We obtain gain of the probe field at multiple frequencies in a closed three-level V-type system using frequency modulated pump field. There is no associated population inversion among the atomic states of the probe transition. We describe both the steady-state and transient dynamics of this system. Under suitable conditions, the system exhibits large gain simultaneously at series of frequencies far removed from resonance. Moreover, the system can be tailored to exhibit multiple frequency regimes where the probe experiences anomalous dispersion accompanied by negligible gain-absorption over a large bandwidth, a desirable feature for obtaining superluminal propagation of pulses with negligible distortion.
physics.optics:An undoped double quantum well (DQW) was driven with a terahertz (THz) electric field of frequency \omega_{THz} polarized in the growth direction, while simultaneously illuminated with a near-infrared (NIR) laser at frequency \omega_{NIR}. The intensity of NIR upconverted sidebands \omega_{sideband}=\omega_{NIR} + \omega_{THz} was maximized when a dc voltage applied in the growth direction tuned the excitonic states into resonance with both the THz and NIR fields. There was no detectable upconversion far from resonance. The results demonstrate the possibility of using gated DQW devices for all-optical wavelength shifting between optical communication channels separated by up to a few THz.
physics.optics:Driving a double-quantum-well excitonic intersubband resonance with a terahertz (THz) electric field of frequency \omega_{THz} generated terahertz optical sidebands \omega=\omega_{THz}+\omega_{NIR} on a weak NIR probe. At high THz intensities, the intersubband dipole energy which coupled two excitons was comparable to the THz photon energy. In this strong-field regime the sideband intensity displayed a non-monotonic dependence on the THz field strength. The oscillating refractive index which gives rise to the sidebands may be understood by the formation of Floquet states, which oscillate with the same periodicity as the driving THz field.
physics.optics:We elaborate on the consequences of the factorization of the transfer matrix of any lossless multilayer in terms of three basic matrices of simple interpretation. By considering the bilinear transformation that this transfer matrix induces in the complex plane, we introduce the concept of multilayer transfer function and study its properties in the unit disk. In this geometrical setting, our factorization translates into three actions that can be viewed as the basic pieces for understanding the multilayer behavior. Additionally, we introduce a simple trace criterion that allows us to classify multilayers in three types with properties closely related to one (and only one) of these three basic matrices. We apply this approach to analyze some practical examples that are representative of these types of matrices.
physics.optics:We consider the problem of radiation into free space from the end-facet of a single-mode photonic crystal fiber (PCF). We calculate the numerical aperture NA=sin theta from the half-divergence angle theta ~ tan^{-1}(lambda/pi w) with pi w^2 being the effective area of the mode in the PCF. For the fiber first presented by Knight et al. we find a numerical aperture NA ~ 0.07 which compares to standard fiber technology. We also study the effect of different hole sizes and demonstrate that the PCF technology provides a large freedom for NA-engineering. Comparing to experiments we find good agreement.
physics.optics:Polarized and azimuthal dependencies of optical second harmonics generation (SHG) at the surface of noncentrosymmetric semiconductor crystals have been measured on polished surfaces of ZnSe(100), using a fundamental wavelength of 1.06$\mu m$. The SHG intensity patterns were analyzed for all four combination of p- and s-polarized incidence and output, considering both the bulk and surface optical nonlinearities in the electric dipole approximation. We found that the measurement using $S_{in}-S_{out}$ is particularly useful in determining the symmetry of the oxdized layer interface, which would lower the effective symmetry of the surface from $C_{4v}$ to $C_{2v}.$ We also have shown that the [011] and [0$\bar{1}$1] directions can be distinguished through the analysis of p-incident and p-output confugration.
physics.optics:Fundamental rules and definitions of Fractional Differintegrals are outlined. Factorizing 1-D and 2-D Helmholtz equations four fractional eigenfunctions are determined. The functions exhibit incident and reflected plane waves as well as diffracted incident and reflected waves of the half-plane edge. They allow to construct the Sommerfeld half-plane diffraction solutions. Parabolic-Wave Equation (PWE, Leontovich-Fock) for paraxial propagation is factorized and differetial fractional solutions of Fresnel-integral type are derived. We arrived at two solutions, which are the mothers of known and new solutions.
physics.optics:In the classical theory, an electromagnetic field obeying Maxwell's equations cannot be absorbed quickly by matter, so that it remains a zero point field. Splitting the total, genuine electromagnetic field into the sum of a conventional field and a zero point field is physically meaningless until a receiver attenuates the genuine field down to the zero point field, or studying the amplification of the zero point field by a source.   In classical optics all optical effects must be written using the genuine field, so that at low light levels the nonlinear effects become linear in relation to the conventional field. The result of the interpretation of all observations, even at low light levels, is exactly the same in quantum electrodynamics and in the semi- classical theory.   The zero point field is stochastic only far from the sources and the receivers; elsewhere, it is shaped by matter, it may be studied through fields visible before an absorption or after an amplification.   A classical study of the reduction of the wave packet extends the domain of equivalence of the classical and quantum zero point field; using both interpretations of this field makes the results more reliable, because the traps are different.
physics.optics:The smaller the size of a light-emitting microcavity, the more important it becomes to understand the effects of the cavity boundary on the optical mode profile. Conventional methods of laser physics, such as the paraxial approximation, become inapplicable in many of the more exotic cavity designs to be discussed here. Cavities in the shape of microdisks, pillars and rings can yield low lasing thresholds in a wide variety of gain media: quantum wells, wires and even dots, as well as quantum cascade superlattices and GaN. An overview of the experimental and theoretical status is provided, with special emphasis on the light extraction problem.
physics.optics:The stationary states of a microlaser are related to the decaying quasibound states of the corresponding passive cavity. These are interpreted classically as originating from sequential escape attempts of an ensemble of rays obeying a curvature-corrected Fresnel formula. Polarization-dependent predictions of this model, and its limitations for stable orbits in partially chaotic systems are discussed.
physics.optics:We measured and calculated transmission spectra of two-dimensional quasiperiodic photonic crystals (PCs) based on a 5-fold (Penrose) or 8-fold (octagonal) symmetric quasiperiodic pattern. The photonic crystal consisted of dielectric cylindrical rods in air placed normal to the basal plane on vertices of tiles composing the quasiperiodic pattern. An isotropic photonic band gap (PBG) appeared in the TM mode, where electric fields were parallel to the rods, even when the real part of a dielectric constant of the rod was as small as 2.4. An isotropic PBG-like dip was seen in tiny Penrose and octagonal PCs with only 6 and 9 rods, respectively. These results indicate that local multiple light scattering within the tiny PC plays an important role in the PBG formation. Besides the isotropic PBG, we found dips depending on the incident angle of the light. This is the first report of anisotropic structures clearly observed in transmission spectra of quasiperiodic PCs. Based on rod-number and rod-arrangement dependence, it is thought that the shapes and positions of the anisotropic dips are determined by global multiple light scattering covering the whole system. In contrast to the isotropic PBG due to local light scattering, we could not find any PBGs due to global light scattering even though we studied transmission spectra of a huge Penrose PC with 466 rods.
physics.optics:A perfect focus telescope is one in which all rays parallel to the axis meet at a point and give equal magnification there. It is shown that these two conditions define the shapes of both primary and secondary mirrors. Apart from scale, the solution depends upon two parameters, $s$, which gives the mirror separation in terms of the effective focal length, and $K$, which gives the relative position of the final focus in that unit. The two conditions ensure that the optical systems have neither spherical aberration nor coma, no matter how fast the $f$ ratio. All known coma--free systems emerge as approximate special cases. In his classical paper, K. Schwarzschild studied all two mirror systems whose profiles were conic sections. We make no such a priori shape conditions but demand a perfect focus and solve for the mirrors' shapes.
physics.optics:The results of experimental testing the existence of intense Lorentzian--like wings with FWHM $\sim 4.5 cm^{-1}$ in the absorption spectra of polyatomic molecules in a gas phase are presented. Two independent experimental methods were used for evaluating the integral intensity of the line wings for a number of substances. In the first case, the cross--section of the far wings of absorption bands in a gas phase spectrum were measured. Then, these band wings were extrapolated inside the contour of absorption band. In the second case, the saturation degree of the linear spectrum of molecules was determined. Radiation of a pulsed $CO_2$--laser was used at low gas pressure ($\sim 16$ mtorr) and averaged excitation level of molecules ${<n>}\sim 0.1$ quanta/molecule. The values obtained by these two independent methods coincide for a variety of molecules. The average relative integral intensity of the line wings varied from $\sim 0.6%$ for $SF_6$ and $SiF_4$ to $\sim 90%$ for $(CF_3)_2O$ and $(CF_3)_2CO$.
physics.optics:It is shown that the direct Fourier synthesization of light beams allows one to create polarity-asymmetric waves, which are able, in the process of nonlinear interaction with a medium, to break its inversion symmetry. As a result, these "polar" waves may show the effect of optical rectification in nonlinear centrosymmetric media by generating light-induced dc electric polarization. At the same time, the waves of this type, due to their unusual symmetry properties, can be used for detecting the direction and sign of a dc electric field applied to the medium. The prospects of application of polar waves to data recording and processing are discussed.
physics.optics:Presented is an analysis of general scaling perturbations in a transmitting fiber. For elliptical perturbations, under some conditions an intermode dispersion parameter characterizing modal PMD is shown to be directly proportional to the mode dispersion.
physics.optics:Extensive Bose-Einstein condensation research activities have recently led to studies of fermionic atoms and optical confinements. Here we present a case of micro-optical fermionic electron phase transition. Optically confined ordering and phase transitions of a fermionic cloud in dynamic steady state are associated with Rayleigh emissions from photonic quantum ring manifold which are generated by nature without any ring lithography. The whispering gallery modes, produced in a semiconductor Rayleigh-Fabry-Perot toroidal cavity at room temperature, exhibit novel properties of ultralow thresholds open to nano-ampere regime, thermal stabilities from square-root-T-dependent spectral shift, and angularly varying intermode spacings. The photonic quantum ring phenomena are associated with a photonic field-driven phase transition of quantum-well-to-quantum-wire and hence the photonic (non-de Broglie) quantum corral effect on the Rayleigh cavity-confined carriers in dynamic steady state. Based upon the intra-cavity fermionic condensation we also offer a prospect for an electrically driven few-quantum dot single photon source from the photonic quantum ring laser for quantum information processors.
physics.optics:Nonlinear optical media that are normally dispersive, support a new type of localized (nondiffractive and nondispersive) wavepackets that are X-shaped in space and time and have slower than exponential decay. High-intensity X-waves, unlike linear ones, can be formed spontaneously through a trigger mechanism of conical emission, thus playing an important role in experiments.
physics.optics:We report three-dimensional laser microfabrication, which enables microstructuring of materials on the scale of 0.2-1 micrometers. The two different types of microfabrication demonstrated and discussed in this work are based on holographic recording, and light-induced damage in transparent dielectric materials. Both techniques use nonlinear optical excitation of materials by ultrashort laser pulses (duration < 1 ps).
physics.optics:We propose a concept for production of high power coherent attosecond pulses in X-ray range. An approach is based on generation of 8th harmonic of radiation in a multistage HGHG FEL (high gain high harmonic free electron laser) configuration starting from shot noise. Single-spike phenomena occurs when electron bunch is passed through the sequence of four relatively short undulators. The first stage is a conventional "long" wavelength (0.8 nm) SASE FEL which operates in the high-gain linear regime. The 0.1 nm wavelength range is reached by successive multiplication (0.8 nm $\to$ 0.4 nm $\to$ 0.2 nm $\to$ 0.1 nm) in a stage sequence. Our study shows that the statistical properties of the high-harmonic radiation from the SASE FEL, operating in linear regime, can be used for selection of radiation pulses with a single spike in time domain. The duration of the spikes is in attosecond range. Selection of single-spike high-harmonic pulses is achieved by using a special trigger in data acquisition system. The potential of X-ray SASE FEL at TESLA at DESY for generating attosecond pulses is demonstrated. Since the design of XFEL laboratory at TESLA is based on the use of long SASE undulators with tunable gap, no special place nor additional FEL undulators are required for attophysics experiments. The use of a 10 GW-level attosecond X-ray pulses at X-ray SASE FEL facility will enable us to track processes inside atoms.
physics.optics:We present an algorithm for the maximization of photonic bandgaps in two-dimensional crystals. Once the translational symmetries of the underlying structure have been imposed, our algorithm finds a global maximal (and complete, if one exists) bandgap. Additionally, we prove two remarkable results related to maximal bandgaps: the so-called `maximum contrast' rule, and about the location in the Brillouin zone of band edges.
physics.optics:We investigate the propagation of electromagnetic waves in finite photonic band gap structures. We analyze the phenomenon of conduction and forbidden bands and we show that two regimes are to be distinguished with respect to the existence of a strong field near the interfaces. We precise the domain for which an effective medium theory is sounded.
physics.optics:The maximum bit-rate of a slab waveguide is ultimately determined by the waveguide dispersion. We show that while the maximum bit rate in a waveguide is inversely proportional to the waveguide's width, bit rate per unit width (i.e., spatial capacity) decreases, and in the limit of a zero-width waveguide it converges to a value, which is independent of the waveguide's refractive indices. This value is qualitatively equivalent to the transmission rate per unit of width in free space. We also show that in a 3D waveguide (e.g., fibers), unlike free space, the spatial capacity vanishes in the same limit.
physics.optics:The photonic band dispersion and density of states (DOS) are calculated for the three-dimensional (3D) hexagonal structure corresponding to a distributed Bragg reflector patterned with a 2D triangular lattice of circular holes. Results for the Si/SiO$_2$ and GaAs/AlGaAs systems determine the optimal parameters for which a gap in the 2D plane occurs and overlaps the 1D gap of the multilayer. The DOS is considerably reduced in correspondence with the overlap of 2D and 1D gaps. Also, the local density of states (i.e., the DOS weighted with the squared electric field at a given point) has strong variations depending on the position. Both results imply substantial changes of spontaneous emission rates and patterns for a local emitter embedded in the structure and make this system attractive for the fabrication of a 3D photonic crystal with controlled radiative properties.
physics.optics:We present a Fourier transform methodology for all-order polarization mode dispersion (PMD) analysis, based on the first Born approximation to the coupled-mode equation solution. Our method predicts wavelength-dependent PMD effects and allows design of filters for their mitigation.
physics.optics:We demonstrate the combination of a hemispherical solid immersion lens with a micro-photoluminescence setup. Two advantages introduced by the SIL, an improved resolution of 0.4 times the wavelength in vacuum and a 5 times enhancement of the collection efficiency, make it an ideal system for spatially resolved spectroscopy applications. The influence of the air gap between the SIL and the sample surface is investigated in detail. We confirm the tolerance of the set--up to an air gap of several micrometers. Such a system is proven to be ideal system in the studies of exciton transport and polarization dependent single quantum dot spectroscopy.
physics.optics:It is assumed, that the clumps of lines do not connected with states mixing and IVR, but they are the result of breaking (destruction) of the process of averaging of momentum of inertia of molecules during the vibration motion of atoms. Rough estimates of the widths of clumps of lines in absorption spectra of some acetylenic derivatives were made with this model. Obtained results are in a satisfactory agreement with the available experimental data. This idea allows also in principle to explain the origin of intensive wings of lines, the existence of which was discussed earlier.
physics.optics:The origin of the Kerr type nonlinearity of the medium as a result of the interaction between photons via the Dirac delta-potential is presented in the formalism adopted from the photon wave function approach. In the view of the result the optical soliton may be treated as a bound state (cluster) of many photons.
physics.optics:The propagation of photon in a dielectric may be described with the help of the scalar and vector potentials of the medium. The main novelty of the paper is that the concept of the vector potential (which is connected with the velocity of the medium) can be extended to relativistic velocities of the medium. The position-dependent photon wave function was used to describe the propagation of the photon. The new concepts of the velocity of photon as particle and the photon mass in the dielectric medium were proposed.
physics.optics:Mathematical aspects of the SU(1,1) group parameter x dynamics governed by Hamiltonians exhibiting some special types of time dependence has been presented on an elementary level from the point of view of Moebius transformation of complex plane. The trajectories of x in continuous and mappings in discrete dynamics are considered. Some simple examples have been examined. Analytical considerations and numerical results have been given.
physics.optics:Propagation of the TE electromagnetic waves in self-focusing medium is governed by the nonlinear Schroedinger equation. In this paper the stationary solutions of this equation have been systematically presented. The phase-plane method, qualitative analysis, and mechanical interpretation of the differential equations are widely used. It is well known that TE waves can be guided by the single interface between two semi-infinite media, providing that one of the media has a self-focusing (Kerr type) nonlinearity. This special solution is called a spatial soliton. In this paper our interests are not restricted to the soliton solutions. In the context of the nonlinear substrate and cladding we have found solutions which could be useful to describe also the incident light in nonlinear medium. This result is the main point of the paper. Some of the presented stationary solutions were already used in similar optical context in literature but we show a little wider class of solutions. In the last section we review and illustrate some results concerning the spatial soliton solution.
physics.optics:We present angle- and polarization-resolved measurements of the optical transmission of a subwavelength hole array. These results give a (far-field) visualization of the corresponding (near-field) propagation of the excited surface plasmons and allow for a simple analysis of their polarization properties.
physics.optics:A monochromatic linear source of light is rotated with certain angular frequency and when such light is analysed after reflection then a change of frequency or wavelength may be observed depending on the location of the observer. This change of frequency or wavelength is different from the classical Doppler effect [1] or relativistic Doppler effect [2]. The reason behind this shift in wavelength is that a certain time interval observed by an observer in the rotating frame is different from that of a stationary observer.
physics.optics:We investigate the spectral response of a Brillouin amplifier in the frequency regime within the SBS bandwidth. This is done by amplitude modulating the pump with a low frequency, and therefore, unlike previous studies, the spectrum of the modulated pump is, in all cases, smaller than the SBS bandwidth. We show both theoretically and experimentally that unlike phase modulation, which was reported in the literature, the amplitude modulation increases the Brillouin amplifier gain, and that this effect has a very narrow bandwidth. Only modulation frequencies that are lower than a certain cut-off frequency increase the gain. This cut-off frequency is inversely proportional to the fiber's length, and can therefore be arbitrarily small.
physics.optics:The Phase Diverse Speckle (PDS) problem is formulated mathematically as Multi Frame Blind Deconvolution (MFBD) together with a set of Linear Equality Constraints (LECs) on the wavefront expansion parameters. This MFBD-LEC formulation is quite general and, in addition to PDS, it allows the same code to handle a variety of different data collection schemes specified as data, the LECs, rather than in the code. It also relieves us from having to derive new expressions for the gradient of the wavefront parameter vector for each type of data set. The idea is first presented with a simple formulation that accommodates Phase Diversity, Phase Diverse Speckle, and Shack-Hartmann wavefront sensing. Then various generalizations are discussed, that allows many other types of data sets to be handled.
physics.optics:A Monte Carlo simulation has been performed to track light rays in cylindrical fibres by ray optics. The trapping efficiencies for skew and meridional rays in active fibres and distributions of characteristic quantities for all trapped light rays have been calculated. The simulation provides new results for curved fibres, where the analytical expressions are too complex to be solved. The light losses due to sharp bending of fibres are presented as a function of the ratio of curvature to fibre radius and bending angle. It is shown that a radius of curvature to fibre radius ratio of greater than 65 results in a loss of less than 10% with the loss occuring in the initial stage of the bend (at bending angles Phi circa pi/8 rad).
physics.optics:We have measured the photonic bandgap in the transmission of microwaves through a two-dimensional photonic crystal slab. The structure was constructed by cementing acrylic rods in a hexagonal closed-packed array to form rectangular stacks. We find a bandgap centered at approximately 11 GHz, whose depth, width and center frequency vary with the number of layers in the slab, angle of incidence and microwave polarization.
physics.optics:We study forward stimulated Raman emission from weakly fluorescent dye 4'-diethylamino-N-methyl-4-stilbazolium tosylate (DEST) in 1,2,dichloroethane solution excited by a 28 ps, 532 nm Nd: YAG laser. Neat 1, 2, dichloroethane emits the first Stokes line at 631 nm with a spectral width of 1.6 nm corresponding to a Raman shift of 2956 per cm. We observe reduction of spectral width with the addition of DEST in 1, 2, dichloroethane solution. The single pass conversion efficiency for forward Raman emission is as high as 20 percent in a 1 cm path length sample. The pulse duration of forward stimulated Raman emission measured by a third order autocorrelation technique is 10 ps in neat 1, 2, dichloroethane, whereas it is nearly 3 ps for 0.04 mM of DEST solution.
physics.optics:Distribution of centrosymmetrical molecules of an impurity (p-diclorobenzene) in monocrystals of solid solutions in two different matrixes with centrosymmetrical (p-dibrombenzene) and noncentrosymmetrical (p-bromchlorbenzene) molecules by the method of a Raman Effect is determined.
physics.optics:We demonstrate that twisting one part of a chiral photonic structure about its helical axis produces a single circularly polarized localized mode that gives rise to an anomalous crossover in propagation. Up to a crossover thickness, this defect results in a peak in transmission and exponential scaling of the linewidth for a circularly polarized wave with the same handedness as structure. Above the crossover, however, the linewidth saturates and the defect mode can be excited only by the oppositely polarized wave, resulting in a peak in reflection instead of transmission.
physics.optics:We study experimentally and theoretically the polarization alternation during the switch-on transient of a quasi-isotropic CO$_2$ laser emitting on the fundamental mode. The observed transient dynamics is well reproduced by means of a model which provides a quantitative discrimination between the intrinsic asymmetry due to the kinetic coupling of molecules with different angular momenta, and the extrinsic anisotropies, due to a tilted intracavity window. Furthermore, the experiment provides a numerical assignment for the decay rate of the coherence term for a CO$_2$ laser.
physics.optics:In this paper we present an analysis of information transfer time based on holomorphism, causality and the classical principle of stationary phase. We also make a preliminary study of the effect of noise on information transfer time, and find that noise tends to increase transfer times. Noise and information signals are both essentially acausal, such that analytic continuation (i.e. prediction) is impossible, which also implies that their frequency spectra cannot be holomorphic. This leads to the paradox of a non-holomorphic information-bearing light signal, yet whose underlying Maxwell equations governing the propagation of the EM wave describe a holomorphic function in spacetime. We find that application of stationary phase and entropy arguments circumvents this difficulty, with stationary phase only suggesting the most likely transfer times of an information signal in the presence of noise. Faster transit times are not excluded, but are highly improbable. Stationary phase solutions, by definition, do not include signal forerunners, whose detection in the presence of noise is also unreliable. Hence a finite information capacity ensues, as expected from Shannon's law, and information cannot be transferred faster than c. We also find that the method of stationary phase implies complex transfer times. However, by considering spacetime to be isomorphic with the complex temporal plane, we find that an imaginary time is equivalent to a real distance, and can be interpreted as the uncertainty in the spatial position of the information pulse. Finally, we apply our theory to a photonic band gap crystal, and find that information transfer speed and tunneling is always subluminal.
physics.optics:Enhancement of optical Kerr nonlinearity for self-action by electro-magnetically induced transparency in a four-level atomic system including dephasing between the ground states is studied in detail by solving the density matrix equations for the atomic levels. We discern three major contributions, from energy shifts of the ground states induced by the probe light, to the third-order susceptibility in the four-level system. In this four-level system with the frequency-degenerate probes, quantum interference amongst the three contributions can, not only enhance the third-order susceptibility more effectively than in the three-level system with the same characteristic parameters, but also make the ratio between its real and imaginary part controllable. Due to dephasing between the two ground states and constructive quantum interference, the most effective enhancement generally occurs at an offset that is determined by the atomic transition frequency difference and the coupling Rabi frequency.
physics.optics:We numerically study supercontinuum (SC) generation in photonic crystal fibers pumped with low-power 30-ps pulses close to the zero dispersion wavelength 647nm. We show how the efficiency is significantly improved by designing the dispersion to allow widely separated spectral lines generated by degenerate four-wave-mixing (FWM) directly from the pump to broaden and merge. By proper modification of the dispersion profile the generation of additional FWM Stokes and anti-Stokes lines results in efficient generation of an 800nm wide SC. Simulations show that the predicted efficient SC generation is more robust and can survive fiber imperfections modelled as random fluctuations of the dispersion coefficients along the fiber length.
physics.optics:We numerically study the possibilities for improved large-mode area endlessly single mode photonic crystal fibers for use in high-power delivery applications. By carefully choosing the optimal hole diameter we find that a triangular core formed by three missing neighboring air holes considerably improves the mode area and loss properties compared to the case with a core formed by one missing air hole. In a realized fiber we demonstrate an enhancement of the mode area by ~30 % without a corresponding increase in the attenuation.
physics.optics:A new type of perturbative expansion is built in order to give a rigorous derivation and to clarify the range of validity of some commonly used model equations.   This model describes the evolution of the modulation of two short and localized pulses, fundamental and second harmonic, propagating together in a bulk uniaxial crystal with non-vanishing second order susceptibility $\chi^(2)$ and interacting through the nonlinear effect known as ``cascading'' in nonlinear optics.   The perturbative method mixes a multi-scale expansion with a power series expansion of the susceptibility, and must be carefully adapted to the physical situation. It allows the determination of the physical conditions under which the model is valid: the order of magnitude of the walk-off, phase-mismatch,and anisotropy must have determined values.
physics.optics:Nonlinear phase noise, often called the Gordon-Mollenauer effect, can be compensated electronically by subtracting from the received phase a correction proportional to the received intensity. The optimal scaling factor is derived analytically and found to be approximately equal to half of the ratio of mean nonlinear phase noise and the mean received intensity. Using optimal compensation, the standard deviation of residual phase noise is halved, doubling the transmission distance in systems limited by nonlinear phase noise.
physics.optics:On the basis of the data given in the works of different authors a criterion of phase-photometric method of measurement of energy angle of divergence has been formulated. Validity of application of the obtained relations for a ray beam with an arbitrary diameter and an arbitrary shape of the wave front has been proved. Advantages of the proposed phase-photometric method in comparison with the focal-spot method have been confirmed. Necessity and possibility of building a standard solid angle has been proved.
physics.optics:This document contains my detailed calculation of the Generalised Few-cycle Envelope Approximation (GFEA) propagation equation reported and used in Phys. Rev. A (submitted) and its associated longer version at arXiv.org. This GFEA propagation equation is intended to be applicable to optical pulses only a few cycles long, a regime where the standard Slowly Varying Envelope Approximation (SVEA) fails.
physics.optics:We present a comprehensive framework for treating the nonlinear interaction of few-cycle pulses using an envelope description that goes beyond the traditional SVEA method. This is applied to a range of simulations that demonstrate how the effect of a $\chi^{(2)}$ nonlinearity differs between the many-cycle and few-cycle cases. Our approach, which includes diffraction, dispersion, multiple fields, and a wide range of nonlinearities, builds upon the work of Brabec and Krausz[1] and Porras[2]. No approximations are made until the final stage when a particular problem is considered.   The original version (v1) of this arXiv paper is close to the published Phys.Rev.A. version, and much smaller in size.
physics.optics:Broadband noise on supercontinuum spectra generated in microstructure fiber is shown to lead to amplitude fluctuations as large as 50 % for certain input laser pulse parameters. We study this noise using both experimental measurements and numerical simulations with a generalized stochastic nonlinear Schroedinger equation, finding good quantitative agreement over a range of input pulse energies and chirp values. This noise is shown to arise from nonlinear amplification of two quantum noise inputs: the input pulse shot noise and the spontaneous Raman scattering down the fiber.
physics.optics:The probability density function of Kerr effect phase noise, often called the Gordon-Mollenauer effect, is derived analytically. The Kerr effect phase noise can be accurately modeled as the summation of a Gaussian random variable and a noncentral chi-square random variable with two degrees of freedom. Using the received intensity to correct for the phase noise, the residual Kerr effect phase noise can be modeled as the summation of a Gaussian random variable and the difference of two noncentral chi-square random variables with two degrees of freedom. The residual phase noise can be approximated by Gaussian distribution better than the Kerr effect phase noise without correction.
physics.optics:Steady-state and dynamics of the self-phase-locked (3\omega ==> 2\omega, \omega) subharmonic optical parametric oscillator are analyzed in the pump-and-signal resonant configuration, using an approximate analytical model and a full propagation model. The upper branch solutions are found always stable, regardless of the degree of pump enhancement. The domain of existence of stationary states is found to critically depend on the phase-mismatch of the competing second-harmonic process.
physics.optics:In the present paper we investigate the transmission and reflection band behavior for a plane electromagnetic wave falling obliquely on an ideal layered structure. The dependence of this behavior on the problem parameters and wave incident angle is considered. It is shown, that in general case the band width is a non-monotonous function of the problem parameters. A condition is found, which defines the possibility of the contact of the transmission bands. This condition has the same form for s and p waves. It is also shown that irrespective of the wave polarization, the transmission coefficient equals to the unit at the contact points.
physics.optics:The problem of determination of the maximum of second harmonic generation in the potential well containing a rectangular barrier is considered. It is shown that, in general, the problem of finding the ensemble of structures with equidistant first three levels has two types of solutions.   For the first type the second and third energy levels are located above a rectangular barrier, and for the second type the third level is located above the barrier only. It is also shown, that generation corresponding to the second type of solution always is less than generation for the first one. Taking into account the effective mass changes the problem of finding the generation maximum for a finite depth well is exactly solved.
physics.optics:We investigate numerically optical properties of novel two-dimensional photonic materials where parallel dielectric rods are randomly placed with the restriction that the distance between rods is larger than a certain value. A large complete photonic gap (PG) is found when rods have sufficient density and dielectric contrast. Our result shows that neither long-range nor short-range order is an essential prerequisite to the formation of PGs. A universal principle is proposed for designing arbitrarily shaped waveguides, where waveguides are fenced with side walls of periodic rods and surrounded by the novel photonic materials. We observe highly efficient transmission of light for various waveguides. Due to structural uniformity, the novel photonic materials are best suited for filling up the outer region of waveguides of arbitrary shape and dimension comparable with the wavelength.
physics.optics:We suggest an effective method for controlling nonlinear switching in arrays of weakly coupled optical waveguides. We demonstrate the digitized switching of a narrow input beam for up to eleven waveguides in the engineered waveguide arrays.
physics.optics:Plane waves in Kerr media spontaneously generate paraxial X-waves (i.e. non-dispersive and non-diffractive pulsed beams) that get amplified along propagation. This effect can be considered a form of conical emission (i.e. spatio-temporal modulational instability), and can be used as a key for the interpretation of the out of axis energy emission in the splitting process of focused pulses in normally dispersive materials. A new class of spatio-temporal localized wave patterns is identified. X-waves instability, and nonlinear X-waves, are also expected in periodical Bose condensed gases.
physics.optics:The Dicke superradiance on vibronic transitions of impurity crystals is considered. It is shown that parameters of the superradiance (duration and intensity of the superradiance pulse and delay times) on each vibronic transition depend on the strength of coupling of electronic states with the intramolecular impurity vibration (responsible for the vibronic structure of the optical spectrum in the form of vibrational replicas of the pure electronic line) and on the crystal temperature through the Debye-Waller factor of the lattice vibrations. Theoretical estimates of the ratios of the time delays, as well as of the superradiance pulse intensities for different vibronic transitions well agree with the results of experimental observations of two-color superradiance in the polar dielectric KCl:O2-. In addition, the theory describes qualitatively correctly the critical temperature dependence of the superradiance effect.
physics.optics:This work is concerned with the propagation of electromagnetic waves in isotropic chiral media and with the effects produced by a plane boundary between two such media. In analogy with the phenomena of reflection and refraction of plane electromagnetic waves in ordinary dielectrics, the kinematical and dynamical aspects of these phenomena are studied, such as the intensity of the various wave components and the change in the polarization of the wave as it crosses the boundary. As a prerequisite of this, we show that the plane wave solution must be written as a suitable superposition of the circularly amplitudes on both sides of the interface, we elucidate which is the appropriate set of conditions that the solution must satisfy at the boundary, and we set down the minimal, and complete, set of equations that must be solved for the coefficient amplitudes in order to satisfy the boundary conditions. The equations are solved explicitly for some particular cases and configurations (e.g., normal incidence), the salient features of those solutions are analyzed in some detail, and the general solution to the equations is given as well.
physics.optics:We study the class of endlessly single-mode all-silica photonic crystal fibers with a triangular air-hole cladding. We consider the sensibility to longitudinal nonuniformities and the consequences and limitations for realizing low-loss large-mode area photonic crystal fibers. We also discuss the dominating scattering mechanism and experimentally we confirm that both macro and micro-bending can be the limiting factor.
physics.optics:Some aspects of lasing at vibronic transitions in impurity crystals are theoretically studied. The threshold conditions for a vibronic laser are shown to be dependent on the strength of interaction of optical centers with a local vibration, which forms the vibronic spectrum, and the crystal lattice temperature. The theory can be easily generalized to the spectrum containing a structureless phonon sideband and well agrees with the experimental temperature dependence of the output power of a Mg2SiO4:Cr4+ forsterite laser.
physics.optics:We investigate the characteristics of guided wave modes in planar coupled waveguides. In particular, we calculate the dispersion relations for TM modes in which one or both of the guiding layers consists of negative index media (NIM)-where the permittivity and permeability are both negative. We find that the Poynting vector within the NIM waveguide axis can change sign and magnitude, a feature that is reflected in the dispersion curves.
physics.optics:It has recently been shown that periodic layered media can reflect strongly for all incident angles and polarizations in a given frequency range. The standard treatment gets these band gaps from an eigenvalue equation for the Bloch factor in an infinite periodic structure. We argue that such a procedure may become meaningless when dealing with structures with not very many periods. We propose an alternative approach based on a factorization of the multilayer transfer matrix in terms of three fundamental matrices of simple interpretation. We show that the trace of the transfer matrix sorts the periodic structures into three types with properties closely related to one (and only one) of the three fundamental matrices. We present the reflectance associated to each one of these types, which can be considered as universal features of the reflection in these media.
physics.optics:We study effects of finite height and surrounding material on photonic crystal slabs of one- and two-dimensional photonic crystals with a pseudo-spectral method and finite difference time domain simulation methods. The band gap is shown to be strongly modified by the boundary material. As an application we suggest reflection and guiding of light by patterning the material on top/below the slab.
physics.optics:The characteristics of an imaging system formed by a slab of a lossy left-handed material (LHM) are studied. The transfer function of the LHM imaging system is written in an appropriate product form with each term having a clear physical interpretation. A tiny loss of the LHM may suppress the transmission of evanescent waves through the LHM slab and this is explained physically. An analytical expression for the resolution of the imaging system is derived. It is shown that it is impossible to make a subwavelength imaging by using a realistic LHM imaging system unless the LHM slab is much thinner than the wavelength.
physics.optics:We observe the formation of an intense optical wavepacket fully localized in all dimensions, i.e. both longitudinally (in time) and in the transverse plane, with an extension of a few tens of fsec and microns, respectively. Our measurements show that the self-trapped wave is a X-shaped light bullet spontaneously generated from a standard laser wavepacket via the nonlinear material response (i.e., second-harmonic generation), which extend the soliton concept to a new realm, where the main hump coexists with conical tails which reflect the symmetry of linear dispersion relationship.
physics.optics:The statistical properties of nonlinear phase noise, often called the Gordon-Mollenauer effect, is studied analytically when the number of fiber spans is very large. The joint characteristic functions of the nonlinear phase noise with electric field, received intensity, and the phase of amplifier noise are all derived analytically. Based on the joint characteristic function of nonlinear phase noise with the phase of amplifier noise, the error probability of signal having nonlinear phase noise is calculated using the Fourier series expansion of the probability density function. The error probability is increased due to the dependence between nonlinear phase noise and the phase of amplifier noise. When the received intensity is used to compensate the nonlinear phase noise, the optimal linear and nonlinear minimum mean-square error compensators are derived analytically using the joint characteristic function of nonlinear phase noise and received intensity. Using the joint probability density of received amplitude and phase, the optimal maximum a posteriori probability detector is derived analytically. The nonlinear compensator always performs better than linear compensator.
physics.optics:The exact Green function for the scalar wave equation in a plane with any set of perfectly reflecting straight mirrors, which may be joined to form corners, is given as a diffraction scattering series. Instances would be slit diffraction in optics, or the Schrodinger equation inside (or outside) a general polygonal enclosure ('quantum polygon billiards'). The method is based on the seminal 1896 Riemann helicoid surface solution by Sommerfeld for optical diffraction by a single corner. It is generalised to account for multiple scatter by adapting the analysis of Stovicek for a closely related problem: a collection of magnetic flux lines (points) in a plane, the multi-flux Aharonov-Bohm effect. The short wavelength limit is shown to yield the 'geometrical theory of diffraction'. For slit diffraction the exact series is shown to coincide with that of Schwarzschild in 1902.
physics.optics:We experimentally demonstrate for the first time that a linearly polarized beam is focussed to an asymmetric spot when using a high-numerical aperture focussing system. This asymmetry was predicted by Richards and Wolf [Proc.R.Soc.London A, 253, 358 (1959)] and can only be measured when a polarization insensitive sensor is placed in the focal region. We used a specially modified photodiode in a knife edge type set up to obtain highly resolved images of the total electric energy density distribution at the focus. The results are in good agreement with the predictions of a vectorial focussing theory.
physics.optics:A simple model is used to estimate the Q factor in numerical simulations of differential phase shift keying (DPSK) with optical delay demodulation and balanced detection. It is found that an alternative definition of Q is needed for DPSK in order to have a more accurate prediction of the bit error ratio (BER).
physics.optics:With using of point-dipole model the theoretical calculations of main refractive indices and orientation of indicatrix of 18 minerals are performed. The feature of studied minerals is the statistically disordered arrangement of CO3, SO4, SO2, PO4 groups and also separate ions. The optical characters of uniaxial minerals and orientation of indicatrix of orthorhombic and monoclinic minerals, obtained by results of calculations, agree with experimental definitions.
physics.optics:The features of a compact, single pass, multi-pixel optical parametric generator are discussed. Several hundreds of independent high spatial-quality tunable ultrashort pulses were produced by pumping a bulk lithium triborate crystal with an array of tightly focussed intense beams. The array of beams was produced by shining a microlenses array with a large pump beam. Overall conversion efficiency to signal and idler up to 30% of the pump beam has been reported. Shot-to-shot energy fluctuation down to 3% was achieved for the generated radiation.
physics.optics:We use a spatially resolved cavity ring-down technique to show that the 2D eigenmode of an unstable optical cavity has a fractal pattern, i.e. it looks the same at different length scales. In agreement with theory, we find that this pattern has the maximum conceivable roughness, i.e., its fractal dimension is 3.01 plus\minus 0.04. This insight in the nature of unstable cavity eigenmodes may lead to better understanding of wave dynamics in open systems, for both light and matter waves.
physics.optics:Numerical Calculations are employed to study the modulation of light by surface acoustic waves (SAWs) in photonic band gap (PBG) structures. The on/off contrast ratio in PBG switch based on optical cavity is determined as a function of the SAW induced dielectric modulation. We show that these structures exhibit high contrast ratios even for moderate acousto-optic coupling
physics.optics:By combining the definition of the Wigner distribution function (WDF) and the matrix method of optical system modeling, we can evaluate the transformation of the former in centered systems with great complexity. The effect of stops and lens diameter are also considered and are shown to be responsible for non-linear clipping of the resulting WDF in the case of coherent illumination and non-linear modulation of the WDF when the illumination is incoherent. As an example, the study of a single lens imaging systems illustrates the applicability of the method.
physics.optics:Free-space propagation can be described as a shearing of the Wigner distribution function in the spatial coordinate; this shearing is linear in paraxial approximation but assumes a more complex shape for wide-angle propagation. Integration in the frequency domain allows the determination of near-field diffraction, leading to the well known Fresnel diffraction when small angles are considered and allowing exact prediction of wide-angle diffraction. The authors use this technique to demonstrate evanescent wave formation and diffraction elimination for very small apertures.
physics.optics:We reelaborate on the basic properties of lossless multilayers by using bilinear transformations. We study some interesting properties of the multilayer transfer function in the unit disk, showing that hyperbolic geometry turns out to be an essential tool for understanding multilayer action. We use a simple trace criterion to classify multilayers into three classes that represent rotations, translations, or parallel displacements. Moreover, we show that these three actions can be decomposed as a product of two reflections in hyperbolic lines. Therefore, we conclude that hyperbolic reflections can be considered as the basic pieces for a deeper understanding of multilayer optics.
physics.optics:In a coherent monoenergetic beam of non-interacting particles, the phase velocity and the particle transport velocity are functions of position, with the strongest variation being in the focal region. These velocities are everywhere parallel to each other, and their product is constant in space. For a coherent monochromatic electromagnetic beam, the energy transport velocity is never greater than the speed of light, and can even be zero. The phase velocities (one each for the non-zero components of the electric and magnetic fields, in general) can be different from each other and from the energy transport velocity, both in direction and in magnitude. The phase velocities at a given point are independent of time, for both particle and electromagnetic beams. The energy velocity is independent of time for the particle beam, but in general oscillates (with angular frequency 2w) in magnitude and direction about its mean value at a given point in the electromagnetic beam. However, there exist electromagnetic steady beams, within which the energy flux, energy density and energy velocity are all independent of time.
physics.optics:The polarization properties of monochromatic light beams are studied. In contrast to the idealization of an electromagnetic plane wave, finite beams which are everywhere linearly polarized in the same direction do not exist. Neither do beams which are everywhere circularly polarized in a fixed plane. It is also shown that transversely finite beams cannot be purely transverse in both their electric and magnetic vectors, and that their electromagnetic energy travels at less than c. The electric and magnetic fields in an electromagnetic beam have different polarization properties in general, but there exists a class of steady beams in which the electric and magnetic polarizations are the same (and in which energy density and energy flux are independent of time). Examples are given of exactly and approximately linearly polarized beams, and of approximately circularly polarized beams.
physics.optics:We suggest a geometrical framework to discuss periodic layered structures in the unit disk. The band gaps appear when the point representing the system approaches the unit circle. We show that the trace of the matrix describing the basic period allows for a classification in three families of orbits with quite different properties. The laws of convergence of the iterates to the unit circle can be then considered as universal features of the reflection.
physics.optics:The effect of the Kerr nonlinearity on linear non-diffractive Bessel beams is investigated analytically and numerically using the nonlinear Schr\"odinger equation. The nonlinearity is shown to primarily affect the central parts of the Bessel beam, giving rise to radial compression or decompression depending on whether the nonlinearity is focusing or defocusing, respectively. The dynamical properties of Gaussian-truncated Bessel beams are also analysed in the presence of a Kerr nonlinearity. It is found that although a condition for width balance in the root-mean-square sense exists, the beam profile becomes strongly deformed during propagation and may exhibit the phenomena of global and partial collapse.
physics.optics:We describe an optical technique based on the statistical analysis of the random intensity distribution due to the interference of the near-field scattered light with the strong transmitted beam. It is shown that, from the study of the two-dimensional power spectrum of the intensity, one derives the scattered intensity as a function of the scattering wave vector. Near-field conditions are specified and discussed. The substantial advantages over traditional scattering technique are pointed out, and is indicated that the technique could be of interest for wave lengths other than visible light.
physics.optics:The usual computation of the spontaneous emission uses a mixture of classical and quantum postulates. A purely classical computation shows that a source of electromagnetic field absorbs light in the eigenmode it is able to emit. Thus in an excitation by an other mode, the component of this mode on the eigenmode is absorbed, while the remainder is scattered. This loss of energy does not apply to the zero point field which has its regular energy in the eigenmode, so that the zero point field seems more effective than the other fields for the stimulation of light emission.
physics.optics:We perform numerical studies of the effect of sidewall imperfections on the resonant state broadening of the optical microdisk cavities for lasing applications. We demonstrate that even small edge roughness causes a drastic degradation of high-Q whispering gallery (WG) mode resonances reducing their Q-values by many orders of magnitude. At the same time, low-Q WG resonances are rather insensitive to the surface roughness. The results of numerical simulation obtained using the scattering matrix technique, are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surface of sections in the classical ray picture.
physics.optics:We consider plane waves propagating in quadratic nonlinear slab waveguides with nonlinear quasi-phase-matching gratings. We predict analytically and verify numerically the complete gain spectrum for transverse modulational instability, including hitherto undescribed higher order gain bands.
physics.optics:Optical near field has been generated by Laguarre-Gaussian doughnut beam on inner surface of "atom funnel". The resulting optical near field has been measured with the help of fiber probe and a consequent effect on cold atoms- released from MOT, has been estimated. Atoms with temperature less than 10 micro_kelvin can be reflected by the optical near field.
physics.optics:We report on an attempt to generate highly stable continuous terahertz (THz) wave by using optical frequency comb (OFC). About 10-nm wide OFC has been generated through a deep phase modulation of a 852 nm laser line in lithium niobate crystal cavity. The multiple optical modes (side bands) of the OFC, which are equally separated from each other by the modulation frequency (=6 GHz) are taken as the frequency reference. When another semiconductor laser is frequency locked, the stability of the difference frequency between the master laser and the second laser is improved on the same order of the RF modulator. An ultra-narrow line and tunable THz radiation source can be achieved by photomixing of this stable difference-frequency optical beat in a photoconductive antenna.
physics.optics:Doughnut shaped light beam has been generated from Gaussian mode ($TEM_{00}$) cw-Ti sapphire laser. After splitting the pump beam into two equal intensity components and introducing unequal convergence and phase delay while they are recombined it results in doughnut mode. Such a beam is tunable and have long propagation length. The evanescent field generated by 360 mW (at 780 nm wavelength) of such a beam creates optical field of 600 nm decay length with a 5.75 neV repulsive dipole potential. Thus cold Rb atoms (at 10{$\mu$}K or less temperature) released from MOT can be reflected by the surface so that the atoms are collected ultimately at the bottom of the prism. By focussing such doughnut beam with 8 cm focal length converging lens, the dark radius reduces to 22{$\mu$}. We also observe such beam to contain azimuthal phase as well as radial phase distribution.
physics.optics:Ferroelectric ordering, the electroclinic effect and chiral smectic C (SmC*) - smectic A (SmA*) phase transitions in thin planar ferroelectric liquid crystal (FLC) cells are studied by means of linear electrooptic and second harmonic generation techniques. The ferroelectric switching is detected in biased FLC cells by measuring azimuthal dependences of linear and nonlinear responses. The applied DC-electric field rotates the FLC symmetry axis with initial and final orientations in the cell plane. Comparative studies of the switching behavior in reflection and transmission allows to distinguish the contributions from the bulk and the sub-surface layers of the cell. The analysis of temperature dependence shows the existence of a strong surface coupling. The temperature dependent nonlinear polarization shows a critical behavior corresponding to the superfluid model.
physics.optics:We have generated high power doughnut beam suitable for atom funnel experiment with the conversion efficiency of about 50 %.
physics.optics:We consider large-mode area photonic crystal fibers for visible applications where micro-deformation induced attenuation becomes a potential problem when the effective area A_eff is sufficiently large compared to lambda^2. We argue how a slight increase in fiber diameter D can be used in screening the high-frequency components of the micro-deformation spectrum mechanically and we confirm this experimentally for both 15 and 20 micron core fibers. For typical bending-radii (R~16 cm) the operating band-width increases by ~3-400 nm to the low-wavelength side.
physics.optics:We investigate general properties of spatial 1-dimensional bright photorefractive solitons and suggest various analytical approximations for the soliton profile and the half width, both depending on an intensity parameter r.
physics.optics:A model for a non-Kerr cylindrical nematic fiber is presented. We use the multiple scales method to show the possibility of constructing different kinds of wavepackets of transverse magnetic (TM) modes propagating through the fiber. This procedure allows us to generate different hierarchies of nonlinear partial differential equations (PDEs) which describe the propagation of optical pulses along the fiber. We go beyond the usual weakly nonlinear limit of a Kerr medium and derive an extended Nonlinear Schrodinger equation (eNLS) with a third order derivative nonlinearity, governing the dynamics for the amplitude of the wavepacket. In this derivation the dispersion, self-focussing and diffraction in the nematic are taken into account. Although the resulting nonlinear $PDE$ may be reduced to the modified Korteweg de Vries equation (mKdV), it also has additional complex solutions which include two-parameter families of bright and dark complex solitons. We show analytically that under certain conditions, the bright solitons are actually double embedded solitons. We explain why these solitons do not radiate at all, even though their wavenumbers are contained in the linear spectrum of the system. Finally, we close the paper by making comments on the advantages as well as the limitations of our approach, and on further generalizations of the model and method presented.
physics.optics:The modal cut-off is investigated experimentally in a series of high quality non-linear photonic crystal fibers. We demonstrate a suitable measurement technique to determine the cut-off wavelength and verify it by inspecting the near field of the modes that may be excited below and above the cut-off. We observe a double peak structure in the cut-off spectra, which is attributed to a splitting of the higher order modes. The cut-off is measured for seven different fiber geometries with different pitches and relative hole size, and a very good agreement with recent theoretical work is found.
physics.optics:We address the long-standing unresolved problem concerning the V-parameter in a photonic crystal fiber (PCF). Formulate the parameter appropriate for a core-defect in a periodic structure we argue that the multi-mode cut-off occurs at a wavelength lambda* which satisfies V_PCF(lambda*)=pi. Comparing to numerics and recent cut-off calculations we confirm this result.
physics.optics:We propose in this article an unambiguous definition of the local density of electromagnetic states (LDOS) in a vacuum near an interface in an equilibrium situation at temperature $T$. We show that the LDOS depends only on the electric field Green function of the system but does not reduce in general to the trace of its imaginary part as often used in the literature. We illustrate this result by a study of the LDOS variations with the distance to an interface and point out deviations from the standard definition. We show nevertheless that this definition remains correct at frequencies close to the material resonances such as surface polaritons. We also study the feasability of detecting such a LDOS with apetureless SNOM techniques. We first show that a thermal near-field emission spectrum above a sample should be detectable and that this measurement could give access to the electromagnetic LDOS. It is further shown that the apertureless SNOM is the optical analog of the scanning tunneling microscope which is known to detect the electronic LDOS. We also discuss some recent SNOM experiments aimed at detecting the electromagnetic LDOS.
physics.optics:The nonlinearity of a transmission fiber may be compensated by a specialty fiber and an optical phase conjugator. Such combination may be used to pre-distort signals before each fiber span so to linearize an entire transmission line.
physics.optics:Two fiber lines may compensate each other for nonlinearity with the help of optical phase conjugation. The pair of fiber lines and the optical signals in them may be either mirror-symmetric or translationally symmetric about the conjugator.
physics.optics:We have developed a scattering-matrix approach for numerical calculation of resonant states and Q-values of a nonideal optical disk cavity of an arbitrary shape and of an arbitrary varying refraction index. The developed method has been applied to study the effect of surface roughness and inhomogeneity of the refraction index on Q-values of microdisk cavities for lasing applications. We demonstrate that even small surface roughness can lead to a drastic degradation of high-Q cavity modes by many orders of magnitude. The results of numerical simulation are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surfaces of section and Husimi distributions.
physics.optics:We analyse theoretically for the first time to our knowledge the perfect phase matching of guided TE and TM modes with a multilayer waveguide composed of linear isotropic dielectric materials. Alongside strict investigation into dispersion relations for multilayer systems, we give an explicit qualitative explanation for the phenomenon of mode matching on the basis of the standard one-dimensional homogenization technique, and discuss the minimum number of layers and the refractive index profile for the proposed device scheme. Direct applications of the scheme include polarization-insensitive, intermodal dispersion-free planar propagation, efficient fibre-to-planar waveguide coupling and, potentially, mode filtering. As a self-sufficient result, we present compact analytical expressions for the mode dispersion in a finite, N-period, three-layer dielectric superlattice.
physics.optics:The characteristic function of soliton phase jitter is found analytically when the soliton is perturbed by amplifier noise. In additional to that from amplitude jitter, the nonlinear phase noise due to frequency and timing jitter is also analyzed. Because the nonlinear phase noise is not Gaussian distributed, the overall phase jitter is also non-Gaussian. For a fixed mean nonlinear phase shift, the contribution of nonlinear phase noise from frequency and timing jitter decreases with distance and signal-to-noise ratio.
physics.optics:The field energy distributions and effective mode areas of silica-based photonic bandgap fibers with a honeycomb airhole structure in the cladding and an extra airhole defining the core are investigated. We present a generalization of the common effective area definition, suitable for the problem at hand, and compare the results for the photonic bandgap fibers with those of index-guiding microstructured fibers. While the majority of the field energy in the honeycomb photonic bandgap fibers is found to reside in the silica, a substantial fraction (up to ~30%) can be located in the airholes. This property may show such fibers particularly interesting for sensor applications, especially those based on nonlinear effects or interaction with other structures (e.g. Bragg gratings) in the glass.
physics.optics:The waveguiding properties of two silica-based airguiding photonic bandgap fiber designs are investigated with special emphasis on material effects. The nonlinear coefficients are found to be 1-2 orders of magnitude smaller than those obtained in index-guiding microstructured fibers with large mode areas. The material dispersion of silica makes a significant contribution to the total chromatic dispersion although less than 10% of the field energy is located in the silica regions of the fibers. These findings suggest that dispersion engineering through the choice of base material may be a possibility in this type of fibers.
physics.optics:Lithium thioindate (LiInS$_{2}$) is a new nonlinear chalcogenide biaxial material transparent from 0.4 to 12 $\mu$m, that has been successfully grown in large sizes and good optical quality. We report on new physical properties that are relevant for laser and nonlinear optics applications. With respect to AgGaS(e)$_2$ ternary chalcopyrite materials, LiInS$_{2}$ displays a nearly-isotropic thermal expansion behavior, a 5-times larger thermal conductivity associated with high optical damage thresholds, and an extremely low intensity-dependent absorption allowing direct high-power downconversion from the near-IR to the deep mid-IR. Continuous-wave difference-frequency generation (5-11$ \mu$m) of Ti:sapphire laser sources is reported for the first time.
physics.optics:We find that the function that describes the surface of spherical aberration free lenses can be used for both positive and negative refractive index media. With the inclusion of negative index, this function assumes the form of all the conic sections and expands the theory of aplanatic optical surfaces. There are two different symmetry centers with respect to the index that create an asymmetric relationship between positive and negative index lens profiles. In the thin lens limit the familiar formulas for image position and magnification hold for any index.
physics.optics:We report the surprising observation of directional tunneling escape from nearly spherical fused-silica optical resonators, in which most of the phase space is filled with nonchaotic regular trajectories. Experimental and theoretical studies of the dependence of the far-field emission pattern on both the degree of deformation and the excitation condition show that nonperturbative phase-space structures in the internal ray dynamics profoundly affect tunneling leakage of the whispering-gallery modes.
physics.optics:Intermodal interactions displayed through the phenomena of mode coupling and conversion in optical systems are treated by means of the Lindstedt-Poincare perturbation method of strained parameters more widely known in classical quantum mechanics and quantum chemistry as the stationary perturbation technique. The focus here is on the mode conversion at the points of virtual phase matching (otherwise called anticrossings or avoided crossings) associated with the maximum conversion efficiency. The method is shown to provide a convenient tool to deal with intermodal interactions at anticrossings -- interactions induced by any kind of perturbation in dielectric index profile of the waveguide, embracing optical inhomogeneity, magnetization of arbitrary orientation, and nonlinearity. Closed-form analytic expressions are derived for the minimum value of mode mismatch and for the length of complete mode conversion (the coupling length, or the beat length) in generic waveguiding systems exhibiting anticrossings. Demonstrating the effectiveness of the method, these general expressions are further applied to the case of TE -- TM mode conversion in (i) a multilayer gyrotropic waveguide under piecewise-constant, arbitrarily oriented magnetization, and (ii) an optically-inhomogeneous planar dielectric waveguide -- an example which the standard coupled-mode theory fails to describe.
physics.optics:We exploit a slightly noncollinear second-harmonic cross-correlation scheme to map the 3D space-time intensity distribution of an unknown complex-shaped ultrashort optical pulse. We show the capability of the technique to reconstruct both the amplitude and the phase of the field through the coherence of the nonlinear interaction down to a resolution of 10 $\mu$m in space and 200 fs in time. This implies that the concept of second-harmonic holography can be employed down to the sub-ps time scale, and used to discuss the features of the technique in terms of the reconstructed fields.
physics.optics:Nonlinear mode coupling in a coaxial waveguide filled with Faraday material has been considered. The picture of mode interaction is shown to resemble Coulomb interaction of charges: higher modes with nonzero angular momentum interact like effective charges via exchange of zero angular momentum quanta of the fundamental mode. Thus, at large distances this interaction becomes the dominant mechanism of mode coupling. The developed model may be used in designing coaxial photonic crystal fibers with strong tailored mode interaction.
physics.optics:We develop a general theory of spatial solitons in a liquid crystalline medium exhibiting a nonlinearity with an arbitrary degree of effective nonlocality. The model accounts the observability of "accessible solitons" and establishes an important link with parametric solitons.
physics.optics:Theoretical analysis is presented on quantum state evolution of polarization light waves at frequencies $\omega_{o}$ and $\omega_{e}$ in a periodically poled nonlinear crystal (PPNC). It is shown that the variances of all the four Stokes parameters can be squeezed.
physics.optics:Counter-propagating light fields have the ability to create self-organized one-dimensional optically bound arrays of microscopic particles, where the light fields adapt to the particle locations and vice versa. We develop a theoretical model to describe this situation and show good agreement with recent experimental data (Phys. Rev. Lett. 89, 128301 (2002)) for two and three particles, if the scattering force is assumed to dominate the axial trapping of the particles. The extension of these ideas to two and three dimensional optically bound states is also discussed.
physics.optics:The inversion of a diffraction pattern offers aberration-free diffraction-limited 3D images without the resolution and depth-of-field limitations of lens-based tomographic systems, the only limitation being radiation damage. We review our experimental results, discuss the fundamental limits of this technique and future plans.
physics.optics:The stability of two-dimensional bright vortex solitons in a media with focusing cubic and defocusing quintic nonlinearities is investigated analytically and numerically. It is proved that above some critical beam powers not only one- and two-charged but also multiple-charged stable vortex solitons do exist. A vortex soliton occurs robust with respect to symmetry-breaking modulational instability in the self-defocusing regime provided that its radial profile becomes flattened, so that a self-trapped wave beam gets a pronounced surface. It is demonstrated that the dynamics of a slightly perturbed stable vortex soliton resembles an oscillation of a liquid stream having a surface tension. Using the idea of sustaining effective surface tension for spatial vortex soliton in a media with competing nonlinearities the explanation of a suppression of the modulational instability is proposed.
physics.optics:Quasi error-free 10 Gbit/s data transmission is demonstrated over a novel type of 50 micron core diameter photonic crystal fiber with as much as 100 m length. Combined with 850$ nm VCSEL sources, this fiber is an attractive alternative to graded-index multi-mode fibers for datacom applications. A comparison to numerical simulations suggests that the high bit-rate may be partly explained by inter-modal diffusion.
physics.optics:The Casimir force between metallic plates made of realistic materials is evaluated for distances in the nanometer range. A spectrum over real frequencies is introduced and shows narrow peaks due to surface resonances (plasmon polaritons or phonon polaritons) that are coupled across the vacuum gap. We demonstrate that the Casimir force originates from the attraction (repulsion) due to the corresponding symmetric (antisymmetric) eigenmodes, respectively. This picture is used to derive a simple analytical estimate of the Casimir force at short distances. We recover the result known for Drude metals without absorption and compute the correction for weakly absorbing materials.
physics.optics:In recent years there has been an explosive development of interest in the measurement of forces at the microscopic level, such as within living cells, as well as the properties of fluids and suspensions on this scale, using optically trapped particles as probes. The next step would be to measure torques and associated rotational motion. This would allow measurement on very small scales since no translational motion is needed. It could also provide an absolute measurement of the forces holding a stationary non-rotating particle in place. The laser-induced torque acting on an optically trapped microscopic birefringent particle can be used for these measurements. Here we present a new method for simple, robust, accurate, simultaneous measurement of the rotation speed of a laser trapped birefringent particle, and the optical torque acting on it, by measuring the change in angular momentum of the light from passing through the particle. This method does not depend on the size or shape of the particle or the laser beam geometry, nor does it depend on the properties of the surrounding medium. This could allow accurate measurement of viscosity on a microscopic scale.
physics.optics:Optical tweezers are widely used for the manipulation of cells and their internal structures. However, the degree of manipulation possible is limited by poor control over the orientation of trapped cells. We show that it is possible to controllably align or rotate disc shaped cells - chloroplasts of Spinacia oleracea - in a plane polarised Gaussian beam trap, using optical torques resulting predominantly from circular polarisation induced in the transmitted beam by the non-spherical shape of the cells.
physics.optics:Optical trapping is a widely used technique, with many important applications in biology and metrology. Complete modelling of trapping requires calculation of optical forces, primarily a scattering problem, and non-optical forces. The T-matrix method is used to calculate forces acting on spheroidal and cylindrical particles.
physics.optics:Optical trapping, where microscopic particles are trapped and manipulated by light is a powerful and widespread technique, with the single-beam gradient trap (also known as optical tweezers) in use for a large number of biological and other applications.   The forces and torques acting on a trapped particle result from the transfer of momentum and angular momentum from the trapping beam to the particle.   Despite the apparent simplicity of a laser trap, with a single particle in a single beam, exact calculation of the optical forces and torques acting on particles is difficult. Calculations can be performed using approximate methods, but are only applicable within their ranges of validity, such as for particles much larger than, or much smaller than, the trapping wavelength, and for spherical isotropic particles.   This leaves unfortunate gaps, since wavelength-scale particles are of great practical interest because they are readily and strongly trapped and are used to probe interesting microscopic and macroscopic phenomena, and non-spherical or anisotropic particles, biological, crystalline, or other, due to their frequent occurance in nature, and the possibility of rotating such objects or controlling or sensing their orientation.   The systematic application of electromagnetic scattering theory can provide a general theory of laser trapping, and render results missing from existing theory. We present here calculations of force and torque on a trapped particle obtained from this theory and discuss the possible applications, including the optical measurement of the force and torque.
physics.optics:Multipole expansion of an incident radiation field - that is, representation of the fields as sums of vector spherical wavefunctions - is essential for theoretical light scattering methods such as the T-matrix method and generalised Lorenz-Mie theory (GLMT). In general, it is theoretically straightforward to find a vector spherical wavefunction representation of an arbitrary radiation field. For example, a simple formula results in the useful case of an incident plane wave. Laser beams present some difficulties. These problems are not a result of any deficiency in the basic process of spherical wavefunction expansion, but are due to the fact that laser beams, in their standard representations, are not radiation fields, but only approximations of radiation fields. This results from the standard laser beam representations being solutions to the paraxial scalar wave equation. We present an efficient method for determining the multipole representation of an arbitrary focussed beam.
physics.optics:The T-matrix method is widely used for the calculation of scattering by particles of sizes on the order of the illuminating wavelength. Although the extended boundary condition method (EBCM) is the most commonly used technique for calculating the T-matrix, a variety of methods can be used.   We consider some general principles of calculating T-matrices, and apply the point-matching method to calculate the T-matrix for particles devoid of symmetry. This method avoids the time-consuming surface integrals required by the EBCM.
physics.optics:Light-induced rotation of absorbing microscopic particles by transfer of angular momentum from light to the material raises the possibility of optically driven micromachines. The phenomenon has been observed using elliptically polarized laser beams or beams with helical phase structure. But it is difficult to develop high power in such experiments because of overheating and unwanted axial forces, limiting the achievable rotation rates to a few hertz. This problem can in principle be overcome by using transparent particles, transferring angular momentum by a mechanism first observed by Beth in 1936, when he reported a tiny torque developed in a quartz waveplate due to the change in polarization of transmitted light. Here we show that an optical torque can be induced on microscopic birefringent particles of calcite held by optical tweezers. Depending on the polarization of the incident beam, the particles either become aligned with the plane of polarization (and thus can be rotated through specified angles) or spin with constant rotation frequency. Because these microscopic particles are transparent, they can be held in three-dimensional optical traps at very high power without heating. We have observed rotation rates in excess of 350 Hz.
physics.optics:This report contains a tutorial introduction to the method of importance sampling. The use of this method is illustrated for simulations of the noise-induced energy jitter of return-to-zero pulses in optical communication systems.
physics.optics:The 3-dimensional coherence matrix is interpreted by emphasising its invariance with respect to spatial rotations. Under these transformations, it naturally decomposes into a real symmetric positive definite matrix, interpreted as the moment of inertia of the ensemble (and the corresponding ellipsoid), and a real axial vector, corresponding to the mean angular momentum of the ensemble. This vector and tensor are related by several inequalities, and the interpretation is compared to those in which unitary invariants of the coherence matrix are studied.
physics.optics:We propose novel multi-phase-matched process that starts with generation of a pair of symmetric second-harmonic waves. Each of them interacts again with the fundamental wave to produce two constructively interfering third harmonic waves collinear to the fundamental input wave.
physics.optics:We numerically calculate the equivalent mode-field radius of the fundamental mode in a photonic crystal fiber (PCF) and show that this is a function of the V-parameter only and not the relative hole size. This dependency is similar to what is found for graded-index standard fibers and we furthermore show that the relation for the PCF can be excellently approximated with the same general mathematical expression. This is to our knowledge the first semi-analytical description of the mode-field radius of a PCF.
physics.optics:The output spectrum of both gas and semiconductor lasers usually contains more than one frequency. Multimode operation in gas versus semiconductor lasers arises from different physics. In gas lasers, slow equilibration of the electron populations at different energies makes each frequency an independent single-mode laser. The slow electron diffusion in semiconductor lasers, combined with the spatially varying optical intensity patterns of the modes, makes each region of space an independent single-mode laser. We develop a rate equation model for the photon number in each mode which captures all these effects. Plotting the photon number versus pumping rate for the competing modes, in both subthreshold and above threshold operation, illustrates the changes in the laser output spectrum due to either slow equilibration or slow diffusion of electrons.
physics.optics:A cascaded iterative Fourier transform (CIFT) algorithm is presented for optical security applications. Two phase-masks are designed and located in the input and the Fourier domains of a 4-f correlator respectively, in order to implement the optical encryption or authenticity verification. Compared with previous methods, the proposed algorithm employs an improved searching strategy: modifying the phase-distributions of both masks synchronously as well as enlarging the searching space. Computer simulations show that the algorithm results in much faster convergence and better image quality for the recovered image. Each of these masks is assigned to different person. Therefore, the decrypted image can be obtained only when all these masks are under authorization. This key-assignment strategy may reduce the risk of being intruded.
physics.optics:Generic wave dislocations (phase singularities, optical vortices) in three dimensions have anisotropic local structure, which is analysed, with emphasis on the twist of surfaces of equal phase along the singular line, and the rotation of the local anisotropy ellipse (twirl). Various measures of twist and twirl are compared in specific examples, and a theorem is found relating the (quantised) topological twist and twirl for a closed dislocation loop with the anisotropy C line index threading the loop.
physics.optics:Diffraction is a fundamental property of light propagation. Owing to this phenomenon,light diffracts out in all directions when it passes through a subwavelength slit.This imposes a fundamental limit on the transverse size of a light beam at a given distance from the aperture. We show that a subwavelength-sized beam propagating without diffractive broadening can be produced in free space by the constructive interference of multiple beams of a Fresnel source of the respective high-refraction-index waveguide. Moreover, it is shown that such a source can be constructed not only for continuous waves, but also for ultra-short (near single-cycle) pulses. The results theoretically demonstrate the feasibility of completely diffraction-free subwavelength-beam optics, for both continuous waves and ultra-short pulses. The approach extends operation of the near-field subwavelength-beam optics, such as near-field scanning optical microscopy and spectroscopy,to the "not-too-distant" field regime (0.5 to about 10 wavelengths).
physics.optics:In recent years the topic of localized wave solutions of the homogeneous scalar wave equation, i.e., the wave fields that propagate without any appreciable spread or drop in intensity, has been discussed in many aspects in numerous publications. In this review the main results of this rather disperse theoretical material are presented in a single mathematical representation - the Fourier decomposition by means of angular spectrum of plane waves. This unified description is shown to lead to a transparent physical understanding of the phenomenon as such and yield the means of optical generation of such wave fields.
cs.LG:This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.
cs.LG:Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.
cs.LG:The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.
cs.LG:We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.
cs.LG:Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a {\it natural} way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus.   Keywords: Kolmogorov complexity; Algorithmic information theory.
cs.LG:In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.
cs.LG:An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.
cs.LG:When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.   Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).   As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.
cs.LG:In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a {\it stigmergic} approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(\lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.
cs.LG:Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.
cs.LG:The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.
cs.LG:In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.
cs.LG:To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur.   We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree \Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures.   The complexity for searching the optimal Bayesian networks of in-degree \Delta increases only polynomially in the number of random varibales for constant \Delta and the optimal joint measure associated with a given graph can be found by convex optimization.
cs.LG:We make progress on two important problems regarding attribute efficient learnability.   First, we give an algorithm for learning decision lists of length $k$ over $n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time $n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight.   Second, we give an algorithm for learning an unknown parity function on $k$ out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$. For $k=o(\log n)$ this yields a polynomial time algorithm with sample complexity $o(n)$. This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.
cs.LG:Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.
cs.LG:It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipf's distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable "progress in studies".
cs.LG:For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.
cs.LG:We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) := 1/N sum_i (f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when y_i is a noisy version of f*(x_i) for some function f* in H, the output of the algorithm converges to f* as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N -> infinity. For the case where N -> infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we describe the limiting 'non-noisy', 'non-regularized' function f*, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.
cs.LG:We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2.   To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated.   Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.
cs.LG:We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space $[0,1]$ and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.
cs.LG:We consider a general class of forecasting protocols, called "linear protocols", and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecaster's predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptic's capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.
cs.LG:This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.
cs.LG:We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.
cs.LG:We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed "bag of components" representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects' structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.
cs.LG:This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for "reactive" experts problems, which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.
cs.LG:A main problem of "Follow the Perturbed Leader" strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t^(1/2)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t^2 log t) samples in each step.
cs.LG:Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.
cs.LG:We consider the problem of learning unions of rectangles over the domain $[b]^n$, in the uniform distribution membership query learning setting, where both b and n are "large". We obtain poly$(n, \log b)$-time algorithms for the following classes:   - poly$(n \log b)$-way Majority of $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   - Union of poly$(\log(n \log b))$ many $O(\frac{\log^2 (n \log b)} {(\log \log(n \log b) \log \log \log (n \log b))^2})$-dimensional rectangles.   - poly$(n \log b)$-way Majority of poly$(n \log b)$-Or of disjoint $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   Our main algorithmic tool is an extension of Jackson's boosting- and Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$, building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to obtain the results stated above are techniques from exact learning [Beimel, Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$ circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean functions as thresholds of parities [Klivans, Servedio 2001].
cs.LG:We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithm's performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is "universal" (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.
cs.LG:The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t - x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, >...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x*. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may impose practical limitations on an agent's performance, as O(eps^-4) queries are made before the queries converge to x* with eps accuracy.
cs.LG:A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).
cs.LG:We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a "regret term" of O(N^(-1/2)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.
cs.LG:Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.
cs.LG:It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.
cs.LG:Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.
cs.LG:Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m->infinity and V for k->infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.
cs.LG:Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.
cs.LG:Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.
cs.LG:A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their $L_1$ distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the $L_1$ metric into a distribution that is good under the KL-divergence.
cs.LG:In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.
cs.LG:In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.
cs.LG:We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as "learning from possibilities" that formalizes these ideas, then we present a more specific learning setting, referred to as "assumption-based learning" that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.
cs.LG:We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learner/classifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. "Nested tree," "embedded tree" and "recursive tree" are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.
cs.LG:We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a "leading prediction strategy", which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.
cs.LG:Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.
cs.LG:We present basic notions of Gold's "learnability in the limit" paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.
cs.LG:An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.
cs.LG:Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictor's goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.
cs.LG:We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon.   In this scenario, we give a poly(n/epsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the ``clustering'' model, where such assumptions are unavoidable.   Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.
cs.LG:Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for "hedging" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.
cs.LG:This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.
cs.LG:Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2^D \sqrt{n}), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.
cs.LG:We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean $n$-sphere $\s^n$ is $\Theta(n)$. We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the ``intrinsic dimensionality'' of Ch\'avez et al.)
cs.LG:This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.
cs.LG:We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.
cs.LG:Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such "VC dimensions" exist for models taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations appropriate for the missing case, the one of models with values in R^Q. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.
cs.LG:We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.
cs.LG:Supervised learning deals with the inference of a distribution over an output or label space $\CY$ conditioned on points in an observation space $\CX$, given a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of applications of interest, acquisition of large amounts of observations is easy, while the process of generating labels is time-consuming or costly. One way to deal with this problem is {\em active} learning, where points to be labelled are selected with the aim of creating a model with better performance than that of an model trained on an equal number of randomly sampled points. In this paper, we instead propose to deal with the labelling cost directly: The learning goal is defined as the minimisation of a cost which is a function of the expected model performance and the total cost of the labels used. This allows the development of general strategies and specific algorithms for (a) optimal stopping, where the expected cost dictates whether label acquisition should continue (b) empirical evaluation, where the cost is used as a performance metric for a given combination of inference, stopping and sampling methods. Though the main focus of the paper is optimal stopping, we also aim to provide the background for further developments and discussion in the related field of active learning.
cs.LG:The method of defensive forecasting is applied to the problem of prediction with expert advice for binary outcomes. It turns out that defensive forecasting is not only competitive with the Aggregating Algorithm but also handles the case of "second-guessing" experts, whose advice depends on the learner's prediction; this paper assumes that the dependence on the learner's prediction is continuous.
cs.LG:Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms. There are two known varieties of defensive forecasting: "continuous", in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and "randomized", in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized. This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous.
cs.LG:Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is $O(n^3)$, where $n$ is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity $O(n^2)$. We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.
cs.LG:Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical
cs.LG:For a classification problem described by the joint density $P(\omega,x)$, models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have been shown to be an optimal similarity measure for nearest neighbor classification. This paper analyzes demonstrates several additional properties of that conditional distribution. The paper first shows that we can reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$ given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class labels, and gives an asymptotically Bayes-optimal classification procedure. It also shows, given such an optimal similarity measure, how to construct a classifier that outperforms the nearest neighbor classifier and achieves Bayes-optimal classification rates. The paper then analyzes Bayesian similarity in a framework where a classifier faces a number of related classification tasks (multitask learning) and illustrates that reconstruction of the class posterior distribution is not possible in general. Finally, the paper identifies a distinct class of classification problems using $P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to solve those problems is the Bayes optimal solution.
cs.LG:Learning machines which have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither the Bayes a posteriori distribution converges to the normal distribution nor the maximum likelihood estimator satisfies asymptotic normality. This is the main reason why it has been difficult to predict their generalization performances from trained states. In this paper, we study four errors, (1) Bayes generalization error, (2) Bayes training error, (3) Gibbs generalization error, and (4) Gibbs training error, and prove that there are mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that Bayes and Gibbs generalization errors are estimated by Bayes and Gibbs training errors, and propose widely applicable information criteria which can be applied to both regular and singular statistical models.
cs.LG:We consider the problem of choosing a density estimate from a set of distributions F, minimizing the L1-distance to an unknown distribution (Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the problem: Scheffe tournament winner and minimum distance estimate. The Scheffe tournament estimate requires fewer computations than the minimum distance estimate, but has strictly weaker guarantees than the latter.   We focus on the computational aspect of density estimation. We present two algorithms, both with the same guarantee as the minimum distance estimate. The first one, a modification of the minimum distance estimate, uses the same number (quadratic in |F|) of computations as the Scheffe tournament. The second one, called ``efficient minimum loss-weight estimate,'' uses only a linear number of computations, assuming that F is preprocessed.   We also give examples showing that the guarantees of the algorithms cannot be improved and explore randomized algorithms for density estimation.
cs.LG:Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.
cs.LG:The cross-entropy method is a simple but efficient method for global optimization. In this paper we provide two online variants of the basic CEM, together with a proof of convergence.
cs.LG:We prove that the optimal assignment kernel, proposed recently as an attempt to embed labeled graphs and more generally tuples of basic data to a Hilbert space, is in fact not always positive definite.
cs.LG:Given R groups of numerical variables X1, ... XR, we assume that each group is the result of one underlying latent variable, and that all latent variables are bound together through a linear equation system. Moreover, we assume that some explanatory latent variables may interact pairwise in one or more equations. We basically consider PLS Path Modelling's algorithm to estimate both latent variables and the model's coefficients. New "external" estimation schemes are proposed that draw latent variables towards strong group structures in a more flexible way. New "internal" estimation schemes are proposed to enable PLSPM to make good use of variable group complementarity and to deal with interactions. Application examples are given.
cs.LG:We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators from "users" to the "objects" they rate. Recent low-rank type matrix completion approaches to CF are shown to be special cases. However, unlike existing regularization based CF methods, our approach can be used to also incorporate information such as attributes of the users or the objects -- a limitation of existing regularization based CF methods. We then provide novel representer theorems that we use to develop new estimation methods. We provide learning algorithms based on low-rank decompositions, and test them on a standard CF dataset. The experiments indicate the advantages of generalizing the existing regularization based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.
cs.LG:We study the problem of learning k-juntas given access to examples drawn from a number of different product distributions. Thus we wish to learn a function f : {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best known algorithms for the general problem of learning a k-junta require running time of n^k * poly(n,2^k), we show that given access to k different product distributions with biases separated by \gamma>0, the functions may be learned in time poly(n,2^k,\gamma^{-k}). More generally, given access to t <= k different product distributions, the functions may be learned in time n^{k/t} * poly(n,2^k,\gamma^{-k}). Our techniques involve novel results in Fourier analysis relating Fourier expansions with respect to different biases and a generalization of Russo's formula.
cs.LG:The use of computational intelligence techniques for classification has been used in numerous applications. This paper compares the use of a Multi Layer Perceptron Neural Network and a new Relational Network on classifying the HIV status of women at ante-natal clinics. The paper discusses the architecture of the relational network and its merits compared to a neural network and most other computational intelligence classifiers. Results gathered from the study indicate comparable classification accuracies as well as revealed relationships between data features in the classification data. Much higher classification accuracies are recommended for future research in the area of HIV classification as well as missing data estimation.
cs.LG:This paper aims to showcase the measure of structural diversity of an ensemble of 9 classifiers and then map a relationship between this structural diversity and accuracy. The structural diversity was induced by having different architectures or structures of the classifiers The Genetical Algorithms (GA) were used to derive the relationship between diversity and the classification accuracy by evolving the classifiers and then picking 9 classifiers out on an ensemble of 60 classifiers. It was found that as the ensemble became diverse the accuracy improved. However at a certain diversity measure the accuracy began to drop. The Kohavi-Wolpert variance method is used to measure the diversity of the ensemble. A method of voting is used to aggregate the results from each classifier. The lowest error was observed at a diversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024 as maximum diversity measured. The parameters that were varied were: the number of hidden nodes, learning rate and the activation function.
cs.LG:Using a support vector machine requires to set two types of hyperparameters: the soft margin parameter C and the parameters of the kernel. To perform this model selection task, the method of choice is cross-validation. Its leave-one-out variant is known to produce an estimator of the generalization error which is almost unbiased. Its major drawback rests in its time requirement. To overcome this difficulty, several upper bounds on the leave-one-out error of the pattern recognition SVM have been derived. Among those bounds, the most popular one is probably the radius-margin bound. It applies to the hard margin pattern recognition SVM, and by extension to the 2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2, as a direct extension of the 2-norm SVM to the multi-class case. For this machine, a generalized radius-margin bound is then established.
cs.LG:We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel \citep{GaeFlaWro03,BorOngSchVisetal05}, marginalized graph kernel \citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04}, and geometric kernel on graphs \citep{Gaertner02}. Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels \citep{KonLaf02}, regularization on graphs \citep{SmoKon03}, and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels \citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized to graphs reduce to the random walk graph kernel.
cs.LG:We study probability distributions over free algebras of trees. Probability distributions can be seen as particular (formal power) tree series [Berstel et al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. We argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. First, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. Note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. Second, the algebraic representation can be easily extended to deal with unranked trees (like XML trees where a symbol may have an unbounded number of children). Both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that Hidden Markov Models are equivalent to nondeterministic string automata); nowadays applications for Web Information Extraction, Web Services and document processing consider unranked trees.
cs.LG:We present a novel graphical framework for modeling non-negative sequential data with hierarchical structure. Our model corresponds to a network of coupled non-negative matrix factorization (NMF) modules, which we refer to as a positive factor network (PFN). The data model is linear, subject to non-negativity constraints, so that observation data consisting of an additive combination of individually representable observations is also representable by the network. This is a desirable property for modeling problems in computational auditory scene analysis, since distinct sound sources in the environment are often well-modeled as combining additively in the corresponding magnitude spectrogram. We propose inference and learning algorithms that leverage existing NMF algorithms and that are straightforward to implement. We present a target tracking example and provide results for synthetic observation data which serve to illustrate the interesting properties of PFNs and motivate their potential usefulness in applications such as music transcription, source separation, and speech recognition. We show how a target process characterized by a hierarchical state transition model can be represented as a PFN. Our results illustrate that a PFN which is defined in terms of a single target observation can then be used to effectively track the states of multiple simultaneous targets. Our results show that the quality of the inferred target states degrades gradually as the observation noise is increased. We also present results for an example in which meaningful hierarchical features are extracted from a spectrogram. Such a hierarchical representation could be useful for music transcription and source separation applications. We also propose a network for language modeling.
cs.LG:We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the inner product then the learned vector is a linear combination of the input data. This result, known as the {\em representer theorem}, is at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, thereby completing the characterization of kernel methods based on regularization. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition for these class of matrix regularizers and highlight them with some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing function.
cs.LG:In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.
cs.LG:We consider the task of learning a classifier from the feature space $\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the features can be partitioned into class-conditionally independent feature sets $\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classifier from $\mathcal{X}_2$ to $\mathcal{X}_1$ and 2) learning the class-conditional distribution of the feature set $\mathcal{X}_1$. This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples. We present experimental evaluation of the idea in two real world applications.
cs.LG:Maximum Variance Unfolding (MVU) and its variants have been very successful in embedding data-manifolds in lower dimensional spaces, often revealing the true intrinsic dimension. In this paper we show how to also incorporate supervised class information into an MVU-like method without breaking its convexity. We call this method the Isometric Separation Map and we show that the resulting kernel matrix can be used as a binary/multiclass Support Vector Machine-like method in a semi-supervised (transductive) framework. We also show that the method always finds a kernel matrix that linearly separates the training data exactly without projecting them in infinite dimensional spaces. In traditional SVMs we choose a kernel and hope that the data become linearly separable in the kernel space. In this paper we show how the hyperplane can be chosen ad-hoc and the kernel is trained so that data are always linearly separable. Comparisons with Large Margin SVMs show comparable performance.
cs.LG:In this paper, I expand Shannon's definition of entropy into a new form of entropy that allows integration of information from different random events. Shannon's notion of entropy is a special case of my more general definition of entropy. I define probability using a so-called performance function, which is de facto an exponential distribution. Assuming that my general notion of entropy reflects the true uncertainty about a probabilistic event, I understand that our perceived uncertainty differs. I claim that our perception is the result of two opposing forces similar to the two famous antagonists in Chinese philosophy: Yin and Yang. Based on this idea, I show that our perceived uncertainty matches the true uncertainty in points determined by the golden ratio. I demonstrate that the well-known sigmoid function, which we typically employ in artificial neural networks as a non-linear threshold function, describes the actual performance. Furthermore, I provide a motivation for the time dilation in Einstein's Special Relativity, basically claiming that although time dilation conforms with our perception, it does not correspond to reality. At the end of the paper, I show how to apply this theoretical framework to practical applications. I present recognition rates for a pattern recognition problem, and also propose a network architecture that can take advantage of general entropy to solve complex decision problems.
cs.LG:Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence.   This paper studies the scenario where the observations are drawn from a stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary phi-mixing and beta-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios.   We also illustrate the application of our phi-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios.
cs.LG:Ensemble classification is an emerging approach to land cover mapping whereby the final classification output is a result of a consensus of classifiers. Intuitively, an ensemble system should consist of base classifiers which are diverse i.e. classifiers whose decision boundaries err differently. In this paper ensemble feature selection is used to impose diversity in ensembles. The features of the constituent base classifiers for each ensemble were created through an exhaustive search algorithm using different separability indices. For each ensemble, the classification accuracy was derived as well as a diversity measure purported to give a measure of the inensemble diversity. The correlation between ensemble classification accuracy and diversity measure was determined to establish the interplay between the two variables. From the findings of this paper, diversity measures as currently formulated do not provide an adequate means upon which to constitute ensembles for land cover mapping.
cs.LG:The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.
cs.LG:We present a convex formulation of dictionary learning for sparse signal decomposition. Convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm. In particular, our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using a large set of synthetic examples, we compare the estimation abilities of the convex and non-convex approaches, showing that while the convex formulation has a single local minimum, this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation.
cs.LG:We introduce a simple and computationally trivial method for binary classification based on the evaluation of potential functions. We demonstrate that despite the conceptual and computational simplicity of the method its performance can match or exceed that of standard Support Vector Machine methods.
cs.LG:We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an $r$-dimensional random vector $\mathbf{Z} \in \mathbb{R}^r$, where $r \geq 2$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\Theta(r \sqrt{T})$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form $O(r \sqrt{T} \log^{3/2} T)$.
cs.LG:We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.
cs.LG:Cooperative decision making is a vision of future network management and control. Distributed connection preemption is an important example where nodes can make intelligent decisions on allocating resources and controlling traffic flows for multi-class service networks. A challenge is that nodal decisions are spatially dependent as traffic flows trespass multiple nodes in a network. Hence the performance-complexity trade-off becomes important, i.e., how accurate decisions are versus how much information is exchanged among nodes. Connection preemption is known to be NP-complete. Centralized preemption is optimal but computationally intractable. Decentralized preemption is computationally efficient but may result in a poor performance. This work investigates distributed preemption where nodes decide whether and which flows to preempt using only local information exchange with neighbors. We develop, based on the probabilistic graphical models, a near-optimal distributed algorithm. The algorithm is used by each node to make collectively near-optimal preemption decisions. We study trade-offs between near-optimal performance and complexity that corresponds to the amount of information-exchange of the distributed algorithm. The algorithm is validated by both analysis and simulation.
cs.LG:In statistical problems, a set of parameterized probability distributions is used to estimate the true probability distribution. If Fisher information matrix at the true distribution is singular, then it has been left unknown what we can estimate about the true distribution from random samples. In this paper, we study a singular regression problem and prove a limit theorem which shows the relation between the singular regression problem and two birational invariants, a real log canonical threshold and a singular fluctuation. The obtained theorem has an important application to statistics, because it enables us to estimate the generalization error from the training error without any knowledge of the true probability distribution.
cs.LG:Scenarios for the emergence or bootstrap of a lexicon involve the repeated interaction between at least two agents who must reach a consensus on how to name N objects using H words. Here we consider minimal models of two types of learning algorithms: cross-situational learning, in which the individuals determine the meaning of a word by looking for something in common across all observed uses of that word, and supervised operant conditioning learning, in which there is strong feedback between individuals about the intended meaning of the words. Despite the stark differences between these learning schemes, we show that they yield the same communication accuracy in the realistic limits of large N and H, which coincides with the result of the classical occupancy problem of randomly assigning N objects to H words.
cs.LG:In this paper, we propose a technique to extract constrained formal concepts.
cs.LG:Recently, different works proposed a new way to mine patterns in databases with pathological size. For example, experiments in genome biology usually provide databases with thousands of attributes (genes) but only tens of objects (experiments). In this case, mining the "transposed" database runs through a smaller search space, and the Galois connection allows to infer the closed patterns of the original database. We focus here on constrained pattern mining for those unusual databases and give a theoretical framework for database and constraint transposition. We discuss the properties of constraint transposition and look into classical constraints. We then address the problem of generating the closed patterns of the original database satisfying the constraint, starting from those mined in the "transposed" database. Finally, we show how to generate all the patterns satisfying the constraint from the closed ones.
cs.LG:We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -- that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.
cs.LG:This paper formalises the concept of learning symbolic rules from multisource data in a cardiac monitoring context. Our sources, electrocardiograms and arterial blood pressure measures, describe cardiac behaviours from different viewpoints. To learn interpretable rules, we use an Inductive Logic Programming (ILP) method. We develop an original strategy to cope with the dimensionality issues caused by using this ILP technique on a rich multisource language. The results show that our method greatly improves the feasibility and the efficiency of the process while staying accurate. They also confirm the benefits of using multiple sources to improve the diagnosis of cardiac arrhythmias.
cs.LG:The problem of completing a low-rank matrix from a subset of its entries is often encountered in the analysis of incomplete data sets exhibiting an underlying factor model with applications in collaborative filtering, computer vision and control. Most recent work had been focused on constructing efficient algorithms for exact or approximate recovery of the missing matrix entries and proving lower bounds for the number of known entries that guarantee a successful recovery with high probability. A related problem from both the mathematical and algorithmic point of view is the distance geometry problem of realizing points in a Euclidean space from a given subset of their pairwise distances. Rigidity theory answers basic questions regarding the uniqueness of the realization satisfying a given partial set of distances. We observe that basic ideas and tools of rigidity theory can be adapted to determine uniqueness of low-rank matrix completion, where inner products play the role that distances play in rigidity theory. This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the completion matrix, that serves as the analogue of the rigidity matrix.
cs.LG:We introduce a new protocol for prediction with expert advice in which each expert evaluates the learner's and his own performance using a loss function that may change over time and may be different from the loss functions used by the other experts. The learner's goal is to perform better or not much worse than each expert, as evaluated by that expert, for all experts simultaneously. If the loss functions used by the experts are all proper scoring rules and all mixable, we show that the defensive forecasting algorithm enjoys the same performance guarantee as that attainable by the Aggregating Algorithm in the standard setting and known to be optimal. This result is also applied to the case of "specialist" (or "sleeping") experts. In this case, the defensive forecasting algorithm reduces to a simple modification of the Aggregating Algorithm.
cs.LG:We present multiplicative updates for solving hard and soft margin support vector machines (SVM) with non-negative kernels. They follow as a natural extension of the updates for non-negative matrix factorization. No additional param- eter setting, such as choosing learning, rate is required. Ex- periments demonstrate rapid convergence to good classifiers. We analyze the rates of asymptotic convergence of the up- dates and establish tight bounds. We test the performance on several datasets using various non-negative kernels and report equivalent generalization errors to that of a standard SVM.
cs.LG:Collecting large labeled data sets is a laborious and expensive task, whose scaling up requires division of the labeling workload between many teachers. When the number of classes is large, miscorrespondences between the labels given by the different teachers are likely to occur, which, in the extreme case, may reach total inconsistency. In this paper we describe how globally consistent labels can be obtained, despite the absence of teacher coordination, and discuss the possible efficiency of this process in terms of human labor. We define a notion of label efficiency, measuring the ratio between the number of globally consistent labels obtained and the number of labels provided by distributed teachers. We show that the efficiency depends critically on the ratio alpha between the number of data instances seen by a single teacher, and the number of classes. We suggest several algorithms for the distributed labeling problem, and analyze their efficiency as a function of alpha. In addition, we provide an upper bound on label efficiency for the case of completely uncoordinated teachers, and show that efficiency approaches 0 as the ratio between the number of labels each teacher provides and the number of classes drops (i.e. alpha goes to 0).
cs.LG:This paper has been retracted.
cs.LG:A $p$-adic modification of the split-LBG classification method is presented in which first clusterings and then cluster centers are computed which locally minimise an energy function. The outcome for a fixed dataset is independent of the prime number $p$ with finitely many exceptions. The methods are applied to the construction of $p$-adic classifiers in the context of learning.
cs.LG:This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It also shows that a number of widely used transductive regression algorithms are in fact unstable. Finally, it reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, for one of the algorithms, in particular for determining the radius of the local neighborhood used by the algorithm.
cs.LG:Motivation: Several different threads of research have been proposed for modeling and mining temporal data. On the one hand, approaches such as dynamic Bayesian networks (DBNs) provide a formal probabilistic basis to model relationships between time-indexed random variables but these models are intractable to learn in the general case. On the other, algorithms such as frequent episode mining are scalable to large datasets but do not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature.   Results: We present a unification of these two seemingly diverse threads of research, by demonstrating how dynamic (discrete) Bayesian networks can be inferred from the results of frequent episode mining. This helps bridge the modeling emphasis of the former with the counting emphasis of the latter. First, we show how, under reasonable assumptions on data characteristics and on influences of random variables, the optimal DBN structure can be computed using a greedy, local, algorithm. Next, we connect the optimality of the DBN structure with the notion of fixed-delay episodes and their counts of distinct occurrences. Finally, to demonstrate the practical feasibility of our approach, we focus on a specific (but broadly applicable) class of networks, called excitatory networks, and show how the search for the optimal DBN structure can be conducted using just information from frequent episodes. Application on datasets gathered from mathematical models of spiking neurons as well as real neuroscience datasets are presented.   Availability: Algorithmic implementations, simulator codebases, and datasets are available from our website at http://neural-code.cs.vt.edu/dbn
cs.LG:Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).
cs.LG:In this paper, we consider the coherent theory of (epistemic) uncertainty of Walley, in which beliefs are represented through sets of probability distributions, and we focus on the problem of modeling prior ignorance about a categorical random variable. In this setting, it is a known result that a state of prior ignorance is not compatible with learning. To overcome this problem, another state of beliefs, called \emph{near-ignorance}, has been proposed. Near-ignorance resembles ignorance very closely, by satisfying some principles that can arguably be regarded as necessary in a state of ignorance, and allows learning to take place. What this paper does, is to provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is \emph{latent}. We argue that such a setting is by far the most common case in practice, and we provide, for the case of categorical latent variables (and general \emph{manifest} variables) a condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied even in the most common statistical problems. We regard these results as a strong form of evidence against the possibility to adopt a condition of prior near-ignorance in real statistical problems.
cs.LG:Engine assembly is a complex and heavily automated distributed-control process, with large amounts of faults data logged everyday. We describe an application of temporal data mining for analyzing fault logs in an engine assembly plant. Frequent episode discovery framework is a model-free method that can be used to deduce (temporal) correlations among events from the logs in an efficient manner. In addition to being theoretically elegant and computationally efficient, frequent episodes are also easy to interpret in the form actionable recommendations. Incorporation of domain-specific information is critical to successful application of the method for analyzing fault logs in the manufacturing domain. We show how domain-specific knowledge can be incorporated using heuristic rules that act as pre-filters and post-filters to frequent episode discovery. The system described here is currently being used in one of the engine assembly plants of General Motors and is planned for adaptation in other plants. To the best of our knowledge, this paper presents the first real, large-scale application of temporal data mining in the manufacturing domain. We believe that the ideas presented in this paper can help practitioners engineer tools for analysis in other similar or related application domains as well.
cs.LG:This paper presents a new hybrid learning algorithm for unsupervised classification tasks. We combined Fuzzy c-means learning algorithm and a supervised version of Minimerror to develop a hybrid incremental strategy allowing unsupervised classifications. We applied this new approach to a real-world database in order to know if the information contained in unlabeled features of a Geographic Information System (GIS), allows to well classify it. Finally, we compared our results to a classical supervised classification obtained by a multilayer perceptron.
cs.LG:We analyze the expected cost of a greedy active learning algorithm. Our analysis extends previous work to a more general setting in which different queries have different costs. Moreover, queries may have more than two possible responses and the distribution over hypotheses may be non uniform. Specific applications include active learning with label costs, active learning for multiclass and partial label queries, and batch mode active learning. We also discuss an approximate version of interest when there are very many queries.
cs.LG:We present three related ways of using Transfer Learning to improve feature selection. The three methods address different problems, and hence share different kinds of information between tasks or feature classes, but all three are based on the information theoretic Minimum Description Length (MDL) principle and share the same underlying Bayesian interpretation. The first method, MIC, applies when predictive models are to be built simultaneously for multiple tasks (``simultaneous transfer'') that share the same set of features. MIC allows each feature to be added to none, some, or all of the task models and is most beneficial for selecting a small set of predictive features from a large pool of features, as is common in genomic and biological datasets. Our second method, TPC (Three Part Coding), uses a similar methodology for the case when the features can be divided into feature classes. Our third method, Transfer-TPC, addresses the ``sequential transfer'' problem in which the task to which we want to transfer knowledge may not be known in advance and may have different amounts of data than the other tasks. Transfer-TPC is most beneficial when we want to transfer knowledge between tasks which have unequal amounts of labeled data, for example the data for disambiguating the senses of different verbs. We demonstrate the effectiveness of these approaches with experimental results on real world data pertaining to genomics and to Word Sense Disambiguation (WSD).
cs.LG:Many learning machines that have hierarchical structure or hidden variables are now being used in information science, artificial intelligence, and bioinformatics. However, several learning machines used in such fields are not regular but singular statistical models, hence their generalization performance is still left unknown. To overcome these problems, in the previous papers, we proved new equations in statistical learning, by which we can estimate the Bayes generalization loss from the Bayes training loss and the functional variance, on the condition that the true distribution is a singularity contained in a learning machine. In this paper, we prove that the same equations hold even if a true distribution is not contained in a parametric model. Also we prove that, the proposed equations in a regular case are asymptotically equivalent to the Takeuchi information criterion. Therefore, the proposed equations are always applicable without any condition on the unknown true distribution.
cs.LG:The problem of classifying sonar signals from rocks and mines first studied by Gorman and Sejnowski has become a benchmark against which many learning algorithms have been tested. We show that both the training set and the test set of this benchmark are linearly separable, although with different hyperplanes. Moreover, the complete set of learning and test patterns together, is also linearly separable. We give the weights that separate these sets, which may be used to compare results found by other algorithms.
cs.LG:Clusters of genes that have evolved by repeated segmental duplication present difficult challenges throughout genomic analysis, from sequence assembly to functional analysis. Improved understanding of these clusters is of utmost importance, since they have been shown to be the source of evolutionary innovation, and have been linked to multiple diseases, including HIV and a variety of cancers. Previously, Zhang et al. (2008) developed an algorithm for reconstructing parsimonious evolutionary histories of such gene clusters, using only human genomic sequence data. In this paper, we propose a probabilistic model for the evolution of gene clusters on a phylogeny, and an MCMC algorithm for reconstruction of duplication histories from genomic sequences in multiple species. Several projects are underway to obtain high quality BAC-based assemblies of duplicated clusters in multiple species, and we anticipate that our method will be useful in analyzing these valuable new data sets.
cs.LG:In this paper, we present two classes of Bayesian approaches to the two-sample problem. Our first class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as flexible nonparametric priors over the unknown distributions.
cs.LG:In this paper, we present the step by step knowledge acquisition process by choosing a structured method through using a questionnaire as a knowledge acquisition tool. Here we want to depict the problem domain as, how to evaluate teachers performance in higher education through the use of expert system technology. The problem is how to acquire the specific knowledge for a selected problem efficiently and effectively from human experts and encode it in the suitable computer format. Acquiring knowledge from human experts in the process of expert systems development is one of the most common problems cited till yet. This questionnaire was sent to 87 domain experts within all public and private universities in Pakistani. Among them 25 domain experts sent their valuable opinions. Most of the domain experts were highly qualified, well experienced and highly responsible persons. The whole questionnaire was divided into 15 main groups of factors, which were further divided into 99 individual questions. These facts were analyzed further to give a final shape to the questionnaire. This knowledge acquisition technique may be used as a learning tool for further research work.
cs.LG:We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.
cs.LG:The maze traversal problem (finding the shortest distance to the goal from any position in a maze) has been an interesting challenge in computational intelligence. Recent work has shown that the cellular simultaneous recurrent neural network (CSRN) can solve this problem for simple mazes. This thesis focuses on exploiting relevant information about the maze to improve learning and decrease the training time for the CSRN to solve mazes. Appropriate variables are identified to create useful clusters using relevant information. The CSRN was next modified to allow for an additional external input. With this additional input, several methods were tested and results show that clustering the mazes improves the overall learning of the traversal problem for the CSRN.
cs.LG:We propose a randomized algorithm for training Support vector machines(SVMs) on large datasets. By using ideas from Random projections we show that the combinatorial dimension of SVMs is $O({log} n)$ with high probability. This estimate of combinatorial dimension is used to derive an iterative algorithm, called RandSVM, which at each step calls an existing solver to train SVMs on a randomly chosen subset of size $O({log} n)$. The algorithm has probabilistic guarantees and is capable of training SVMs with Kernels for both classification and regression problems. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up existing SVM learners, without loss of accuracy.
cs.LG:We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.
cs.LG:In Data Mining, the usefulness of association rules is strongly limited by the huge amount of delivered rules. In this paper we propose a new approach to prune and filter discovered rules. Using Domain Ontologies, we strengthen the integration of user knowledge in the post-processing task. Furthermore, an interactive and iterative framework is designed to assist the user along the analyzing task. On the one hand, we represent user domain knowledge using a Domain Ontology over database. On the other hand, a novel technique is suggested to prune and to filter discovered rules. The proposed framework was applied successfully over the client database provided by Nantes Habitat.
cs.LG:Gaussian processes (GPs) provide a probabilistic nonparametric representation of functions in regression, classification, and other problems. Unfortunately, exact learning with GPs is intractable for large datasets. A variety of approximate GP methods have been proposed that essentially map the large dataset into a small set of basis points. The most advanced of these, the variable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have its own length scale. However, VSGP was only derived for regression. We describe how VSGP can be applied to classification and other problems, by deriving it as an expectation propagation algorithm. In this view, sparse GP approximations correspond to a KL-projection of the true posterior onto a compact exponential family of GPs. VSGP constitutes one such family, and we show how to enlarge this family to get additional accuracy. In particular, we show that endowing each basis point with its own full covariance matrix provides a significant increase in approximation power.
cs.LG:In this paper we discuss the techniques involved in the design of the famous statistical spam filters that include Naive Bayes, Term Frequency-Inverse Document Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes Additive Regression Tree. We compare these techniques with each other in terms of accuracy, recall, precision, etc. Further, we discuss the effectiveness and limitations of statistical filters in filtering out various types of spam from legitimate e-mails.
cs.LG:We study the problem of online regression. We prove a theoretical bound on the square loss of Ridge Regression. We do not make any assumptions about input vectors or outcomes. We also show that Bayesian Ridge Regression can be thought of as an online algorithm competing with all the Gaussian linear experts.
cs.LG:We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on $n$-point nominal data. Anomalies are declared whenever the score of a test sample falls below $\alpha$, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, $\alpha$, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.
cs.LG:In this paper, we prove a crucial theorem called Mirroring Theorem which affirms that given a collection of samples with enough information in it such that it can be classified into classes and subclasses then (i) There exists a mapping which classifies and subclassifies these samples (ii) There exists a hierarchical classifier which can be constructed by using Mirroring Neural Networks (MNNs) in combination with a clustering algorithm that can approximate this mapping. Thus, the proof of the Mirroring theorem provides a theoretical basis for the existence and a practical feasibility of constructing hierarchical classifiers, given the maps. Our proposed Mirroring Theorem can also be considered as an extension to Kolmogrovs theorem in providing a realistic solution for unsupervised classification. The techniques we develop, are general in nature and have led to the construction of learning machines which are (i) tree like in structure, (ii) modular (iii) with each module running on a common algorithm (tandem algorithm) and (iv) selfsupervised. We have actually built the architecture, developed the tandem algorithm of such a hierarchical classifier and demonstrated it on an example problem.
cs.LG:This paper describes a methodology for detecting anomalies from sequentially observed and potentially noisy data. The proposed approach consists of two main elements: (1) {\em filtering}, or assigning a belief or likelihood to each successive measurement based upon our ability to predict it from previous noisy observations, and (2) {\em hedging}, or flagging potential anomalies by comparing the current belief against a time-varying and data-adaptive threshold. The threshold is adjusted based on the available feedback from an end user. Our algorithms, which combine universal prediction with recent work on online convex programming, do not require computing posterior distributions given all current observations and involve simple primal-dual parameter updates. At the heart of the proposed approach lie exponential-family models which can be used in a wide variety of contexts and applications, and which yield methods that achieve sublinear per-round regret against both static and slowly varying product distributions with marginals drawn from the same exponential family. Moreover, the regret against static distributions coincides with the minimax value of the corresponding online strongly convex game. We also prove bounds on the number of mistakes made during the hedging step relative to the best offline choice of the threshold with access to all estimated beliefs and feedback signals. We validate the theory on synthetic data drawn from a time-varying distribution over binary vectors of high dimensionality, as well as on the Enron email dataset.
cs.LG:We present in this paper a study on the ability and the benefits of using a keystroke dynamics authentication method for collaborative systems. Authentication is a challenging issue in order to guarantee the security of use of collaborative systems during the access control step. Many solutions exist in the state of the art such as the use of one time passwords or smart-cards. We focus in this paper on biometric based solutions that do not necessitate any additional sensor. Keystroke dynamics is an interesting solution as it uses only the keyboard and is invisible for users. Many methods have been published in this field. We make a comparative study of many of them considering the operational constraints of use for collaborative systems.
cs.LG:This document describes concisely the ubiquitous class of exponential family distributions met in statistics. The first part recalls definitions and summarizes main properties and duality with Bregman divergences (all proofs are skipped). The second part lists decompositions and related formula of common exponential family distributions. We recall the Fisher-Rao-Riemannian geometries and the dual affine connection information geometries of statistical manifolds. It is intended to maintain and update this document and catalog by adding new distribution items.
cs.LG:One of the most popular algorithms for clustering in Euclidean space is the $k$-means algorithm; $k$-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is {\em well-clustered}. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of $k$-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian -- or, in other words, when the input comes from a mixture of Gaussians.   We analyze three aspects of the $k$-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components.   Finally, we study the sample requirement of $k$-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of $k$-means is {\em near-optimal}.
cs.LG:In this paper, we consider delay-optimal power and subcarrier allocation design for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base station. There are $K$ queues at the base station for the downlink traffic to the $K$ mobiles with heterogeneous packet arrivals and delay requirements. We shall model the problem as a $K$-dimensional infinite horizon average reward Markov Decision Problem (MDP) where the control actions are assumed to be a function of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). This problem is challenging because it corresponds to a stochastic Network Utility Maximization (NUM) problem where general solution is still unknown. We propose an {\em online stochastic value iteration} solution using {\em stochastic approximation}. The proposed power control algorithm, which is a function of both the CSI and the QSI, takes the form of multi-level water-filling. We prove that under two mild conditions in Theorem 1 (One is the stepsize condition. The other is the condition on accessibility of the Markov Chain, which can be easily satisfied in most of the cases we are interested.), the proposed solution converges to the optimal solution almost surely (with probability 1) and the proposed framework offers a possible solution to the general stochastic NUM problem. By exploiting the birth-death structure of the queue dynamics, we obtain a reduced complexity decomposed solution with linear $\mathcal{O}(KN_F)$ complexity and $\mathcal{O}(K)$ memory requirement.
cs.LG:Association rule mining plays vital part in knowledge mining. The difficult task is discovering knowledge or useful rules from the large number of rules generated for reduced support. For pruning or grouping rules, several techniques are used such as rule structure cover methods, informative cover methods, rule clustering, etc. Another way of selecting association rules is based on interestingness measures such as support, confidence, correlation, and so on. In this paper, we study how rule clusters of the pattern Xi - Y are distributed over different interestingness measures.
cs.LG:This paper presents a tumor detection algorithm from mammogram. The proposed system focuses on the solution of two problems. One is how to detect tumors as suspicious regions with a very weak contrast to their background and another is how to extract features which categorize tumors. The tumor detection method follows the scheme of (a) mammogram enhancement. (b) The segmentation of the tumor area. (c) The extraction of features from the segmented tumor area. (d) The use of SVM classifier. The enhancement can be defined as conversion of the image quality to a better and more understandable level. The mammogram enhancement procedure includes filtering, top hat operation, DWT. Then the contrast stretching is used to increase the contrast of the image. The segmentation of mammogram images has been playing an important role to improve the detection and diagnosis of breast cancer. The most common segmentation method used is thresholding. The features are extracted from the segmented breast area. Next stage include, which classifies the regions using the SVM classifier. The method was tested on 75 mammographic images, from the mini-MIAS database. The methodology achieved a sensitivity of 88.75%.
cs.LG:Among all the partition based clustering algorithms K-means is the most popular and well known method. It generally shows impressive results even in considerably large data sets. The computational complexity of K-means does not suffer from the size of the data set. The main disadvantage faced in performing this clustering is that the selection of initial means. If the user does not have adequate knowledge about the data set, it may lead to erroneous results. The algorithm Automatic Initialization of Means (AIM), which is an extension to K-means, has been proposed to overcome the problem of initial mean generation. In this paper an attempt has been made to compare the performance of the algorithms through implementation
cs.LG:Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.
cs.LG:In this paper we consider the problem of reconstructing a hidden weighted hypergraph of constant rank using additive queries. We prove the following: Let $G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$ hyperedges. For any $m$ there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log n}{\log m}) $$ additive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal Query Complexity Bounds for Finding Graphs. {\em STOC}, 749--758,~2008].   When the weights of the hypergraph are integers that are less than $O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for unweighted hypergraphs) there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log \frac{n^d}{m}}{\log m}). $$ additive queries.   Using the information theoretic bound the above query complexities are tight.
cs.LG:Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.
cs.LG:Discovering latent representations of the observed world has become increasingly more relevant in data analysis. Much of the effort concentrates on building latent variables which can be used in prediction problems, such as classification and regression. A related goal of learning latent structure from data is that of identifying which hidden common causes generate the observations, such as in applications that require predicting the effect of policies. This will be the main problem tackled in our contribution: given a dataset of indicators assumed to be generated by unknown and unmeasured common causes, we wish to discover which hidden common causes are those, and how they generate our data. This is possible under the assumption that observed variables are linear functions of the latent causes with additive noise. Previous results in the literature present solutions for the case where each observed variable is a noisy function of a single latent variable. We show how to extend the existing results for some cases where observed variables measure more than one latent variable.
cs.LG:Bayes statistics and statistical physics have the common mathematical structure, where the log likelihood function corresponds to the random Hamiltonian. Recently, it was discovered that the asymptotic learning curves in Bayes estimation are subject to a universal law, even if the log likelihood function can not be approximated by any quadratic form. However, it is left unknown what mathematical property ensures such a universal law. In this paper, we define a renormalizable condition of the statistical estimation problem, and show that, under such a condition, the asymptotic learning curves are ensured to be subject to the universal law, even if the true distribution is unrealizable and singular for a statistical model. Also we study a nonrenormalizable case, in which the learning curves have the different asymptotic behaviors from the universal law.
cs.LG:Associative Classifier is a novel technique which is the integration of Association Rule Mining and Classification. The difficult task in building Associative Classifier model is the selection of relevant rules from a large number of class association rules (CARs). A very popular method of ordering rules for selection is based on confidence, support and antecedent size (CSA). Other methods are based on hybrid orderings in which CSA method is combined with other measures. In the present work, we study the effect of using different interestingness measures of Association rules in CAR rule ordering and selection for associative classifier.
cs.LG:This paper presents a framework aimed at monitoring the behavior of aircraft in a given airspace. Nominal trajectories are determined and learned using data driven methods. Standard procedures are used by air traffic controllers (ATC) to guide aircraft, ensure the safety of the airspace, and to maximize the runway occupancy. Even though standard procedures are used by ATC, the control of the aircraft remains with the pilots, leading to a large variability in the flight patterns observed. Two methods to identify typical operations and their variability from recorded radar tracks are presented. This knowledge base is then used to monitor the conformance of current operations against operations previously identified as standard. A tool called AirTrajectoryMiner is presented, aiming at monitoring the instantaneous health of the airspace, in real time. The airspace is "healthy" when all aircraft are flying according to the nominal procedures. A measure of complexity is introduced, measuring the conformance of current flight to nominal flight patterns. When an aircraft does not conform, the complexity increases as more attention from ATC is required to ensure a safe separation between aircraft.
cs.LG:The paper deals with on-line regression settings with signals belonging to a Banach lattice. Our algorithms work in a semi-online setting where all the inputs are known in advance and outcomes are unknown and given step by step. We apply the Aggregating Algorithm to construct a prediction method whose cumulative loss over all the input vectors is comparable with the cumulative loss of any linear functional on the Banach lattice. As a by-product we get an algorithm that takes signals from an arbitrary domain. Its cumulative loss is comparable with the cumulative loss of any predictor function from Besov and Triebel-Lizorkin spaces. We describe several applications of our setting.
cs.LG:The performance in higher secondary school education in India is a turning point in the academic lives of all students. As this academic performance is influenced by many factors, it is essential to develop predictive data mining model for students' performance so as to identify the slow learners and study the influence of the dominant factors on their academic performance. In the present investigation, a survey cum experimental methodology was adopted to generate a database and it was constructed from a primary and a secondary source. While the primary data was collected from the regular students, the secondary data was gathered from the school and office of the Chief Educational Officer (CEO). A total of 1000 datasets of the year 2006 from five different schools in three different districts of Tamilnadu were collected. The raw data was preprocessed in terms of filling up missing values, transforming values in one form into another and relevant attribute/ variable selection. As a result, we had 772 student records, which were used for CHAID prediction model construction. A set of prediction rules were extracted from CHIAD prediction model and the efficiency of the generated CHIAD prediction model was found. The accuracy of the present model was compared with other model and it has been found to be satisfactory.
cs.LG:The recent increase in dimensionality of data has thrown a great challenge to the existing dimensionality reduction methods in terms of their effectiveness. Dimensionality reduction has emerged as one of the significant preprocessing steps in machine learning applications and has been effective in removing inappropriate data, increasing learning accuracy, and improving comprehensibility. Feature redundancy exercises great influence on the performance of classification process. Towards the better classification performance, this paper addresses the usefulness of truncating the highly correlated and redundant attributes. Here, an effort has been made to verify the utility of dimensionality reduction by applying LVQ (Learning Vector Quantization) method on two Benchmark datasets of 'Pima Indian Diabetic patients' and 'Lung cancer patients'.
cs.LG:A key problem in sensor networks is to decide which sensors to query when, in order to obtain the most useful information (e.g., for performing accurate prediction), subject to constraints (e.g., on power and bandwidth). In many applications the utility function is not known a priori, must be learned from data, and can even change over time. Furthermore for large sensor networks solving a centralized optimization problem to select sensors is not feasible, and thus we seek a fully distributed solution. In this paper, we present Distributed Online Greedy (DOG), an efficient, distributed algorithm for repeatedly selecting sensors online, only receiving feedback about the utility of the selected sensors. We prove very strong theoretical no-regret guarantees that apply whenever the (unknown) utility function satisfies a natural diminishing returns property called submodularity. Our algorithm has extremely low communication requirements, and scales well to large sensor deployments. We extend DOG to allow observation-dependent sensor selection. We empirically demonstrate the effectiveness of our algorithm on several real-world sensing tasks.
cs.LG:Recently Kutin and Niyogi investigated several notions of algorithmic stability--a property of a learning map conceptually similar to continuity--showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.
cs.LG:We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.
cs.LG:We introduce a natural generalization of submodular set cover and exact active learning with a finite hypothesis class (query learning). We call this new problem interactive submodular set cover. Applications include advertising in social networks with hidden information. We give an approximation guarantee for a novel greedy algorithm and give a hardness of approximation result which matches up to constant factors. We also discuss negative results for simpler approaches and present encouraging early experimental results.
cs.LG:India is a multi-lingual country where Roman script is often used alongside different Indic scripts in a text document. To develop a script specific handwritten Optical Character Recognition (OCR) system, it is therefore necessary to identify the scripts of handwritten text correctly. In this paper, we present a system, which automatically separates the scripts of handwritten words from a document, written in Bangla or Devanagri mixed with Roman scripts. In this script separation technique, we first, extract the text lines and words from document pages using a script independent Neighboring Component Analysis technique. Then we have designed a Multi Layer Perceptron (MLP) based classifier for script separation, trained with 8 different wordlevel holistic features. Two equal sized datasets, one with Bangla and Roman scripts and the other with Devanagri and Roman scripts, are prepared for the system evaluation. On respective independent text samples, word-level script identification accuracies of 99.29% and 98.43% are achieved.
cs.LG:We address the problem of learning in an online, bandit setting where the learner must repeatedly select among $K$ actions, but only receives partial feedback based on its choices. We establish two new facts: First, using a new algorithm called Exp4.P, we show that it is possible to compete with the best in a set of $N$ experts with probability $1-\delta$ while incurring regret at most $O(\sqrt{KT\ln(N/\delta)})$ over $T$ time steps. The new algorithm is tested empirically in a large-scale, real-world dataset. Second, we give a new algorithm called VE that competes with a possibly infinite set of policies of VC-dimension $d$ while incurring regret at most $O(\sqrt{T(d\ln(T) + \ln (1/\delta))})$ with probability $1-\delta$. These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing supervised learning type guarantees for the contextual bandit setting.
cs.LG:We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared, and modify it only via a single time-dependent parameter. Our algorithm's regret bounds are worst-case optimal, and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent, which means they can exploit the structure of the actual problem instance. Critically, however, our algorithm does not need to know this structure in advance. Rather, we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.
cs.LG:Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.
cs.LG:Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled dataset. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional datasets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever.
cs.LG:A key issue in statistics and machine learning is to automatically select the "right" model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) - for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.
cs.LG:Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufficient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy.
cs.LG:We present an approach to semi-supervised learning based on an exponential family characterization. Our approach generalizes previous work on coupled priors for hybrid generative/discriminative models. Our model is more flexible and natural than previous approaches. Experimental results on several data sets show that our approach also performs better in practice.
cs.LG:In recent years, predicting the user's next request in web navigation has received much attention. An information source to be used for dealing with such problem is the left information by the previous web users stored at the web access log on the web servers. Purposed systems for this problem work based on this idea that if a large number of web users request specific pages of a website on a given session, it can be concluded that these pages are satisfying similar information needs, and therefore they are conceptually related. In this study, a new clustering approach is introduced that employs logical path storing of a website pages as another parameter which is regarded as a similarity parameter and conceptual relation between web pages. The results of simulation have shown that the proposed approach is more than others precise in determining the clusters.
cs.LG:Most Web page classification models typically apply the bag of words (BOW) model to represent the feature space. The original BOW representation, however, is unable to recognize semantic relationships between terms. One possible solution is to apply the topic model approach based on the Latent Dirichlet Allocation algorithm to cluster the term features into a set of latent topics. Terms assigned into the same topic are semantically related. In this paper, we propose a novel hierarchical classification method based on a topic model and by integrating additional term features from neighboring pages. Our hierarchical classification method consists of two phases: (1) feature representation by using a topic model and integrating neighboring pages, and (2) hierarchical Support Vector Machines (SVM) classification model constructed from a confusion matrix. From the experimental results, the approach of using the proposed hierarchical SVM model by integrating current page with neighboring pages via the topic model yielded the best performance with the accuracy equal to 90.33% and the F1 measure of 90.14%; an improvement of 5.12% and 5.13% over the original SVM model, respectively.
cs.LG:We apply the method of defensive forecasting, based on the use of game-theoretic supermartingales, to prediction with expert advice. In the traditional setting of a countable number of experts and a finite number of outcomes, the Defensive Forecasting Algorithm is very close to the well-known Aggregating Algorithm. Not only the performance guarantees but also the predictions are the same for these two methods of fundamentally different nature. We discuss also a new setting where the experts can give advice conditional on the learner's future decision. Both the algorithms can be adapted to the new setting and give the same performance guarantees as in the traditional setting. Finally, we outline an application of defensive forecasting to a setting with several loss functions.
cs.LG:This paper proposes a novel similarity measure for clustering sequential data. We first construct a common state-space by training a single probabilistic model with all the sequences in order to get a unified representation for the dataset. Then, distances are obtained attending to the transition matrices induced by each sequence in that state-space. This approach solves some of the usual overfitting and scalability issues of the existing semi-parametric techniques, that rely on training a model for each sequence. Empirical studies on both synthetic and real-world datasets illustrate the advantages of the proposed similarity measure for clustering sequences.
cs.LG:In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to $2\lambda/n$, where $\lambda$ is the real log canonical threshold and $n$ is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.
cs.LG:We present a solution to the problem of understanding a system that produces a sequence of temporally ordered observations. Our solution is based on generating and interpreting a set of temporal decision rules. A temporal decision rule is a decision rule that can be used to predict or retrodict the value of a decision attribute, using condition attributes that are observed at times other than the decision attribute's time of observation. A rule set, consisting of a set of temporal decision rules with the same decision attribute, can be interpreted by our Temporal Investigation Method for Enregistered Record Sequences (TIMERS) to signify an instantaneous, an acausal or a possibly causal relationship between the condition attributes and the decision attribute. We show the effectiveness of our method, by describing a number of experiments with both synthetic and real temporal data.
cs.LG:In this work we investigate the relationship between Bregman distances and regularized Logistic Regression model. We present a detailed study of Bregman Distance minimization, a family of generalized entropy measures associated with convex functions. We convert the L1-regularized logistic regression into this more general framework and propose a primal-dual method based algorithm for learning the parameters. We pose L1-regularized logistic regression into Bregman distance minimization and then apply non-linear constrained optimization techniques to estimate the parameters of the logistic model.
cs.LG:We describe and analyze efficient algorithms for learning a linear predictor from examples when the learner can only view a few attributes of each training example. This is the case, for instance, in medical research, where each patient participating in the experiment is only willing to go through a small number of tests. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on each training example. We demonstrate the efficiency of our algorithms by showing that when running on digit recognition data, they obtain a high prediction accuracy even when the learner gets to see only four pixels of each image.
cs.LG:We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.
cs.LG:In Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the {\it effective} sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning.
cs.LG:In this paper, we consider the distributive queue-aware power and subband allocation design for a delay-optimal OFDMA uplink system with one base station, $K$ users and $N_F$ independent subbands. Each mobile has an uplink queue with heterogeneous packet arrivals and delay requirements. We model the problem as an infinite horizon average reward Markov Decision Problem (MDP) where the control actions are functions of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the subband allocation Q-factor by the sum of the per-user subband allocation Q-factor and derive a distributive online stochastic learning algorithm to estimate the per-user Q-factor and the Lagrange multipliers (LM) simultaneously and determine the control actions using an auction mechanism. We show that under the proposed auction mechanism, the distributive online learning converges almost surely (with probability 1). For illustration, we apply the proposed distributive stochastic learning framework to an application example with exponential packet size distribution. We show that the delay-optimal power control has the {\em multi-level water-filling} structure where the CSI determines the instantaneous power allocation and the QSI determines the water-level. The proposed algorithm has linear signaling overhead and computational complexity $\mathcal O(KN)$, which is desirable from an implementation perspective.
cs.LG:This paper presents a method for automated healing as part of off-line automated troubleshooting. The method combines statistical learning with constraint optimization. The automated healing aims at locally optimizing radio resource management (RRM) or system parameters of cells with poor performance in an iterative manner. The statistical learning processes the data using Logistic Regression (LR) to extract closed form (functional) relations between Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. These functional relations are then processed by an optimization engine which proposes new parameter values. The advantage of the proposed formulation is the small number of iterations required by the automated healing method to converge, making it suitable for off-line implementation. The proposed method is applied to heal an Inter-Cell Interference Coordination (ICIC) process in a 3G Long Term Evolution (LTE) network which is based on soft-frequency reuse scheme. Numerical simulations illustrate the benefits of the proposed approach.
cs.LG:Although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications such as communications. In this work, we focus our attention on the complex gaussian kernel and its possible application in the complex Kernel LMS algorithm. In order to derive the gradients needed to develop the complex kernel LMS (CKLMS), we employ the powerful tool of Wirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, the notion of Writinger's calculus is extended to include complex RKHSs. Experiments verify that the CKLMS offers significant performance improvements over the traditional complex LMS or Widely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. However, so far, the emphasis has been on batch techniques. It is only recently, that online adaptive techniques have been considered in the context of signal processing tasks. To the best of our knowledge, no kernel-based strategy has been developed, so far, that is able to deal with complex valued signals. In this paper, we take advantage of a technique called complexification of real RKHSs to attack this problem. In order to derive gradients and subgradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool ofWirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, in this paper, the notion of Writinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be used to derive nonlinear stable algorithms, which offer significant performance improvements over the traditional complex LMS orWidely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Semi-supervised support vector machines (S3VMs) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data. Though S3VMs have been found helpful in many situations, they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only. In this paper, we try to reduce the chance of performance degeneration of S3VMs. Our basic idea is that, rather than exploiting all unlabeled data, the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited, while some highly risky unlabeled instances are avoided. We propose the S3VM-\emph{us} method by using hierarchical clustering to select the unlabeled instances. Experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of S3VM-\emph{us} is much smaller than that of existing S3VMs.
cs.LG:We study prediction with expert advice in the setting where the losses are accumulated with some discounting---the impact of old losses may gradually vanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm for Regression to this case, propose a suitable new variant of exponential weights algorithm, and prove respective loss bounds.
cs.LG:In this paper, we formulate a novel problem for finding blackhole and volcano patterns in a large directed graph. Specifically, a blackhole pattern is a group which is made of a set of nodes in a way such that there are only inlinks to this group from the rest nodes in the graph. In contrast, a volcano pattern is a group which only has outlinks to the rest nodes in the graph. Both patterns can be observed in real world. For instance, in a trading network, a blackhole pattern may represent a group of traders who are manipulating the market. In the paper, we first prove that the blackhole mining problem is a dual problem of finding volcanoes. Therefore, we focus on finding the blackhole patterns. Along this line, we design two pruning schemes to guide the blackhole finding process. In the first pruning scheme, we strategically prune the search space based on a set of pattern-size-independent pruning rules and develop an iBlackhole algorithm. The second pruning scheme follows a divide-and-conquer strategy to further exploit the pruning results from the first pruning scheme. Indeed, a target directed graphs can be divided into several disconnected subgraphs by the first pruning scheme, and thus the blackhole finding can be conducted in each disconnected subgraph rather than in a large graph. Based on these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally, experimental results on real-world data show that the iBlackhole-DC algorithm can be several orders of magnitude faster than the iBlackhole algorithm, which has a huge computational advantage over a brute-force method.
cs.LG:We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is "similar" to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work.
cs.LG:We study online learning when individual instances are corrupted by adversarially chosen random noise. We assume the noise distribution is unknown, and may change over time with no restriction other than having zero mean and bounded variance. Our technique relies on a family of unbiased estimators for non-linear functions, which may be of independent interest. We show that a variant of online gradient descent can learn functions in any dot-product (e.g., polynomial) or Gaussian kernel space with any analytic convex loss function. Our variant uses randomized estimates that need to query a random number of noisy copies of each instance, where with high probability this number is upper bounded by a constant. Allowing such multiple queries cannot be avoided: Indeed, we show that online learning is in general impossible when only one noisy copy of each instance can be accessed.
cs.LG:We consider the question of the stability of evolutionary algorithms to gradual changes, or drift, in the target concept. We define an algorithm to be resistant to drift if, for some inverse polynomial drift rate in the target function, it converges to accuracy 1 -- \epsilon , with polynomial resources, and then stays within that accuracy indefinitely, except with probability \epsilon , at any one time. We show that every evolution algorithm, in the sense of Valiant (2007; 2009), can be converted using the Correlational Query technique of Feldman (2008), into such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean conjunctions, we give bounds on the rates of drift that they can resist. We develop some new evolution algorithms that are resistant to significant drift. In particular, we give an algorithm for evolving linear separators over the spherically symmetric distribution that is resistant to a drift rate of O(\epsilon /n), and another algorithm over the more general product normal distributions that resists a smaller drift rate.   The above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations.
cs.LG:We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the \emph{zero-one} loss function. Unlike most previous formulations which rely on surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural zero-one loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time $\poly(\exp(L\log(L/\epsilon)))$, for $\emph{any}$ distribution, where $L$ is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most $\epsilon$. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in $L$.
cs.LG:This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method.
cs.LG:The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\widetilde{O}(\log\frac{1}{\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\epsilon$ is related to the parameter in Tsybakov noise.
cs.LG:In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.
cs.LG:Exchangeable random variables form an important and well-studied generalization of i.i.d. variables, however simple examples show that no nontrivial concept or function classes are PAC learnable under general exchangeable data inputs $X_1,X_2,\ldots$. Inspired by the work of Berti and Rigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new paradigm, adequate for learning from exchangeable data: predictive PAC learnability. A learning rule $\mathcal L$ for a function class $\mathscr F$ is predictive PAC if for every $\e,\delta>0$ and each function $f\in {\mathscr F}$, whenever $\abs{\sigma}\geq s(\delta,\e)$, we have with confidence $1-\delta$ that the expected difference between $f(X_{n+1})$ and the image of $f\vert\sigma$ under $\mathcal L$ does not exceed $\e$ conditionally on $X_1,X_2,\ldots,X_n$. Thus, instead of learning the function $f$ as such, we are learning to a given accuracy $\e$ the predictive behaviour of $f$ at the future points $X_i(\omega)$, $i>n$ of the sample path. Using de Finetti's theorem, we show that if a universally separable function class $\mathscr F$ is distribution-free PAC learnable under i.i.d. inputs, then it is distribution-free predictive PAC learnable under exchangeable inputs, with a slightly worse sample complexity.
cs.LG:The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.
cs.LG:In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present the first model for dyadic prediction that satisfies several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) it is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to very large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are ordinal and ignore side-information when it is present. Experimental results show that the new method is competitive with state-of-the-art methods for the special cases of collaborative filtering and link prediction, and that it makes accurate predictions on nominal data.
cs.LG:We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. The primary mathematical tool employed in these methods is the notion of the Reproducing Kernel Hilbert Space. However, so far, the emphasis has been on batch techniques. It is only recently, that online techniques have been considered in the context of adaptive signal processing tasks. Moreover, these efforts have only been focussed on real valued data sequences. To the best of our knowledge, no adaptive kernel-based strategy has been developed, so far, for complex valued signals. Furthermore, although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications that deal with complex signals, with Communications being a typical example. In this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called \textit{complexification} of real RKHSs, or complex reproducing kernels, highlighting the use of the complex gaussian kernel. In order to derive gradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool of Wirtinger's Calculus, which has recently attracted attention in the signal processing community. To this end, in this paper, the notion of Wirtinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive several realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS offers significant performance improvements over several linear and nonlinear algorithms, when dealing with nonlinearities.
cs.LG:This paper studies the MINLIP estimator for the identification of Wiener systems consisting of a sequence of a linear FIR dynamical model, and a monotonically increasing (or decreasing) static function. Given $T$ observations, this algorithm boils down to solving a convex quadratic program with $O(T)$ variables and inequality constraints, implementing an inference technique which is based entirely on model complexity control. The resulting estimates of the linear submodel are found to be almost consistent when no noise is present in the data, under a condition of smoothness of the true nonlinearity and local Persistency of Excitation (local PE) of the data. This result is novel as it does not rely on classical tools as a 'linearization' using a Taylor decomposition, nor exploits stochastic properties of the data. It is indicated how to extend the method to cope with noisy data, and empirical evidence contrasts performance of the estimator against other recently proposed techniques.
cs.LG:In response to a 1997 problem of M. Vidyasagar, we state a necessary and sufficient condition for distribution-free PAC learnability of a concept class $\mathscr C$ under the family of all non-atomic (diffuse) measures on the domain $\Omega$. Clearly, finiteness of the classical Vapnik-Chervonenkis dimension of $\mathscr C$ is a sufficient, but no longer necessary, condition. Besides, learnability of $\mathscr C$ under non-atomic measures does not imply the uniform Glivenko-Cantelli property with regard to non-atomic measures. Our learnability criterion is stated in terms of a combinatorial parameter $\VC({\mathscr C}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$ modulo countable sets. The new parameter is obtained by ``thickening up'' single points in the definition of VC dimension to uncountable ``clusters''. Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if every countable subclass of $\mathscr C$ has VC dimension $\leq d$ outside a countable subset of $\Omega$. The new parameter can be also expressed as the classical VC dimension of $\mathscr C$ calculated on a suitable subset of a compactification of $\Omega$. We do not make any measurability assumptions on $\mathscr C$, assuming instead the validity of Martin's Axiom (MA).
cs.LG:We present a new latent-variable model employing a Gaussian mixture integrated with a feature selection procedure (the Bernoulli part of the model) which together form a "Latent Bernoulli-Gauss" distribution. The model is applied to MAP estimation, clustering, feature selection and collaborative filtering and fares favorably with the state-of-the-art latent-variable models.
cs.LG:We address in this paper the problem of multi-channel signal sequence labeling. In particular, we consider the problem where the signals are contaminated by noise or may present some dephasing with respect to their labels. For that, we propose to jointly learn a SVM sample classifier with a temporal filtering of the channels. This will lead to a large margin filtering that is adapted to the specificity of each channel (noise and time-lag). We derive algorithms to solve the optimization problem and we discuss different filter regularizations for automated scaling or selection of channels. Our approach is tested on a non-linear toy example and on a BCI dataset. Results show that the classification performance on these problems can be improved by learning a large margin filtering.
cs.LG:We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a non-recursive function, etc. We further observe that Sontag's ANN is not Glivenko-Cantelli under any input distribution having a non-atomic part.
cs.LG:This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.
cs.LG:Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query `independence day' usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.
cs.LG:Recently, applying the novel data mining techniques for evaluating enterprise financial distress has received much research alternation. Support Vector Machine (SVM) and back propagation neural (BPN) network has been applied successfully in many areas with excellent generalization results, such as rule extraction, classification and evaluation. In this paper, a model based on SVM with Gaussian RBF kernel is proposed here for enterprise financial distress evaluation. BPN network is considered one of the simplest and are most general methods used for supervised training of multilayered neural network. The comparative results show that through the difference between the performance measures is marginal; SVM gives higher precision and lower error rates.
cs.LG:Most image-search approaches today are based on the text based tags associated with the images which are mostly human generated and are subject to various kinds of errors. The results of a query to the image database thus can often be misleading and may not satisfy the requirements of the user. In this work we propose our approach to automate this tagging process of images, where image results generated can be fine filtered based on a probabilistic tagging mechanism. We implement a tool which helps to automate the tagging process by maintaining a training database, wherein the system is trained to identify certain set of input images, the results generated from which are used to create a probabilistic tagging mechanism. Given a certain set of segments in an image it calculates the probability of presence of particular keywords. This probability table is further used to generate the candidate tags for input images.
cs.LG:We present a framework for discriminative sequence classification where the learner works directly in the high dimensional predictor space of all subsequences in the training set. This is possible by employing a new coordinate-descent algorithm coupled with bounding the magnitude of the gradient for selecting discriminative subsequences fast. We characterize the loss functions for which our generic learning algorithm can be applied and present concrete implementations for logistic regression (binomial log-likelihood loss) and support vector machines (squared hinge loss). Application of our algorithm to protein remote homology detection and remote fold recognition results in performance comparable to that of state-of-the-art methods (e.g., kernel support vector machines). Unlike state-of-the-art classifiers, the resulting classification models are simply lists of weighted discriminative subsequences and can thus be interpreted and related to the biological problem.
cs.LG:We present three generalisations of Kernel Principal Components Analysis (KPCA) which incorporate knowledge of the class labels of a subset of the data points. The first, MV-KPCA, penalises within class variances similar to Fisher discriminant analysis. The second, LSKPCA is a hybrid of least squares regression and kernel PCA. The final LR-KPCA is an iteratively reweighted version of the previous which achieves a sigmoid loss function on the labeled points. We provide a theoretical risk bound as well as illustrative experiments on real and toy data sets.
cs.LG:In this paper the sequential prediction problem with expert advice is considered for the case where losses of experts suffered at each step cannot be bounded in advance. We present some modification of Kalai and Vempala algorithm of following the perturbed leader where weights depend on past losses of the experts. New notions of a volume and a scaled fluctuation of a game are introduced. We present a probabilistic algorithm protected from unrestrictedly large one-step losses. This algorithm has the optimal performance in the case when the scaled fluctuations of one-step losses of experts of the pool tend to zero.
cs.LG:In prediction with expert advice the goal is to design online prediction algorithms that achieve small regret (additional loss on the whole data) compared to a reference scheme. In the simplest such scheme one compares to the loss of the best expert in hindsight. A more ambitious goal is to split the data into segments and compare to the best expert on each segment. This is appropriate if the nature of the data changes between segments. The standard fixed-share algorithm is fast and achieves small regret compared to this scheme.   Fixed share treats the experts as black boxes: there are no assumptions about how they generate their predictions. But if the experts are learning, the following question arises: should the experts learn from all data or only from data in their own segment? The original algorithm naturally addresses the first case. Here we consider the second option, which is more appropriate exactly when the nature of the data changes between segments. In general extending fixed share to this second case will slow it down by a factor of T on T outcomes. We show, however, that no such slowdown is necessary if the experts are hidden Markov models.
cs.LG:A problem posed by Freund is how to efficiently track a small pool of experts out of a much larger set. This problem was solved when Bousquet and Warmuth introduced their mixing past posteriors (MPP) algorithm in 2001.   In Freund's problem the experts would normally be considered black boxes. However, in this paper we re-examine Freund's problem in case the experts have internal structure that enables them to learn. In this case the problem has two possible interpretations: should the experts learn from all data or only from the subsequence on which they are being tracked? The MPP algorithm solves the first case. Our contribution is to generalise MPP to address the second option. The results we obtain apply to any expert structure that can be formalised using (expert) hidden Markov models. Curiously enough, for our interpretation there are \emph{two} natural reference schemes: freezing and sleeping. For each scheme, we provide an efficient prediction strategy and prove the relevant loss bound.
cs.LG:We propose a novel feature selection strategy to discover language-independent acoustic features that tend to be responsible for emotions regardless of languages, linguistics and other factors. Experimental results suggest that the language-independent feature subset discovered yields the performance comparable to the full feature set on various emotional speech corpora.
cs.LG:The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results demonstrate the efficiency and effectiveness of the proposed algorithm.
cs.LG:This paper introduces an approach to Reinforcement Learning Algorithm by comparing their immediate rewards using a variation of Q-Learning algorithm. Unlike the conventional Q-Learning, the proposed algorithm compares current reward with immediate reward of past move and work accordingly. Relative reward based Q-learning is an approach towards interactive learning. Q-Learning is a model free reinforcement learning method that used to learn the agents. It is observed that under normal circumstances algorithm take more episodes to reach optimal Q-value due to its normal reward or sometime negative reward. In this new form of algorithm agents select only those actions which have a higher immediate reward signal in comparison to previous one. The contribution of this article is the presentation of new Q-Learning Algorithm in order to maximize the performance of algorithm and reduce the number of episode required to reach optimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20 Grid world deterministic environment and the result for the two forms of Q-Learning Algorithms is given.
cs.LG:We study three families of online convex optimization algorithms: follow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual averaging (RDA), and composite-objective mirror descent. We first prove equivalence theorems that show all of these algorithms are instantiations of a general FTRL update. This provides theoretical insight on previous experimental observations. In particular, even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly, it has been observed that RDA is even more effective at producing sparsity. Our results demonstrate that FOBOS uses subgradient approximations to the L1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two, and outperforms both on a large, real-world dataset.   Our second contribution is a unified analysis which produces regret bounds that match (up to logarithmic terms) or improve the best previously known bounds. This analysis also extends these algorithms in two important ways: we support a more general type of composite objective and we analyze implicit updates, which replace the subgradient approximation of the current loss function with an exact optimization.
cs.LG:We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels - specifically, the gap in per observation probabilities between the most likely labels. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs.   We demonstrate empirically that the hybrid loss typically performs as least as well as - and often better than - both of its constituent losses on variety of tasks. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction and the effects of label dominance on these results.
cs.LG:In this paper we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in a top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy to assess the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node, are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree.
cs.LG:Margin theory provides one of the most popular explanations to the success of \texttt{AdaBoost}, where the central point lies in the recognition that \textit{margin} is the key for characterizing the performance of \texttt{AdaBoost}. This theory has been very influential, e.g., it has been used to argue that \texttt{AdaBoost} usually does not overfit since it tends to enlarge the margin even after the training error reaches zero. Previously the \textit{minimum margin bound} was established for \texttt{AdaBoost}, however, \cite{Breiman1999} pointed out that maximizing the minimum margin does not necessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006} emphasized that the margin distribution rather than minimum margin is crucial to the performance of \texttt{AdaBoost}. In this paper, we first present the \textit{$k$th margin bound} and further study on its relationship to previous work such as the minimum margin bound and Emargin bound. Then, we improve the previous empirical Bernstein bounds \citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on such findings, we defend the margin-based explanation against Breiman's doubts by proving a new generalization error bound that considers exactly the same factors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than \cite{Breiman1999}'s minimum margin bound. By incorporating factors such as average margin and variance, we present a generalization error bound that is heavily related to the whole margin distribution. We also provide margin distribution bounds for generalization error of voting classifiers in finite VC-dimension space.
cs.LG:In this work, we propose a new optimization framework for multiclass boosting learning. In the literature, AdaBoost.MO and AdaBoost.ECC are the two successful multiclass boosting algorithms, which can use binary weak learners. We explicitly derive these two algorithms' Lagrange dual problems based on their regularized loss functions. We show that the Lagrange dual formulations enable us to design totally-corrective multiclass algorithms by using the primal-dual optimization technique. Experiments on benchmark data sets suggest that our multiclass boosting can achieve a comparable generalization capability with state-of-the-art, but the convergence speed is much faster than stage-wise gradient descent boosting. In other words, the new totally corrective algorithms can maximize the margin more aggressively.
cs.LG:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for empirical risk minimization with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n}, this translates to a learning rate of O(RH/n) in the separable (L*=0) case and O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization with a smooth non-negative objective.
cs.LG:Sparse learning has recently received increasing attention in many areas including machine learning, statistics, and applied mathematics. The mixed-norm regularization based on the L1/Lq norm with q > 1 is attractive in many applications of regression and classification in that it facilitates group sparsity in the model. The resulting optimization problem is, however, challenging to solve due to the structure of the L1/Lq -regularization. Existing work deals with special cases including q = 2,infinity, and they cannot be easily extended to the general case. In this paper, we propose an efficient algorithm based on the accelerated gradient method for solving the L1/Lq -regularized problem, which is applicable for all values of q larger than 1, thus significantly extending existing work. One key building block of the proposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our theoretical analysis reveals the key properties of EP1q and illustrates why EP1q for the general q is significantly more challenging to solve than the special cases. Based on our theoretical analysis, we develop an efficient algorithm for EP1q by solving two zero finding problems. Experimental results demonstrate the efficiency of the proposed algorithm.
cs.LG:An instance-weighted variant of the support vector machine (SVM) has attracted considerable attention recently since they are useful in various machine learning tasks such as non-stationary data analysis, heteroscedastic data modeling, transfer learning, learning to rank, and transduction. An important challenge in these scenarios is to overcome the computational bottleneck---instance weights often change dynamically or adaptively, and thus the weighted SVM solutions must be repeatedly computed. In this paper, we develop an algorithm that can efficiently and exactly update the weighted SVM solutions for arbitrary change of instance weights. Technically, this contribution can be regarded as an extension of the conventional solution-path algorithm for a single regularization parameter to multiple instance-weight parameters. However, this extension gives rise to a significant problem that breakpoints (at which the solution path turns) have to be identified in high-dimensional space. To facilitate this, we introduce a parametric representation of instance weights. We also provide a geometric interpretation in weight space using a notion of critical region: a polyhedron in which the current affine solution remains to be optimal. Then we find breakpoints at intersections of the solution path and boundaries of polyhedrons. Through extensive experiments on various practical applications, we demonstrate the usefulness of the proposed algorithm.
cs.LG:Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the model's estimate of the objective and the uncertainty at any given point. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm's performance.
cs.LG:We consider the problem of energy-efficient point-to-point transmission of delay-sensitive data (e.g. multimedia data) over a fading channel. Existing research on this topic utilizes either physical-layer centric solutions, namely power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM); however, there is currently no rigorous and unified framework for simultaneously utilizing both physical-layer centric and system-level techniques to achieve the minimum possible energy consumption, under delay constraints, in the presence of stochastic and a priori unknown traffic and channel conditions. In this report, we propose such a framework. We formulate the stochastic optimization problem as a Markov decision process (MDP) and solve it online using reinforcement learning. The advantages of the proposed online method are that (i) it does not require a priori knowledge of the traffic arrival and channel statistics to determine the jointly optimal power-control, AMC, and DPM policies; (ii) it exploits partial information about the system so that less information needs to be learned than when using conventional reinforcement learning algorithms; and (iii) it obviates the need for action exploration, which severely limits the adaptation speed and run-time performance of conventional reinforcement learning algorithms. Our results show that the proposed learning algorithms can converge up to two orders of magnitude faster than a state-of-the-art learning algorithm for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms.
cs.LG:We propose a focus of attention mechanism to speed up the Perceptron algorithm. Focus of attention speeds up the Perceptron algorithm by lowering the number of features evaluated throughout training and prediction. Whereas the traditional Perceptron evaluates all the features of each example, the Attentive Perceptron evaluates less features for easy to classify examples, thereby achieving significant speedups and small losses in prediction accuracy. Focus of attention allows the Attentive Perceptron to stop the evaluation of features at any interim point and filter the example. This creates an attentive filter which concentrates computation at examples that are hard to classify, and quickly filters examples that are easy to classify.
cs.LG:In this paper, we consider a queue-aware distributive resource control algorithm for two-hop MIMO cooperative systems. We shall illustrate that relay buffering is an effective way to reduce the intrinsic half-duplex penalty in cooperative systems. The complex interactions of the queues at the source node and the relays are modeled as an average-cost infinite horizon Markov Decision Process (MDP). The traditional approach solving this MDP problem involves centralized control with huge complexity. To obtain a distributive and low complexity solution, we introduce a linear structure which approximates the value function of the associated Bellman equation by the sum of per-node value functions. We derive a distributive two-stage two-winner auction-based control policy which is a function of the local CSI and local QSI only. Furthermore, to estimate the best fit approximation parameter, we propose a distributive online stochastic learning algorithm using stochastic approximation theory. Finally, we establish technical conditions for almost-sure convergence and show that under heavy traffic, the proposed low complexity distributive control is global optimal.
cs.LG:To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.
cs.LG:This paper describes algorithms for nonnegative matrix factorization (NMF) with the beta-divergence (beta-NMF). The beta-divergence is a family of cost functions parametrized by a single shape parameter beta that takes the Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito divergence as special cases (beta = 2,1,0, respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization (MM) algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a beta-dependent power exponent. The monotonicity of the heuristic algorithm can however be proven for beta in (0,1) using the proposed auxiliary function. Then we introduce the concept of majorization-equalization (ME) algorithm which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate the faster convergence of the ME approach. The paper also describes how the proposed algorithms can be adapted to two common variants of NMF : penalized NMF (i.e., when a penalty function of the factors is added to the criterion function) and convex-NMF (when the dictionary is assumed to belong to a known subspace).
cs.LG:Hardness results for maximum agreement problems have close connections to hardness results for proper learning in computational learning theory. In this paper we prove two hardness results for the problem of finding a low degree polynomial threshold function (PTF) which has the maximum possible agreement with a given set of labeled examples in $\R^n \times \{-1,1\}.$ We prove that for any constants $d\geq 1, \eps > 0$,   {itemize}   Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a degree-$d$ PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a degree-$d$ PTF that is consistent with a $1-\eps$ fraction of the examples.   It is $\NP$-hard to find a degree-2 PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a halfspace (degree-1 PTF) that is consistent with a $1 - \eps$ fraction of the examples.   {itemize}   These results immediately imply the following hardness of learning results: (i) Assuming the Unique Games Conjecture, there is no better-than-trivial proper learning algorithm that agnostically learns degree-$d$ PTFs under arbitrary distributions; (ii) There is no better-than-trivial learning algorithm that outputs degree-2 PTFs and agnostically learns halfspaces (i.e. degree-1 PTFs) under arbitrary distributions.
cs.LG:A general framework based on Gaussian models and a MAP-EM algorithm is introduced in this paper for solving matrix/table completion problems. The numerical experiments with the standard and challenging movie ratings data show that the proposed approach, based on probably one of the simplest probabilistic models, leads to the results in the same ballpark as the state-of-the-art, at a lower computational cost.
cs.LG:This paper considers the clustering problem for large data sets. We propose an approach based on distributed optimization. The clustering problem is formulated as an optimization problem of maximizing the classification gain. We show that the optimization problem can be reformulated and decomposed into small-scale sub optimization problems by using the Dantzig-Wolfe decomposition method. Generally speaking, the Dantzig-Wolfe method can only be used for convex optimization problems, where the duality gaps are zero. Even though, the considered optimization problem in this paper is non-convex, we prove that the duality gap goes to zero, as the problem size goes to infinity. Therefore, the Dantzig-Wolfe method can be applied here. In the proposed approach, the clustering problem is iteratively solved by a group of computers coordinated by one center processor, where each computer solves one independent small-scale sub optimization problem during each iteration, and only a small amount of data communication is needed between the computers and center processor. Numerical results show that the proposed approach is effective and efficient.
cs.LG:We give sublinear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L2-SVM, for which sublinear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sublinear time algorithms achieving arbitrary approximation factor.
cs.LG:Nesterov's accelerated gradient methods (AGM) have been successfully applied in many machine learning areas. However, their empirical performance on training max-margin models has been inferior to existing specialized solvers. In this paper, we first extend AGM to strongly convex and composite objective functions with Bregman style prox-functions. Our unifying framework covers both the $\infty$-memory and 1-memory styles of AGM, tunes the Lipschiz constant adaptively, and bounds the duality gap. Then we demonstrate various ways to apply this framework of methods to a wide range of machine learning problems. Emphasis will be given on their rate of convergence and how to efficiently compute the gradient and optimize the models. The experimental results show that with our extensions AGM outperforms state-of-the-art solvers on max-margin models.
cs.LG:An importance weight quantifies the relative importance of one example over another, coming up in applications of boosting, asymmetric classification costs, reductions, and active learning. The standard approach for dealing with importance weights in gradient descent is via multiplication of the gradient. We first demonstrate the problems of this approach when importance weights are large, and argue in favor of more sophisticated ways for dealing with them. We then develop an approach which enjoys an invariance property: that updating twice with importance weight $h$ is equivalent to updating once with importance weight $2h$. For many important losses this has a closed form update which satisfies standard regret guarantees when all examples have $h=1$. We also briefly discuss two other reasonable approaches for handling large importance weights. Empirically, these approaches yield substantially superior prediction with similar computational performance while reducing the sensitivity of the algorithm to the exact setting of the learning rate. We apply these to online active learning yielding an extraordinarily fast active learning algorithm that works even in the presence of adversarial noise.
cs.LG:The note presents a modified proof of a loss bound for the exponentially weighted average forecaster with time-varying potential. The regret term of the algorithm is upper-bounded by sqrt{n ln(N)} (uniformly in n), where N is the number of experts and n is the number of steps.
cs.LG:Recommendation systems are emerging as an important business application with significant economic impact. Currently popular systems include Amazon's book recommendations, Netflix's movie recommendations, and Pandora's music recommendations. In this paper we address the problem of estimating probabilities associated with recommendation system data using non-parametric kernel smoothing. In our estimation we interpret missing items as randomly censored observations and obtain efficient computation schemes using combinatorial properties of generating functions. We demonstrate our approach with several case studies involving real world movie recommendation data. The results are comparable with state-of-the-art techniques while also providing probabilistic preference estimates outside the scope of traditional recommender systems.
cs.LG:We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.
cs.LG:In this paper, we propose a two-timescale delay-optimal dynamic clustering and power allocation design for downlink network MIMO systems. The dynamic clustering control is adaptive to the global queue state information (GQSI) only and computed at the base station controller (BSC) over a longer time scale. On the other hand, the power allocations of all the BSs in one cluster are adaptive to both intra-cluster channel state information (CCSI) and intra-cluster queue state information (CQSI), and computed at the cluster manager (CM) over a shorter time scale. We show that the two-timescale delay-optimal control can be formulated as an infinite-horizon average cost Constrained Partially Observed Markov Decision Process (CPOMDP). By exploiting the special problem structure, we shall derive an equivalent Bellman equation in terms of Pattern Selection Q-factor to solve the CPOMDP. To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the Pattern Selection Q-factor by the sum of Per-cluster Potential functions and propose a novel distributive online learning algorithm to estimate the Per-cluster Potential functions (at each CM) as well as the Lagrange multipliers (LM) (at each BS). We show that the proposed distributive online learning algorithm converges almost surely (with probability 1). By exploiting the birth-death structure of the queue dynamics, we further decompose the Per-cluster Potential function into sum of Per-cluster Per-user Potential functions and formulate the instantaneous power allocation as a Per-stage QSI-aware Interference Game played among all the CMs. We also propose a QSI-aware Simultaneous Iterative Water-filling Algorithm (QSIWFA) and show that it can achieve the Nash Equilibrium (NE).
cs.LG:To attain the best learning accuracy, people move on with difficulties and frustrations. Though one can optimize the empirical objective using a given set of samples, its generalization ability to the entire sample distribution remains questionable. Even if a fair generalization guarantee is offered, one still wants to know what is to happen if the regularizer is removed, and/or how well the artificial loss (like the hinge loss) relates to the accuracy.   For such reason, this report surveys four different trials towards the learning accuracy, embracing the major advances in supervised learning theory in the past four years. Starting from the generic setting of learning, the first two trials introduce the best optimization and generalization bounds for convex learning, and the third trial gets rid of the regularizer. As an innovative attempt, the fourth trial studies the optimization when the objective is exactly the accuracy, in the special case of binary classification. This report also analyzes the last trial through experiments.
cs.LG:This report explores the use of machine learning techniques to accurately predict travel times in city streets and highways using floating car data (location information of user vehicles on a road network). The aim of this report is twofold, first we present a general architecture of solving this problem, then present and evaluate few techniques on real floating car data gathered over a month on a 5 Km highway in New Delhi.
cs.LG:This article discusses in detail the rating system that won the kaggle competition "Chess Ratings: Elo vs the rest of the world". The competition provided a historical dataset of outcomes for chess games, and aimed to discover whether novel approaches can predict the outcomes of future games, more accurately than the well-known Elo rating system. The winning rating system, called Elo++ in the rest of the article, builds upon the Elo rating system. Like Elo, Elo++ uses a single rating per player and predicts the outcome of a game, by using a logistic curve over the difference in ratings of the players. The major component of Elo++ is a regularization technique that avoids overfitting these ratings. The dataset of chess games and outcomes is relatively small and one has to be careful not to draw "too many conclusions" out of the limited data. Many approaches tested in the competition showed signs of such an overfitting. The leader-board was dominated by attempts that did a very good job on a small test dataset, but couldn't generalize well on the private hold-out dataset. The Elo++ regularization takes into account the number of games per player, the recency of these games and the ratings of the opponents. Finally, Elo++ employs a stochastic gradient descent scheme for training the ratings, and uses only two global parameters (white's advantage and regularization constant) that are optimized using cross-validation.
cs.LG:It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet from a regularization perspective: the EigenNet has an adaptive eigenspace-based composite regularizer, which naturally generalizes the $l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and real data show that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.
cs.LG:Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.
cs.LG:We consider the problem of learning an unknown product distribution $X$ over $\{0,1\}^n$ using samples $f(X)$ where $f$ is a \emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework.   Information-theoretic arguments show that for every transformation function $f$ the corresponding learning problem can be solved to accuracy $\eps$, using $\tilde{O}(n/\eps^2)$ examples, by a generic algorithm whose running time may be exponential in $n.$ We show that this learning problem can be computationally intractable even for constant $\eps$ and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function $f(x)=w \cdot x$ with integer weights $w_i \leq n$ and prove that the corresponding learning problem requires $\Omega(n)$ samples.   As our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function $f(x)= \sum_{i=1}^n x_i$. Our algorithm learns to $\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\eps)$ number of samples that is independent of $n.$ We also give an efficient algorithm that uses $\log n \cdot \poly(1/\eps)$ samples but has running time that is only $\poly(\log n, 1/\eps).$
cs.LG:Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a two-layer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-of-the-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM$^{perf}$ in terms of $F_1$-score.
cs.LG:In this work we study parallelization of online learning, a core primitive in machine learning. In a parallel environment all known approaches for parallel online learning lead to delayed updates, where the model is updated using out-of-date information. In the worst case, or when examples are temporally correlated, delay can have a very adverse effect on the learning algorithm. Here, we analyze and present preliminary empirical results on a set of learning architectures based on a feature sharding approach that present various tradeoffs between delay, degree of parallelism, representation power and empirical performance.
cs.LG:Supervised learning is all about the ability to generalize knowledge. Specifically, the goal of the learning is to train a classifier using training data, in such a way that it will be capable of classifying new unseen data correctly. In order to acheive this goal, it is important to carefully design the learner, so it will not overfit the training data. The later can is done usually by adding a regularization term. The statistical learning theory explains the success of this method by claiming that it restricts the complexity of the learned model. This explanation, however, is rather abstract and does not have a geometric intuition. The generalization error of a classifier may be thought of as correlated with its robustness to perturbations of the data: a classifier that copes with disturbance is expected to generalize well. Indeed, Xu et al. [2009] have shown that the SVM formulation is equivalent to a robust optimization (RO) formulation, in which an adversary displaces the training and testing points within a ball of pre-determined radius. In this work we explore a different kind of robustness, namely changing each data point with a Gaussian cloud centered at the sample. Loss is evaluated as the expectation of an underlying loss function on the cloud. This setup fits the fact that in many applications, the data is sampled along with noise. We develop an RO framework, in which the adversary chooses the covariance of the noise. In our algorithm named GURU, the tuning parameter is a spectral bound on the noise, thus it can be estimated using physical or applicative considerations. Our experiments show that this framework performs as well as SVM and even slightly better in some cases. Generalizations for Mercer kernels and for the multiclass case are presented as well. We also show that our framework may be further generalized, using the technique of convex perspective functions.
cs.LG:We propose a new clustering technique that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.
cs.LG:A fundamental result of statistical learnig theory states that a concept class is PAC learnable if and only if it is a uniform Glivenko-Cantelli class if and only if the VC dimension of the class is finite. However, the theorem is only valid under special assumptions of measurability of the class, in which case the PAC learnability even becomes consistent. Otherwise, there is a classical example, constructed under the Continuum Hypothesis by Dudley and Durst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a concept class of VC dimension one which is neither uniform Glivenko-Cantelli nor consistently PAC learnable. We show that, rather surprisingly, under an additional set-theoretic hypothesis which is much milder than the Continuum Hypothesis (Martin's Axiom), PAC learnability is equivalent to finite VC dimension for every concept class.
cs.LG:Q-learning is a reliable but inefficient off-policy temporal-difference method, backing up reward only one step at a time. Replacing traces, using a recency heuristic, are more efficient but less reliable. In this work, we introduce model-free, off-policy temporal difference methods that make better use of experience than Watkins' Q(\lambda). We introduce both Optimistic Q(\lambda) and the temporal second difference trace (TSDT). TSDT is particularly powerful in deterministic domains. TSDT uses neither recency nor frequency heuristics, storing (s,a,r,s',\delta) so that off-policy updates can be performed after apparently suboptimal actions have been taken. There are additional advantages when using state abstraction, as in MAXQ. We demonstrate that TSDT does significantly better than both Q-learning and Watkins' Q(\lambda) in a deterministic cliff-walking domain. Results in a noisy cliff-walking domain are less advantageous for TSDT, but demonstrate the efficacy of Optimistic Q(\lambda), a replacing trace with some of the advantages of TSDT.
cs.LG:In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.
cs.LG:In this paper we present methods for attacking and defending $k$-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior's or process' $k$-order statistics to build a stochastic process that has those same $k$-order stationary statistics but possesses different, deliberately designed, $(k+1)$-order statistics if desired. Such a model realizes a "complexification" of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed $(k+1)$-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the $k$-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an {\em arms race} in the sense that the advantage goes to the party that has more computing resources applied to the problem.
cs.LG:Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing $x^*$ that minimizes the expression $\sum_{f\in F}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where we wish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$ corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$, and we seek a shape $x$ that minimizes the sum of distances from each point in $P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and $f(x)$ is the distance from $p$ to its closest shape in $x$.   Our main result is a unified framework for constructing {\em coresets} and {\em approximate clustering} for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of $\varepsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation" of the input set $F$. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model).   We show how to generalize the results of our framework for squared distances (as in $k$-mean), distances to the $q$th power, and deterministic constructions.
cs.LG:The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.
cs.LG:This paper introduces a machine learning based collaborative multi-band spectrum sensing policy for cognitive radios. The proposed sensing policy guides secondary users to focus the search of unused radio spectrum to those frequencies that persistently provide them high data rate. The proposed policy is based on machine learning, which makes it adaptive with the temporally and spatially varying radio spectrum. Furthermore, there is no need for dynamic modeling of the primary activity since it is implicitly learned over time. Energy efficiency is achieved by minimizing the number of assigned sensors per each subband under a constraint on miss detection probability. It is important to control the missed detections because they cause collisions with primary transmissions and lead to retransmissions at both the primary and secondary user. Simulations show that the proposed machine learning based sensing policy improves the overall throughput of the secondary network and improves the energy efficiency while controlling the miss detection probability.
cs.LG:This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. In particular, we observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure between the observed variables - separating out the spurious interactions caused by the (marginalizing out of the) latent variables' time series. We develop a new method, based on convex optimization, to do so in the case when the number of latent variables is smaller than the number of observed ones. For the case when the dependency structure between the observed variables is sparse, we theoretically establish a high-dimensional scaling result for structure recovery. We verify our theoretical result with both synthetic and real data (from the stock market).
cs.LG:Estimator algorithms in learning automata are useful tools for adaptive, real-time optimization in computer science and engineering applications. This paper investigates theoretical convergence properties for a special case of estimator algorithms: the pursuit learning algorithm. In this note, we identify and fill a gap in existing proofs of probabilistic convergence for pursuit learning. It is tradition to take the pursuit learning tuning parameter to be fixed in practical applications, but our proof sheds light on the importance of a vanishing sequence of tuning parameters in a theoretical convergence analysis.
cs.LG:One of the major challenges of ECoG-based Brain-Machine Interfaces is the movement prediction of a human subject. Several methods exist to predict an arm 2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is to predict individual finger movements (5-D trajectory). The difficulty lies in the fact that there is no simple relation between ECoG signals and finger movement. We propose in this paper to decode finger flexions using switching models. This method permits to simplify the system as it is now described as an ensemble of linear models depending on an internal state. We show that an interesting accuracy prediction can be obtained by such a model.
cs.LG:Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.
cs.LG:This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using epsilon-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.
cs.LG:Motivated by the amount of code that goes unidentified on the web, we introduce a practical method for algorithmically identifying the programming language of source code. Our work is based on supervised learning and intelligent statistical features. We also explored, but abandoned, a grammatical approach. In testing, our implementation greatly outperforms that of an existing tool that relies on a Bayesian classifier. Code is written in Python and available under an MIT license.
cs.LG:Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.
cs.LG:Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.
