cs.AI:Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.
cs.AI:Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.
cs.AI:We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.
cs.AI:As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.
cs.AI:To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime.
cs.AI:Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.
cs.AI:A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.
cs.AI:Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.
cs.AI:The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.
cs.AI:The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.
cs.AI:We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.
cs.AI:This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.
cs.AI:In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.
cs.AI:Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,...,N} that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.
cs.AI:Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.
cs.AI:This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.
cs.AI:This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.
cs.AI:The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.
cs.AI:This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.
cs.AI:For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.
cs.AI:Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k ``classes''). The definition is acquired by studying collections of training examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.
cs.AI:The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.
cs.AI:Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda &gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.
cs.AI:This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search.
cs.AI:Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.
cs.AI:Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n^2d^2) complexity (instead of O(n^3d^3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.
cs.AI:We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.
cs.AI:Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.
cs.AI:We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional ``basecase'' oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.
cs.AI:In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.
cs.AI:There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the "best" possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.
cs.AI:This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).
cs.AI:ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.
cs.AI:Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.
cs.AI:Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.
cs.AI:This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.
cs.AI:Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset.
cs.AI:Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a ``measure of goodness'' or ``membership value'' with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.
cs.AI:This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.
cs.AI:OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.
cs.AI:The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.
cs.AI:In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.
cs.AI:We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.
cs.AI:Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.
cs.AI:This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.
cs.AI:We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.
cs.AI:Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.
cs.AI:The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.
cs.AI:Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.
cs.AI:We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.
cs.AI:We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.
cs.AI:A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.
cs.AI:For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.
cs.AI:Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a ``difference matching'' procedure. The critic then proposes lemmas and generalizations which ``ripple'' these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.
cs.AI:Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.
cs.AI:Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.
cs.AI:This paper presents new experimental evidence against the utility of Occam's razor. A~systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.
cs.AI:The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.
cs.AI:This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
cs.AI:Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.
cs.AI:Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.
cs.AI:A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.
cs.AI:An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.
cs.AI:Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.
cs.AI:Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods. We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines.
cs.AI:Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Omega_1,Omega_2,..., with the following properties: first, Omega_1 is the class of all stratified knowledge bases; second, if a knowledge base Pi is in Omega_k, then Pi has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find the minimum k such that Pi belongs to Omega_k in time polynomial in the size of Pi; and, last, where K is the class of all knowledge bases, it is the case that union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some class in the hierarchy.
cs.AI:We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by UCPOP to select plans for refinement. The other is based on preferring ``zero commitment'' (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of UCPOP, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default UCPOP search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices.
cs.AI:Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.
cs.AI:This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol (UNP) is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals. Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one 'strict', the other 'tolerant', and analyze their affects on the stability and efficiency of negotiation outcomes.
cs.AI:First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization.
cs.AI:This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP.
cs.AI:A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as ``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.
cs.AI:Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a personUs name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains.
cs.AI:Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data.
cs.AI:Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.
cs.AI:Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken- language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.
cs.AI:Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic muALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualified number restrictions, muALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of muALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus.
cs.AI:We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the Toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment.
cs.AI:We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks (we show how they can be generated using clustering and conditioning algorithms). The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching.
cs.AI:An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the REGENT algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.
cs.AI:Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.
cs.AI:We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total.
cs.AI:The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions.
cs.AI:This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time.
cs.AI:Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.
cs.AI:Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.
cs.AI:SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.
cs.AI:Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.
cs.AI:Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.
cs.AI:The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback, while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.
cs.AI:Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms.
cs.AI:The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered.
cs.AI:Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.
cs.AI:An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.
cs.AI:Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.
cs.AI:This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.
cs.AI:In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.
cs.AI:In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.
cs.AI:This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.
cs.AI:One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.
cs.AI:We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the process of proving that certain planning problems are complete for NP^PP, we introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.
cs.AI:In this paper we describe SYNERGY, which is a highly parallelizable, linear planning system that is based on the genetic programming paradigm. Rather than reasoning about the world it is planning for, SYNERGY uses artificial selection, recombination and fitness measure to generate linear plans that solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase problem and a few variants of the robot navigation problem), and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by UCPOP. In order to facilitate the search reduction and to enhance the expressive power of SYNERGY, we also propose two major extensions to our planning system: a formalism for using hierarchical planning operators, and a framework for planning in dynamic environments.
cs.AI:We show that several constraint propagation algorithms (also called (local) consistency, consistency enforcing, Waltz, filtering or narrowing algorithms) are instances of algorithms that deal with chaotic iteration. To this end we propose a simple abstract framework that allows us to classify and compare these algorithms and to establish in a uniform way their basic properties.
cs.AI:This paper examines the phenomenon of daydreaming: spontaneously recalling or imagining personal or vicarious experiences in the past or future. The following important roles of daydreaming in human cognition are postulated: plan preparation and rehearsal, learning from failures and successes, support for processes of creativity, emotion regulation, and motivation.   A computational theory of daydreaming and its implementation as the program DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based on relaxed planning, 2) a dynamic episodic memory of experiences used by the scenario generator, 3) a collection of personal goals and control goals which guide the scenario generator, 4) an emotion component in which daydreams initiate, and are initiated by, emotional states arising from goal outcomes, and 5) domain knowledge of interpersonal relations and common everyday occurrences.   The role of emotions and control goals in daydreaming is discussed. Four control goals commonly used in guiding daydreaming are presented: rationalization, failure/success reversal, revenge, and preparation. The role of episodic memory in daydreaming is considered, including how daydreamed information is incorporated into memory and later used. An initial version of DAYDREAMER which produces several daydreams (in English) is currently running.
cs.AI:Real world combinatorial optimization problems such as scheduling are typically too complex to solve with exact methods. Additionally, the problems often have to observe vaguely specified constraints of different importance, the available data may be uncertain, and compromises between antagonistic criteria may be necessary. We present a combination of approximate reasoning based constraints and iterative optimization based heuristics that help to model and solve such problems in a framework of C++ software libraries called StarFLIP++. While initially developed to schedule continuous caster units in steel plants, we present in this paper results from reusing the library components in a shift scheduling system for the workforce of an industrial production plant.
cs.AI:The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman & Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.
cs.AI:How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) "iconic representations," which are analogs of the proximal sensory projections of distal objects and events, and (2) "categorical representations," which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) "symbolic representations," grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., "An X is a Y that is Z").
cs.AI:In tree search problem the best-first search algorithm needs too much of space . To remove such drawbacks of these algorithms the IDA* was developed which is both space and time cost efficient. But again IDA* can give an optimal solution for real valued problems like Flow shop scheduling, Travelling Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further modifications are done on it and the Iterative Deepening Branch and Bound Search Algorithms is developed which meets the requirements. We have tried using this algorithm for the Flow Shop Scheduling Problem and have found that it is quite effective.
cs.AI:Agents are small programs that autonomously take actions based on changes in their environment or ``state.'' Over the last few years, there have been an increasing number of efforts to build agents that can interact and/or collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top of legacy code. However, their framework assumes that agent states are completely determined, and there is no uncertainty in an agent's state. Thus, their framework allows an agent developer to specify how his agents will react when the agent is 100% sure about what is true/false in the world state. In this paper, we propose the concept of a \emph{probabilistic agent program} and show how, given an arbitrary program written in any imperative language, we may build a declarative ``probabilistic'' agent program on top of it which supports decision making in the presence of uncertainty. We provide two alternative semantics for probabilistic agent programs. We show that the second semantics, though more epistemically appealing, is more complex to compute. We provide sound and complete algorithms to compute the semantics of \emph{positive} agent programs.
cs.AI:The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.
cs.AI:We revisit the issue of connections between two leading formalisms in nonmonotonic reasoning: autoepistemic logic and default logic. For each logic we develop a comprehensive semantic framework based on the notion of a belief pair. The set of all belief pairs together with the so called knowledge ordering forms a complete lattice. For each logic, we introduce several semantics by means of fixpoints of operators on the lattice of belief pairs. Our results elucidate an underlying isomorphism of the respective semantic constructions. In particular, we show that the interpretation of defaults as modal formulas proposed by Konolige allows us to represent all semantics for default logic in terms of the corresponding semantics for autoepistemic logic. Thus, our results conclusively establish that default logic can indeed be viewed as a fragment of autoepistemic logic. However, as we also demonstrate, the semantics of Moore and Reiter are given by different operators and occupy different locations in their corresponding families of semantics. This result explains the source of the longstanding difficulty to formally relate these two semantics. In the paper, we also discuss approximating skeptical reasoning with autoepistemic and default logics and establish constructive principles behind such approximations.
cs.AI:Randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables. However, these algorithms are not complete. That is, they provide no guarantee that a satisfying assignment, if one exists, will be found. Thus, when studying randomized algorithms, there are two important characteristics that need to be considered: the running time and, even more importantly, the accuracy --- a measure of likelihood that a satisfying assignment will be found, provided one exists. In fact, we argue that without a reference to the accuracy, the notion of the running time for randomized algorithms is not well-defined. In this paper, we introduce a formal notion of accuracy. We use it to define a concept of the running time. We use both notions to study the random walk strategy GSAT algorithm. We investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments. We demonstrate that the running time of GSAT grows exponentially in the number of variables of the input formula for randomly generated 3-CNF formulas and for the formulas encoding 3- and 4-colorability of graphs.
cs.AI:Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentioned as a process that may have been used to obtain a primordial biological code necessary open-ended evolution.
cs.AI:This paper presents a method of computing a revision of a function-free normal logic program. If an added rule is inconsistent with a program, that is, if it leads to a situation such that no stable model exists for a new program, then deletion and addition of rules are performed to avoid inconsistency. We specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules. To compute such deletion and addition, we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule. We compute a minimally revised program, by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure.
cs.AI:This is a system description for the OSCAR defeasible reasoner.
cs.AI:Diagnostic reasoning has been characterized logically as consistency-based reasoning or abductive reasoning. Previous analyses in the literature have shown, on the one hand, that choosing the (in general more restrictive) abductive definition may be appropriate or not, depending on the content of the knowledge base [Console&Torasso91], and, on the other hand, that, depending on the choice of the definition the same knowledge should be expressed in different form [Poole94].   Since in Model-Based Diagnosis a major problem is finding the right way of abstracting the behavior of the system to be modeled, this paper discusses the relation between modeling, and in particular abstraction in the model, and the notion of diagnosis.
cs.AI:ACLP is a system which combines abductive reasoning and constraint solving by integrating the frameworks of Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). It forms a general high-level knowledge representation environment for abductive problems in Artificial Intelligence and other areas. In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems. The ACLP system is currently implemented on top of the CLP language of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver for finite domains. It has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the (lower level) constraint solving framework of CLP on which it is built. These experiments provide evidence that the abductive framework of ACLP does not compromise significantly the computational efficiency of the solutions. Other experiments show the natural ability of ACLP to accommodate easily and in a robust way new or changing requirements of the original problem.
cs.AI:We present a method for relevance sensitive non-monotonic inference from belief sequences which incorporates insights pertaining to prioritized inference and relevance sensitive, inconsistency tolerant belief revision.   Our model uses a finite, logically open sequence of propositional formulas as a representation for beliefs and defines a notion of inference from maxiconsistent subsets of formulas guided by two orderings: a temporal sequencing and an ordering based on relevance relations between the conclusion and formulas in the sequence. The relevance relations are ternary (using context as a parameter) as opposed to standard binary axiomatizations. The inference operation thus defined easily handles iterated revision by maintaining a revision history, blocks the derivation of inconsistent answers from a possibly inconsistent sequence and maintains the distinction between explicit and implicit beliefs. In doing so, it provides a finitely presented formalism and a plausible model of reasoning for automated agents.
cs.AI:We propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. In detail, we generalize the notions of Pearl's entailment in system Z, Lehmann's lexicographic entailment, and Geffner's conditional entailment to conditional constraints. We give some examples that show that the new notions of z-, lexicographic, and conditional entailment have similar properties like their classical counterparts. Moreover, we show that the new notions of z-, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints.
cs.AI:This paper describes a system, called PLP, for compiling ordered logic programs into standard logic programs under the answer set semantics. In an ordered logic program, rules are named by unique terms, and preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Since the result of the translation is an extended logic program, existing logic programming systems can be used as underlying reasoning engine. In particular, PLP is conceived as a front-end to the logic programming systems dlv and smodels.
cs.AI:The SLDNFA-system results from the LP+ project at the K.U.Leuven, which investigates logics and proof procedures for these logics for declarative knowledge representation. Within this project inductive definition logic (ID-logic) is used as representation logic. Different solvers are being developed for this logic and one of these is SLDNFA. A prototype of the system is available and used for investigating how to solve efficiently problems represented in ID-logic.
cs.AI:We describe an approach for compiling preferences into logic programs under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows both the specification of static orderings (as found in most previous work), in which preferences are external to a logic program, as well as orderings on sets of rules. In large part then, we are interested in describing a general methodology for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.
cs.AI:This paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base. The abductive inference processes described here depend on the semantic of the rule. We distinguish two classes of interpretation of a fuzzy rule, certainty generation rules and possible generation rules. In this paper we present the architecture of abductive inference in the first class of interpretation. We give two kinds of problem that we can resolve by using the proposed models of inference.
cs.AI:The goal of the LP+ project at the K.U.Leuven is to design an expressive logic, suitable for declarative knowledge representation, and to develop intelligent systems based on Logic Programming technology for solving computational problems using the declarative specifications. The ID-logic is an integration of typed classical logic and a definition logic. Different abductive solvers for this language are being developed. This paper is a report of the integration of high order aggregates into ID-logic and the consequences on the solver SLDNFA.
cs.AI:We propose a new approach to belief revision that provides a way to change knowledge bases with a minimum of effort. We call this way of revising belief states optimal belief revision. Our revision method gives special attention to the fact that most belief revision processes are directed to a specific informational objective. This approach to belief change is founded on notions such as optimal context and accessibility. For the sentential model of belief states we provide both a formal description of contexts as sub-theories determined by three parameters and a method to construct contexts. Next, we introduce an accessibility ordering for belief sets, which we then use for selecting the best (optimal) contexts with respect to the processing effort involved in the revision. Then, for finitely axiomatizable knowledge bases, we characterize a finite accessibility ranking from which the accessibility ordering for the entire base is generated and show how to determine the ranking of an arbitrary sentence in the language. Finally, we define the adjustment of the accessibility ranking of a revised base of a belief set.
cs.AI:High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like \congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting \ccgolog, a variant of \congolog which is based on the extended situation calculus. One benefit of \ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism.
cs.AI:The Smodels system implements the stable model semantics for normal logic programs. It handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints. On top of this core engine more involved systems can be built. As an example, we have implemented total and partial stable model computation for disjunctive logic programs. An interesting application method is based on answer set programming, i.e., encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules. Smodels has been applied to a number of areas including planning, model checking, reachability analysis, product configuration, dynamic constraint satisfaction, and feature interaction.
cs.AI:E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.
cs.AI:In this paper, we outline the prototype of an automated inference tool, called QUIP, which provides a uniform implementation for several nonmonotonic reasoning formalisms. The theoretical basis of QUIP is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae.
cs.AI:Over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information. One such language is the language of epistemic specifications. As is usual with logic programming languages, the problem of answering queries is intractable in the general case. For extended disjunctive logic programs, an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets. In this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications. Furthermore, an extension of the splitting set theorem will be presented. Also, a characterization of stratified epistemic specifications will be given in terms of splitting sets. This characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs.
cs.AI:The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.
cs.AI:We generalize a theorem by Francois Fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure. The study of this relationship is important in connection with the emergence of answer set programming. Whenever the two semantics are equivalent, answer sets can be computed by a satisfiability solver, and the use of answer set solvers such as smodels and dlv is unnecessary. A logic programming representation of the blocks world due to Ilkka Niemelae is discussed as an example.
cs.AI:We introduced decomposable negation normal form (DNNF) recently as a tractable form of propositional theories, and provided a number of powerful logical operations that can be performed on it in polynomial time. We also presented an algorithm for compiling any conjunctive normal form (CNF) into DNNF and provided a structure-based guarantee on its space and time complexity. We present in this paper a linear-time algorithm for converting an ordered binary decision diagram (OBDD) representation of a propositional theory into an equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the previous complexity guarantees on compiling DNNF continue to hold for this stricter subclass, which has stronger properties. In particular, we present a new operation on d-DNNF which allows us to count its models under the assertion, retraction and flipping of every literal by traversing the d-DNNF twice. That is, after such traversal, we can test in constant-time: the entailment of any literal by the d-DNNF, and the consistency of the d-DNNF under the retraction or flipping of any literal. We demonstrate the significance of these new operations by showing how they allow us to implement linear-time, complete truth maintenance systems and linear-time, complete belief revision systems for two important classes of propositional theories.
cs.AI:The paper reports on first preliminary results and insights gained in a project aiming at implementing the fluent calculus using methods and techniques based on binary decision diagrams. After reporting on an initial experiment showing promising results we discuss our findings concerning various techniques and heuristics used to speed up the reasoning process.
cs.AI:Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.
cs.AI:In an earlier work, we have presented operations of belief change which only affect the relevant part of a belief base. In this paper, we propose the application of the same strategy to the problem of model-based diangosis. We first isolate the subset of the system description which is relevant for a given observation and then solve the diagnosis problem for this subset.
cs.AI:We present a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such ``extension'' represents the revised state; alternately revision consists of the intersection of all such extensions.   The most general formulation of our approach is flexible enough to express other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: the choice revision operator gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A.
cs.AI:SATEN is an object-oriented web-based extraction and belief revision engine. It runs on any computer via a Java 1.1 enabled browser such as Netscape 4. SATEN performs belief revision based on the AGM approach. The extraction and belief revision reasoning engines operate on a user specified ranking of information. One of the features of SATEN is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm. We describe here an ASP system, DATALOG with constraints or DC, based on non-monotonic logic. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax, use and implementation of DC and provide experimental results.
cs.AI:Answer-set programming (ASP) has emerged recently as a viable programming paradigm well attuned to search problems in AI, constraint satisfaction and combinatorics. Propositional logic is, arguably, the simplest ASP system with an intuitive semantics supporting direct modeling of problem constraints. However, for some applications, especially those requiring that transitive closure be computed, it requires additional variables and results in large theories. Consequently, it may not be a practical computational tool for such problems. On the other hand, ASP systems based on nonmonotonic logics, such as stable logic programming, can handle transitive closure computation efficiently and, in general, yield very concise theories as problem representations. Their semantics is, however, more complex. Searching for the middle ground, in this paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax and semantics of DC, and study its properties. We discuss an implementation of DC and present results of experimental study of the effectiveness of DC, comparing it with CSAT, a satisfiability checker and SMODELS implementation of stable logic programming. Our results show that DC is competitive with the other two approaches, in case of many search problems, often yielding much more efficient solutions.
cs.AI:We study here the well-known propagation rules for Boolean constraints. First we propose a simple notion of completeness for sets of such rules and establish a completeness result. Then we show an equivalence in an appropriate sense between Boolean constraint propagation and unit propagation, a form of resolution for propositional logic.   Subsequently we characterize one set of such rules by means of the notion of hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify the status of a similar, though different, set of rules introduced in (Simonis 1989a) and more fully in (Codognet and Diaz 1996).
cs.AI:A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.
cs.AI:In this paper we present a rule based formalism for filtering variables domains of constraints. This formalism is well adapted for solving dynamic CSP. We take diagnosis as an instance problem to illustrate the use of these rules. A diagnosis problem is seen like finding all the minimal sets of constraints to be relaxed in the constraint network that models the device to be diagnosed
cs.AI:Despite the effort of many researchers in the area of multi-agent systems (MAS) for designing and programming agents, a few years ago the research community began to take into account that common features among different MAS exists. Based on these common features, several tools have tackled the problem of agent development on specific application domains or specific types of agents. As a consequence, their scope is restricted to a subset of the huge application domain of MAS. In this paper we propose a generic infrastructure for programming agents whose name is Brainstorm/J. The infrastructure has been implemented as an object oriented framework. As a consequence, our approach supports a broader scope of MAS applications than previous efforts, being flexible and reusable.
cs.AI:In fuzzy propositional logic, to a proposition a partial truth in [0,1] is assigned. It is well known that under certain circumstances, fuzzy logic collapses to classical logic. In this paper, we will show that under dual conditions, fuzzy logic collapses to four-valued (relevance) logic, where propositions have truth-value true, false, unknown, or contradiction. As a consequence, fuzzy entailment may be considered as ``in between'' four-valued (relevance) entailment and classical entailment.
cs.AI:We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.
cs.AI:Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are systems based on stable model semantics, abductive systems, and first order logic model generators which compute solutions as models of some theory. This paper compares these different approaches from the point of view of knowledge representation (how declarative are the programs) and from the point of view of performance (how good are they at solving typical problems).
cs.AI:In this paper, we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery. This theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules. We show how to realize this theory in a learning system named CFRule. CFRule is a parallel weight-based model, but it departs from traditional neural computing in that its internal knowledge is comprehensible. Furthermore, when the model converges upon training, each channel converges to a target rule. The model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent. Since, however, gradient descent only guarantees local optimization, a multi-channel regression-based optimization strategy is developed to effectively deal with this problem. Formally, we prove that the CFRule model can explicitly and precisely encode any given rule set. Also, we prove a property related to asynchronous parallel convergence, which is a critical element of the multi-channel parallel adaptation theory for rule learning. Thanks to the quantizability nature of the CFRule model, rules can be extracted completely and soundly via a threshold-based mechanism. Finally, the practical application of the theory is demonstrated in DNA promoter recognition and hepatitis prognosis prediction.
cs.AI:We present an approach for modelling the structure and coarse content of legal documents with a view to providing automated support for the drafting of contracts and contract database retrieval. The approach is designed to be applicable where contract drafting is based on model-form contracts or on existing examples of a similar type. The main features of the approach are: (1) the representation addresses the structure and the interrelationships between the constituent parts of contracts, but not the text of the document itself; (2) the representation of documents is separated from the mechanisms that manipulate it; and (3) the drafting process is subject to a collection of explicitly stated constraints that govern the structure of the documents. We describe the representation of document instances and of 'generic documents', which are data structures used to drive the creation of new document instances, and we show extracts from a sample session to illustrate the features of a prototype system implemented in MacProlog.
cs.AI:One influential approach to assessing the "goodness" of arguments is offered by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an approach that originates in discourse analysis. In p-d terms an argument is good if it avoids committing a fallacy, whereas in RST terms an argument is good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for providing only a partially functional account of argument, and similar criticisms have been raised in the Natural Language Generation (NLG) community-particularly by Moore & Pollack (1992)- with regards to its account of intentionality in text in general. Mann and Thompson themselves note that although RST can be successfully applied to a wide range of texts from diverse domains, it fails to characterise some types of text, most notably legal contracts. There is ongoing research in the Artificial Intelligence and Law community exploring the potential for providing electronic support to contract negotiators, focusing on long-term, complex engineering agreements (see for example Daskalopulu & Sergot 1997). This paper provides a brief introduction to RST and illustrates its shortcomings with respect to contractual text. An alternative approach for modelling argument structure is presented which not only caters for contractual text, but also overcomes the aforementioned limitations of RST.
cs.AI:Information Integration is a young and exciting field with enormous research and commercial significance in the new world of the Information Society. It stands at the crossroad of Databases and Artificial Intelligence requiring novel techniques that bring together different methods from these fields. Information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible, transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program. The field although relatively young has already found many practical applications particularly for integrating information over the World Wide Web. This paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas. It attempts to evaluate the current and potential role of Computational Logic in this and suggests some of the problems where logic-based techniques could be used.
cs.AI:Constraint propagation is a general algorithmic approach for pruning the search space of a CSP. In a uniform way, K. R. Apt has defined a computation as an iteration of reduction functions over a domain. He has also demonstrated the need for integrating static properties of reduction functions (commutativity and semi-commutativity) to design specialized algorithms such as AC3 and DAC. We introduce here a set of operators for modeling compositions of reduction functions. Two of the major goals are to tackle parallel computations, and dynamic behaviours (such as slow convergence).
cs.AI:We consider an approach to update nonmonotonic knowledge bases represented as extended logic programs under answer set semantics. New information is incorporated into the current knowledge base subject to a causal rejection principle enforcing that, in case of conflicts, more recent rules are preferred and older rules are overridden. Such a rejection principle is also exploited in other approaches to update logic programs, e.g., in dynamic logic programming by Alferes et al. We give a thorough analysis of properties of our approach, to get a better understanding of the causal rejection principle. We review postulates for update and revision operators from the area of theory change and nonmonotonic reasoning, and some new properties are considered as well. We then consider refinements of our semantics which incorporate a notion of minimality of change. As well, we investigate the relationship to other approaches, showing that our approach is semantically equivalent to inheritance programs by Buccafurri et al. and that it coincides with certain classes of dynamic logic programs, for which we provide characterizations in terms of graph conditions. Therefore, most of our results about properties of causal rejection principle apply to these approaches as well. Finally, we deal with computational complexity of our approach, and outline how the update semantics and its refinements can be implemented on top of existing logic programming engines.
cs.AI:We introduce a learning method called ``gradient-based reinforcement planning'' (GREP). Unlike traditional DP methods that improve their policy backwards in time, GREP is a gradient-based method that plans ahead and improves its policy before it actually acts in the environment. We derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments.
cs.AI:Much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty. In this work we extend competitive analysis to the context of multi-agent systems. Unlike classical competitive analysis where the behavior of an agent's environment is taken to be arbitrary, we consider the case where an agent's environment consists of other agents. These agents will usually obey some (minimal) rationality constraints. This leads to the definition of rational competitive analysis. We introduce the concept of rational competitive analysis, and initiate the study of competitive analysis for multi-agent systems. We also discuss the application of rational competitive analysis to the context of bidding games, as well as to the classical one-way trading problem.
cs.AI:This article aims at clarifying the language and practice of scientific experiment, mainly by hooking observability on calculability.
cs.AI:Many systems that exhibit nonmonotonic behavior have been described and studied already in the literature. The general notion of nonmonotonic reasoning, though, has almost always been described only negatively, by the property it does not enjoy, i.e. monotonicity. We study here general patterns of nonmonotonic reasoning and try to isolate properties that could help us map the field of nonmonotonic reasoning by reference to positive properties. We concentrate on a number of families of nonmonotonic consequence relations, defined in the style of Gentzen. Both proof-theoretic and semantic points of view are developed in parallel. The former point of view was pioneered by D. Gabbay, while the latter has been advocated by Y. Shoham in. Five such families are defined and characterized by representation theorems, relating the two points of view. One of the families of interest, that of preferential relations, turns out to have been studied by E. Adams. The "preferential" models proposed here are a much stronger tool than Adams' probabilistic semantics. The basic language used in this paper is that of propositional logic. The extension of our results to first order predicate calculi and the study of the computational complexity of the decision problems described in this paper will be treated in another paper.
cs.AI:This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type "if ... then ...", represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, S. Kraus and the authors defined and studied "preferential" consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, "rational" relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a "ranked" preferential model, or by a (non-standard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.
cs.AI:It is shown that Darwiche and Pearl's postulates imply an interesting property, not noticed by the authors.
cs.AI:A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions.
cs.AI:We give a semantics to iterated update by a preference relation on possible developments. An iterated update is a sequence of formulas, giving (incomplete) information about successive states of the world. A development is a sequence of models, describing a possible trajectory through time. We assume a principle of inertia and prefer those developments, which are compatible with the information, and avoid unnecessary changes. The logical properties of the updates defined in this way are considered, and a representation result is proved.
cs.AI:A. Tarski proposed the study of infinitary consequence operations as the central topic of mathematical logic. He considered monotonicity to be a property of all such operations. In this paper, we weaken the monotonicity requirement and consider more general operations, inference operations. These operations describe the nonmonotonic logics both humans and machines seem to be using when infering defeasible information from incomplete knowledge. We single out a number of interesting families of inference operations. This study of infinitary inference operations is inspired by the results of Kraus, Lehmann and Magidor on finitary nonmonotonic operations, but this paper is self-contained.
cs.AI:The Expansion property considered by researchers in Social Choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the {\em pure}, i.e., not involving connectives, version of a previously known weak rationality condition. The assumption that the union of two definable sets of models is definable is needed for the soundness part of the result.
cs.AI:The lexicographic closure of any given finite set D of normal defaults is defined. A conditional assertion "if a then b" is in this lexicographic closure if, given the defaults D and the fact a, one would conclude b. The lexicographic closure is essentially a rational extension of D, and of its rational closure, defined in a previous paper. It provides a logic of normal defaults that is different from the one proposed by R. Reiter and that is rich enough not to require the consideration of non-normal defaults. A large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind Reiter's logic of defaults.
cs.AI:We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).
cs.AI:Stereotypical reasoning assumes that the situation at hand is one of a kind and that it enjoys the properties generally associated with that kind of situation. It is one of the most basic forms of nonmonotonic reasoning. A formal model for stereotypical reasoning is proposed and the logical properties of this form of reasoning are studied. Stereotypical reasoning is shown to be cumulative under weak assumptions.
cs.AI:We introduce a methodology and framework for expressing general preference information in logic programming under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of form s < t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed program correspond with the preferred answer sets of the original program. Our approach allows the specification of dynamic orderings, in which preferences can appear arbitrarily within a program. Static orderings (in which preferences are external to a logic program) are a trivial restriction of the general dynamic case. First, we develop a specific approach to reasoning with preferences, wherein the preference ordering specifies the order in which rules are to be applied. We then demonstrate the wide range of applicability of our framework by showing how other approaches, among them that of Brewka and Eiter, can be captured within our framework. Since the result of each of these transformations is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a publicly available compiler as a front-end for these programming systems.
cs.AI:Prioritized default reasoning has illustrated its rich expressiveness and flexibility in knowledge representation and reasoning. However, many important aspects of prioritized default reasoning have yet to be thoroughly explored. In this paper, we investigate two properties of prioritized logic programs in the context of answer set semantics. Specifically, we reveal a close relationship between mutual defeasibility and uniqueness of the answer set for a prioritized logic program. We then explore how the splitting technique for extended logic programs can be extended to prioritized logic programs. We prove splitting theorems that can be used to simplify the evaluation of a prioritized logic program under certain conditions.
cs.AI:The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula, but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed.
cs.AI:We study fixpoints of operators on lattices. To this end we introduce the notion of an approximation of an operator. We order approximations by means of a precision ordering. We show that each lattice operator O has a unique most precise or ultimate approximation. We demonstrate that fixpoints of this ultimate approximation provide useful insights into fixpoints of the operator O.   We apply our theory to logic programming and introduce the ultimate Kripke-Kleene, well-founded and stable semantics. We show that the ultimate Kripke-Kleene and well-founded semantics are more precise then their standard counterparts We argue that ultimate semantics for logic programming have attractive epistemological properties and that, while in general they are computationally more complex than the standard semantics, for many classes of theories, their complexity is no worse.
cs.AI:Representing defeasibility is an important issue in common sense reasoning. In reasoning about action and change, this issue becomes more difficult because domain and action related defeasible information may conflict with general inertia rules. Furthermore, different types of defeasible information may also interfere with each other during the reasoning. In this paper, we develop a prioritized logic programming approach to handle defeasibilities in reasoning about action. In particular, we propose three action languages {\cal AT}^{0}, {\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in action domains named defeasible constraints, defeasible observations and actions with defeasible and abnormal effects respectively. Each language with a higher superscript can be viewed as an extension of the language with a lower superscript. These action languages inherit the simple syntax of {\cal A} language but their semantics is developed in terms of transition systems where transition functions are defined based on prioritized logic programs. By illustrating various examples, we show that our approach eventually provides a powerful mechanism to handle various defeasibilities in temporal prediction and postdiction. We also investigate semantic properties of these three action languages and characterize classes of action domains that present more desirable solutions in reasoning about action within the underlying action languages.
cs.AI:An anticipatory system for guiding plot development in interactive narratives is described. The executable model is a finite automaton that provides the implemented system with a look-ahead. The identification of undesirable future states in the model is used to guide the player, in a transparent manner. In this way, too radical twists of the plot can be avoided. Since the player participates in the development of the plot, such guidance can have many forms, depending on the environment of the player, on the behavior of the other players, and on the means of player interaction. We present a design method for interactive narratives which produces designs suitable for the implementation of anticipatory mechanisms. Use of the method is illustrated by application to our interactive computer game Kaktus.
cs.AI:Open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics. There are obvious analogies between open predicates and abducible predicates. However, despite superficial similarities, there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa. Similarly, open programs cannot be immediately simulated with answer set programming (ASP). In this paper we start a thorough investigation of the relationships between open inference, abduction and ASP. We shall prove that open programs generalize the other two frameworks. The generalized framework suggests interesting extensions of abduction under the generalized stable model semantics. In some cases, we will be able to reduce open inference to abduction and ASP, thereby estimating its computational complexity. At the same time, the aforementioned reduction opens the way to new applications of abduction and ASP.
cs.AI:In this paper we consider three different kinds of domain-dependent control knowledge (temporal, procedural and HTN-based) that are useful in planning. Our approach is declarative and relies on the language of logic programming with answer set semantics (AnsProlog*). AnsProlog* is designed to plan without control knowledge. We show how temporal, procedural and HTN-based control knowledge can be incorporated into AnsProlog* by the modular addition of a small number of domain-dependent rules, without the need to modify the planner. We formally prove the correctness of our planner, both in the absence and presence of the control knowledge. Finally, we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length.
cs.AI:Dung's abstract framework for argumentation enables a study of the interactions between arguments based solely on an ``attack'' binary relation on the set of arguments. Various ways to solve conflicts between contradictory pieces of information have been proposed in the context of argumentation, nonmonotonic reasoning or logic programming, and can be captured by appropriate semantics within Dung's framework. A common feature of these semantics is that one can always maximize in some sense the set of acceptable arguments. We propose in this paper to extend Dung's framework in order to allow for the representation of what we call ``restricted'' arguments: these arguments should only be used if absolutely necessary, that is, in order to support other arguments that would otherwise be defeated. We modify Dung's preferred semantics accordingly: a set of arguments becomes acceptable only if it contains a minimum of restricted arguments, for a maximum of unrestricted arguments.
cs.AI:We address a general representation problem for belief change, and describe two interrelated representations for iterative non-prioritized change: a logical representation in terms of persistent epistemic states, and a constructive representation in terms of flocks of bases.
cs.AI:An extension of an abstract argumentation framework, called collective argumentation, is introduced in which the attack relation is defined directly among sets of arguments. The extension turns out to be suitable, in particular, for representing semantics of disjunctive logic programs. Two special kinds of collective argumentation are considered in which the opponents can share their arguments.
cs.AI:Logic programs with ordered disjunction (LPODs) combine ideas underlying Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming. Logic programming under answer set semantics is extended with a new connective called ordered disjunction. The new connective allows us to represent alternative, ranked options for problem solutions in the heads of rules: A \times B intuitively means: if possible A, but if A is not possible then at least B. The semantics of logic programs with ordered disjunction is based on a preference relation on answer sets. LPODs are useful for applications in design and configuration and can serve as a basis for qualitative decision making.
cs.AI:In this paper, we investigate the extent to which knowledge compilation can be used to improve inference from propositional weighted bases. We present a general notion of compilation of a weighted base that is parametrized by any equivalence--preserving compilation function. Both negative and positive results are presented. On the one hand, complexity results are identified, showing that the inference problem from a compiled weighted base is as difficult as in the general case, when the prime implicates, Horn cover or renamable Horn cover classes are targeted. On the other hand, we show that the inference problem becomes tractable whenever DNNF-compilations are used and clausal queries are considered. Moreover, we show that the set of all preferred models of a DNNF-compilation of a weighted base can be computed in time polynomial in the output size. Finally, we sketch how our results can be used in model-based diagnosis in order to compute the most probable diagnoses of a system.
cs.AI:This paper studies the problem of modeling complex domains of actions and change within high-level action description languages. We investigate two main issues of concern: (a) can we represent complex domains that capture together different problems such as ramifications, non-determinism and concurrency of actions, at a high-level, close to the given natural ontology of the problem domain and (b) what features of such a representation can affect, and how, its computational behaviour. The paper describes the main problems faced in this representation task and presents the results of an empirical study, carried out through a series of controlled experiments, to analyze the computational performance of reasoning in these representations. The experiments compare different representations obtained, for example, by changing the basic ontology of the domain or by varying the degree of use of indirect effect laws through domain constraints. This study has helped to expose the main sources of computational difficulty in the reasoning and suggest some methodological guidelines for representing complex domains. Although our work has been carried out within one particular high-level description language, we believe that the results, especially those that relate to the problems of representation, are independent of the specific modeling language.
cs.AI:This paper introduces the notion of value-based argumentation frameworks, an extension of the standard argumentation frameworks proposed by Dung, which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote.
cs.AI:We analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory. We start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics. We then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs. The suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics. In particular, our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics. Finally, we indicate how an existing preferred well-founded semantics can be captured within our semantical framework.
cs.AI:In this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems. This transformation allows to characterize all notions of Reiter's default logic in the framework of argumentation systems. As a consequence, computing extensions, or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic. The transformation proposed is linear in the number of defaults.
cs.AI:In the present paper, the existence and multiplicity problems of extensions are addressed. The focus is on extension of the stable type. The main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification, a close cousin of the notion of admissibility. The characterization is given in the context of the particular logic for dialectical argumentation DEFLOG. The results are of direct relevance for several well-established models of defeasible reasoning (like default logic, logic programming and argumentation frameworks), since elsewhere dialectical argumentation has been shown to have close formal connections with these models.
cs.AI:Recently, it has been shown that probabilistic entailment under coherence is weaker than model-theoretic probabilistic entailment. Moreover, probabilistic entailment under coherence is a generalization of default entailment in System P. In this paper, we continue this line of research by presenting probabilistic generalizations of more sophisticated notions of classical default entailment that lie between model-theoretic probabilistic entailment and probabilistic entailment under coherence. That is, the new formalisms properly generalize their counterparts in classical default reasoning, they are weaker than model-theoretic probabilistic entailment, and they are stronger than probabilistic entailment under coherence. The new formalisms are useful especially for handling probabilistic inconsistencies related to conditioning on zero events. They can also be applied for probabilistic belief revision. More generally, in the same spirit as a similar previous paper, this paper sheds light on exciting new formalisms for probabilistic reasoning beyond the well-known standard ones.
cs.AI:We seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic. Rather than stipulating that the conclusion of an inference be true in all models in which the premises are true, we require that the conclusion of a nonmonotonic inference be true in ``almost all'' models of a certain sort in which the premises are true. This ``certain sort'' specification picks out the models that are relevant to the inference, taking into account factors such as specificity and vagueness, and previous inferences. The frequencies characterizing the relevant models reflect known frequencies in our actual world. The criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension. We show that this avoids the implausibilities that might otherwise result from the chaining of default inferences. The model proportions, when construed in terms of frequencies, provide a verifiable grounding of default rules, and can become the basis for generating default rules from statistics.
cs.AI:About ten years ago, various notions of preferential entailment have been introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM), one of the main competitor being a more general version defined by Makinson (MAK). These two versions have already been compared, but it is time to revisit these comparisons. Here are our three main results: (1) These two notions are equivalent, provided that we restrict our attention, as done in KLM, to the cases where the entailment respects logical equivalence (on the left and on the right). (2) A serious simplification of the description of the fundamental cases in which MAK is equivalent to KLM, including a natural passage in both ways. (3) The two previous results are given for preferential entailments more general than considered in some of the original texts, but they apply also to the original definitions and, for this particular case also, the models can be simplified.
cs.AI:This work analyses main features that should be present in knowledge representation. It suggests a model for representation and a way to implement this model in software. Representation takes care of both low-level sensor information and high-level concepts.
cs.AI:We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.
cs.AI:Recently, several approaches to updating knowledge bases modeled as extended logic programs have been introduced, ranging from basic methods to incorporate (sequences of) sets of rules into a logic program, to more elaborate methods which use an update policy for specifying how updates must be incorporated. In this paper, we introduce a framework for reasoning about evolving knowledge bases, which are represented as extended logic programs and maintained by an update policy. We first describe a formal model which captures various update approaches, and we define a logical language for expressing properties of evolving knowledge bases. We then investigate semantical and computational properties of our framework, where we focus on properties of knowledge states with respect to the canonical reasoning task of whether a given formula holds on a given evolving knowledge base. In particular, we present finitary characterizations of the evolution for certain classes of framework instances, which can be exploited for obtaining decidability results. In more detail, we characterize the complexity of reasoning for some meaningful classes of evolving knowledge bases, ranging from polynomial to double exponential space complexity.
cs.AI:In this thesis I present a virtual laboratory which implements five different models for controlling animats: a rule-based system, a behaviour-based system, a concept-based system, a neural network, and a Braitenberg architecture. Through different experiments, I compare the performance of the models and conclude that there is no "best" model, since different models are better for different things in different contexts.   The models I chose, although quite simple, represent different approaches for studying cognition. Using the results as an empirical philosophical aid,   I note that there is no "best" approach for studying cognition, since different approaches have all advantages and disadvantages, because they study different aspects of cognition from different contexts. This has implications for current debates on "proper" approaches for cognition: all approaches are a bit proper, but none will be "proper enough". I draw remarks on the notion of cognition abstracting from all the approaches used to study it, and propose a simple classification for different types of cognition.
cs.AI:This paper deals with the revision of partially ordered beliefs. It proposes a semantic representation of epistemic states by partial pre-orders on interpretations and a syntactic representation by partially ordered belief bases. Two revision operations, the revision stemming from the history of observations and the possibilistic revision, defined when the epistemic state is represented by a total pre-order, are generalized, at a semantic level, to the case of a partial pre-order on interpretations, and at a syntactic level, to the case of a partially ordered belief base. The equivalence between the two representations is shown for the two revision operations.
cs.AI:This is the first in a series of connected papers discussing the problem of a dynamically reconfigurable universal learning neurocomputer that could serve as a computational model for the whole human brain. The whole series is entitled "The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal Learning Neurocomputer." (For more information visit the website www.brain0.com.) This introductory paper is concerned with general methodology. Its main goal is to explain why it is critically important for both neural modeling and cognitive modeling to pay much attention to the basic requirements of the whole brain as a complex computing system. The author argues that it can be easier to develop an adequate computational model for the whole "unprogrammed" (untrained) human brain than to find adequate formal representations of some nontrivial parts of brain's performance. (In the same way as, for example, it is easier to describe the behavior of a complex analytical function than the behavior of its real and/or imaginary part.) The "curse of dimensionality" that plagues purely phenomenological ("brainless") cognitive theories is a natural penalty for an attempt to represent insufficiently large parts of brain's performance in a state space of insufficiently high dimensionality. A "partial" modeler encounters "Catch 22." An attempt to simplify a cognitive problem by artificially reducing its dimensionality makes the problem more difficult.
cs.AI:As a part of our effort for studying the evolution and development of cognition, we present results derived from synthetic experimentations in a virtual laboratory where animats develop koncepts adaptively and ground their meaning through action. We introduce the term "koncept" to avoid confusions and ambiguity derived from the wide use of the word "concept". We present the models which our animats use for abstracting koncepts from perceptions, plastically adapt koncepts, and associate koncepts with actions. On a more philosophical vein, we suggest that knowledge is a property of a cognitive system, not an element, and therefore observer-dependent.
cs.AI:This paper presents a model for dynamic adjustment of the motivation degree, using a reinforcement learning approach, in an action selection mechanism previously developed by the authors. The learning takes place in the modification of a parameter of the model of combination of internal and external stimuli. Experiments that show the claimed properties are presented, using a VR simulation developed for such purposes. The importance of adaptation by learning in action selection is also discussed.
cs.AI:This article analyses the properties of the Internal Behaviour network, an action selection mechanism previously proposed by the authors, with the aid of a simulation developed for such ends. A brief review of the Internal Behaviour network is followed by the explanation of the implementation of the simulation. Then, experiments are presented and discussed analysing the properties of the action selection in the proposed model.
cs.AI:This paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent, based in an action selection mechanism previously proposed by the authors. This combination model includes additive and multiplicative elements, which allows to incorporate new properties, which enhance the action selection. A given parameter a, which is part of the proposed model, allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity.
cs.AI:Reinforcement learning (RL) involves sequential decision making in uncertain environments. The aim of the decision-making agent is to maximize the benefit of acting in its environment over an extended period of time. Finding an optimal policy in RL may be very slow. To speed up learning, one often used solution is the integration of planning, for example, Sutton's Dyna algorithm, or various other methods using macro-actions.   Here we suggest to separate plannable, i.e., close to deterministic parts of the world, and focus planning efforts in this domain. A novel reinforcement learning method called plannable RL (pRL) is proposed here. pRL builds a simple model, which is used to search for macro actions. The simplicity of the model makes planning computationally inexpensive. It is shown that pRL finds an optimal policy, and that plannable macro actions found by pRL are near-optimal. In turn, it is unnecessary to try large numbers of macro actions, which enables fast learning. The utility of pRL is demonstrated by computer simulations.
cs.AI:Optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length. For interacting agents in time-critical applications, learning of the possibility of scheduling of subtasks (events) or the full task is an additional relevant issue. Besides, there exist highly stochastic problems where the actual trajectories show great variety from episode to episode, but completing the task takes almost the same amount of time. The identification of sub-problems of this nature may promote e.g., planning, scheduling and segmenting Markov decision processes. In this work, formulae for the average duration as well as the standard deviation of the duration of events are derived. The emerging Bellman-type equation is a simple extension of Sobel's work (1982). Methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension. Computer demonstration on a toy problem serve to highlight the principle.
cs.AI:Much work has been done on extending the well-founded semantics to general disjunctive logic programs and various approaches have been proposed. However, these semantics are different from each other and no consensus is reached about which semantics is the most intended. In this paper we look at disjunctive well-founded reasoning from different angles. We show that there is an intuitive form of the well-founded reasoning in disjunctive logic programming which can be characterized by slightly modifying some exisitng approaches to defining disjunctive well-founded semantics, including program transformations, argumentation, unfounded sets (and resolution-like procedure). We also provide a bottom-up procedure for this semantics. The significance of our work is not only in clarifying the relationship among different approaches, but also shed some light on what is an intended well-founded semantics for disjunctive logic programs.
cs.AI:We provide a semantic framework for preference handling in answer set programming. To this end, we introduce preference preserving consequence operators. The resulting fixpoint characterizations provide us with a uniform semantic framework for characterizing preference handling in existing approaches. Although our approach is extensible to other semantics by means of an alternating fixpoint theory, we focus here on the elaboration of preferences under answer set semantics. Alternatively, we show how these approaches can be characterized by the concept of order preservation. These uniform semantic characterizations provide us with new insights about interrelationships and moreover about ways of implementation.
cs.AI:The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions.   In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis.   The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents.
cs.AI:Cooperative constraint solving is an area of constraint programming that studies the interaction between constraint solvers with the aim of discovering the interaction patterns that amplify the positive qualities of individual solvers. Automatisation and formalisation of such studies is an important issue of cooperative constraint solving.   In this paper we present a constraint-based analysis of composite solvers that integrates reasoning about the individual solvers and the processed data. The idea is to approximate this reasoning by resolution of set constraints on the finite sets representing the predicates that express all the necessary properties. We illustrate application of our analysis to two important cooperation patterns: deterministic choice and loop.
cs.AI:There is a growing interest in using Kalman-filter models for brain modelling. In turn, it is of considerable importance to represent Kalman-filter in connectionist forms with local Hebbian learning rules. To our best knowledge, Kalman-filter has not been given such local representation. It seems that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a connectionist representation is presented, which is derived by means of the recursive prediction error method. We show that this method gives rise to attractive local learning rules and can adapt the Kalman-gain.
cs.AI:We discuss philosophical issues concerning the notion of cognition basing ourselves in experimental results in cognitive sciences, especially in computer simulations of cognitive systems. There have been debates on the "proper" approach for studying cognition, but we have realized that all approaches can be in theory equivalent. Different approaches model different properties of cognitive systems from different perspectives, so we can only learn from all of them. We also integrate ideas from several perspectives for enhancing the notion of cognition, such that it can contain other definitions of cognition as special cases. This allows us to propose a simple classification of different types of cognition.
cs.AI:The paper studies an implementation methodology for partial and disjunctive stable models where partiality and disjunctions are unfolded from a logic program so that an implementation of stable models for normal (disjunction-free) programs can be used as the core inference engine. The unfolding is done in two separate steps. Firstly, it is shown that partial stable models can be captured by total stable models using a simple linear and modular program transformation. Hence, reasoning tasks concerning partial stable models can be solved using an implementation of total stable models. Disjunctive partial stable models have been lacking implementations which now become available as the translation handles also the disjunctive case. Secondly, it is shown how total stable models of disjunctive programs can be determined by computing stable models for normal programs. Hence, an implementation of stable models of normal programs can be used as a core engine for implementing disjunctive programs. The feasibility of the approach is demonstrated by constructing a system for computing stable models of disjunctive programs using the smodels system as the core engine. The performance of the resulting system is compared to that of dlv which is a state-of-the-art special purpose system for disjunctive programs.
cs.AI:When tracking a large number of targets, it is often computationally expensive to represent the full joint distribution over target states. In cases where the targets move independently, each target can instead be tracked with a separate filter. However, this leads to a model-data association problem. Another approach to solve the problem with computational complexity is to track only the first moment of the joint distribution, the probability hypothesis density (PHD). The integral of this distribution over any area S is the expected number of targets within S. Since no record of object identity is kept, the model-data association problem is avoided.   The contribution of this paper is a particle filter implementation of the PHD filter mentioned above. This PHD particle filter is applied to tracking of multiple vehicles in terrain, a non-linear tracking problem. Experiments show that the filter can track a changing number of vehicles robustly, achieving near-real-time performance.
cs.AI:Search in cyclic AND/OR graphs was traditionally known to be an unsolved problem. In the recent past several important studies have been reported in this domain. In this paper, we have taken a fresh look at the problem. First, a new and comprehensive theoretical framework for cyclic AND/OR graphs has been presented, which was found missing in the recent literature. Based on this framework, two best-first search algorithms, S1 and S2, have been developed. S1 does uninformed search and is a simple modification of the Bottom-up algorithm by Martelli and Montanari. S2 performs a heuristically guided search and replicates the modification in Bottom-up's successors, namely HS and AO*. Both S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles. We then present a detailed analysis for the correctness and complexity results of S1 and S2, using the proposed framework. We have observed through experiments that S1 and S2 output correct results in all cases.
cs.AI:Thomas M. Strat has developed a decision-theoretic apparatus for Dempster-Shafer theory (Decision analysis using belief functions, Intern. J. Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility intervals are constructed for different choices. The choice with the highest expected utility is preferable to others. However, to find the preferred choice when the expected utility interval of one choice is included in that of another, it is necessary to interpolate a discerning point in the intervals. This is done by the parameter rho, defined as the probability that the ambiguity about the utility of every nonsingleton focal element will turn out as favorable as possible. If there are several different decision makers, we might sometimes be more interested in having the highest expected utility among the decision makers rather than only trying to maximize our own expected utility regardless of choices made by other decision makers. The preference of each choice is then determined by the probability of yielding the highest expected utility. This probability is equal to the maximal interval length of rho under which an alternative is preferred. We must here take into account not only the choices already made by other decision makers but also the rational choices we can assume to be made by later decision makers. In Strats apparatus, an assumption, unwarranted by the evidence at hand, has to be made about the value of rho. We demonstrate that no such assumption is necessary. It is sufficient to assume a uniform probability distribution for rho to be able to discern the most preferable choice. We discuss when this approach is justifiable.
cs.AI:Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete. This is a fundamental problem in general, and of particular interest for Bayesian networks. Recently, Grunwald and Halpern have shown that commonly used updating strategies fail in this case, except under very special assumptions. In this paper we propose a new method for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no assumptions about the so-called incompleteness mechanism that associates complete with incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we use only coherence arguments to turn prior into posterior probabilities. In general, this new approach to updating produces lower and upper posterior probabilities and expectations, as well as partially determinate decisions. This is a logical consequence of the existing ignorance about the incompleteness mechanism. We apply the new approach to the problem of classification of new evidence in probabilistic expert systems, where it leads to a new, so-called conservative updating rule. In the special case of Bayesian networks constructed using expert knowledge, we provide an exact algorithm for classification based on our updating rule, which has linear-time complexity for a class of networks wider than polytrees. This result is then extended to the more general framework of credal networks, where computations are often much harder than with Bayesian nets. Using an example, we show that our rule appears to provide a solid basis for reliable updating with incomplete observations, when no strong assumptions about the incompleteness mechanism are justified.
cs.AI:As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a ``naive space'', which does not take into account the protocol used, can often lead to counterintuitive results. Here we examine why. A criterion known as CAR (``coarsening at random'') in the statistical literature characterizes when ``naive'' conditioning in a naive space works. We show that the CAR condition holds rather infrequently, and we provide a procedural characterization of it, by giving a randomized algorithm that generates all and only distributions for which CAR holds. This substantially extends previous characterizations of CAR. We also consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE). We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, and show that there exist some very simple settings in which MRE essentially never gives the right results. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.
cs.AI:Configuring consists in simulating the realization of a complex product from a catalog of component parts, using known relations between types, and picking values for object attributes. This highly combinatorial problem in the field of constraint programming has been addressed with a variety of approaches since the foundation system R1(McDermott82). An inherent difficulty in solving configuration problems is the existence of many isomorphisms among interpretations. We describe a formalism independent approach to improve the detection of isomorphisms by configurators, which does not require to adapt the problem model. To achieve this, we exploit the properties of a characteristic subset of configuration problems, called the structural sub-problem, which canonical solutions can be produced or tested at a limited cost. In this paper we present an algorithm for testing the canonicity of configurations, that can be added as a symmetry breaking constraint to any configurator. The cost and efficiency of this canonicity test are given.
cs.AI:This article introduces the idea that probabilistic reasoning (PR) may be understood as "information compression by multiple alignment, unification and search" (ICMAUS). In this context, multiple alignment has a meaning which is similar to but distinct from its meaning in bio-informatics, while unification means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic.   A software model, SP61, has been developed for the discovery and formation of 'good' multiple alignments, evaluated in terms of information compression. The model is described in outline.   Using examples from the SP61 model, this article describes in outline how the ICMAUS framework can model various kinds of PR including: PR in best-match pattern recognition and information retrieval; one-step 'deductive' and 'abductive' PR; inheritance of attributes in a class hierarchy; chains of reasoning (probabilistic decision networks and decision trees, and PR with 'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with default values; modelling the function of a Bayesian network.
cs.AI:This article presents an overview of the idea that "information compression by multiple alignment, unification and search" (ICMAUS) may serve as a unifying principle in computing (including mathematics and logic) and in such aspects of human cognition as the analysis and production of natural language, fuzzy pattern recognition and best-match information retrieval, concept hierarchies with inheritance of attributes, probabilistic reasoning, and unsupervised inductive learning. The ICMAUS concepts are described together with an outline of the SP61 software model in which the ICMAUS concepts are currently realised. A range of examples is presented, illustrated with output from the SP61 model.
cs.AI:We propose a calculus integrating two calculi well-known in Qualitative Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus, and a coarser version of Freksa's relative orientation calculus. An original constraint propagation procedure is presented, which implements the interaction between the two integrated calculi. The importance of taking into account the interaction is shown with a real example providing an inconsistent knowledge base, whose inconsistency (a) cannot be detected by reasoning separately about each of the two components of the knowledge, just because, taken separately, each is consistent, but (b) is detected by the proposed algorithm, thanks to the interaction knowledge propagated from each of the two compnents to the other.
cs.AI:We define a ternary Relation Algebra (RA) of relative position relations on two-dimensional directed lines (d-lines for short). A d-line has two degrees of freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The representation of the RDF of a d-line will be handled by an RA of 2D orientations, CYC_t, known in the literature. A second algebra, TA_t, which will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and TA_t, will constitute, respectively, the translational and the rotational components of the RA, PA_t, of relative position relations on d-lines: the PA_t atoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that are compatible. We present in detail the RA PA_t, with its converse table, its rotation table and its composition tables. We show that a (polynomial) constraint propagation algorithm, known in the literature, is complete for a subset of PA_t relations including almost all of the atomic relations. We will discuss the application scope of the RA, which includes incidence geometry, GIS (Geographic Information Systems), shape representation, localisation in (multi-)robot navigation, and the representation of motion prepositions in NLP (Natural Language Processing). We then compare the RA to existing ones, such as an algebra for reasoning about rectangles parallel to the axes of an (orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval algebra, and an algebra for reasoning about 2D segments.
cs.AI:An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, it will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases, that can include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly {from} the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other non-deductive reasoning systems.
cs.AI:This paper describes an approach to the representation and processing of ontologies in the Semantic Web, based on the ICMAUS theory of computation and AI. This approach has strengths that complement those of languages based on the Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main benefits of the ICMAUS approach are simplicity and comprehensibility in the representation of ontologies, an ability to cope with errors and uncertainties in knowledge, and a versatile reasoning system with capabilities in the kinds of probabilistic reasoning that seem to be required in the Semantic Web.
cs.AI:Interactions are patterns between several attributes in data that cannot be inferred from any subset of these attributes. While mutual information is a well-established approach to evaluating the interactions between two attributes, we surveyed its generalizations as to quantify interactions between several attributes. We have chosen McGill's interaction information, which has been independently rediscovered a number of times under various names in various disciplines, because of its many intuitively appealing properties. We apply interaction information to visually present the most important interactions of the data. Visualization of interactions has provided insight into the structure of data on a number of domains, identifying redundant attributes and opportunities for constructing new features, discovering unexpected regularities in data, and have helped during construction of predictive models; we illustrate the methods on numerous examples. A machine learning method that disregards interactions may get caught in two traps: myopia is caused by learning algorithms assuming independence in spite of interactions, whereas fragmentation arises from assuming an interaction in spite of independence.
cs.AI:In this paper we develop an evidential force aggregation method intended for classification of evidential intelligence into recognized force structures. We assume that the intelligence has already been partitioned into clusters and use the classification method individually in each cluster. The classification is based on a measure of fitness between template and fused intelligence that makes it possible to handle intelligence reports with multiple nonspecific and uncertain propositions. With this measure we can aggregate on a level-by-level basis, starting from general intelligence to achieve a complete force structure with recognized units on all hierarchical levels.
cs.AI:Article discusses the application of Kullback-Leibler divergence to the recognition of speech signals and suggests three algorithms implementing this divergence criterion: correlation algorithm, spectral algorithm and filter algorithm. Discussion covers an approach to the problem of speech variability and is illustrated with the results of experimental modeling of speech signals. The article gives a number of recommendations on the choice of appropriate model parameters and provides a comparison to some other methods of speech recognition.
cs.AI:Richard Cox [1] set the axiomatic foundations of probable inference and the algebra of propositions. He showed that consistency within these axioms requires certain rules for updating belief. In this paper we use the analogy between probability and utility introduced in [2] to propose an axiomatic foundation for utility inference and the algebra of preferences. We show that consistency within these axioms requires certain rules for updating preference. We discuss a class of utility functions that stems from the axioms of utility inference and show that this class is the basic building block for any general multiattribute utility function. We use this class of utility functions together with the algebra of preferences to construct utility functions represented by logical operations on the attributes.
cs.AI:Recent literature in the last Maximum Entropy workshop introduced an analogy between cumulative probability distributions and normalized utility functions. Based on this analogy, a utility density function can de defined as the derivative of a normalized utility function. A utility density function is non-negative and integrates to unity. These two properties form the basis of a correspondence between utility and probability. A natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values. Maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment, and helps assign utility values based on partial preference information. This paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility.
cs.AI:Abduction, first proposed in the setting of classical logics, has been studied with growing interest in the logic programming area during the last years.   In this paper we study abduction with penalization in the logic programming framework. This form of abductive reasoning, which has not been previously analyzed in logic programming, turns out to represent several relevant problems, including optimization problems, very naturally. We define a formal model for abduction with penalization over logic programs, which extends the abductive framework proposed by Kakas and Mancarella. We address knowledge representation issues, encoding a number of problems in our abductive framework. In particular, we consider some relevant problems, taken from different domains, ranging from optimization theory to diagnosis and planning; their encodings turn out to be simple and elegant in our formalism. We thoroughly analyze the computational complexity of the main problems arising in the context of abduction with penalization from logic programs. Finally, we implement a system supporting the proposed abductive framework on top of the DLV engine. To this end, we design a translation from abduction problems with penalties into logic programs with weak constraints. We prove that this approach is sound and complete.
cs.AI:We study local-search satisfiability solvers for propositional logic extended with cardinality atoms, that is, expressions that provide explicit ways to model constraints on cardinalities of sets. Adding cardinality atoms to the language of propositional logic facilitates modeling search problems and often results in concise encodings. We propose two ``native'' local-search solvers for theories in the extended language. We also describe techniques to reduce the problem to standard propositional satisfiability and allow us to use off-the-shelf SAT solvers. We study these methods experimentally. Our general finding is that native solvers designed specifically for the extended language perform better than indirect methods relying on SAT solvers.
cs.AI:We describe WSAT(cc), a local-search solver for computing models of theories in the language of propositional logic extended by cardinality atoms. WSAT(cc) is a processing back-end for the logic PS+, a recently proposed formalism for answer-set programming.
cs.AI:This paper presents duality between probability distributions and utility functions.
cs.AI:Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge Representation and Reasoning (KRR). \DLP is very expressive in a precise mathematical sense: it allows to express every property of finite structures that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$). Importantly, the \DLP encodings are often simple and natural.   In this paper, we single out some limitations of \DLP for KRR, which cannot naturally express problems where the size of the disjunction is not known ``a priori'' (like N-Coloring), but it is part of the input. To overcome these limitations, we further enhance the knowledge modelling abilities of \DLP, by extending this language by {\em Parametric Connectives (OR and AND)}. These connectives allow us to represent compactly the disjunction/conjunction of a set of atoms having a given property. We formally define the semantics of the new language, named $DLP^{\bigvee,\bigwedge}$ and we show the usefulness of the new constructs on relevant knowledge-based problems. We address implementation issues and discuss related works.
cs.AI:The research field of Agent-Oriented Software Engineering (AOSE) aims to find abstractions, languages, methodologies and toolkits for modeling, verifying, validating and prototyping complex applications conceptualized as Multiagent Systems (MASs). A very lively research sub-field studies how formal methods can be used for AOSE. This paper presents a detailed survey of six logic-based executable agent specification languages that have been chosen for their potential to be integrated in our ARPEGGIO project, an open framework for specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each executable language, the logic foundations are described and an example of use is shown. A comparison of the six languages and a survey of similar approaches complete the paper, together with considerations of the advantages of using logic-based languages in MAS modeling and prototyping.
cs.AI:We propose a generalization of expected utility that we call generalized EU (GEU), where a decision maker's beliefs are represented by plausibility measures, and the decision maker's tastes are represented by general (i.e.,not necessarily real-valued) utility functions. We show that every agent, ``rational'' or not, can be modeled as a GEU maximizer. We then show that we can customize GEU by selectively imposing just the constraints we want. In particular, we show how each of Savage's postulates corresponds to constraints on GEU.
cs.AI:Many different rules for decision making have been introduced in the literature. We show that a notion of generalized expected utility proposed in Part I of this paper is a universal decision rule, in the sense that it can represent essentially all other decision rules.
cs.AI:This paper describes a novel approach to grammar induction that has been developed within a framework designed to integrate learning with other aspects of computing, AI, mathematics and logic. This framework, called "information compression by multiple alignment, unification and search" (ICMAUS), is founded on principles of Minimum Length Encoding pioneered by Solomonoff and others. Most of the paper describes SP70, a computer model of the ICMAUS framework that incorporates processes for unsupervised learning of grammars. An example is presented to show how the model can infer a plausible grammar from appropriate input. Limitations of the current model and how they may be overcome are briefly discussed.
cs.AI:We consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations, well-known in QSR. The more general, integrating language we consider is based on convex constraints of the qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$, with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning of the quantitative constraint, in particular, is that point $x$ belongs to the (convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and $\beta$. The general form of a constraint is a disjunction of the form $[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha _{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha _i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above: the meaning of such a general constraint is that, for some $i=1... n_1$, $r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A conjunction of such general constraints is a $\tcsp$-like CSP, which we will refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective solution search algorithm for an $\scsp$ will be described, which uses (1) constraint propagation, based on a composition operation to be defined, as the filtering method during the search, and (2) the Simplex algorithm, guaranteeing completeness, at the leaves of the search tree. The approach is particularly suited for large-scale high-level vision, such as, e.g., satellite-like surveillance of a geographic area.
cs.AI:Object oriented constraint programs (OOCPs) emerge as a leading evolution of constraint programming and artificial intelligence, first applied to a range of industrial applications called configuration problems. The rich variety of technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP, Terminological systems, constraint programs with set variables ...) is a source of difficulty. No universally accepted formal language exists for communicating about OOCPs, which makes the comparison of systems difficult. We present here a Z based specification of OOCPs which avoids the falltrap of hidden object semantics. The object system is part of the specification, and captures all of the most advanced notions from the object oriented modeling standard UML. The paper illustrates these issues and the conciseness and precision of Z by the specification of a working OOCP that solves an historical AI problem : parsing a context free grammar. Being written in Z, an OOCP specification also supports formal proofs. The whole builds the foundation of an adaptative and evolving framework for communicating about constrained object models and programs.
cs.AI:In this paper we suggest an architecture for a software agent which operates a physical device and is capable of making observations and of testing and repairing the device's components. We present simplified definitions of the notions of symptom, candidate diagnosis, and diagnosis which are based on the theory of action language ${\cal AL}$. The definitions allow one to give a simple account of the agent's behavior in which many of the agent's tasks are reduced to computing stable models of logic programs.
cs.AI:We compare two recent extensions of the answer set (stable model) semantics of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the bodies and heads of rules to contain nested expressions. The other, due to Niemela and Simons, uses weight constraints. We show that there is a simple, modular translation from the language of weight constraints into the language of nested expressions that preserves the program's answer sets. Nested expressions can be eliminated from the result of this translation in favor of additional atoms. The translation makes it possible to compute answer sets for some programs with weight constraints using satisfiability solvers, and to prove the strong equivalence of programs with weight constraints using the logic of here-and there.
cs.AI:(We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend Schlipf's result to prove that SLP solves all search problems in the class $\mathit{NP}$. Moreover, we do this in a uniform way as defined in \cite{mt99}. Specifically, we show that there is a single $\mathrm{DATALOG}^{\neg}$ program $P_{\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$ with non-negative integer coefficients and any input $\sigma$ of size $n$ over a fixed alphabet $\Sigma$, there is an extensional database $\mathit{edb}_{M,p,\sigma}$ such that there is a one-to-one correspondence between the stable models of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ and the accepting computations of the machine $M$ that reach the final state in at most $p(n)$ steps. Moreover, $\mathit{edb}_{M,p,\sigma}$ can be computed in polynomial time from $p$, $\sigma$ and the description of $M$ and the decoding of such accepting computations from its corresponding stable model of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ can be computed in linear time. A similar statement holds for Default Logic with respect to $\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result involves additional technical complications and will be a subject of another publication.}.
cs.AI:This book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as "information compression by multiple alignment, unification and search". This "SP theory", which has been under development since 1987, provides a unified view of such things as the workings of a universal Turing machine, the nature of 'knowledge', the interpretation and production of natural language, pattern recognition and best-match information retrieval, several kinds of probabilistic reasoning, planning and problem solving, unsupervised learning, and a range of concepts in mathematics and logic. The theory also provides a basis for the design of an 'SP' computer with several potential advantages compared with traditional digital computers.
cs.AI:In rule-based systems, goal-oriented computations correspond naturally to the possible ways that an observation may be explained. In some applications, we need to compute explanations for a series of observations with the same domain. The question whether previously computed answers can be recycled arises. A yes answer could result in substantial savings of repeated computations. For systems based on classic logic, the answer is YES. For nonmonotonic systems however, one tends to believe that the answer should be NO, since recycling is a form of adding information. In this paper, we show that computed answers can always be recycled, in a nontrivial way, for the class of rewrite procedures that we proposed earlier for logic programs with negation. We present some experimental results on an encoding of the logistics domain.
cs.AI:Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. This framework can be successfully applied to the investigation of the performance of Artificial Intelligence (AI) inference and cognitive systems. In this paper, we delineate a categorical formalisation of memory as a control structure driving performance in inference systems. Abstracting away control mechanisms from three widely used representations of memory in cognitive systems (scripts, production rules and clusters) we explain how categorical triples capture the interaction between learning and problem-solving.
cs.AI:The field of machine learning (ML) is concerned with the question of how to construct algorithms that automatically improve with experience. In recent years many successful ML applications have been developed, such as datamining programs, information-filtering systems, etc. Although ML algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems, most of these algorithms are based on quantitative reasoning, as they rely on training data in order to infer so-called target functions.   In the last years defeasible argumentation has proven to be a sound setting to formalize common-sense qualitative reasoning. This approach can be combined with other inference techniques, such as those provided by machine learning theory.   In this paper we outline different alternatives for combining defeasible argumentation and machine learning techniques. We suggest how different aspects of a generic argument-based framework can be integrated with other ML-based approaches.
cs.AI:Stable model semantics has become a very popular approach for the management of negation in logic programming. This approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called Gelfond-Lifschitz transformation.   The primary goal of this work is to present an alternative and epistemic-based characterization of stable model semantics, to the Gelfond-Lifschitz transformation. In particular, we show that stable model semantics can be defined entirely as an extension of the Kripke-Kleene semantics. Indeed, we show that the closed world assumption can be seen as an additional source of `falsehood' to be added cumulatively to the Kripke-Kleene semantics. Our approach is purely algebraic and can abstract from the particular formalism of choice as it is based on monotone operators (under the knowledge order) over bilattices only.
cs.AI:We address the problem of the development of representations and their relationship to the environment. We study a software agent which develops in a network a representation of its simple environment which captures and integrates the relationships between agent and environment through a closure mechanism. The inclusion of a variable behavior modifier allows better representation development. This can be confirmed with an internal description of the closure mechanism, and with an external description of the properties of the representation network.
cs.AI:This document describes syntax, semantics and implementation guidelines in order to enrich the DLV system with the possibility to make external C function calls. This feature is realized by the introduction of parametric external predicates, whose extension is not specified through a logic program but implicitly computed through external code.
cs.AI:This document describes the functions as they are treated in the DLV system. We give first the language, then specify the main implementation issues.
cs.AI:We introduce Ak, an extension of the action description language A (Gelfond and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. We also present a translation of Ak domain descriptions into epistemic logic programs.
cs.AI:In this paper, we examine the performance of four fuzzy rule generation methods on Wisconsin breast cancer data. The first method generates fuzzy if then rules using the mean and the standard deviation of attribute values. The second approach generates fuzzy if then rules using the histogram of attributes values. The third procedure generates fuzzy if then rules with certainty of each attribute into homogeneous fuzzy sets. In the fourth approach, only overlapping areas are partitioned. The first two approaches generate a single fuzzy if then rule for each class by specifying the membership function of each antecedent fuzzy set using the information about attribute values of training patterns. The other two approaches are based on fuzzy grids with homogeneous fuzzy partitions of each attribute. The performance of each approach is evaluated on breast cancer data sets. Simulation results show that the Modified grid approach has a high classification rate of 99.73 %.
cs.AI:The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.
cs.AI:Neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems. This paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on Box--Jenkins autoregressive integrated moving average (ARIMA) model to predict electricity demand in the State of Victoria, Australia. The soft computing methods considered are an evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN) trained using scaled conjugate gradient algorithm (CGA) and backpropagation (BP) algorithm. The forecast accuracy is compared with the forecasts used by Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models. Test results show that the neuro-fuzzy system performed better than neural networks, ARIMA model and the VPX forecasts.
cs.AI:Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model.
cs.AI:Long-term rainfall prediction is a challenging task especially in the modern world where we are facing the major environmental problem of global warming. In general, climate and rainfall are highly non-linear phenomena in nature exhibiting what is known as the butterfly effect. While some regions of the world are noticing a systematic decrease in annual rainfall, others notice increases in flooding and severe storms. The global nature of this phenomenon is very complicated and requires sophisticated computer modeling and simulation to predict accurately. In this paper, we report a performance analysis for Multivariate Adaptive Regression Splines (MARS)and artificial neural networks for one month ahead prediction of rainfall. To evaluate the prediction efficiency, we made use of 87 years of rainfall data in Kerala state, the southern part of the Indian peninsula situated at latitude -longitude pairs (8o29'N - 76o57' E). We used an artificial neural network trained using the scaled conjugate gradient algorithm. The neural network and MARS were trained with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that MARS is a good forecasting tool and performed better than the considered neural network.
cs.AI:Classification of texture pattern is one of the most important problems in pattern recognition. In this paper, we present a classification method based on the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works on gray level image, the color scheme of each image is transformed into gray levels. For classifying the images using DCT we used two popular soft computing techniques namely neurocomputing and neuro-fuzzy computing. We used a feedforward neural network trained using the backpropagation learning and an evolving fuzzy neural network to classify the textures. The soft computing models were trained using 80% of the texture data and remaining was used for testing and validation purposes. A performance comparison was made among the soft computing models for the texture classification problem. We also analyzed the effects of prolonged training of neural networks. It is observed that the proposed neuro-fuzzy model performed better than neural network.
cs.AI:Sorting by reversals is an important problem in inferring the evolutionary relationship between two genomes. The problem of sorting unsigned permutation has been proven to be NP-hard. The best guaranteed error bounded is the 3/2- approximation algorithm. However, the problem of sorting signed permutation can be solved easily. Fast algorithms have been developed both for finding the sorting sequence and finding the reversal distance of signed permutation. In this paper, we present a way to view the problem of sorting unsigned permutation as signed permutation. And the problem can then be seen as searching an optimal signed permutation in all n2 corresponding signed permutations. We use genetic algorithm to conduct the search. Our experimental result shows that the proposed method outperform the 3/2-approximation algorithm.
cs.AI:Past few years have witnessed a growing recognition of intelligent techniques for the construction of efficient and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDS) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given.
cs.AI:The aim of our research was to apply well-known data mining techniques (such as linear neural networks, multi-layered perceptrons, probabilistic neural networks, classification and regression trees, support vector machines and finally a hybrid decision tree neural network approach) to the problem of predicting the quality of service in call centers; based on the performance data actually collected in a call center of a large insurance company. Our aim was two-fold. First, to compare the performance of models built using the above-mentioned techniques and, second, to analyze the characteristics of the input sensitivity in order to better understand the relationship between the perform-ance evaluation process and the actual performance and in this way help improve the performance of call centers. In this paper we summarize our findings.
cs.AI:The use of intelligent systems for stock market predictions has been widely established. In this paper, we investigate how the seemingly chaotic behavior of stock markets could be well represented using several connectionist paradigms and soft computing techniques. To demonstrate the different techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4 year's NIFTY index values. This paper investigates the development of a reliable and efficient technique to model the seemingly chaotic behavior of stock markets. We considered an artificial neural network trained using Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper briefly explains how the different connectionist paradigms could be formulated using different learning methods and then investigates whether they can provide the required level of performance, which are sufficiently good and robust so as to provide a reliable forecast model for stock market indices. Experiment results reveal that all the connectionist paradigms considered could represent the stock indices behavior very accurately.
cs.AI:The purpose of this paper is to point to the usefulness of applying a linear mathematical formulation of fuzzy multiple criteria objective decision methods in organising business activities. In this respect fuzzy parameters of linear programming are modelled by preference-based membership functions. This paper begins with an introduction and some related research followed by some fundamentals of fuzzy set theory and technical concepts of fuzzy multiple objective decision models. Further a real case study of a manufacturing plant and the implementation of the proposed technique is presented. Empirical results clearly show the superiority of the fuzzy technique in optimising individual objective functions when compared to non-fuzzy approach. Furthermore, for the problem considered, the optimal solution helps to infer that by incorporating fuzziness in a linear programming model either in constraints, or both in objective functions and constraints, provides a similar (or even better) level of satisfaction for obtained results compared to non-fuzzy linear programming.
cs.AI:In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial Neural Network), an automatic computational framework for the adaptive optimization of artificial neural networks wherein the neural network architecture, activation function, connection weights; learning algorithm and its parameters are adapted according to the problem. We explored the performance of MLEANN and conventionally designed artificial neural networks for function approximation problems. To evaluate the comparative performance, we used three different well-known chaotic time series. We also present the state of the art popular neural network learning algorithms and some experimentation results related to convergence speed and generalization performance. We explored the performance of backpropagation algorithm; conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt algorithm for the three chaotic time series. Performances of the different learning algorithms were evaluated when the activation functions and architecture were changed. We further present the theoretical background, algorithm, design strategy and further demonstrate how effective and inevitable is the proposed MLEANN framework to design a neural network, which is smaller, faster and with a better generalization performance.
cs.AI:The phylogenetic tree construction is to infer the evolutionary relationship between species from the experimental data. However, the experimental data are often imperfect and conflicting each others. Therefore, it is important to extract the motif from the imperfect data. The largest compatible subset problem is that, given a set of experimental data, we want to discard the minimum such that the remaining is compatible. The largest compatible subset problem can be viewed as the vertex cover problem in the graph theory that has been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary Computing (EC) method for this problem. The proposed method combines the EC approach and the algorithmic approach for special structured graphs. As a result, the complexity of the problem is dramatically reduced. Experiments were performed on randomly generated graphs with different edge densities. The vertex covers produced by the proposed method were then compared to the vertex covers produced by a 2-approximation algorithm. The experimental results showed that the proposed method consistently outperformed a classical 2- approximation algorithm. Furthermore, a significant improvement was found when the graph density was small.
cs.AI:Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing technologies that underlie the conception, design and utilization of intelligent systems. Several works have been done where engineers and scientists have applied intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a concurrent fuzzy-neural network approach combining unsupervised and supervised learning techniques to develop the Tactical Air Combat Decision Support System (TACDSS). Experiment results clearly demonstrate the efficiency of the proposed technique.
cs.AI:In a universe with a single currency, there would be no foreign exchange market, no foreign exchange rates, and no foreign exchange. Over the past twenty-five years, the way the market has performed those tasks has changed enormously. The need for intelligent monitoring systems has become a necessity to keep track of the complex forex market. The vast currency market is a foreign concept to the average individual. However, once it is broken down into simple terms, the average individual can begin to understand the foreign exchange market and use it as a financial instrument for future investing. In this paper, we attempt to compare the performance of hybrid soft computing and hard computing techniques to predict the average monthly forex rates one month ahead. The soft computing models considered are a neural network trained by the scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive Regression Splines (MARS), Classification and Regression Trees (CART) and a hybrid CART-MARS technique. We considered the exchange rates of Australian dollar with respect to US dollar, Singapore dollar, New Zealand dollar, Japanese yen and United Kingdom pounds. The models were trained using 70% of the data and remaining was used for testing and validation purposes. It is observed that the proposed hybrid models could predict the forex rates more accurately than all the techniques when applied individually. Empirical results also reveal that the hybrid hard computing approach also improved some of our previous work using a neuro-fuzzy approach.
cs.AI:The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. In this paper, we present the important concepts of Web usage mining and its various practical applications. We further present a novel approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy inference system to analyze the Web site visitor trends. A hybrid evolutionary fuzzy clustering algorithm is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Proposed approach is compared with self-organizing maps (to discover patterns) and several function approximation techniques like neural networks, linear genetic programming and Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are graphically illustrated and the practical significance is discussed in detail. Empirical results clearly show that the proposed Web usage-mining framework is efficient.
cs.AI:Normally a decision support system is build to solve problem where multi-criteria decisions are involved. The knowledge base is the vital part of the decision support containing the information or data that is used in decision-making process. This is the field where engineers and scientists have applied several intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a hybrid neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference system for the Tactical Air Combat Decision Support System (TACDSS). Some simulation results demonstrating the difference of the learning techniques and are also provided.
cs.AI:Several adaptation techniques have been investigated to optimize fuzzy inference systems. Neural network learning algorithms have been used to determine the parameters of fuzzy inference system. Such models are often called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model there is no guarantee that the neural network learning algorithm converges and the tuning of fuzzy inference system will be successful. Success of evolutionary search procedures for optimization of fuzzy inference system is well proven and established in many application areas. In this paper, we will explore how the optimization of fuzzy inference systems could be further improved using a meta-heuristic approach combining neural network learning and evolutionary computation. The proposed technique could be considered as a methodology to integrate neural networks, fuzzy inference systems and evolutionary search procedures. We present the theoretical frameworks and some experimental results to demonstrate the efficiency of the proposed technique.
cs.AI:Evolutionary artificial neural networks (EANNs) refer to a special class of artificial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. Evolutionary algorithms are used to adapt the connection weights, network architecture and learning algorithms according to the problem environment. Even though evolutionary algorithms are well known as efficient global search algorithms, very often they miss the best local solutions in the complex solution space. In this paper, we propose a hybrid meta-heuristic learning approach combining evolutionary learning and local search methods (using 1st and 2nd order error information) to improve the learning and faster convergence obtained using a direct evolutionary approach. The proposed technique is tested on three different chaotic time series and the test results are compared with some popular neuro-fuzzy systems and a recently developed cutting angle method of global optimization. Empirical results reveal that the proposed technique is efficient in spite of the computational complexity.
cs.AI:The academic literature suggests that the extent of exporting by multinational corporation subsidiaries (MCS) depends on their product manufactured, resources, tax protection, customers and markets, involvement strategy, financial independence and suppliers' relationship with a multinational corporation (MNC). The aim of this paper is to model the complex export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order to determine the actual volume of MCS export output (sales exported). The proposed fuzzy inference system is optimised by using neural network learning and evolutionary computation. Empirical results clearly show that the proposed approach could model the export behaviour reasonable well compared to a direct neural network approach.
cs.AI:The costs of fatalities and injuries due to traffic accident have a great impact on society. This paper presents our research to model the severity of injury resulting from traffic accidents using artificial neural networks and decision trees. We have applied them to an actual data set obtained from the National Automotive Sampling System (NASS) General Estimates System (GES). Experiment results reveal that in all the cases the decision tree outperforms the neural network. Our research analysis also shows that the three most important factors in fatal injury are: driver's seat belt usage, light condition of the roadway, and driver's alcohol usage.
cs.AI:This paper presents a comparative study of six soft computing models namely multilayer perceptron networks, Elman recurrent neural network, radial basis function network, Hopfield model, fuzzy inference system and hybrid fuzzy neural network for the hourly electricity demand forecast of Czech Republic. The soft computing models were trained and tested using the actual hourly load data for seven years. A comparison of the proposed techniques is presented for predicting 2 day ahead demands for electricity. Simulation results indicate that hybrid fuzzy neural network and radial basis function networks are the best candidates for the analysis and forecasting of electricity demand.
cs.AI:Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing (SC) technologies that underlie the conception, design and utilization of intelligent systems. In this paper, we present different SC paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm, two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems. We demonstrate the efficiency of the different algorithms by developing a decision support system for a Tactical Air Combat Environment (TACE). Some empirical comparisons between the different algorithms are also provided.
cs.AI:In this paper, we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ the 0-approximation [Son & Baral 2001] to define the regression function. In binary domains, the use of 0-approximation means using 3-valued states. Although planning using this approach is incomplete with respect to the full semantics, we adopt it to have a lower complexity. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one. We then develop a conditional planner that utilizes our regression function. We prove the soundness and completeness of our planning algorithm and present experimental results with respect to several well known planning problems in the literature.
cs.AI:Defeasible logic is a rule-based nonmonotonic logic, with both strict and defeasible rules, and a priority relation on rules. We show that inference in the propositional form of the logic can be performed in linear time. This contrasts markedly with most other propositional nonmonotonic logics, in which inference is intractable.
cs.AI:Defeasible argumentation has experienced a considerable growth in AI in the last decade. Theoretical results have been combined with development of practical applications in AI & Law, Case-Based Reasoning and various knowledge-based systems. However, the dialectical process associated with inference is computationally expensive. This paper focuses on speeding up this inference process by pruning the involved search space. Our approach is twofold. On one hand, we identify distinguished literals for computing defeat. On the other hand, we restrict ourselves to a subset of all possible conflicting arguments by introducing dialectical constraints.
cs.AI:Main purposes of the paper are followings: 1) To show examples of the calculations in domain of QFT via ``derivative rules'' of an expert system; 2) To consider advantages and disadvantage that technology of the calculations; 3) To reflect about how one would develop new physical theories, what knowledge would be useful in their investigations and how this problem can be connected with designing an expert system.
cs.AI:We will try to tackle both the theoretical and practical aspects of a very important problem in chess programming as stated in the title of this article - the issue of draw detection by move repetition. The standard approach that has so far been employed in most chess programs is based on utilising positional matrices in original and compressed format as well as on the implementation of the so-called bitboard format.   The new approach that we will be trying to introduce is based on using variant strings generated by the search algorithm (searcher) during the tree expansion in decision making. We hope to prove that this approach is more efficient than the standard treatment of the issue, especially in positions with few pieces (endgames). To illustrate what we have in mind a machine language routine that implements our theoretical assumptions is attached. The routine is part of the Axon chess program, developed by the authors. Axon, in its current incarnation, plays chess at master strength (ca. 2400-2450 Elo, based on both Axon vs computer programs and Axon vs human masters in over 3000 games altogether).
cs.AI:Learning to respond to voice-text input involves the subject's ability in understanding the phonetic and text based contents and his/her ability to communicate based on his/her experience. The neuro-cognitive facility of the subject has to support two important domains in order to make the learning process complete. In many cases, though the understanding is complete, the response is partial. This is one valid reason why we need to support the information from the subject with scalable techniques such as Natural Language Processing (NLP) for abstraction of the contents from the output. This paper explores the feasibility of using NLP modules interlaced with Neural Networks to perform the required task in autogenic training related to medical applications.
cs.AI:Generalized evolutionary algorithm based on Tsallis canonical distribution is proposed. The algorithm uses Tsallis generalized canonical distribution to weigh the configurations for `selection' instead of Gibbs-Boltzmann distribution. Our simulation results show that for an appropriate choice of non-extensive index that is offered by Tsallis statistics, evolutionary algorithms based on this generalization outperform algorithms based on Gibbs-Boltzmann distribution.
cs.AI:In this paper we present and evaluate a search strategy called Decomposition Based Search (DBS) which is based on two steps: subproblem generation and subproblem solution. The generation of subproblems is done through value ranking and domain splitting. Subdomains are explored so as to generate, according to the heuristic chosen, promising subproblems first.   We show that two well known search strategies, Limited Discrepancy Search (LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First we present a tuning of DBS that visits the same search nodes as IB, but avoids restarts. Then we compare both theoretically and computationally DBS and LDS using the same heuristic. We prove that DBS has a higher probability of being successful than LDS on a comparable number of nodes, under realistic assumptions. Experiments on a constraint satisfaction problem and an optimization problem show that DBS is indeed very effective if compared to LDS.
cs.AI:Solution techniques for Constraint Satisfaction and Optimisation Problems often make use of backtrack search methods, exploiting variable and value ordering heuristics. In this paper, we propose and analyse a very simple method to apply in case the value ordering heuristic produces ties: postponing the branching decision. To this end, we group together values in a tie, branch on this sub-domain, and defer the decision among them to lower levels of the search tree. We show theoretically and experimentally that this simple modification can dramatically improve the efficiency of the search strategy. Although in practise similar methods may have been applied already, to our knowledge, no empirical or theoretical study has been proposed in the literature to identify when and to what extent this strategy should be used.
cs.AI:In this paper, we propose an effective search procedure that interleaves two steps: subproblem generation and subproblem solution. We mainly focus on the first part. It consists of a variable domain value ranking based on reduced costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search tree, the most promising subproblems first. An interesting result is that reduced costs provide a very precise ranking that allows to almost always find the optimal solution in the first generated subproblem, even if its dimension is significantly smaller than that of the original problem. Concerning the proof of optimality, we exploit a way to increase the lower bound for subproblems at higher discrepancies. We show experimental results on the TSP and its time constrained variant to show the effectiveness of the proposed approach, but the technique could be generalized for other problems.
cs.AI:One proposes a first alternative rule of combination to WAO (Weighted Average Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are particular cases of WO (the Weighted Operator) because the conflicting mass is redistributed with respect to some weighting factors. In this first PCR rule, the proportionalization is done for each non-empty set with respect to the non-zero sum of its corresponding mass matrix - instead of its mass column average as in WAO, but the results are the same as Ph. Smets has pointed out. Also, we extend WAO (which herein gives no solution) for the degenerate case when all column sums of all non-empty sets are zero, and then the conflicting mass is transferred to the non-empty disjunctive form of all non-empty sets together; but if this disjunctive form happens to be empty, then one considers an open world (i.e. the frame of discernment might contain new hypotheses) and thus all conflicting mass is transferred to the empty set. In addition to WAO, we propose a general formula for PCR1 (WAO for non-degenerate cases).
cs.AI:In this paper one proposes a simple algorithm of combining the fusion rules, those rules which first use the conjunctive rule and then the transfer of conflicting mass to the non-empty sets, in such a way that they gain the property of associativity and fulfill the Markovian requirement for dynamic fusion. Also, a new rule, SDL-improved, is presented.
cs.AI:FLUX is a programming method for the design of agents that reason logically about their actions and sensor information in the presence of incomplete knowledge. The core of FLUX is a system of Constraint Handling Rules, which enables agents to maintain an internal model of their environment by which they control their own behavior. The general action representation formalism of the fluent calculus provides the formal semantics for the constraint solver. FLUX exhibits excellent computational behavior due to both a carefully restricted expressiveness and the inference paradigm of progression.
cs.AI:Boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis. However, Boltzmann selection is not used in practice because a good annealing schedule for the `inverse temperature' parameter is lacking. In this paper we propose a Cauchy annealing schedule for Boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge. To formalize these aspects, we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength. In this paper, we prove an important result, by which we derive an annealing schedule called Cauchy annealing schedule. We demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms.
cs.AI:In this paper we propose five versions of a Proportional Conflict Redistribution rule (PCR) for information fusion together with several examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of the rules and also the exactitude of the redistribution of conflicting masses. PCR1 restricted from the hyper-power set to the power set and without degenerate cases gives the same result as the Weighted Average Operator (WAO) proposed recently by J{\o}sang, Daniel and Vannoorenberghe but does not satisfy the neutrality property of vacuous belief assignment. That's why improved PCR rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's rules. The PCR rules redistribute the conflicting mass, after the conjunctive rule has been applied, proportionally with some functions depending on the masses assigned to their corresponding columns in the mass matrix. There are infinitely many ways these functions (weighting factors) can be chosen depending on the complexity one wants to deal with in specific applications and fusion systems. Any fusion combination rule is at some degree ad-hoc.
cs.AI:This paper presents in detail the generalized pignistic transformation (GPT) succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a tool for decision process. The GPT allows to provide a subjective probability measure from any generalized basic belief assignment given by any corpus of evidence. We mainly focus our presentation on the 3D case and provide the complete result obtained by the GPT and its validation drawn from the probability theory.
cs.AI:Since no fusion theory neither rule fully satisfy all needed applications, the author proposes a Unification of Fusion Theories and a combination of fusion rules in solving problems/applications. For each particular application, one selects the most appropriate model, rule(s), and algorithm of implementation. We are working in the unification of the fusion theories and rules, which looks like a cooking recipe, better we'd say like a logical chart for a computer programmer, but we don't see another method to comprise/unify all things. The unification scenario presented herein, which is now in an incipient form, should periodically be updated incorporating new discoveries from the fusion and engineering research.
cs.AI:Normal forms for logic programs under stable/answer set semantics are introduced. We argue that these forms can simplify the study of program properties, mainly consistency. The first normal form, called the {\em kernel} of the program, is useful for studying existence and number of answer sets. A kernel program is composed of the atoms which are undefined in the Well-founded semantics, which are those that directly affect the existence of answer sets. The body of rules is composed of negative literals only. Thus, the kernel form tends to be significantly more compact than other formulations. Also, it is possible to check consistency of kernel programs in terms of colorings of the Extended Dependency Graph program representation which we previously developed. The second normal form is called {\em 3-kernel.} A 3-kernel program is composed of the atoms which are undefined in the Well-founded semantics. Rules in 3-kernel programs have at most two conditions, and each rule either belongs to a cycle, or defines a connection between cycles. 3-kernel programs may have positive conditions. The 3-kernel normal form is very useful for the static analysis of program consistency, i.e., the syntactic characterization of existence of answer sets. This result can be obtained thanks to a novel graph-like representation of programs, called Cycle Graph which presented in the companion article \cite{Cos04b}.
cs.AI:This paper may look like a glossary of the fusion rules and we also introduce new ones presenting their formulas and examples: Conjunctive, Disjunctive, Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule, Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache classical and hybrid rules, Murphy's average rule, Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as particular cases: Iganaki's parameterized rule, Weighting Average Operator, minC (M. Daniel), and newly Proportional Conflict Redistribution rules (Smarandache-Dezert) among which PCR5 is the most exact way of redistribution of the conflicting mass to non-empty sets following the path of the conjunctive rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to information fusion (Tchamova-Smarandache). Introducing the degree of union and degree of inclusion with respect to the cardinal of sets not with the fuzzy set point of view, besides that of intersection, many fusion rules can be improved. There are corner cases where each rule might have difficulties working or may not get an expected result.
cs.AI:There are many examples in the literature that suggest that indistinguishability is intransitive, despite the fact that the indistinguishability relation is typically taken to be an equivalence relation (and thus transitive). It is shown that if the uncertainty perception and the question of when an agent reports that two things are indistinguishable are both carefully modeled, the problems disappear, and indistinguishability can indeed be taken to be an equivalence relation. Moreover, this model also suggests a logic of vagueness that seems to solve many of the problems related to vagueness discussed in the philosophical literature. In particular, it is shown here how the logic can handle the sorites paradox.
cs.AI:A careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony. The implications of this analysis for van Fraassen's Reflection Principle and Savage's Sure-Thing Principle are considered.
cs.AI:The paper is an attempt to generalize a methodology, which is similar to the bounded-input bounded-output method currently widely used for the system stability studies. The presented earlier methodology allows decomposition of input space into bounded subspaces and defining for each subspace its bounding surface. It also defines a corresponding predefined control, which maps any point of a bounded input into a desired bounded output subspace. This methodology was improved by providing a mechanism for the fast defining a bounded surface. This paper presents enhanced bounded-input bounded-predefined-control bounded-output approach, which provides adaptability feature to the control and allows transferring of a controlled system along a suboptimal trajectory.
cs.AI:The number of probability distributions required to populate a conditional probability table (CPT) in a Bayesian network, grows exponentially with the number of parent-nodes associated with that table. If the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier. In this paper we devise an algorithm to populate the CPT while easing the extent of knowledge acquisition. The input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node, and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes. These are elicited from the domain expert. The set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations. The algorithm is used to populate the CPT by computing appropriate weighted sums of the elicited distributions. We invoke the methods of information geometry to demonstrate how these weighted sums capture the expert's judgemental strategy.
cs.AI:Consider the problem of tracking a set of moving targets. Apart from the tracking result, it is often important to know where the tracking fails, either to steer sensors to that part of the state-space, or to inform a human operator about the status and quality of the obtained information. An intuitive quality measure is the correlation between two tracking results based on uncorrelated observations. In the case of Bayesian trackers such a correlation measure could be the Kullback-Leibler difference.   We focus on a scenario with a large number of military units moving in some terrain. The units are observed by several types of sensors and "meta-sensors" with force aggregation capabilities. The sensors register units of different size. Two separate multi-target probability hypothesis density (PHD) particle filters are used to track some type of units (e.g., companies) and their sub-units (e.g., platoons), respectively, based on observations of units of those sizes. Each observation is used in one filter only.   Although the state-space may well be the same in both filters, the posterior PHD distributions are not directly comparable -- one unit might correspond to three or four spatially distributed sub-units. Therefore, we introduce a mapping function between distributions for different unit size, based on doctrine knowledge of unit configuration.   The mapped distributions can now be compared -- locally or globally -- using some measure, which gives the correlation between two PHD distributions in a bounded volume of the state-space. To locate areas where the tracking fails, a discretized quality map of the state-space can be generated by applying the measure locally to different parts of the space.
cs.AI:We describe the recently introduced extremal optimization algorithm and apply it to target detection and association problems arising in pre-processing for multi-target tracking.   Here we consider the problem of pre-processing for multiple target tracking when the number of sensor reports received is very large and arrives in large bursts. In this case, it is sometimes necessary to pre-process reports before sending them to tracking modules in the fusion system. The pre-processing step associates reports to known tracks (or initializes new tracks for reports on objects that have not been seen before). It could also be used as a pre-process step before clustering, e.g., in order to test how many clusters to use.   The pre-processing is done by solving an approximate version of the original problem. In this approximation, not all pair-wise conflicts are calculated. The approximation relies on knowing how many such pair-wise conflicts that are necessary to compute. To determine this, results on phase-transitions occurring when coloring (or clustering) large random instances of a particular graph ensemble are used.
cs.AI:The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this chapter, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach. The last part of this chapter concerns the presentation of the neutrosophic logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and neutrosophic logic are useful tools in decision making after fusioning the information using the DSm hybrid rule of combination of masses.
cs.AI:In this paper, we propose a new method based on Hidden Markov Models to interpret temporal sequences of sensor data from mobile robots to automatically detect features. Hidden Markov Models have been used for a long time in pattern recognition, especially in speech recognition. Their main advantages over other methods (such as neural networks) are their ability to model noisy temporal signals of variable length. We show in this paper that this approach is well suited for interpretation of temporal sequences of mobile-robot sensor data. We present two distinct experiments and results: the first one in an indoor environment where a mobile robot learns to detect features like open doors or T-intersections, the second one in an outdoor environment where a different mobile robot has to identify situations like climbing a hill or crossing a rock.
cs.AI:In this paper, we present a rich semantic network based on a differential analysis. We then detail implemented measures that take into account common and differential features between words. In a last section, we describe some industrial applications.
cs.AI:Answer set programming (ASP) with disjunction offers a powerful tool for declaratively representing and solving hard problems. Many NP-complete problems can be encoded in the answer set semantics of logic programs in a very concise and intuitive way, where the encoding reflects the typical "guess and check" nature of NP problems: The property is encoded in a way such that polynomial size certificates for it correspond to stable models of a program. However, the problem-solving capacity of full disjunctive logic programs (DLPs) is beyond NP, and captures a class of problems at the second level of the polynomial hierarchy. While these problems also have a clear "guess and check" structure, finding an encoding in a DLP reflecting this structure may sometimes be a non-obvious task, in particular if the "check" itself is a coNP-complete problem; usually, such problems are solved by interleaving separate guess and check programs, where the check is expressed by inconsistency of the check program. In this paper, we present general transformations of head-cycle free (extended) disjunctive logic programs into stratified and positive (extended) disjunctive logic programs based on meta-interpretation techniques. The answer sets of the original and the transformed program are in simple correspondence, and, moreover, inconsistency of the original program is indicated by a designated answer set of the transformed program. Our transformations facilitate the integration of separate "guess" and "check" programs, which are often easy to obtain, automatically into a single disjunctive logic program. Our results complement recent results on meta-interpretation in ASP, and extend methods and techniques for a declarative "guess and check" problem solving paradigm through ASP.
cs.AI:This paper presents an approach to enhance search engines with information about word senses available in WordNet. The approach exploits information about the conceptual relations within the lexical-semantic net. In the wrapper for search engines presented, WordNet information is used to specify user's request or to classify the results of a publicly available web search engine, like google, yahoo, etc.
cs.AI:This paper reports about experiments with GermaNet as a resource within domain specific document analysis. The main question to be answered is: How is the coverage of GermaNet in a specific domain? We report about results of a field test of GermaNet for analyses of autopsy protocols and present a sketch about the integration of GermaNet inside XDOC. Our remarks will contribute to a GermaNet user's wish list.
cs.AI:The aim of the project presented in this paper is to design a system for an NLG architecture, which supports the documentation process of eBusiness models. A major task is to enrich the formal description of an eBusiness model with additional information needed in an NLG task.
cs.AI:Lexical semantic resources, like WordNet, are often used in real applications of natural language document processing. For example, we integrated GermaNet in our document suite XDOC of processing of German forensic autopsy protocols. In addition to the hypernymy and synonymy relation, we want to adapt GermaNet's verb frames for our analysis. In this paper we outline an approach for the domain related enrichment of GermaNet verb frames by corpus based syntactic and co-occurred data analyses of real documents.
cs.AI:Real applications of natural language document processing are very often confronted with domain specific lexical gaps during the analysis of documents of a new domain. This paper describes an approach for the derivation of domain specific concepts for the extension of an existing ontology. As resources we need an initial ontology and a partially processed corpus of a domain. We exploit the specific characteristic of the sublanguage in the corpus. Our approach is based on syntactical structures (noun phrases) and compound analyses to extract information required for the extension of GermaNet's lexical resources.
cs.AI:We suggest to employ techniques from Natural Language Processing (NLP) and Knowledge Representation (KR) to transform existing documents into documents amenable for the Semantic Web. Semantic Web documents have at least part of their semantics and pragmatics marked up explicitly in both a machine processable as well as human readable manner. XML and its related standards (XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and methodologies developed for different application scenarios.
cs.AI:This text introduces the twin deadlocks of strong artificial life. Conceptualization of life is a deadlock both because of the existence of a continuum between the inert and the living, and because we only know one instance of life. Computationalism is a second deadlock since it remains a matter of faith. Nevertheless, artificial life realizations quickly progress and recent constructions embed an always growing set of the intuitive properties of life. This growing gap between theory and realizations should sooner or later crystallize in some kind of paradigm shift and then give clues to break the twin deadlocks.
cs.AI:In this chapter we describe new neural-network techniques developed for visual mining clinical electroencephalograms (EEGs), the weak electrical potentials invoked by brain activity. These techniques exploit fruitful ideas of Group Method of Data Handling (GMDH). Section 2 briefly describes the standard neural-network techniques which are able to learn well-suited classification modes from data presented by relevant features. Section 3 introduces an evolving cascade neural network technique which adds new input nodes as well as new neurons to the network while the training error decreases. This algorithm is applied to recognize artifacts in the clinical EEGs. Section 4 presents the GMDH-type polynomial networks learnt from data. We applied this technique to distinguish the EEGs recorded from an Alzheimer and a healthy patient as well as recognize EEG artifacts. Section 5 describes the new neural-network technique developed to induce multi-class concepts from data. We used this technique for inducing a 16-class concept from the large-scale clinical EEG data. Finally we discuss perspectives of applying the neural-network techniques to clinical EEGs.
cs.AI:Bayesian averaging over classification models allows the uncertainty of classification outcomes to be evaluated, which is of crucial importance for making reliable decisions in applications such as financial in which risks have to be estimated. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the diversity of a classifier ensemble and the required performance. The interpretability of classification models can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models. The required diversity of the DT ensemble can be achieved by using the Bayesian model averaging all possible DTs. In practice, the Bayesian approach can be implemented on the base of a Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior distribution. For sampling large DTs, the MCMC method is extended by Reversible Jump technique which allows inducing DTs under given priors. For the case when the prior information on the DT size is unavailable, the sweeping technique defining the prior implicitly reveals a better performance. Within this Chapter we explore the classification uncertainty of the Bayesian MCMC techniques on some datasets from the StatLog Repository and real financial data. The classification uncertainty is compared within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. This technique provides realistic estimates of the classification uncertainty which can be easily interpreted in statistical terms with the aim of risk evaluation.
cs.AI:Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of classification outcomes that is of crucial importance for safety critical applications. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the classifier diversity and the required performance. The interpretability of MCSs can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models for experts. The required diversity of MCSs exploiting such classification models can be achieved by using two techniques, the Bayesian model averaging and the randomised DT ensemble. Both techniques have revealed promising results when applied to real-world problems. In this paper we experimentally compare the classification uncertainty of the Bayesian model averaging with a restarting strategy and the randomised DT ensemble on a synthetic dataset and some domain problems commonly used in the machine learning community. To make the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo technique. The classification uncertainty is evaluated within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. Exploring a full posterior distribution, this technique produces realistic estimates which can be easily interpreted in statistical terms. In our experiments we found out that the Bayesian DTs are superior to the randomised DT ensembles within the Uncertainty Envelope technique.
cs.AI:Artificial intelligence (AI) research has evolved over the last few decades and knowledge acquisition research is at the core of AI research. PKAW-04 is one of three international knowledge acquisition workshops held in the Pacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong emphasis on incremental knowledge acquisition, machine learning, neural nets and active mining.   The proceedings contain 19 papers that were selected by the program committee among 24 submitted papers. All papers were peer reviewed by at least two reviewers. The papers in these proceedings cover the methods and tools as well as the applications related to develop expert systems or knowledge based systems.
cs.AI:In the frame of designing a knowledge discovery system, we have developed stochastic models based on high-order hidden Markov models. These models are capable to map sequences of data into a Markov chain in which the transitions between the states depend on the \texttt{n} previous states according to the order of the model. We study the process of achieving information extraction fromspatial and temporal data by means of an unsupervised classification. We use therefore a French national database related to the land use of a region, named Teruti, which describes the land use both in the spatial and temporal domain. Land-use categories (wheat, corn, forest, ...) are logged every year on each site regularly spaced in the region. They constitute a temporal sequence of images in which we look for spatial and temporal dependencies. The temporal segmentation of the data is done by means of a second-order Hidden Markov Model (\hmmd) that appears to have very good capabilities to locate stationary segments, as shown in our previous work in speech recognition. Thespatial classification is performed by defining a fractal scanning ofthe images with the help of a Hilbert-Peano curve that introduces atotal order on the sites, preserving the relation ofneighborhood between the sites. We show that the \hmmd performs aclassification that is meaningful for the agronomists.Spatial and temporal classification may be achieved simultaneously by means of a 2 levels \hmmd that measures the \aposteriori probability to map a temporal sequence of images onto a set of hidden classes.
cs.AI:Our ongoing work aims at defining an ontology-centered approach for building expertise models for the CommonKADS methodology. This approach (which we have named "OntoKADS") is founded on a core problem-solving ontology which distinguishes between two conceptualization levels: at an object level, a set of concepts enable us to define classes of problem-solving situations, and at a meta level, a set of meta-concepts represent modeling primitives. In this article, our presentation of OntoKADS will focus on the core ontology and, in particular, on roles - the primitive situated at the interface between domain knowledge and reasoning, and whose ontological status is still much debated. We first propose a coherent, global, ontological framework which enables us to account for this primitive. We then show how this novel characterization of the primitive allows definition of new rules for the construction of expertise models.
cs.AI:Automatic or assisted workflow composition is a field of intense research for applications to the world wide web or to business process modeling. Workflow composition is traditionally addressed in various ways, generally via theorem proving techniques. Recent research observed that building a composite workflow bears strong relationships with finite model search, and that some workflow languages can be defined as constrained object metamodels . This lead to consider the viability of applying configuration techniques to this problem, which was proven feasible. Constrained based configuration expects a constrained object model as input. The purpose of this document is to formally specify the constrained object model involved in ongoing experiments and research using the Z specification language.
cs.AI:To the reduct problems of decision system, the paper proposes the notion of dynamic core according to the dynamic reduct model. It describes various formal definitions of dynamic core, and discusses some properties about dynamic core. All of these show that dynamic core possesses the essential characters of the feature core.
cs.AI:Correlated time series are time series that, by virtue of the underlying process to which they refer, are expected to influence each other strongly. We introduce a novel approach to handle such time series, one that models their interaction as a two-dimensional cellular automaton and therefore allows them to be treated as a single entity. We apply our approach to the problems of filling gaps and predicting values in rainfall time series. Computational results show that the new approach compares favorably to Kalman smoothing and filtering.
cs.AI:ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems. Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed. The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results.
cs.AI:We present a declarative language, PP, for the high-level specification of preferences between possible solutions (or trajectories) of a planning problem. This novel language allows users to elegantly express non-trivial, multi-dimensional preferences and priorities over such preferences. The semantics of PP allows the identification of most preferred trajectories for a given goal. We also provide an answer set programming implementation of planning problems with PP preferences.
cs.AI:Clustering is a widely used technique in data mining applications for discovering patterns in underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either numeric or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining applications. In this paper, we propose a novel divide-and-conquer technique to solve this problem. First, the original mixed dataset is divided into two sub-datasets: the pure categorical dataset and the pure numeric dataset. Next, existing well established clustering algorithms designed for different types of datasets are employed to produce corresponding clusters. Last, the clustering results on the categorical and numeric dataset are combined as a categorical dataset, on which the categorical data clustering algorithm is used to get the final clusters. Our contribution in this paper is to provide an algorithm framework for the mixed attributes clustering problem, in which existing clustering algorithms can be easily integrated, the capabilities of different kinds of clustering algorithms and characteristics of different types of datasets could be fully exploited. Comparisons with other clustering algorithms on real life datasets illustrate the superiority of our approach.
cs.AI:Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-histogram, a new efficient algorithm for clustering categorical data. The k-histogram algorithm extends the k-means algorithm to categorical domain by replacing the means of clusters with histograms, and dynamically updates histograms in the clustering process. Experimental results on real datasets show that k-histogram algorithm can produce better clustering results than k-modes algorithm, the one related with our work most closely.
cs.AI:This report describes a new version of the OntoSpec methodology for ontology building. Defined by the LaRIA Knowledge Engineering Team (University of Picardie Jules Verne, Amiens, France), OntoSpec aims at helping builders to model ontological knowledge (upstream of formal representation). The methodology relies on a set of rigorously-defined modelling primitives and principles. Its application leads to the elaboration of a semi-informal ontology, which is independent of knowledge representation languages. We recently enriched the OntoSpec methodology by endowing it with a new resource, the DOLCE top-level ontology defined at the LOA (IST-CNR, Trento, Italy). The goal of this integration is to provide modellers with additional help in structuring application ontologies, while maintaining independence vis-\`{a}-vis formal representation languages. In this report, we first provide an overview of the OntoSpec methodology's general principles and then describe the DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a specification of DOLCE in the semi-informal OntoSpec language) is presented in an appendix.
cs.AI:In this paper we present a new approach for marker less human motion capture from conventional camera feeds. The aim of our study is to recover 3D positions of key points of the body that can serve for gait analysis. Our approach is based on foreground segmentation, an articulated body model and particle filters. In order to be generic and simple no restrictive dynamic modelling was used. A new modified particle filtering algorithm was introduced. It is used efficiently to search the model configuration space. This new algorithm which we call Interval Particle Filtering reorganizes the configurations search space in an optimal deterministic way and proved to be efficient in tracking natural human movement. Results for human motion capture from a single camera are presented and compared to results obtained from a marker based system. The system proved to be able to track motion successfully even in partial occlusions.
cs.AI:The aim of our study is to detect balance disorders and a tendency towards the falls in the elderly, knowing gait parameters. In this paper we present a new tool for gait analysis based on markerless human motion capture, from camera feeds. The system introduced here, recovers the 3D positions of several key points of the human body while walking. Foreground segmentation, an articulated body model and particle filtering are basic elements of our approach. No dynamic model is used thus this system can be described as generic and simple to implement. A modified particle filtering algorithm, which we call Interval Particle Filtering, is used to reorganise and search through the model's configurations search space in a deterministic optimal way. This algorithm was able to perform human movement tracking with success. Results from the treatment of a single cam feeds are shown and compared to results obtained using a marker based human motion capture system.
cs.AI:An agent often has a number of hypotheses, and must choose among them based on observations, or outcomes of experiments. Each of these observations can be viewed as providing evidence for or against various hypotheses. All the attempts to formalize this intuition up to now have assumed that associated with each hypothesis h there is a likelihood function \mu_h, which is a probability measure that intuitively describes how likely each observation is, conditional on h being the correct hypothesis. We consider an extension of this framework where there is uncertainty as to which of a number of likelihood functions is appropriate, and discuss how one formal approach to defining evidence, which views evidence as a function from priors to posteriors, can be generalized to accommodate this uncertainty.
cs.AI:Being able to analyze and interpret signal coming from electroencephalogram (EEG) recording can be of high interest for many applications including medical diagnosis and Brain-Computer Interfaces. Indeed, human experts are today able to extract from this signal many hints related to physiological as well as cognitive states of the recorded subject and it would be very interesting to perform such task automatically but today no completely automatic system exists. In previous studies, we have compared human expertise and automatic processing tools, including artificial neural networks (ANN), to better understand the competences of each and determine which are the difficult aspects to integrate in a fully automatic system. In this paper, we bring more elements to that study in reporting the main results of a practical experiment which was carried out in an hospital for sleep pathology study. An EEG recording was studied and labeled by a human expert and an ANN. We describe here the characteristics of the experiment, both human and neuronal procedure of analysis, compare their performances and point out the main limitations which arise from this study.
cs.AI:Train timetabling is a difficult and very tightly constrained combinatorial problem that deals with the construction of train schedules. We focus on the particular problem of local reconstruction of the schedule following a small perturbation, seeking minimisation of the total accumulated delay by adapting times of departure and arrival for each train and allocation of resources (tracks, routing nodes, etc.). We describe a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic to gradually reconstruct the schedule by inserting trains one after the other following the permutation. This algorithm can be hybridised with ILOG commercial MIP programming tool CPLEX in a coarse-grained manner: the evolutionary part is used to quickly obtain a good but suboptimal solution and this intermediate solution is refined using CPLEX. Experimental results are presented on a large real-world case involving more than one million variables and 2 million constraints. Results are surprisingly good as the evolutionary algorithm, alone or hybridised, produces excellent solutions much faster than CPLEX alone.
cs.AI:Evolutionary computing (EC) is an exciting development in Computer Science. It amounts to building, applying and studying algorithms based on the Darwinian principles of natural selection. In this paper we briefly introduce the main concepts behind evolutionary computing. We present the main components all evolutionary algorithms (EA), sketch the differences between different types of EAs and survey application areas ranging from optimization, modeling and simulation to entertainment.
cs.AI:This article is taken out.
cs.AI:A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. Among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi (RFV) model, a representation for recurrent fuzzy systems. It is an extension of the FV model proposed by Kavka and Schoenauer that extends the application domain to include temporal problems. The FV model is a representation for fuzzy controllers based on Voronoi diagrams that can represent fuzzy systems with synergistic rules, fulfilling the $\epsilon$-completeness property and providing a simple way to introduce a priory knowledge. In the proposed representation, the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs. These internal units act as memory elements. In the RFV model, the semantic of the internal units can be specified together with the a priori rules. The geometric interpretation of the rules allows the use of geometric variational operators during the evolution. The representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics.
cs.AI:When solving numerical constraints such as nonlinear equations and inequalities, solvers often exploit pruning techniques, which remove redundant value combinations from the domains of variables, at pruning steps. To find the complete solution set, most of these solvers alternate the pruning steps with branching steps, which split each problem into subproblems. This forms the so-called branch-and-prune framework, well known among the approaches for solving numerical constraints. The basic branch-and-prune search strategy that uses domain bisections in place of the branching steps is called the bisection search. In general, the bisection search works well in case (i) the solutions are isolated, but it can be improved further in case (ii) there are continuums of solutions (this often occurs when inequalities are involved). In this paper, we propose a new branch-and-prune search strategy along with several variants, which not only allow yielding better branching decisions in the latter case, but also work as well as the bisection search does in the former case. These new search algorithms enable us to employ various pruning techniques in the construction of inner and outer approximations of the solution set. Our experiments show that these algorithms speed up the solving process often by one order of magnitude or more when solving problems with continuums of solutions, while keeping the same performance as the bisection search when the solutions are isolated.
cs.AI:IS success is a complex concept, and its evaluation is complicated, unstructured and not readily quantifiable. Numerous scientific publications address the issue of success in the IS field as well as in other fields. But, little efforts have been done for processing indeterminacy and uncertainty in success research. This paper shows a formal method for mapping success using Neutrosophic Success Map. This is an emerging tool for processing indeterminacy and uncertainty in success research. EIS success have been analyzed using this tool.
cs.AI:In this paper, a mathematical schema theory is developed. This theory has three roots: brain theory schemas, grid automata, and block-shemas. In Section 2 of this paper, elements of the theory of grid automata necessary for the mathematical schema theory are presented. In Section 3, elements of brain theory necessary for the mathematical schema theory are presented. In Section 4, other types of schemas are considered. In Section 5, the mathematical schema theory is developed. The achieved level of schema representation allows one to model by mathematical tools virtually any type of schemas considered before, including schemas in neurophisiology, psychology, computer science, Internet technology, databases, logic, and mathematics.
cs.AI:Data-based classification is fundamental to most branches of science. While recent years have brought enormous progress in various areas of statistical computing and clustering, some general challenges in clustering remain: model selection, robustness, and scalability to large datasets. We consider the important problem of deciding on the optimal number of clusters, given an arbitrary definition of space and clusteriness. We show how to construct a cluster information criterion that allows objective model selection. Differing from other approaches, our truecluster method does not require specific assumptions about underlying distributions, dissimilarity definitions or cluster models. Truecluster puts arbitrary clustering algorithms into a generic unified (sampling-based) statistical framework. It is scalable to big datasets and provides robust cluster assignments and case-wise diagnostics. Truecluster will make clustering more objective, allows for automation, and will save time and costs. Free R software is available.
cs.AI:An original approach, termed Divide-and-Evolve is proposed to hybridize Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the domain of Temporal Planning Problems (TPPs). Whereas standard Memetic Algorithms use local search methods to improve the evolutionary solutions, and thus fail when the local method stops working on the complete problem, the Divide-and-Evolve approach splits the problem at hand into several, hopefully easier, sub-problems, and can thus solve globally problems that are intractable when directly fed into deterministic OR algorithms. But the most prominent advantage of the Divide-and-Evolve approach is that it immediately opens up an avenue for multi-objective optimization, even though the OR method that is used is single-objective. Proof of concept approach on the standard (single-objective) Zeno transportation benchmark is given, and a small original multi-objective benchmark is proposed in the same Zeno framework to assess the multi-objective capabilities of the proposed methodology, a breakthrough in Temporal Planning.
cs.AI:This article considers evidence from physical and biological sciences to show machines are deficient compared to biological systems at incorporating intelligence. Machines fall short on two counts: firstly, unlike brains, machines do not self-organize in a recursive manner; secondly, machines are based on classical logic, whereas Nature's intelligence may depend on quantum mechanics.
cs.AI:Constraint Programming (CP) has proved an effective paradigm to model and solve difficult combinatorial satisfaction and optimisation problems from disparate domains. Many such problems arising from the commercial world are permeated by data uncertainty. Existing CP approaches that accommodate uncertainty are less suited to uncertainty arising due to incomplete and erroneous data, because they do not build reliable models and solutions guaranteed to address the user's genuine problem as she perceives it. Other fields such as reliable computation offer combinations of models and associated methods to handle these types of uncertain data, but lack an expressive framework characterising the resolution methodology independently of the model.   We present a unifying framework that extends the CP formalism in both model and solutions, to tackle ill-defined combinatorial problems with incomplete or erroneous data. The certainty closure framework brings together modelling and solving methodologies from different fields into the CP paradigm to provide reliable and efficient approches for uncertain constraint problems. We demonstrate the applicability of the framework on a case study in network diagnosis. We define resolution forms that give generic templates, and their associated operational semantics, to derive practical solution methods for reliable solutions.
cs.AI:The application of Genetic Programming to the discovery of empirical laws is often impaired by the huge size of the search space, and consequently by the computer resources needed. In many cases, the extreme demand for memory and CPU is due to the massive growth of non-coding segments, the introns. The paper presents a new program evolution framework which combines distribution-based evolution in the PBIL spirit, with grammar-based genetic programming; the information is stored as a probability distribution on the gra mmar rules, rather than in a population. Experiments on a real-world like problem show that this approach gives a practical solution to the problem of intron growth.
cs.AI:This paper deals with the problem of classifying signals. The new method for building so called local classifiers and local features is presented. The method is a combination of the lifting scheme and the support vector machines. Its main aim is to produce effective and yet comprehensible classifiers that would help in understanding processes hidden behind classified signals. To illustrate the method we present the results obtained on an artificial and a real dataset.
cs.AI:Open answer set programming (OASP) is an extension of answer set programming where one may ground a program with an arbitrary superset of the program's constants. We define a fixed point logic (FPL) extension of Clark's completion such that open answer sets correspond to models of FPL formulas and identify a syntactic subclass of programs, called (loosely) guarded programs. Whereas reasoning with general programs in OASP is undecidable, the FPL translation of (loosely) guarded programs falls in the decidable (loosely) guarded fixed point logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP, enabling for the first time, a characterization of an answer set semantics by muLGF formulas. We further extend the open answer set semantics for programs with generalized literals. Such generalized programs (gPs) have interesting properties, e.g., the ability to express infinity axioms. We restrict the syntax of gPs such that both rules and generalized literals are guarded. Via a translation to guarded fixed point logic, we deduce 2-exptime-completeness of satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted GgPs with exptime-complete satisfiability checking, but still sufficiently expressive to optimally simulate computation tree logic (CTL). We translate Datalog lite programs to GgPs, establishing equivalence of GgPs under an open answer set semantics, alternation-free muGF, and Datalog lite.
cs.AI:Consistency check has been the only criterion for theory evaluation in logic-based approaches to reasoning about actions. This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description in reasoning about actions should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied together we have a modular action theory. Besides being easier to understand and more elaboration tolerant in McCarthy's sense, modular theories have interesting properties. We point out the problems that arise when the postulates about modularity are violated and propose algorithmic checks that can help the designer of an action theory to overcome them.
cs.AI:The estimation of linear causal models (also known as structural equation models) from data is a well-known problem which has received much attention in the past. Most previous work has, however, made an explicit or implicit assumption of gaussianity, limiting the identifiability of the models. We have recently shown (Shimizu et al, 2005; Hoyer et al, 2006) that for non-gaussian distributions the full causal model can be estimated in the no hidden variables case. In this contribution, we discuss the estimation of the model when confounding latent variables are present. Although in this case uniqueness is no longer guaranteed, there is at most a finite set of models which can fit the data. We develop an algorithm for estimating this set, and describe numerical simulations which confirm the theoretical arguments and demonstrate the practical viability of the approach. Full Matlab code is provided for all simulations.
cs.AI:Shock physics experiments are often complicated and expensive. As a result, researchers are unable to conduct as many experiments as they would like - leading to sparse data sets. In this paper, Support Vector Machines for regression are applied to velocimetry data sets for shock damaged and melted tin metal. Some success at interpolating between data sets is achieved. Implications for future work are discussed.
cs.AI:In this paper, we study clustering with respect to the k-modes objective function, a natural formulation of clustering for categorical data. One of the main contributions of this paper is to establish the connection between k-modes and k-median, i.e., the optimum of k-median is at most twice the optimum of k-modes for the same categorical data clustering problem. Based on this observation, we derive a deterministic algorithm that achieves an approximation factor of 2. Furthermore, we prove that the distance measure in k-modes defines a metric. Hence, we are able to extend existing approximation algorithms for metric k-median to k-modes. Empirical results verify the superiority of our method.
cs.AI:A model of an organism as an autonomous intelligent system has been proposed. This model was used to analyze learning of an organism in various environmental conditions. Processes of learning were divided into two types: strong and weak processes taking place in the absence and the presence of aprioristic information about an object respectively. Weak learning is synonymous to adaptation when aprioristic programs already available in a system (an organism) are started. It was shown that strong learning is impossible for both an organism and any autonomous intelligent system. It was shown also that the knowledge base of an organism cannot be updated. Therefore, all behavior programs of an organism are congenital. A model of a conditioned reflex as a series of consecutive measurements of environmental parameters has been advanced. Repeated measurements are necessary in this case to reduce the error during decision making.
cs.AI:This paper presents two new promising rules of combination for the fusion of uncertain and potentially highly conflicting sources of evidences in the framework of the theory of belief functions in order to palliate the well-know limitations of Dempster's rule and to work beyond the limits of applicability of the Dempster-Shafer theory. We present both a new class of adaptive combination rules (ACR) and a new efficient Proportional Conflict Redistribution (PCR) rule allowing to deal with highly conflicting sources for static and dynamic fusion applications.
cs.AI:Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a formal model of computing with values. Motivated by Zadeh's paradigm of computing with words rather than numbers, Ying proposed a kind of fuzzy automata, whose input alphabet consists of all fuzzy subsets of a set of symbols, as a formal model of computing with all words. In this paper, we introduce a somewhat general formal model of computing with (some special) words. The new features of the model are that the input alphabet only comprises some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy transition function can be specified arbitrarily. By employing the methodology of fuzzy control, we establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling fuzzy inputs. These principles show that computing with values and computing with all words can be respectively implemented by computing with words. Some algebraic properties of retractions and generalized extensions are addressed as well.
cs.AI:Through the Internet and the World-Wide Web, a vast number of information sources has become available, which offer information on various subjects by different providers, often in heterogeneous formats. This calls for tools and methods for building an advanced information-processing infrastructure. One issue in this area is the selection of suitable information sources in query answering. In this paper, we present a knowledge-based approach to this problem, in the setting where one among a set of information sources (prototypically, data repositories) should be selected for evaluating a user query. We use extended logic programs (ELPs) to represent rich descriptions of the information sources, an underlying domain theory, and user queries in a formal query language (here, XML-QL, but other languages can be handled as well). Moreover, we use ELPs for declarative query analysis and generation of a query description. Central to our approach are declarative source-selection programs, for which we define syntax and semantics. Due to the structured nature of the considered data items, the semantics of such programs must carefully respect implicit context information in source-selection rules, and furthermore combine it with possible user preferences. A prototype implementation of our approach has been realized exploiting the DLV KR system and its plp front-end for prioritized ELPs. We describe a representative example involving specific movie databases, and report about experimental results.
cs.AI:It is well known that perspective alignment plays a major role in the planning and interpretation of spatial language. In order to understand the role of perspective alignment and the cognitive processes involved, we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings. We show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place, either implicitly or based on explicit marking.
cs.AI:We extend the 0-approximation of sensing actions and incomplete information in [Son and Baral 2000] to action theories with static causal laws and prove its soundness with respect to the possible world semantics. We also show that the conditional planning problem with respect to this approximation is NP-complete. We then present an answer set programming based conditional planner, called ASCP, that is capable of generating both conformant plans and conditional plans in the presence of sensing actions, incomplete information about the initial state, and static causal laws. We prove the correctness of our implementation and argue that our planner is sound and complete with respect to the proposed approximation. Finally, we present experimental results comparing ASCP to other planners.
cs.AI:Computing and storing probabilities is a hard problem as soon as one has to deal with complex distributions over multiple random variables. The problem of efficient representation of probability distributions is central in term of computational efficiency in the field of probabilistic reasoning. The main problem arises when dealing with joint probability distributions over a set of random variables: they are always represented using huge probability arrays. In this paper, a new method based on binary-tree representation is introduced in order to store efficiently very large joint distributions. Our approach approximates any multidimensional joint distributions using an adaptive discretization of the space. We make the assumption that the lower is the probability mass of a particular region of feature space, the larger is the discretization step. This assumption leads to a very optimized representation in term of time and memory. The other advantages of our approach are the ability to refine dynamically the distribution every time it is needed leading to a more accurate representation of the probability distribution and to an anytime representation of the distribution.
cs.AI:In order to more effectively cope with the real-world problems of vagueness, {\it fuzzy discrete event systems} (FDESs) were proposed recently, and the supervisory control theory of FDESs was developed. In view of the importance of failure diagnosis, in this paper, we present an approach of the failure diagnosis in the framework of FDESs. More specifically: (1) We formalize the definition of diagnosability for FDESs, in which the observable set and failure set of events are {\it fuzzy}, that is, each event has certain degree to be observable and unobservable, and, also, each event may possess different possibility of failure occurring. (2) Through the construction of observability-based diagnosers of FDESs, we investigate its some basic properties. In particular, we present a necessary and sufficient condition for diagnosability of FDESs. (3) Some examples serving to illuminate the applications of the diagnosability of FDESs are described. To conclude, some related issues are raised for further consideration.
cs.AI:Classification of ordinal data is one of the most important tasks of relation learning. In this thesis a novel framework for ordered classes is proposed. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Compared with a well-known approach using pairwise objects as training samples, the new algorithm has a reduced complexity and training time. A second novel model, the unimodal model, is also introduced and a parametric version is mapped into neural networks. Several case studies are presented to assert the validity of the proposed models.
cs.AI:Imagination is the critical point in developing of realistic artificial intelligence (AI) systems. One way to approach imagination would be simulation of its properties and operations. We developed two models: AI-Brain Network Hierarchy of Languages and Semantical Holographic Calculus as well as simulation system ScriptWriter that emulate the process of imagination through an automatic animation of English texts. The purpose of this paper is to demonstrate the model and to present ScriptWriter system http://nvo.sdsc.edu/NVO/JCSG/get_SRB_mime_file2.cgi//home/tamara.sdsc/test/demo.zip?F=/home/tamara.sdsc/test/demo.zip&M=application/x-gtar for simulation of the imagination.
cs.AI:In Dempster-Shafer belief theory, general beliefs are expressed as belief mass distribution functions over frames of discernment. In Subjective Logic beliefs are expressed as belief mass distribution functions over binary frames of discernment. Belief representations in Subjective Logic, which are called opinions, also contain a base rate parameter which express the a priori belief in the absence of evidence. Philosophically, beliefs are quantitative representations of evidence as perceived by humans or by other intelligent agents. The basic operators of classical probability calculus, such as addition and multiplication, can be applied to opinions, thereby making belief calculus practical. Through the equivalence between opinions and Beta probability density functions, this also provides a calculus for Beta probability density functions. This article explains the basic elements of belief calculus.
cs.AI:The problem of combining beliefs in the Dempster-Shafer belief theory has attracted considerable attention over the last two decades. The classical Dempster's Rule has often been criticised, and many alternative rules for belief combination have been proposed in the literature. The consensus operator for combining beliefs has nice properties and produces more intuitive results than Dempster's rule, but has the limitation that it can only be applied to belief distribution functions on binary state spaces. In this paper we present a generalisation of the consensus operator that can be applied to Dirichlet belief functions on state spaces of arbitrary size. This rule, called the cumulative rule of belief combination, can be derived from classical statistical theory, and corresponds well with human intuition.
cs.AI:Artificial Intelligence (AI) has recently become a real formal science: the new millennium brought the first mathematically sound, asymptotically optimal, universal problem solvers, providing a new, rigorous foundation for the previously largely heuristic field of General AI and embedded agents. At the same time there has been rapid progress in practical methods for learning true sequence-processing programs, as opposed to traditional methods limited to stationary pattern association. Here we will briefly review some of the new results, and speculate about future developments, pointing out that the time intervals between the most notable events in over 40,000 years or 2^9 lifetimes of human history have sped up exponentially, apparently converging to zero within the next few decades. Or is this impression just a by-product of the way humans allocate memory space to past events?
cs.AI:In this paper we propose a new family of Belief Conditioning Rules (BCRs) for belief revision. These rules are not directly related with the fusion of several sources of evidence but with the revision of a belief assignment available at a given time according to the new truth (i.e. conditioning constraint) one has about the space of solutions of the problem.
cs.AI:In this note we introduce the notion of islands for restricting local search. We show how we can construct islands for CNF SAT problems, and how much search space can be eliminated by restricting search to the island.
cs.AI:Knowing the norms of a domain is crucial, but there exist no repository of norms. We propose a method to extract them from texts: texts generally do not describe a norm, but rather how a state-of-affairs differs from it. Answers concerning the cause of the state-of-affairs described often reveal the implicit norm. We apply this idea to the domain of driving, and validate it by designing algorithms that identify, in a text, the "basic" norms to which it refers implicitly.
cs.AI:Norms are essential to extend inference: inferences based on norms are far richer than those based on logical implications. In the recent decades, much effort has been devoted to reason on a domain, once its norms are represented. How to extract and express those norms has received far less attention. Extraction is difficult: as the readers are supposed to know them, the norms of a domain are seldom made explicit. For one thing, extracting norms requires a language to represent them, and this is the topic of this paper. We apply this language to represent norms in the domain of driving, and show that it is adequate to reason on the causes of accidents, as described by car-crash reports.
cs.AI:In this paper we consider and analyze the behavior of two combinational rules for temporal (sequential) attribute data fusion for target type estimation. Our comparative analysis is based on Dempster's fusion rule proposed in Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a very efficient Target Type Tracking and reduces drastically the latency delay for correct Target Type decision with respect to Demspter's rule. For cases presenting some short Target Type switches, Demspter's rule is proved to be unable to detect the switches and thus to track correctly the Target Type changes. The approach proposed here is totally new, efficient and promising to be incorporated in real-time Generalized Data Association - Multi Target Tracking systems (GDA-MTT) and provides an important result on the behavior of PCR5 with respect to Dempster's rule. The MatLab source code is provided in
cs.AI:This paper introduces the notion of qualitative belief assignment to model beliefs of human experts expressed in natural language (with linguistic labels). We show how qualitative beliefs can be efficiently combined using an extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical quantitative reasoning to qualitative reasoning. We propose a new arithmetic on linguistic labels which allows a direct extension of classical DSm fusion rule or DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed jointly with a Qualitative Average Operator. We also show how crisp or interval mappings can be used to deal indirectly with linguistic labels. A very simple example is provided to illustrate our qualitative fusion rules.
cs.AI:The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this introduction, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach.
cs.AI:We study an alternative to the prevailing approach to modelling qualitative spatial reasoning (QSR) problems as constraint satisfaction problems. In the standard approach, a relation between objects is a constraint whereas in the alternative approach it is a variable. The relation-variable approach greatly simplifies integration and implementation of QSR. To substantiate this point, we discuss several QSR algorithms from the literature which in the relation-variable approach reduce to the customary constraint propagation algorithm enforcing generalised arc-consistency.
cs.AI:I explore the use of sets of probability measures as a representation of uncertainty.
cs.AI:We present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ a three-valued characterization of domains with sensing actions to define the regression function. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one.
cs.AI:A modification of OWL-S regarding parameter description is proposed. It is strictly based on Description Logic. In addition to class description of parameters it also allows the modelling of relations between parameters and the precise description of the size of data to be supplied to a service. In particular, it solves two major issues identified within current proposals for a Semantic Web Service annotation standard.
cs.AI:The paper describes the ALVIS annotation format designed for the indexing of large collections of documents in topic-specific search engines. This paper is exemplified on the biological domain and on MedLine abstracts, as developing a specialized search engine for biologists is one of the ALVIS case studies. The ALVIS principle for linguistic annotations is based on existing works and standard propositions. We made the choice of stand-off annotations rather than inserted mark-up. Annotations are encoded as XML elements which form the linguistic subsection of the document record.
cs.AI:The aim of this paper is to provide a sound framework for addressing a difficult problem: the automatic construction of an autonomous agent's modular architecture. We combine results from two apparently uncorrelated domains: Autonomous planning through Markov Decision Processes and a General Data Clustering Approach using a kernel-like method. Our fundamental idea is that the former is a good framework for addressing autonomy whereas the latter allows to tackle self-organizing problems.
cs.AI:In this paper we elaborate on a specific application in the context of hybrid description logic programs (hybrid DLPs), namely description logic Semantic Web type systems (DL-types) which are used for term typing of LP rules based on a polymorphic, order-sorted, hybrid DL-typed unification as procedural semantics of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates interchange of domain-independent rules over domain boundaries via dynamically typing and mapping of explicitly defined type ontologies.
cs.AI:In this paper we describe an architecture of a system that answer the question : Why did the accident happen? from the textual description of an accident. We present briefly the different parts of the architecture and then we describe with more detail the semantic part of the system i.e. the part in which the norm-based reasoning is performed on the explicit knowlege extracted from the text.
cs.AI:We develop a system which must be able to perform the same inferences that a human reader of an accident report can do and more particularly to determine the apparent causes of the accident. We describe the general framework in which we are situated, linguistic and semantic levels of the analysis and the inference rules used by the system.
cs.AI:The k-modes algorithm has become a popular technique in solving categorical data clustering problems in different application domains. However, the algorithm requires random selection of initial points for the clusters. Different initial points often lead to considerable distinct clustering results. In this paper we present an experimental study on applying a farthest-point heuristic based initialization method to k-modes clustering to improve its performance. Experiments show that new initialization method leads to better clustering accuracy than random selection initialization method for k-modes clustering.
cs.AI:The opening book is an important component of a chess engine, and thus computer chess programmers have been developing automated methods to improve the quality of their books. For chess, which has a very rich opening theory, large databases of high-quality games can be used as the basis of an opening book, from which statistics relating to move choices from given positions can be collected. In order to find out whether the opening books used by modern chess engines in machine versus machine competitions are ``comparable'' to those used by chess players in human versus human competitions, we carried out analysis on 26 test positions using statistics from two opening books one compiled from humans' games and the other from machines' games. Our analysis using several nonparametric measures, shows that, overall, there is a strong association between humans' and machines' choices of opening moves when using a book to guide their choices.
cs.AI:We present a new local approximation algorithm for computing Maximum a Posteriori (MAP) and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say $G$. Our algorithm is based on decomposition of $G$ into {\em appropriately} chosen small components; then computing estimates locally in each of these components and then producing a {\em good} global solution. We show that if the underlying graph $G$ either excludes some finite-sized graph as its minor (e.g. Planar graph) or has low doubling dimension (e.g. any graph with {\em geometry}), then our algorithm will produce solution for both questions within {\em arbitrary accuracy}. We present a message-passing implementation of our algorithm for MAP computation using self-avoiding walk of graph. In order to evaluate the computational cost of this implementation, we derive novel tight bounds on the size of self-avoiding walk tree for arbitrary graph.   As a consequence of our algorithmic result, we show that the normalized log-partition function (also known as free-energy) for a class of {\em regular} MRFs will converge to a limit, that is computable to an arbitrary accuracy.
cs.AI:Creation procedure of associative patterns ensemble in terms of formal logic with using neural net-work (NN) model is formulated. It is shown that the associative patterns set is created by means of unique procedure of NN work which having individual parameters of entrance stimulus transformation. It is ascer-tained that the quantity of the selected associative patterns possesses is a constant.
cs.AI:In case-based reasoning, the adaptation step depends in general on domain-dependent knowledge, which motivates studies on adaptation knowledge acquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge discovery from databases. This system explores the variations within the case base to elicit adaptation knowledge. It has been successfully tested in an application of case-based decision support to breast cancer treatment.
cs.AI:Recently, the diagnosability of {\it stochastic discrete event systems} (SDESs) was investigated in the literature, and, the failure diagnosis considered was {\it centralized}. In this paper, we propose an approach to {\it decentralized} failure diagnosis of SDESs, where the stochastic system uses multiple local diagnosers to detect failures and each local diagnoser possesses its own information. In a way, the centralized failure diagnosis of SDESs can be viewed as a special case of the decentralized failure diagnosis presented in this paper with only one projection. The main contributions are as follows: (1) We formalize the notion of codiagnosability for stochastic automata, which means that a failure can be detected by at least one local stochastic diagnoser within a finite delay. (2) We construct a codiagnoser from a given stochastic automaton with multiple projections, and the codiagnoser associated with the local diagnosers is used to test codiagnosability condition of SDESs. (3) We deal with a number of basic properties of the codiagnoser. In particular, a necessary and sufficient condition for the codiagnosability of SDESs is presented. (4) We give a computing method in detail to check whether codiagnosability is violated. And (5) some examples are described to illustrate the applications of the codiagnosability and its computing method.
cs.AI:The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been and still remains of primal importance for the development of reliable information fusion systems. In this short survey paper, we present the theory of plausible and paradoxical reasoning, known as DSmT (Dezert-Smarandache Theory) in literature, developed for dealing with imprecise, uncertain and potentially highly conflicting sources of information. DSmT is a new paradigm shift for information fusion and recent publications have shown the interest and the potential ability of DSmT to solve fusion problems where Dempster's rule used in Dempster-Shafer Theory (DST) provides counter-intuitive results or fails to provide useful result at all. This paper is focused on the foundations of DSmT and on its main rules of combination (classic, hybrid and Proportional Conflict Redistribution rules). Shafer's model on which is based DST appears as a particular and specific case of DSm hybrid model which can be easily handled by DSmT as well. Several simple but illustrative examples are given throughout this paper to show the interest and the generality of this new theory.
cs.AI:Reaction RuleML is a general, practical, compact and user-friendly XML-serialized language for the family of reaction rules. In this white paper we give a review of the history of event / action /state processing and reaction rule approaches and systems in different domains, define basic concepts and give a classification of the event, action, state processing and reasoning space as well as a discussion of relevant / related work
cs.AI:A fuzzy logic based classification engine has been developed for classifying mass spectra obtained with an imaging internal source Fourier transform mass spectrometer (I^2LD-FTMS). Traditionally, an operator uses the relative abundance of ions with specific mass-to-charge (m/z) ratios to categorize spectra. An operator does this by comparing the spectrum of m/z versus abundance of an unknown sample against a library of spectra from known samples. Automated positioning and acquisition allow I^2LD-FTMS to acquire data from very large grids, this would require classification of up to 3600 spectrum per hour to keep pace with the acquisition. The tedious job of classifying numerous spectra generated in an I^2LD-FTMS imaging application can be replaced by a fuzzy rule base if the cues an operator uses can be encapsulated. We present the translation of linguistic rules to a fuzzy classifier for mineral phases in basalt. This paper also describes a method for gathering statistics on ions, which are not currently used in the rule base, but which may be candidates for making the rule base more accurate and complete or to form new rule bases based on data obtained from known samples. A spatial method for classifying spectra with low membership values, based on neighboring sample classifications, is also presented.
cs.AI:Description Logics (DLs) are appropriate, widely used, logics for managing structured knowledge. They allow reasoning about individuals and concepts, i.e. set of individuals with common properties. Typically, DLs are limited to dealing with crisp, well defined concepts. That is, concepts for which the problem whether an individual is an instance of it is yes/no question. More often than not, the concepts encountered in the real world do not have a precisely defined criteria of membership: we may say that an individual is an instance of a concept only to a certain degree, depending on the individual's properties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In order to deal with fuzzy, incomplete, indeterminate and inconsistent concepts, we need to extend the fuzzy DLs, combining the neutrosophic logic with a classical DL. In particular, concepts become neutrosophic (here neutrosophic means fuzzy, incomplete, indeterminate, and inconsistent), thus reasoning about neutrosophic concepts is supported. We'll define its syntax, its semantics, and describe its properties.
cs.AI:Support Vector Machines (SVMs) are well-established Machine Learning (ML) algorithms. They rely on the fact that i) linear learning can be formalized as a well-posed optimization problem; ii) non-linear learning can be brought into linear learning thanks to the kernel trick and the mapping of the initial search space onto a high dimensional feature space. The kernel is designed by the ML expert and it governs the efficiency of the SVM approach. In this paper, a new approach for the automatic design of kernels by Genetic Programming, called the Evolutionary Kernel Machine (EKM), is presented. EKM combines a well-founded fitness function inspired from the margin criterion, and a co-evolution framework ensuring the computational scalability of the approach. Empirical validation on standard ML benchmark demonstrates that EKM is competitive using state-of-the-art SVMs with tuned hyper-parameters.
cs.AI:Functional brain imaging is a source of spatio-temporal data mining problems. A new framework hybridizing multi-objective and multi-modal optimization is proposed to formalize these data mining problems, and addressed through Evolutionary Computation (EC). The merits of EC for spatio-temporal data mining are demonstrated as the approach facilitates the modelling of the experts' requirements, and flexibly accommodates their changing goals.
cs.AI:The paper suggests the use of Multi-Valued Decision Diagrams (MDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-terminal path in the search tree. The technique used is an extension of the GAC algorithm for the regular language constraint on finite length input. Our approach adds support for skipped variables, maintains the reduced property of the MDD dynamically and provides domain entailment detection. Finally we also show how to adapt the approach to constraint types that are closely related to MDDs, such as AOMDDs and Case DAGs.
cs.AI:Did natural consciousness and intelligent systems arise out of a path that was co-evolutionary to evolution? Can we explain human self-consciousness as having risen out of such an evolutionary path? If so how could it have been?   In this first part of a two-part paper (titled IXI), we take a learning system perspective to the problem of consciousness and intelligent systems, an approach that may look unseasonable in this age of fMRI's and high tech neuroscience.   We posit conscious intelligent systems in natural environments and wonder how natural factors influence their design paths. Such a perspective allows us to explain seamlessly a variety of natural factors, factors ranging from the rise and presence of the human mind, man's sense of I, his self-consciousness and his looping thought processes to factors like reproduction, incubation, extinction, sleep, the richness of natural behavior, etc. It even allows us to speculate on a possible human evolution scenario and other natural phenomena.
cs.AI:This is the second part of a paper on Conscious Intelligent Systems. We use the understanding gained in the first part (Conscious Intelligent Systems Part 1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the presence of mind affects understanding and intelligent systems; we see that the presence of mind necessitates language. The rise of language in turn has important effects on understanding. We discuss the humanoid question and how the question of self-consciousness (and by association mind/thought/language) would affect humanoids too.
cs.AI:A product configurator which is complete, backtrack free and able to compute the valid domains at any state of the configuration can be constructed by building a Binary Decision Diagram (BDD). Despite the fact that the size of the BDD is exponential in the number of variables in the worst case, BDDs have proved to work very well in practice. Current BDD-based techniques can only handle interactive configuration with small finite domains. In this paper we extend the approach to handle string variables constrained by regular expressions. The user is allowed to change the strings by adding letters at the end of the string. We show how to make a data structure that can perform fast valid domain computations given some assignment on the set of string variables.   We first show how to do this by using one large DFA. Since this approach is too space consuming to be of practical use, we construct a data structure that simulates the large DFA and in most practical cases are much more space efficient. As an example a configuration problem on $n$ string variables with only one solution in which each string variable is assigned to a value of length of $k$ the former structure will use $\Omega(k^n)$ space whereas the latter only need $O(kn)$. We also show how this framework easily can be combined with the recent BDD techniques to allow both boolean, integer and string variables in the configuration problem.
cs.AI:Recently, M. Chertkov and V.Y. Chernyak derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the Belief Propagation solution. By adding correction terms to the BP free energy, one for each "generalized loop" in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce Truncated Loop Series BP (TLSBP), a particular way of truncating the loop series of M. Chertkov and V.Y. Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model, regular random graphs and on Promedas, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to significant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered.
cs.AI:In this paper, the traditional k-modes clustering algorithm is extended by weighting attribute value matches in dissimilarity computation. The use of attribute value weighting technique makes it possible to generate clusters with stronger intra-similarities, and therefore achieve better clustering performance. Experimental results on real life datasets show that these value weighting based k-modes algorithms are superior to the standard k-modes algorithm with respect to clustering accuracy.
cs.AI:In Verification and in (optimal) AI Planning, a successful method is to formulate the application as boolean satisfiability (SAT), and solve it with state-of-the-art DPLL-based procedures. There is a lack of understanding of why this works so well. Focussing on the Planning context, we identify a form of problem structure concerned with the symmetrical or asymmetrical nature of the cost of achieving the individual planning goals. We quantify this sort of structure with a simple numeric parameter called AsymRatio, ranging between 0 and 1. We run experiments in 10 benchmark domains from the International Planning Competitions since 2000; we show that AsymRatio is a good indicator of SAT solver performance in 8 of these domains. We then examine carefully crafted synthetic planning domains that allow control of the amount of structure, and that are clean enough for a rigorous analysis of the combinatorial search space. The domains are parameterized by size, and by the amount of structure. The CNFs we examine are unsatisfiable, encoding one planning step less than the length of the optimal plan. We prove upper and lower bounds on the size of the best possible DPLL refutations, under different settings of the amount of structure, as a function of size. We also identify the best possible sets of branching variables (backdoors). With minimum AsymRatio, we prove exponential lower bounds, and identify minimal backdoors of size linear in the number of variables. With maximum AsymRatio, we identify logarithmic DPLL refutations (and backdoors), showing a doubly exponential gap between the two structural extreme cases. The reasons for this behavior -- the proof arguments -- illuminate the prototypical patterns of structure causing the empirical behavior observed in the competition benchmarks.
cs.AI:This short paper introduces two new fusion rules for combining quantitative basic belief assignments. These rules although very simple have not been proposed in literature so far and could serve as useful alternatives because of their low computation cost with respect to the recent advanced Proportional Conflict Redistribution rules developed in the DSmT framework.
cs.AI:Constraint Programming (CP) has been successfully applied to both constraint satisfaction and constraint optimization problems. A wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution. However, a key outstanding issue is the representation of 'ad-hoc' constraints that do not have an inherent combinatorial nature, and hence are not modeled well using narrowly specialized global constraints. We attempt to address this issue by considering a hybrid of search and compilation. Specifically we suggest the use of Reduced Ordered Multi-Valued Decision Diagrams (ROMDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the MDD. Furthermore we present an approach for incrementally maintaining the reduced property of the MDD during the search, and show how this can be used for providing domain entailment detection. Finally we discuss how to apply our approach to other similar data structures such as AOMDDs and Case DAGs. The technique used can be seen as an extension of the GAC algorithm for the regular language constraint on finite length input.
cs.AI:For academics and practitioners concerned with computers, business and mathematics, one central issue is supporting decision makers. In this paper, we propose a generalization of Decision Matrix Method (DMM), using Neutrosophic logic. It emerges as an alternative to the existing logics and it represents a mathematical model of uncertainty and indeterminacy. This paper proposes the Neutrosophic Decision Matrix Method as a more realistic tool for decision making. In addition, a de-neutrosophication process is included.
cs.AI:This paper constructs a tree structure for the music rhythm using the L-system. It models the structure as an automata and derives its complexity. It also solves the complexity for the L-system. This complexity can resolve the similarity between trees. This complexity serves as a measure of psychological complexity for rhythms. It resolves the music complexity of various compositions including the Mozart effect K488.   Keyword: music perception, psychological complexity, rhythm, L-system, automata, temporal associative memory, inverse problem, rewriting rule, bracketed string, tree similarity
cs.AI:Using qualitative reasoning with geographic information, contrarily, for instance, with robotics, looks not only fastidious (i.e.: encoding knowledge Propositional Logics PL), but appears to be computational complex, and not tractable at all, most of the time. However, knowledge fusion or revision, is a common operation performed when users merge several different data sets in a unique decision making process, without much support. Introducing logics would be a great improvement, and we propose in this paper, means for deciding -a priori- if one application can benefit from a complete revision, under only the assumption of a conjecture that we name the "containment conjecture", which limits the size of the minimal conflicts to revise. We demonstrate that this conjecture brings us the interesting computational property of performing a not-provable but global, revision, made of many local revisions, at a tractable size. We illustrate this approach on an application.
cs.AI:In case-based reasoning, the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement. The reason for this difficulty is that, in general, adaptation strongly depends on domain-dependent knowledge. This fact motivates research on adaptation knowledge acquisition (AKA). This paper presents an approach to AKA based on the principles and techniques of knowledge discovery from databases and data-mining. It is implemented in CABAMAKA, a system that explores the variations within the case base to elicit adaptation knowledge. This system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment.
cs.AI:In these notes we formally describe the functionality of Calculating Valid Domains from the BDD representing the solution space of valid configurations. The formalization is largely based on the CLab configuration framework.
cs.AI:Motivation: Profile hidden Markov Models (pHMMs) are a popular and very useful tool in the detection of the remote homologue protein families. Unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm and tool that tries to improve pHMM performance by using structural information while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs. Each pHMM is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. Properties used were primary, secondary and tertiary structures, accessibility and packing. HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP database to perform our experiments. Throughout, we apply leave-one-family-out cross-validation over protein superfamilies. First, we used the MAMMOTH-mult structural aligner to align the training set proteins. Then, we performed two sets of experiments. In a first experiment, we compared structure weighted models against standard pHMMs and against each other. In a second experiment, we compared the voting model against individual pHMMs. We compare method performance through ROC curves and through Precision/Recall curves, and assess significance through the paired two tailed t-test. Our results show significant performance improvements of all structurally weighted models over default HMMER, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.
cs.AI:This paper proposes an approach to training rough set models using Bayesian framework trained using Markov Chain Monte Carlo (MCMC) method. The prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. Markov Chain Monte Carlo sampling is conducted through sampling in the rough set granule space and Metropolis algorithm is used as an acceptance criteria. The proposed method is tested to estimate the risk of HIV given demographic data. The results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. In addition the Bayesian rough set give the probabilities of the estimated HIV status as well as the linguistic rules describing how the demographic parameters drive the risk of HIV.
cs.AI:Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.
cs.AI:Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-line) or incrementally along evolution (On-line). Experiments on a set of benchmark problems show that Off-line outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.
q-bio.BM:We consider the regime in which the bands of the torsional acoustic (TA) and the hydrogen-bond-stretch (HBS) modes of the DNA interpenetrate each other. Within the framework of a model that accommodates the structure of the double helix, we find the three-wave interaction between the TA- and the HBS-modes, and show that microwave radiation could bring about torsional vibrations that could serve as a pump mode for maintaining the HBS-one. Rayleigh's threshold condition for the parametric resonance provides an estimate for the power density of the mw-field necessary for generating the HBS-mode.
q-bio.BM:Identifying the driving forces and the mechanism of association of huntingtin-exon1, a close marker for the progress of Huntington's disease, is an important prerequisite towards finding potential drug targets, and ultimately a cure. We introduce here a modelling framework based on a key analogy of the physico-chemical properties of the exon1 fragment to block copolymers. We use a systematic mesoscale methodology, based on Dissipative Particle Dynamics, which is capable of overcoming kinetic barriers, thus capturing the dynamics of significantly larger systems over longer times than considered before. Our results reveal that the relative hydrophobicity of the poly-glutamine block as compared to the rest of the (proline-based) exon1 fragment, ignored to date, constitutes a major factor in the initiation of the self-assembly process. We find that the assembly is governed by both the concentration of exon1 and the length of the poly-glutamine stretch, with a low length threshold for association even at the lowest volume fractions we considered. Moreover, this self-association occurs irrespective of whether the glutamine stretch is in random coil or hairpin configuration, leading to spherical or cylindrical assemblies, respectively. We discuss the implications of these results for reinterpretation of existing research within this context, including that the routes towards aggregation of exon1 may be distinct to those of the widely studied homopolymeric poly-glutamine peptides.
q-bio.BM:The molecular mechanism of the solvent motion that is required to instigate the protein structural relaxation above a critical hydration level or transition temperature has yet to be determined. In this work we use quasi-elastic neutron scattering (QENS) and molecular dynamics simulation to investigate hydration water dynamics near a greatly simplified protein surface. We consider the hydration water dynamics near the completely deuterated N-acetyl-leucine-methylamide (NALMA) solute, a hydrophobic amino acid side chain attached to a polar blocked polypeptide backbone, as a function of concentration between 0.5M-2.0M, under ambient conditions. In this Communication, we focus our results of hydration dynamics near a model protein surface on the issue of how enzymatic activity is restored once a critical hydration level is reached, and provide a hypothesis for the molecular mechanism of the solvent motion that is required to trigger protein structural relaxation when above the hydration transition.
q-bio.BM:We analyze the dependence of thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, N. Using lattice Go models we show that DeltaT/T_F ~ N^-1, where T_F is the folding transition temperature and DeltaT is the folding transition width. This finding is consistent with finite size effects expected for the systems undergoing a phase transition from a disordered to an ordered phase. The dependence of the folding rates k_F on N for lattice models and the dataset of 57 proteins and peptides shows that k_F = k_F^0 exp(-CN^beta) provides a good fit, if 0 < beta <= 2/3 and C is a constant. We find that k_F = k_F^0 exp(-1.1N^0.5) with k_F^0 =(0.4x10^-6 s)^-1 can estimate optimal protein folding rates to within an order of magnitude in most cases. By using this fit for a set of proteins with beta-sheet topology we find that k_F^0 is approximately equal to k_U^0, the prefactor for unfolding rates. The maximum ratio of k_U^0/k_F^0 is 10 for this class of proteins.
q-bio.BM:The asymmetry in the shapes of folded and unfolded states are probed using two parameters, one being a measure of the sphericity and the other that describes the shape. For the folded states, whose interiors are densely packed, the radii of gyration (Rg) and these two parameters are calculated using the coordinates of the experimentally determined structures. Although Rg scales as expected for maximally compact structures, the distributions of the shape parameters show that there is considerable asymmetry in the shapes of folded structures. The degree of asymmetry is greater for proteins that form oligomers. Analysis of the two- and three-body contacts in the native structures shows that the presence of near equal number of contacts between backbone and side-chains and between side-chains gives rise to dense packing. We suggest that proteins with relatively large values of shape parameters can tolerate volume mutations without greatly affecting the network of contacts or their stability. To probe shape characteristics of denatured states we have developed a model of a WW-like domain. The shape parameters, which are calculated using Langevin simulations, change dramatically in the course of coil to globule transition. Comparison of the values of shape parameters between the globular state and the folded state of WW domain shows that both energetic (especially dispersion in the hydrophobic interactions) and steric effects are important in determining packing in proteins.
q-bio.BM:Instead of conformation states of single residues, refined conformation states of quintuplets are proposed to reflect conformation correlation. Simple hidden Markov models combining with sliding window scores are used for predicting secondary structure of a protein from its amino acid sequence. Since the length of protein conformation segments varies in a narrow range, we ignore the duration effect of the length distribution. The window scores for residues are a window version of the Chou-Fasman propensities estimated under an approximation of conditional independency. Different window widths are examined, and the optimal width is found to be 17. A high accuracy about 70% is achieved.
q-bio.BM:Is protein secondary structure primarily determined by local interactions between residues closely spaced along the amino acid backbone, or by non-local tertiary interactions? To answer this question we have measured the entropy densities of primary structure and secondary structure sequences, and the local inter-sequence mutual information density. We find that the important inter-sequence interactions are short ranged, that correlations between neighboring amino acids are essentially uninformative, and that only 1/4 of the total information needed to determine the secondary structure is available from local inter-sequence correlations. Since the remaining information must come from non-local interactions, this observation supports the view that the majority of most proteins fold via a cooperative process where secondary and tertiary structure form concurrently. To provide a more direct comparison to existing secondary structure prediction methods, we construct a simple hidden Markov model (HMM) of the sequences. This HMM achieves a prediction accuracy comparable to other single sequence secondary structure prediction algorithms, and can extract almost all of the inter-sequence mutual information. This suggests that these algorithms are almost optimal, and that we should not expect a dramatic improvement in prediction accuracy. However, local correlations between secondary and primary structure are probably of under-appreciated importance in many tertiary structure prediction methods, such as threading.
q-bio.BM:The determination of the folding mechanisms of proteins is critical to understand the topological change that can propagate Alzheimer and Creutzfeld-Jakobs diseases, among others. The computational community has paid considerable attention to this problem; however, the associated time scale, typically on the order of milliseconds or more, represents a formidable challenge. Ab initio protein folding from long molecular dynamics (MD) simulations or ensemble dynamics is not feasible with ordinary computing facilities and new techniques must be introduced. Here we present a detailed study of the folding of a 16-residue beta-hairpin, described by a generic energy model and using the activation-relaxation technique. From a total of 90 trajectories at 300 K, three folding pathways emerge. All involve a simultaneous optimization of the complete hydrophobic and hydrogen bonding interactions. The first two follow closely those observed by previous theoretical studies. The third pathway, never observed by previous all-atom folding, unfolding and equilibrium simulations, can be described as a reptation move of one strand of the beta-sheet with respect to the other. This reptation move indicates that non-native interactions can play a dominant role in the folding of secondary structures. These results point to a more complex folding picture than expected for a simple beta-hairpin.
q-bio.BM:Analytic estimates for the forces and free energy generated by bilayer deformation reveal a compelling and intuitive model for MscL channel gating analogous to the nucleation of a second phase. We argue that the competition between hydrophobic mismatch and tension results in a surprisingly rich story which can provide both a quantitative comparison to measurements of opening tension for MscL when reconstituted in bilayers of different thickness and qualitative insights into the function of the MscL channel and other transmembrane proteins.
q-bio.BM:It is important to understand how protein folding and evolution influences each other. Several studies based on entropy calculation correlating experimental measurement of residue participation in folding nucleus and sequence conservation have reached different conclusions. Here we report analysis of conservation of folding nucleus using an evolutionary model alternative to entropy based approaches. We employ a continuous time Markov model of codon substitution to distinguish mutation fixed by evolution and mutation fixed by chance. This model takes into account bias in codon frequency, bias favoring transition over transversion, as well as explicit phylogenetic information. We measure selection pressure using the ratio $\omega$ of synonymous vs. non-synonymous substitution at individual residue site. The $\omega$-values are estimated using the {\sc Paml} method, a maximum-likelihood estimator. Our results show that there is little correlation between the extent of kinetic participation in protein folding nucleus as measured by experimental $\phi$-value and selection pressure as measured by $\omega$-value. In addition, two randomization tests failed to show that folding nucleus residues are significantly more conserved than the whole protein. These results suggest that at the level of codon substitution, there is no indication that folding nucleus residues are significantly more conserved than other residues. We further reconstruct candidate ancestral residues of the folding nucleus and suggest possible test tube mutation studies of ancient folding nucleus.
q-bio.BM:Many signalling functions in molecular biology require proteins bind to substrates such as DNA in response to environmental signals such as the simultaneous binding to a small molecule. Examples are repressor proteins which may transmit information via a conformational change in response to the ligand binding. An alternative entropic mechanism of ``allostery'' suggests that the inducer ligand changes the intramolecular vibrational entropy not just the static structure. We present a quantitative, coarse-grained model of entropic allostery that suggests design rules for internal cohesive potentials in proteins employing this effect. It also addresses the issue of how the signal information to bind or unbind is transmitted through the protein. The model may be applicable to a wide range of repressors and also to signalling in transmembrane proteins.
q-bio.BM:How DNA repair enzymes find the relatively rare sites of damage is not known in great detail. Recent experiments and molecular data suggest that the individual repair enzymes do not work independently of each other, but rather interact with each other through currents exchanged along DNA. A damaged site in DNA hinders this exchange and this makes it possible to quickly free up resources from error free stretches of DNA. Here the size of the speedup gained from this current exchange mechanism is calculated and the characteristic length and time scales are identified. In particular for Escherichia coli we estimate the speedup to be 50000/N, where N is the number of repair enzymes participating in the current exchange mechanism. Even though N is not exactly known a speedup of order 10 is not entirely unreasonable. Furthermore upon over expression of repair enzymes the detection time only varies as one over the squareroot of N and not as 1/N. This behavior is of interest in assessing the impact of stress full and radioactive environments on individual cell mutation rates.
q-bio.BM:We study DNA adsorption and renaturation in a water-phenol two-phase system, with or without shaking. In very dilute solutions, single-stranded DNA is adsorbed at the interface in a salt-dependent manner. At high salt concentrations the adsorption is irreversible. The adsorption of the single-stranded DNA is specific to phenol and relies on stacking and hydrogen bonding. We establish the interfacial nature of a DNA renaturation at a high salt concentration. In the absence of shaking, this reaction involves an efficient surface diffusion of the single-stranded DNA chains. In the presence of a vigorous shaking, the bimolecular rate of the reaction exceeds the Smoluchowski limit for a three-dimensional diffusion-controlled reaction. DNA renaturation in these conditions is known as the Phenol Emulsion Reassociation Technique or PERT. Our results establish the interfacial nature of PERT. A comparison of this interfacial reaction with other approaches shows that PERT is the most efficient technique and reveals similarities between PERT and the renaturation performed by single-stranded nucleic acid binding proteins. Our results lead to a better understanding of the partitioning of nucleic acids in two-phase systems, and should help design improved extraction procedures for damaged nucleic acids. We present arguments in favor of a role of phenol and water-phenol interface in prebiotic chemistry. The most efficient renaturation reactions (in the presence of condensing agents or with PERT) occur in heterogeneous systems. This reveals the limitations of homogeneous approaches to the biochemistry of nucleic acids. We propose a heterogeneous approach to overcome the limitations of the homogeneous viewpoint.
q-bio.BM:Hydrophobicity is thought to be one of the primary forces driving the folding of proteins. On average, hydrophobic residues occur preferentially in the core, whereas polar residues tends to occur at the surface of a folded protein. By analyzing the known protein structures, we quantify the degree to which the hydrophobicity sequence of a protein correlates with its pattern of surface exposure. We have assessed the statistical significance of this correlation for several hydrophobicity scales in the literature, and find that the computed correlations are significant but far from optimal. We show that this less than optimal correlation arises primarily from the large degree of mutations that naturally occurring proteins can tolerate. Lesser effects are due in part to forces other than hydrophobicity and we quantify this by analyzing the surface exposure distributions of all amino acids. Lastly we show that our database findings are consistent with those found from an off-lattice hydrophobic-polar model of protein folding.
q-bio.BM:The approach for the description of the DNA conformational transformations on the mesoscopic scales in the frame of the double helix is presented. Due to consideration of the joint motions of DNA structural elements along the conformational pathways the models for different transformations may be constructed in the unifying two-component form. One component of the model is the degree of freedom of the elastic rod and another component -- the effective coordinate of the conformational transformation. The internal and external model components are interrelated, as it is characteristic for the DNA structure organization. It is shown that the kinetic energy of the conformational transformation of heterogeneous DNA may be put in homogeneous form. In the frame of the developed approach the static excitations of the DNA structure under the transitions between the stable states are found for internal and external components. The comparison of the data obtained with the experiment on intrinsic DNA deformability shows good qualitative agreement. The conclusion is made that the found excitations in the DNA structure may be classificated as the static conformational solitons.
q-bio.BM:Molecular combing is a powerful and simple method for aligning DNA molecules onto a surface. Using this technique combined with fluorescence microscopy, we observed that the length of lambda-DNA molecules was extended to about 1.6 times their contour length (unextended length, 16.2 micrometers) by the combing method on hydrophobic polymethylmetacrylate (PMMA) coated surfaces. The effects of sodium and magnesium ions and pH of the DNA solution were investigated. Interestingly, we observed force-induced melting of single DNA molecules.
q-bio.BM:Using a Brownian dynamics simulation, we numerically studied the interaction of DNA with histone and proposed an octamer-rotation model to describe the process of nucleosome formation. Nucleosome disruption under stretching was also simulated. The theoretical curves of extension versus time as well as of force versus extension are consistent with previous experimental results.
q-bio.BM:We propose a two-dimensional model for a complete description of the dynamics of molecular motors, including both the processive movement along track filaments and the dissociation from the filaments. The theoretical results on the distributions of the run length and dwell time at a given ATP concentration, the dependences of mean run length, mean dwell time and mean velocity on ATP concentration and load are in good agreement with the previous experimental results.
q-bio.BM:Kinesin motors have been studied extensively both experimentally and theoretically. However, the microscopic mechanism of the processive movement of kinesin is still an open question. In this paper, we propose a hand-over-hand model for the processivity of kinesin, which is based on chemical, mechanical, and electrical couplings. In the model the processive movement does not need to rely on the two heads' coordination in their ATP hydrolysis and mechanical cycles. Rather, the ATP hydrolyses at the two heads are independent. The much higher ATPase rate at the trailing head than the leading head makes the motor walk processively in a natural way, with one ATP being hydrolyzed per step. The model is consistent with the structural study of kinesin and the measured pathway of the kinesin ATPase. Using the model the estimated driving force of ~ 5.8 pN is in agreements with the experimental results (5~7.5 pN). The prediction of the moving time in one step (~10 microseconds) is also consistent with the measured values of 0~50 microseconds. The previous observation of substeps within the 8-nm step is explained. The shapes of velocity-load (both positive and negative) curves show resemblance to previous experimental results.
q-bio.BM:Myosin V and myosin VI are two classes of two-headed molecular motors of the myosin superfamily that move processively along helical actin filaments in opposite directions. Here we present a hand-over-hand model for their processive movements. In the model, the moving direction of a dimeric molecular motor is automatically determined by the relative orientation between its two heads at free state and its head's binding orientation on track filament. This determines that myosin V moves toward the barbed end and myosin VI moves toward the pointed end of actin. During the moving period in one step, one head remains bound to actin for myosin V whereas two heads are detached for myosin VI: The moving manner is determined by the length of neck domain. This naturally explains the similar dynamic behaviors but opposite moving directions of myosin VI and mutant myosin V (the neck of which is truncated to only one-sixth of the native length). Because of different moving manners, myosin VI and mutant myosin V exhibit significantly broader step-size distribution than native myosin V. However, all three motors give the same mean step size of 36 nm (the pseudo-repeat of actin helix). Using the model we study the dynamics of myosin V quantitatively, with theoretical results in agreement with previous experimental ones.
q-bio.BM:We describe a faster and more accurate algorithm for computing the statistical mechanics of DNA denaturation according to the Poland-Scheraga type. Nearest neighbor thermodynamics is included in a complete and general way. The algorithm represents an optimization with respect to algorithmic complexity of the partition function algorithm of Yeramian et al.: We reduce the computation time for a base-pairing probability profile from O(N2) to O(N). This speed-up comes in addition to the speed-up due to a multiexponential approximation of the loop entropy factor as introduced by Fixman and Freire. The speed-up, however, is independent of the multiexponential approximation and reduces time from O(N3) to O(N2) in the exact case. In addition to calculating the standard base-pairing probability profiles, we propose to use the algorithm to calculate various other probabilities (loops, helices, tails) for a more direct view of the melting regions and their positions and sizes.
q-bio.BM:A joint experimental / theoretical investigation of the elastin-like octapeptide GVG(VPGVG) was carried out. In this paper a comprehensive molecular dynamics study of the temperature dependent folding and unfolding of the octapeptide is presented. The current study, as well as its experimental counterpart find that this peptide undergoes an "inverse temperature transition", ITT, leading to a folding at about 310-330 K. In addition, an unfolding transition is identified at unusually high temperatures approaching the boiling point of water. Due to the small size of the system two broad temperature regimes are found: the "ITT regime" (at about 280-320 K) and the "unfolding regime" at about T > 330 K, where the peptide has a maximum probability of being folded at approximately 330 K. A detailed molecular picture involving a thermodynamic order parameter, or reaction coordinate, for this process is presented along with a time-correlation function analysis of the hydrogen bond dynamics within the peptide as well as between the peptide and solvating water molecules. Correlation with experimental evidence and ramifications on the properties of elastin are discussed.
q-bio.BM:A simplified model for the closed circular DNA (ccDNA) is proposed to describe some specific features of the helix-coil transition in such molecule. The Hamiltonian of ccDNA is related to the one introduced earlier for the linear DNA. The basic assumption is that the reduced energy of the hydrogen bond is not constant through the transition process but depends effectively on the fraction of already broken bonds. A transformation formula is obtained which relates the temperature of ccDNA at a given degree of helicity during the transition to the temperature of the corresponding linear chain at the same degree of helicity. The formula provides a simple method to calculate the melting curve for the ccDNA from the experimental melting curve of the linear DNA with the same nucleotide sequence.
q-bio.BM:We develop a simple but rigorous model of protein-protein association kinetics based on diffusional association on free energy landscapes obtained by sampling configurations within and surrounding the native complex binding funnels. Guided by results obtained on exactly solvable model problems, we transform the problem of diffusion in a potential into free diffusion in the presence of an absorbing zone spanning the entrance to the binding funnel. The free diffusion problem is solved using a recently derived analytic expression for the rate of association of asymmetrically oriented molecules. Despite the required high steric specificity and the absence of long-range attractive interactions, the computed rates are typically on the order of 10^4-10^6 M-1 s-1, several orders of magnitude higher than rates obtained using a purely probabilistic model in which the association rate for free diffusion of uniformly reactive molecules is multiplied by the probability of a correct alignment of the two partners in a random collision. As the association rates of many protein-protein complexes are also in the 10^5-10^6 M-1 s-1, our results suggest that free energy barriers arising from desolvation and/or side-chain freezing during complex formation or increased ruggedness within the binding funnel, which are completely neglected in our simple diffusional model, do not contribute significantly to the dynamics of protein-protein association. The transparent physical interpretation of our approach that computes association rates directly from the size and geometry of protein-protein binding funnels makes it a useful complement to Brownian dynamics simulations.
q-bio.BM:Functional proteins must fold with some minimal stability to a structure that can perform a biochemical task. Here we use a simple model to investigate the relationship between the stability requirement and the capacity of a protein to evolve the function of binding to a ligand. Although our model contains no built-in tradeoff between stability and function, proteins evolved function more efficiently when the stability requirement was relaxed. Proteins with both high stability and high function evolved more efficiently when the stability requirement was gradually increased than when there was constant selection for high stability. These results show that in our model, the evolution of function is enhanced by allowing proteins to explore sequences corresponding to marginally stable structures, and that it is easier to improve stability while maintaining high function than to improve function while maintaining high stability. Our model also demonstrates that even in the absence of a fundamental biophysical tradeoff between stability and function, the speed with which function can evolve is limited by the stability requirement imposed on the protein.
q-bio.BM:Using the model for the processive movement of a dimeric kinesin we proposed before, we study the dynamics of a number of mutant homodimeric and heterodimeric kinesins that were constructed by Kaseda et al. (Kaseda, K., Higuchi, H. and Hirose, K. PNAS 99, 16058 (2002)). The theoretical results of ATPase rate per head, moving velocity, and stall force of the motors show good agreement with the experimental results by Kaseda et al.: The puzzling dynamic behaviors of heterodimeric kinesin that consists of two distinct heads compared with its parent homodimers can be easily explained by using independent ATPase rates of the two heads in our model. We also study the collective kinetic behaviors of kinesins in MT-gliding motility. The results explains well that the average MT-gliding velocity is independent of the number of bound motors and is equal to the moving velocity of a single kinesin relative to MT.
q-bio.BM:The simplest approximation of interaction potential between amino-acids in proteins is the contact potential, which defines the effective free energy of a protein conformation by a set of amino acid contacts formed in this conformation. Finding a contact potential capable of predicting free energies of protein states across a variety of protein families will aid protein folding and engineering in silico on a computationally tractable time-scale. We test the ability of contact potentials to accurately and transferably (across various protein families) predict stability changes of proteins upon mutations. We develop a new methodology to determine the contact potentials in proteins from experimental measurements of changes in protein thermodynamic stabilities (ddG) upon mutations. We apply our methodology to derive sets of contact interaction parameters for a hierarchy of interaction models including solvation and multi-body contact parameters. We test how well our models reproduce experimental measurements by statistical tests. We evaluate the maximum accuracy of predictions obtained by using contact potentials and the correlation between parameters derived from different data-sets of experimental ddG values. We argue that it is impossible to reach experimental accuracy and derive fully transferable contact parameters using the contact models of potentials. However, contact parameters can yield reliable predictions of ddG for datasets of mutations confined to specific amino-acid positions in the sequence of a single protein.
q-bio.BM:We first review how to determine the rate of vibrational energy relaxation (VER) using perturbation theory. We then apply those theoretical results to the problem of VER of a CD stretching mode in the protein cytochrome c. We model cytochrome c in vacuum as a normal mode system with the lowest-order anharmonic coupling elements. We find that, for the ``lifetime'' width parameter $\gamma=3 \sim 30$ cm$^{-1}$, the VER time is $0.2 \sim 0.3$ ps, which agrees rather well with the previous classical calculation using the quantum correction factor method, and is consistent with spectroscopic experiments by Romesberg's group. We decompose the VER rate into separate contributions from two modes, and find that the most significant contribution, which depends on the ``lifetime'' width parameter, comes from those modes most resonant with the CD vibrational mode.
q-bio.BM:The three-dimensional structures of two common repeat motifs Val$^1$-Pro$^2$-Gly$^3$-Val$^4$-Gly$^5$ and Val$^1$-Gly$^2$-Val$^3$-Pro$^4$-Gly$^5$-Val$^6$-Gly$^7$-Val$^8$-Pro$^9$ of tropoelastin are investigated by using the multicanonical simulation procedure. By minimizing the energy structures along the trajectory the thermodynamically most stable low-energy microstates of the molecule are determined. The structural predictions are in good agreement with X-ray diffraction experiments.
q-bio.BM:We address the controversial hot question concerning the validity of the loose-coupling versus the lever-arm models in the actomyosin dynamics by re-interpreting and extending the washboard potential model proposed by some of us in a previous paper. In the new theory, a loose-coupling mechanism co-exists with the deterministic lever-arm model. The synergetic action of a random component, originating from the harnessed thermal energy, and of the power-stroke generated by the lever-arm classical mechanism is seen to yield an excellent fit of the set of data obtained in T. Yanagida's laboratory on the sliding of Myosin II heads on actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of theory is tested via different combination of parameters and potential profiles.
q-bio.BM:In simple models side chains are often represented implicitly (e.g., by spin-states) or simplified as one atom. We study side chain effects using square lattice and tetrahedral lattice models, with explicitly side chains of two atoms. We distinguish effects due to chirality and effects due to side chain flexibilities, since residues in proteins are L-residues, and their side chains adopt different rotameric states. Short chains are enumerated exhaustively. For long chains, we sample effectively rare events (eg, compact conformations) and obtain complete pictures of ensemble properties of these models at all compactness region. We find that both chirality and reduced side chain flexibility lower the folding entropy significantly for globally compact conformations, suggesting that they are important properties of residues to ensure fast folding and stable native structure. This corresponds well with our finding that natural amino acid residues have reduced effective flexibility, as evidenced by analysis of rotamer libraries and side chain rotatable bonds. We further develop a method calculating the exact side-chain entropy for a given back bone structure. We show that simple rotamer counting often underestimates side chain entropy significantly, and side chain entropy does not always correlate well with main chain packing. Among compact backbones with maximum side chain entropy, helical structures emerges as the dominating configurations. Our results suggest that side chain entropy may be an important factor contributing to the formation of alpha helices for compact conformations.
q-bio.BM:We show that the contact map of the native structure of globular proteins can be reconstructed starting from the sole knowledge of the contact map's principal eigenvector, and present an exact algorithm for this purpose. Our algorithm yields a unique contact map for all 221 globular structures of PDBselect25 of length $N \le 120$. We also show that the reconstructed contact maps allow in turn for the accurate reconstruction of the three-dimensional structure. These results indicate that the reduced vectorial representation provided by the principal eigenvector of the contact map is equivalent to the protein structure itself. This representation is expected to provide a useful tool in bioinformatics algorithms for protein structure comparison and alignment, as well as a promising intermediate step towards protein structure prediction.
q-bio.BM:Function of proteins or a network of interacting proteins often involves communication between residues that are well separated in sequence. The classic example is the participation of distant residues in allosteric regulation. Bioinformatic and structural analysis methods have been introduced to infer residues that are correlated. Recently, increasing attention has been paid to obtain the sequence properties that determine the tendency of disease related proteins (Abeta peptides, prion proteins, transthyretin etc.) to aggregate and form fibrils. Motivated in part by the need to identify sequence characteristics that indicate a tendency to aggregate, we introduce a general method that probes covariations in charged residues along the sequence in a given protein family. The method, which involves computing the Sequence Correlation Entropy (SCE) using the quenched probability Psk(i,j) of finding a residue pair at a given sequence separation sk, allows us to classify protein families in terms of their SCE. Our general approach may be a useful way in obtaining evolutionary covariations of amino acid residues on a genome wide level.
q-bio.BM:We present an analysis of the effects of global topology on the structural stability of folded proteins in thermal equilibrium with a heat bath. For a large class of single domain proteins, we computed the harmonic spectrum within the Gaussian Network Model (GNM) and determined the spectral dimension, a parameter describing the low frequency behaviour of the density of modes. We find a surprisingly strong correlation between the spectral dimension and the number of amino acids of the protein. Considering that larger spectral dimension value relate to more topologically compact folded state, our results indicate that for a given temperature and length of the protein, the folded structure corresponds to the less compact folding compatible with thermodynamic stability.
q-bio.BM:We present a simple physical model which demonstrates that the native state folds of proteins can emerge on the basis of considerations of geometry and symmetry. We show that the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and hydrophobicity are sufficient to yield a free energy landscape with broad minima even for a homopolymer. These minima correspond to marginally compact structures comprising the menu of folds that proteins choose from to house their native-states in. Our results provide a general framework for understanding the common characteristics of globular proteins.
q-bio.BM:With the aim to study the relationship between protein sequences and their native structures, we adopt vectorial representations for both sequence and structure. The structural representation is based on the Principal Eigenvector of the fold's contact matrix (PE). As recently shown, the latter encodes sufficient information for reconstructing the whole contact matrix. The sequence is represented through a Hydrophobicity Profile (HP), using a generalized hydrophobicity scale that we obtain from the principal eigenvector of a residue-residue interaction matrix and denote it as interactivity scale. Using this novel scale, we define the optimal HP of a protein fold, and predict, by means of stability arguments, that it is strongly correlated with the PE of the fold's contact matrix. This prediction is confirmed through an evolutionary analysis, which shows that the PE correlates with the HP of each individual sequence adopting the same fold and, even more strongly, with the average HP of this set of sequences. Thus, protein sequences evolve in such a way that their average HP is close to the optimal one, implying that neutral evolution can be viewed as a kind of motion in sequence space around the optimal HP. Our results indicate that the correlation coefficient between N-dimensional vectors constitutes a natural metric in the vectorial space in which we represent both protein sequences and protein structures, which we call Vectorial Protein Space. In this way, we define a unified framework for sequence to sequence, sequence to structure, and structure to structure alignments. We show that the interactivity scale is nearly optimal both for the comparison of sequences with sequences and sequences with structures.
q-bio.BM:We fit the Fourier transforms of solvent accessibility and hydrophobicity profiles of a representative set of proteins to a joint multi-variable Gaussian. This allows us to separate the intrinsic tendencies of sequence and structure profiles from the interactions that correlate them; for example, the $\alpha$-helix periodicity in sequence hydrophobicity is dictated by the solvent accessibility of structures. The distinct intrinsic tendencies of sequence and structure profiles are most pronounced at long periods, where sequence hydrophobicity fluctuates more, while solvent accessibility fluctuations are less than average. Interestingly, correlations between the two profiles can be interpreted as the Boltzmann weight of the solvation energy at room temperature.
q-bio.BM:In this paper, we examine the mechanical role of the lipid bilayer in ion channel conformation and function with specific reference to the case of the mechanosensitive channel of large conductance (MscL). In a recent paper (Wiggins and Phillips, 2004), we argued that mechanotransduction very naturally arises from lipid-protein interactions by invoking a simple analytic model of the MscL channel and the surrounding lipid bilayer. In this paper, we focus on improving and expanding this analytic framework for studying lipid-protein interactions with special attention to MscL. Our goal is to generate simple scaling relations which can be used to provide qualitative understanding of the role of membrane mechanics in protein function and to quantitatively interpret experimental results. For the MscL channel, we find that the free energies induced by lipid-protein interaction are of the same order as the free energy differences between conductance states measured by Sukharev et al. (1999). We therefore conclude that the mechanics of the bilayer plays an essential role in determining the conformation and function of the channel. Finally, we compare the predictions of our model to experimental results from the recent investigations of the MscL channel by Perozo et al. (2002), Powl et al. (2003), Yoshimura et al. (2004), and others and suggest a suite of new experiments.
q-bio.BM:The conjunction of insights from structural biology, solution biochemistry, genetics and single molecule biophysics has provided a renewed impetus for the construction of quantitative models of biological processes. One area that has been a beneficiary of these experimental techniques is the study of viruses. In this paper we describe how the insights obtained from such experiments can be utilized to construct physical models of processes in the viral life cycle. We focus on dsDNA bacteriophages and show that the bending elasticity of DNA and its electrostatics in solution can be combined to determine the forces experienced during packaging and ejection of the viral genome. Furthermore, we quantitatively analyze the effect of fluid viscosity and capsid expansion on the forces experienced during packaging. Finally, we present a model for DNA ejection from bacteriophages based on the hypothesis that the energy stored in the tightly packed genome within the capsid leads to its forceful ejection. The predictions of our model can be tested through experiments in vitro where DNA ejection is inhibited by the application of external osmotic pressure.
q-bio.BM:Amyloid fibers are aggregates of proteins. They are built out of a peptide called $\beta$--amyloid (A$\beta$) containing between 41 and 43 residues, produced by the action of an enzyme which cleaves a much larger protein known as the Amyloid Precursor Protein (APP). X-ray diffraction experiments have shown that these fibrils are rich in $\beta$--structures, whereas the shape of the peptide displays an $\alpha$--helix structure within the APP in its biologically active conformation. A realistic model of fibril formation is developed based on the seventeen residues A$\beta$12--28 amyloid peptide, which has been shown to form fibrils structurally similar to those of the whole A$\beta$ peptide. With the help of physical arguments and in keeping with experimental findings, the A$\beta$12--28 monomer is assumed to be in four possible states (i.e., native helix conformation, $\beta$--hairpin, globular low--energy state and unfolded state). Making use of these monomeric states, oligomers (dimers, tertramers and octamers) were constructed. With the help of short, detailed Molecular Dynamics (MD) calculations of the three monomers and of a variety of oligomers, energies for these structures were obtained. Making use of these results within the framework of a simple yet realistic model to describe the entropic terms associated with the variety of amyloid conformations, a phase diagram can be calculated of the whole many--body system, leading to a thermodynamical picture in overall agreement with the experimental findings. In particular, the existence of micellar metastable states seem to be a key issue to determine the thermodynamical properties of the system.
q-bio.BM:The possibility of deriving the contact potentials between amino acids from their frequencies of occurence in proteins is discussed in evolutionary terms. This approach allows the use of traditional thermodynamics to describe such frequencies and, consequently, to develop a strategy to include in the calculations correlations due to the spatial proximity of the amino acids and to their overall tendency of being conserved in proteins. Making use of a lattice model to describe protein chains and defining a "true" potential, we test these strategies by selecting a database of folding model sequences, deriving the contact potentials from such sequences and comparing them with the "true" potential. Taking into account correlations allows for a markedly better prediction of the interaction potentials.
q-bio.BM:While all the information required for the folding of a protein is contained in its amino acid sequence, one has not yet learned how to extract this information to predict the three--dimensional, biologically active, native conformation of a protein whose sequence is known. Using insight obtained from simple model simulations of the folding of proteins, in particular of the fact that this phenomenon is essentially controlled by conserved (native) contacts among (few) strongly interacting ("hot"), as a rule hydrophobic, amino acids, which also stabilize local elementary structures (LES, hidden, incipient secondary structures like $\alpha$--helices and $\beta$--sheets) formed early in the folding process and leading to the postcritical folding nucleus (i.e., the minimum set of native contacts which bring the system pass beyond the highest free--energy barrier found in the whole folding process) it is possible to work out a succesful strategy for reading the native structure of designed proteins from the knowledge of only their amino acid sequence and of the contact energies among the amino acids. Because LES have undergone millions of years of evolution to selectively dock to their complementary structures, small peptides made out of the same amino acids as the LES are expected to selectively attach to the newly expressed (unfolded) protein and inhibit its folding, or to the native (fluctuating) native conformation and denaturate it. These peptides, or their mimetic molecules, can thus be used as effective non--conventional drugs to those already existing (and directed at neutralizing the active site of enzymes), displaying the advantage of not suffering from the uprise of resistance.
q-bio.BM:Methods for alignment of protein sequences typically measure similarity by using substitution matrix with scores for all possible exchanges of one amino acid with another. Although widely used, the matrices derived from homologous sequence segments, such as Dayhoff's PAM matrices and Henikoff's BLOSUM matrices, are not specific for protein conformation identification. Using a different approach, we got many amino acid segment blocks. For each of them, the protein secondary structure is identical. Based on these blocks, we have derived new amino acid substitution matrices. The application of these matrices led to marked improvements in conformation segment search and homologues detection in twilight zone.
q-bio.BM:The advent of new experimental genomic technologies and the massive increase of DNA sequence information is helping researchers better understand how our genes work. Recently, experiments on mRNA abundance (gene expression) have revealed that gene expression shows a stationary organization described by a power-law distribution (scale-free organization) (i.e., gene expression $k$ decays as $k^{-\gamma}$), which is highly conserved in all the major five kingdoms of life, from Bacteria to Human. An underlying gene expression dynamics "rich-travel-more" was suggested to recover that evolutional conservation of transcriptional organization. Here we propose a constructive approach to gene expression dynamics with larger scope. Our gene expression construction restores the stationary state, predicts the power-law exponent for different organisms with natural explanation for small correction at high and low expression levels, describes the intermediate state dynamics (time finite) and elucidates the gene expression stability. This approach requires only one assumption: Markov property.
q-bio.BM:Proteins are minimally frustrated polymers. However, for realistic protein models non-native interactions must be taken into account. In this paper we analyze the effect of non-native interactions on the folding rate and on the folding free energy barrier. We present an analytic theory to account for the modification on the free energy landscape upon introduction of non-native contacts, added as a perturbation to the strong native interactions driving folding. Our theory predicts a rate-enhancement regime at fixed temperature, under the introduction of weak, non-native interactions. We have thoroughly tested this theoretical prediction with simulations of a coarse-grained protein model, by employing an off-lattice $C_\alpha$ model of the src-SH3 domain. The strong agreement between results from simulations and theory confirm the non trivial result that a relatively small amount of non-native interaction energy can actually assist the folding to the native structure.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. For simplified models of proteins where coordinates of only $C_\alpha$ atoms need to be specified, an accurate potential function is important. Such a simplified model is essential for efficient search of conformational space. In this work, we present a formulation of potential function for simplified representations of protein structures. It is based on the combination of descriptors derived from residue-residue contact and sequence-dependent local geometry. The optimal weight coefficients for contact and local geometry is obtained through optimization by maximizing margins among native and decoy structures. The latter are generated by chain growth and by gapless threading. The performance of the potential function in blind test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. This potential function have comparable or better performance than several residue-based potential functions that require in addition coordinates of side chain centers or coordinates of all side chain atoms.
q-bio.BM:Circular permutation connects the N and C termini of a protein and concurrently cleaves elsewhere in the chain, providing an important mechanism for generating novel protein fold and functions. However, their in genomes is unknown because current detection methods can miss many occurances, mistaking random repeats as circular permutation. Here we develop a method for detecting circularly permuted proteins from structural comparison. Sequence order independent alignment of protein structures can be regarded as a special case of the maximum-weight independent set problem, which is known to be computationally hard. We develop an efficient approximation algorithm by repeatedly solving relaxations of an appropriate intermediate integer programming formulation, we show that the approximation ratio is much better then the theoretical worst case ratio of $r = 1/4$. Circularly permuted proteins reported in literature can be identified rapidly with our method, while they escape the detection by publicly available servers for structural alignment.
q-bio.BM:Motivation. Protein design aims to identify sequences compatible with a given protein fold but incompatible to any alternative folds. To select the correct sequences and to guide the search process, a design scoring function is critically important. Such a scoring function should be able to characterize the global fitness landscape of many proteins simultaneously.   Results. To find optimal design scoring functions, we introduce two geometric views and propose a formulation using mixture of nonlinear Gaussian kernel functions. We aim to solve a simplified protein sequence design problem. Our goal is to distinguish each native sequence for a major portion of representative protein structures from a large number of alternative decoy sequences, each a fragment from proteins of different fold. Our scoring function discriminate perfectly a set of 440 native proteins from 14 million sequence decoys. We show that no linear scoring function can succeed in this task. In a blind test of unrelated proteins, our scoring function misclassfies only 13 native proteins out of 194. This compares favorably with about 3-4 times more misclassifications when optimal linear functions reported in literature are used. We also discuss how to develop protein folding scoring function.
q-bio.BM:Being HIV-1-PR an essential enzyme in the viral life cycle, its inhibition can control AIDS. Because the folding of single domain proteins, like HIV-1-PR is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved amino acids) which have evolved over myriads of generations to recognize and strongly attract each other so as to make the protein fold fast, we suggest a novel type of HIV-1-PR inhibitors which interfere with the folding of the protein: short peptides displaying the same amino acid sequence of that of LES. Theoretical and experimental evidence for the specificity and efficiency of such inhibitors are presented.
q-bio.BM:Vibrational energy relaxation (VER) of a selected mode in cytochrome c (hemeprotein) in vacuum is studied using two theoretical approaches: One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes coupled with nonlinear coupling elements. Both methods result in estimates of VER time (sub ps) for a CD stretching mode in the protein at room temperature, that are in accord with the experimental data of Romesberg's group. The applicability of the two methods is examined through a discussion of the validity of Fermi's golden rule on which the two methods are based.
q-bio.BM:The 16-22 amino acid fragment of the beta-amyloid peptide associated with the Alzheimer's disease, Abeta, is capable of forming amyloid fibrils. Here we study the aggregation mechanism of Abeta(16-22) peptides by unbiased thermodynamic simulations at the atomic level for systems of one, three and six Abeta(16-22) peptides. We find that the isolated Abeta(16-22) peptide is mainly a random coil in the sense that both the alpha-helix and beta-strand contents are low, whereas the three- and six-chain systems form aggregated structures with a high beta-sheet content. Furthermore, in agreement with experiments on Abeta(16-22) fibrils, we find that large parallel beta-sheets are unlikely to form. For the six-chain system, the aggregated structures can have many different shapes, but certain particularly stable shapes can be identified.
q-bio.BM:We identified latent periodicity in catalytic domains of approximately 85% of serine/threonine and tyrosine protein kinases. Similar results were obtained for other 22 protein domains. We also designed the method of noise decomposition, which is aimed to distinguish between different periodicity types of the same period length. The method is to be used in conjunction with the cyclic profile alignment, and this combination is able to reveal structure-related or function-related patterns of latent periodicity. Possible origins of the periodic structure of protein kinase active sites are discussed. Summarizing, we presume that latent periodicity is the common property of many catalytic protein domains.
q-bio.BM:We found latent periodicity of 150 protein families now. We suppose that latent periodicity can determine a spectrum of resonance oscillations in proteins.
q-bio.BM:Proteins have regular tertiary structures but irregular amino acid sequences. This made it very difficult to decode the structural information in the protein sequences. Here we demonstrate that many small alpha protein domains have hidden sequence symmetries characteristic of their pseudo-symmetric tertiary structures. We also present a modified method of recurrent plot to reveal this kind of the hidden sequence symmetry. The results may enable us understand parts of the relations between protein sequences and their tertiary structures, i.e, how the primary sequence of a protein determines its tertiary structure.
q-bio.BM:Summary: The F2CS server provides access to the software, F2CS2.00, that implements an automated prediction method of SCOP and CATH classifications of proteins, based on their FSSP Z-scores (Getz et al., 2002), Availability: Free, at http://www.weizmann.ac.il/physics/complex/compphys/f2cs/. Contact: eytan.domany@weizmann.ac.il Supplementary information: The site contains links to additional figures and tables.
q-bio.BM:Activated processes such as protein unfolding are highly sensitive to heterogeneity in the environment. We study a highly simplified model of a protein in a random heterogeneous environment, a model of the in vivo environment. It is found that if the heterogeneity is sufficiently large the total rate of the process is essentially a random variable; this may be the cause of the species-to-species variability in the rate of prion protein conversion found by Deleault et al. [Nature, 425 (2003) 717].
q-bio.BM:Water molecules and molecular chaperones efficiently help the protein folding process. Here we describe their action in the context of the energy and topological networks of proteins. In energy terms water and chaperones were suggested to decrease the activation energy between various local energy minima smoothing the energy landscape, rescuing misfolded proteins from conformational traps and stabilizing their native structure. In kinetic terms water and chaperones may make the punctuated equilibrium of conformational changes less punctuated and help protein relaxation. Finally, water and chaperones may help the convergence of multiple energy landscapes during protein-macromolecule interactions. We also discuss the possibility of the introduction of protein games to narrow the multitude of the energy landscapes when a protein binds to another macromolecule. Both water and chaperones provide a diffuse set of rapidly fluctuating weak links (low affinity and low probability interactions), which allow the generalization of all these statements to a multitude of networks.
q-bio.BM:Nucleosomes organize the folding of DNA into chromatin and significantly influence transcription, replication, regulation and repair. All atom molecular dynamics simulations of a nucleosome and of its 146 basepairs of DNA free in solution have been conducted. DNA helical parameters are extracted from each trajectory to compare the conformation, effective force constants, persistence length measures, and fluctuations of nucleosomal DNA to free DNA. A method for disassembling and reconstructing the conformation and dynamics of the nucleosome using Fourier analysis is presented. Results indicate that the superhelical path of DNA in the nucleosome is irregular. Long length variations in the conformation of nucleosomal DNA are identified other than those associated with helix repeat. These variations are required to create a proposed tetrasome conformation or to qualitatively reconstruct the 1.75 turns of the nuclesomal superhelix. Free DNA achieves enough bend and shear in solution to create an ideal nucleosome superhelix, but these deformations are not organized so the conformation is essentially linear. Reconstruction of free DNA using selected long wavelength variations in conformation can produce either a left-handed or a right-handed superhelix. DNA is less flexible in the nucleosome than when free in solution, however such measures are length scale dependent.
q-bio.BM:Gene expression analysis by means of microarrays is based on the sequence specific binding of mRNA to DNA oligonucleotide probes and its measurement using fluorescent labels. The binding of RNA fragments involving other sequences than the intended target is problematic because it adds a "chemical background" to the signal, which is not related to the expression degree of the target gene. The paper presents a molecular signature of specific and non specific hybridization with potential consequences for gene expression analysis. We analyzed the signal intensities of perfect match (PM) and mismatch (MM) probes of GeneChip microarrays to specify the effect of specific and non specific hybridization. We found that these events give rise to different relations between the PM and MM intensities as function of the middle base of the PMs, namely a triplet- (C>G=T>A>0) and a duplet-like (C=T>0>G=A) pattern of the PM-MM log-intensity difference upon binding of specific and non specific RNA fragments, respectively. The systematic behaviour of the intensity difference can be rationalized on the level of base pairings of DNA/RNA oligonucleotide duplexes in the middle of the probe sequence. Non-specific binding is characterized by the reversal of the central Watson Crick (WC) pairing for each PM/MM probe pair, whereas specific binding refers to the combination of a WC and a self complementary (SC) pairing in PM and MM probes, respectively. The intensity of complementary MM introduces a systematic source of variation which decreases the precision of expression measures based on the MM intensities.
q-bio.BM:We implement the replica exchange molecular dynamics algorithm to study the interactions of a model peptide (WALP-16) with an explicitly represented DPPC membrane bilayer. We observe the spontaneous, unbiased insertion of WALP-16 into the DPPC bilayer and its folding into an a-helix with a trans-bilayer orientation. We observe that the insertion of the peptide into the DPPC bilayer precedes secondary structure formation. Although the peptide has some propensity to form a partially helical structure in the interfacial region of the DPPC/water system, this state is not a productive intermediate but rather an off-pathway trap for WALP-16 insertion. Equilibrium simulations show that the observed insertion/folding pathway mirrors the potential of mean force (PMF). Calculation of the enthalpic and entropic contributions to this PMF show that the surface bound conformation of WALP-16 is significantly lower in energy than other conformations, and that the insertion of WALP-16 into the bilayer without regular secondary structure is enthalpically unfavorable by 5-10 kcal/mol/residue. The observed insertion/folding pathway disagrees with the dominant conceptual model, which is that a surface bound helix is an obligatory intermediate for the insertion of a-helical peptides into lipid bilayers. In our simulations, the observed insertion/folding pathway is favored because of a large (> 100 kcal/mol) increase in system entropy that occurs when the unstructured WALP-16 peptide enters the lipid bilayer interior. The insertion/folding pathway that is lowest in free energy depends sensitively on the near cancellation of large enthalpic and entropic terms. This suggests that intrinsic membrane peptides may have a diversity of insertion/folding behaviors depending on the exact system of peptide and lipid under consideration.
q-bio.BM:Analysis of data from an Affymetrix Latin Square spike-in experiment indicates that measured fluorescence intensities of features on an oligonucleotide microarray are related to spike-in RNA target concentrations via a hyperbolic response function, generally identified as a Langmuir adsorption isotherm. Furthermore the asymptotic signal at high spike-in concentrations is almost invariably lower for a mismatch feature than for its partner perfect match feature. We survey a number of theoretical adsorption models of hybridization at the microarray surface and find that in general they are unable to explain the differing saturation responses of perfect and mismatch features. On the other hand, we find that a simple and consistent explanation can be found in a model in which equilibrium hybridization followed by partial dissociation of duplexes during the post-hybridization washing phase.
q-bio.BM:A model for the processive movement of dynein is presented based on experimental observations available. In the model, the change from strong microtubule-binding to weak binding of dynein is determined naturally by the variation of the relative orientation between the two interacting surfaces of the stalk tip and the microtubule as the stalk rotates from the ADP.Vi-state orientation to the apo-state orientation. This means that the puzzling communication from the ATP binding site in the globular head to the MT-binding site in the tip of the stalk, which is prerequisite in the conventional model, is not required. Using the present model, the previous experimental results, such as (i) the step size of a dynein being an integer times of the period of the MT lattice, (ii) the dependence of the step size on load, i.e., the step size decreasing with the increase of load, and (iii) the stall force being proportional to [ATP] at low [ATP] and becoming saturated at high [ATP], are well explained.
q-bio.BM:We address the controversial hot question concerning the validity of the loose coupling versus the lever-arm theories in the actomyosin dynamics by re-interpreting and extending the phenomenological washboard potential model proposed by some of us in a previous paper. In this new model a Brownian motion harnessing thermal energy is assumed to co-exist with the deterministic swing of the lever-arm, to yield an excellent fit of the set of data obtained by some of us on the sliding of Myosin II heads on immobilized actin filaments under various load conditions. Our theoretical arguments are complemented by accurate numerical simulations, and the robustness of the model is tested via different choices of parameters and potential profiles.
q-bio.BM:In this paper we investigate the role of native geometry on the kinetics of protein folding based on simple lattice models and Monte Carlo simulations. Results obtained within the scope of the Miyazawa-Jernigan indicate the existence of two dynamical folding regimes depending on the protein chain length. For chains larger than 80 amino acids the folding performance is sensitive to the native state's conformation. Smaller chains, with less than 80 amino acids, fold via two-state kinetics and exhibit a significant correlation between the contact order parameter and the logarithmic folding times. In particular, chains with N=48 amino acids were found to belong to two broad classes of folding, characterized by different cooperativity, depending on the contact order parameter. Preliminary results based on the G\={o} model show that the effect of long range contact interaction strength in the folding kinetics is largely dependent on the native state's geometry.
q-bio.BM:Monte Carlo simulations show that long-range interactions play a major role in determining the folding rates of 48-mer three-dimensional lattice polymers modelled by the Go potential. For three target structures with different native geometries we found a sharp increase in the folding time when the relative contribution of the long-range interactions to the native state's energy is decreased from ~50% towards zero. However, the dispersion of the simulated folding times depends strongly on the native geometry and Go polymers folding to one of the target structures exhibit folding times spanning three orders of magnitude. We have also found that, depending on the target geometry, a strong geometric coupling may exist between local and long-range contacts meaning that, when this coupling exists, the formation of long-range contacts is forced by the previous formation of local contacts. The absence of a strong geometric coupling leads to kinetics that are more sensitive to the interaction energy parameters; in this case the formation of local contacts is not sufficient to promote the establishment of long-range ones when these are strongly penalized energetically, leading to longer folding times.
q-bio.BM:A simplified interaction potential for protein folding studies at the atomic level is discussed and tested on a set of peptides with about 20 residues each. The test set contains both alpha-helical (Trp cage, Fs) and beta-sheet (GB1p, GB1m2, GB1m3, Betanova, LLM) peptides. The model, which is entirely sequence-based, is able to fold these different peptides for one and the same choice of model parameters. Furthermore, the melting behavior of the peptides is in good quantitative agreement with experimental data. Apparent folded populations obtained using different observables are compared, and are found to be very different for some of the peptides (e.g., Betanova). In other cases (in particular, GB1m2 and GB1m3), the different estimates agree reasonably well, indicating a more two-state-like melting behavior.
q-bio.BM:Single molecule FRET (fluorescence resonance energy transfer) is a powerful technique for detecting real-time conformational changes and molecular interactions during biological reactions. In this review, we examine different techniques of extending observation times via immobilization and illustrate how useful biological information can be obtained from single molecule FRET time trajectories with or without absolute distance information.
q-bio.BM:An ensemble of directed macromolecules on a lattice is considered, where the constituting molecules are chosen as a random sequence of N different types. The same type of molecules experiences a hard-core (exclusion) interaction. We study the robustness of the macromolecules with respect to breaking and substituting individual molecules, using a 1/N expansion. The properties depend strongly on the density of macromolecules. In particular, the macromolecules are robust against breaking and substituting at high densities.
q-bio.BM:The vibrational dynamics of a DNA molecule with counterions neutralizing the charged phosphate groups have been studied. With the help of elaborated model the conformational vibrations of the DNA double helix with alkaline metal ions have been described both qualitatively and quantitatively. For the complexes of DNA with counterions Li+, Na+, K+, Rb+ and Cs+ the normal modes have been found, and a mode characterized by the most notable ion displacements with respect to the DNA backbone has been determined. The frequency of counterion vibrations has been established to decrease as the ion mass increases. The results of theoretical calculation have been showed to be in good agreement with the experimental data of Raman spectroscopy.
q-bio.BM:To understand the mechanism of TATA-box conformational transformations we model structure mobility and find the types of conformational excitations of DNA macromolecule in heteronomous conformation. We have constructed the two-component model for describing DNA conformational transformation with simultaneous transitions in the furanos rings of the monomer link. Internal component describes the change of the base pair position in the double helix. External component describes the displacement of mass center of the monomer link. Nonlinearity of the system is accounted with a form of potential energy describing C3'-C2' and C2'-C3' sugars transitions in monomer link, and interrelation between monomer conformational transition and macromolecule deformation. The comparison of our results with experimental data allows to confirm that the localized conformational excitations may realise in DNA TATA-box. These excitations cause the deformation of the macromolecule fragment.
q-bio.BM:A simple approach is proposed to investigate the protein structure. Using a low complexity model, a simple pairwise interaction and the concept of global optimization, we are able to calculate ground states of proteins, which are in agreement with experimental data. All possible model structures of small proteins are available below a certain energy threshold. The exact lowenergy landscapes for the trp cage protein (1L2Y) is presented showing the connectivity of all states and energy barriers.
q-bio.BM:The effective DNA-DNA interaction force is calculated by computer simulations with explicit tetravalent counterions and monovalent salt. For overcharged DNA molecules, the interaction force shows a double-minimum structure. The positions and depths of these minima are regulated by the counterion density in the bulk. Using two-dimensional lattice sum and free energy perturbation theories, the coexisting phases for DNA bundles are calculated. A DNA-condensation and redissolution transition and a stable mesocrystal with an intermediate lattice constant for high counterion concentration are obtained.
q-bio.BM:By using a mixture model for the density distribution of the three pseudobond angles formed by $C_\alpha$ atoms of four consecutive residues, the local structural states are discretized as 17 conformational letters of a protein structural alphabet. This coarse-graining procedure converts a 3D structure to a 1D code sequence. A substitution matrix between these letters is constructed based on the structural alignments of the FSSP database.
q-bio.BM:An overview of theories related to vibrational energy relaxation (VER) in proteins is presented. VER of a selected mode in cytochrome c is studied using two theoretical approaches. One is the equilibrium simulation approach with quantum correction factors, and the other is the reduced model approach which describes the protein as an ensemble of normal modes interacting through nonlinear coupling elements. Both methods result in estimates of the VER time (sub ps) for a CD stretching mode in the protein at room temperature. The theoretical predictions are in accord with the experimental data of Romesberg's group. A perspective on future directions for the detailed study of time scales and mechanisms for VER in proteins is presented.
q-bio.BM:Protein one-dimensional (1D) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional (3D) structure of a protein is encoded in the amino acid sequence. However, it has not been clear whether a given set of 1D structures contains sufficient information for recovering the underlying 3D structure. Here we show that the 3D structure of a protein can be recovered from a set of three types of 1D structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. Using simulated annealing molecular dynamics simulations, the structures satisfying the given native 1D structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. By selecting the structures best satisfying the restraints, all the proteins showed a coordinate RMS deviation of less than 4\AA{} from the native structure, and for most of them, the deviation was even less than 2\AA{}. The present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship.
q-bio.BM:The lack of specificity in microarray experiments due to non-specific hybridization raises a serious problem for the analysis of microarray data because the residual chemical background intensity is not related to the expression degree of the gene of interest. We analyzed the concentration dependence of the signal intensity of perfect match (PM) and mismatch (MM) probes in terms using a microscopic binding model using a combination of mean hybridization isotherms and single base related affinity terms. The signal intensities of the PM and MM probes and their difference are assessed with regard to their sensitivity, specificity and resolution for gene expression measures. The presented theory implies the refinement of existing algorithms of probe level analysis to correct microarray data for non-specific background intensities.
q-bio.BM:Residue-wise contact order (RWCO) is a new kind of one-dimensional protein structures which represents the extent of long-range contacts. We have recently shown that a set of three types of one-dimensional structures (secondary structure, contact number, and RWCO) contains sufficient information for reconstructing the three-dimensional structure of proteins. Currently, there exist prediction methods for secondary structure and contact number from amino acid sequence, but none exists for RWCO. Also, the properties of amino acids that affect RWCO is not clearly understood. Here, we present a linear regression-based method to predict RWCO from amino acid sequence, and analyze the regression parameters to identify the properties that correlates with the RWCO. The present method achieves the significant correlation of 0.59 between the native and predicted RWCOs on average. An unusual feature of the RWCO prediction is the remarkably large optimal half window size of 26 residues. The regression parameters for the central and near-central residues of the local sequence segment highly correlate with those of the contact number prediction, and hence with hydrophobicity.
q-bio.BM:In this paper the heat signaling in microtubules (MT) is investigated. It is argued that for the description of the heat signaling phenomena in MT, the hyperbolic heat transport (HHT) equation must be used. It is shown that HHT is the Klein-Gordon (K-G) equation. The general solution for the K-G equation for MT is obtained. For the undistorted signal propagation in MT the Heisenberg uncertainty principle is formulated and discussed.   Key words: Microtubules; Heat signaling; Klein-Gordon equation; Heisenberg principle.
q-bio.BM:In the last years, tens of thousands gene expression profiles for cells of several organisms have been monitored. Gene expression is a complex transcriptional process where mRNA molecules are translated into proteins, which control most of the cell functions. In this process, the correlation among genes is crucial to determine the specific functions of genes. Here, we propose a novel multi-dimensional stochastic approach to deal with the gene correlation phenomena. Interestingly, our stochastic framework suggests that the study of the gene correlation requires only one theoretical assumption -Markov property- and the experimental transition probability, which characterizes the gene correlation system. Finally, a gene expression experiment is proposed for future applications of the model.
q-bio.BM:The importance of understanding the mechanism of protein aggregation into insoluble amyloid fibrils relies not only on its medical consequences, but also on its more basic properties of self--organization. The discovery that a large number of uncorrelated proteins can form, under proper conditions, structurally similar fibrils has suggested that the underlying mechanism is a general feature of polypeptide chains. In the present work, we address the early events preceeding amyloid fibril formation in solutions of zinc--free human insulin incubated at low pH and high temperature. Aside from being a easy--to--handle model for protein fibrillation, subcutaneous aggregation of insulin after injection is a nuisance which affects patients with diabetes. Here, we show by time--lapse atomic force microscopy (AFM) that a steady-state distribution of protein oligomers with an exponential tail is reached within few minutes after heating. This metastable phase lasts for few hours until aggregation into fibrils suddenly occurs. A theoretical explanation of the oligomer pre--fibrillar distribution is given in terms of a simple coagulation--evaporation kinetic model, in which concentration plays the role of a critical parameter. Due to high resolution and sensitivity of AFM technique, the observation of a long-lasting latency time should be considered an actual feature of the aggregation process, and not simply ascribed to instrumental inefficency. These experimental facts, along with the kinetic model used, claim for a critical role of thermal concentration fluctuations in the process of fibril nucleation.
q-bio.BM:Identical objects, regularly assembled, form a helix, which is the principal motif of nucleic acids, proteins, and viral capsids.
q-bio.BM:The ambitious and ultimate research purpose in Systems Biology is the understanding and modelling of the cell's system. Although a vast number of models have been developed in order to extract biological knowledge from complex systems composed of basic elements as proteins, genes and chemical compounds, a need remains for improving our understanding of dynamical features of the systems (i.e., temporal-dependence).   In this article, we analyze the gene expression dynamics (i.e., how the genes expression fluctuates in time) by using a new constructive approach. This approach is based on only two fundamental ingredients: symmetry and the Markov property of dynamics. First, by using experimental data of human and yeast gene expression time series, we found a symmetry in short-time transition probability from time $t$ to time $t+1$. We call it self-similarity symmetry (i.e., surprisingly, the gene expression short-time fluctuations contain a repeating pattern of smaller and smaller parts that are like the whole, but different in size). Secondly, the Markov property of dynamics reflects that the short-time fluctuation governs the full-time behaviour of the system. Here, we succeed in reconstructing naturally the global behavior of the observed distribution of gene expression (i.e., scaling-law) and the local behaviour of the power-law tail of this distribution, by using only these two ingredients: symmetry and the Markov property of dynamics. This approach may represent a step forward toward an integrated image of the basic elements of the whole cell.
q-bio.BM:In this work, the dynamics of fluctuations in gene expression time series is investigated. By using collected data of gene expression from yeast and human organisms, we found that the fluctuations of gene expression level and its average value over time are strongly correlated and obey a scaling law. As this feature is found in yeast and human organisms, it suggests that probably this coupling is a common dynamical organizing property of all living systems. To understand these observations, we propose a stochastic model which can explain these collective fluctuations, and predict the scaling exponent. Interestingly, our results indicate that the observed scaling law emerges from the self-similarity symmetry embedded in gene expression fluctuations.
q-bio.BM:We study the mechanism underlying the attraction between nucleosomes, the fundamental packaging units of DNA inside the chromatin complex. We introduce a simple model of the nucleosome, the eight-tail colloid, consisting of a charged sphere with eight oppositely charged, flexible, grafted chains that represent the terminal histone tails. We demonstrate that our complexes are attracted via the formation of chain bridges and that this attraction can be tuned by changing the fraction of charged monomers on the tails. This suggests a physical mechanism of chromatin compaction where the degree of DNA condensation can be controlled via biochemical means, namely the acetylation and deacetylation of lysines in the histone tails.
q-bio.BM:The effects of monovalent (Na+, K+) and divalent (Mg2+, Ca2+, Mn2+) ions on the interaction between DNA and histone are studied using the molecular combing technique. Lamda-DNA molecules and DNA-histone complexes incubated with metal cations (Na+, K+, Mg2+, Ca2+, Mn2+) are stretched on hydrophobic surfaces, and directly observed by fluorescence microscopy. The results indicate that when these cations are added into the DNA solution, the fluorescence intensities of the stained DNA are reduced differently. The monovalent cations (Na+, K+) inhibit binding of histone to DNA. The divalent cations (Mg2+, Ca2+, Mn2+) enhance significantly the binding of histone to DNA and the binding of the DNA-histone complex to the hydrophobic surface. Mn2+ also induces condensation and aggregation of the DNA-histone complex.
q-bio.BM:PDZ (Post-synaptic density-95/discs large/zonula occludens-1) domains are relatively small (80 to 120 residues) protein binding modules central in the organization of receptor clusters and in the association of cellular proteins. Their main function is to bind C-terminals of selected proteins that are recognized through specific amino-acids in their carboxyl end. Binding is associated with a deformation of the PDZ native structure and is responsible for dynamical changes in regions not in direct contact with the target. We investigate how this deformation is related to the harmonic dynamics of the PDZ structure and show that one low-frequency collective normal mode, characterized by the concerted movements of different secondary structures, is involved in the binding process. Our results suggest that even minimal structural changes are responsible of communication between distant regions of the protein, in agreement with recent Nuclear Magnetic Resonance (NMR) experiments. Thus PDZ domains are a very clear example of how collective normal modes are able to characterize the relation between function and dynamics of proteins, and to provide indications on the precursors of binding/unbonding events.
q-bio.BM:We introduce a new measure of antigenic distance between influenza A vaccine and circulating strains. The measure correlates well with efficacies of the H3N2 influenza A component of the annual vaccine between 1971 and 2004, as do results of a theory of the immune response to influenza following vaccination. This new measure of antigenic distance is correlated with vaccine efficacy to a greater degree than are current state-of-the-art phylogenetic sequence analyzes or ferret antisera inhibition assays. We suggest that this new measure of antigenic distance be used in the design of the annual influenza vaccine and in the interpretation of vaccine efficacy monitoring.
q-bio.BM:Prediction of one-dimensional protein structures such as secondary structures and contact numbers is useful for the three-dimensional structure prediction and important for the understanding of sequence-structure relationship. Here we present a new machine-learning method, critical random networks (CRNs), for predicting one-dimensional structures, and apply it, with position-specific scoring matrices, to the prediction of secondary structures (SS), contact numbers (CN), and residue-wise contact orders (RWCO). The present method achieves, on average, $Q_3$ accuracy of 77.8% for SS, correlation coefficients of 0.726 and 0.601 for CN and RWCO, respectively. The accuracy of the SS prediction is comparable to other state-of-the-art methods, and that of the CN prediction is a significant improvement over previous methods. We give a detailed formulation of critical random networks-based prediction scheme, and examine the context-dependence of prediction accuracies. In order to study the nonlinear and multi-body effects, we compare the CRNs-based method with a purely linear method based on position-specific scoring matrices. Although not superior to the CRNs-based method, the surprisingly good accuracy achieved by the linear method highlights the difficulty in extracting structural features of higher order from amino acid sequence beyond that provided by the position-specific scoring matrices.
q-bio.BM:We describe the results obtained from an improved model for protein folding. We find that a good agreement with the native structure of a 46 residue long, five-letter protein segment is obtained by carefully tuning the parameters of the self-avoiding energy. In particular we find an improved free-energy profile. We also compare the efficiency of the multidimensional replica exchange method with the widely used parallel tempering.
q-bio.BM:We use single-particle tracking to study the elastic properties of single microtubules grafted to a substrate. Thermal fluctuations of the free microtubule's end are recorded, in order to measure position distribution functions from which we calculate the persistence length of microtubules with contour lengths between 2.6 and 48 micrometers. We find the persistence length to vary by more than a factor of 20 over the total range of contour lengths. Our results support the hypothesis that shearing between protofilaments contributes significantly to the mechanics of microtubules.
q-bio.BM:Optimal structure of proteins is described by linear stochastic differential equation with mean decrease of free energy and volatility. Structure determining strategy is given by a twin of stochastic variables for which empirical conditions are not postulated. Optimal structure determination will be deformed to be adoptive to trading strategy employing martingale property where stochastic integral w.r.t. analytical solution of stochastic differential equation will be employed.
q-bio.BM:One of the main problems of drug design is that of optimizing the drug--target interaction. In the case in which the target is a viral protein displaying a high mutation rate, a second problem arises, namely the eventual development of resistance. We wish to suggest a scheme for the design of non--conventional drugs which do not face any of these problems and apply it to the case of HIV--1 protease. It is based on the knowledge that the folding of single--domain proteins, like e.g. each of the monomers forming the HIV--1--PR homodimer, is controlled by local elementary structures (LES), stabilized by local contacts among hydrophobic, strongly interacting and highly conserved amino acids which play a central role in the folding process. Because LES have evolved over myriads of generations to recognize and strongly interact with each other so as to make the protein fold fast as well as to avoid aggregation with other proteins, highly specific (and thus little toxic) as well as effective folding--inhibitor drugs suggest themselves: short peptides (or eventually their mimetic molecules), displaying the same amino acid sequence of that of LES (p--LES). Aside from being specific and efficient, these inhibitors are expected not to induce resistance: in fact, mutations which successfully avoid their action imply the destabilization of one or more LES and thus should lead to protein denaturation. Making use of Monte Carlo simulations within the framework of a simple although not oversimplified model, which is able to reproduce the main thermodynamic as well as dynamic properties of monoglobular proteins, we first identify the LES of the HIV--1--PR and then show that the corresponding p--LES peptides act as effective inhibitors of the folding of the protease which do not create resistance.
q-bio.BM:Protein structure is generally conceptualized as the global arrangement or of smaller, local motifs of helices, sheets, and loops. These regular, recurring secondary structural elements have well-understood and standardized definitions in terms of amino acid backbone geometry and the manner in which hydrogen bonding requirements are satisfied. Recently, "tube" models have been proposed to explain protein secondary structure in terms of the geometrically optimal packing of a featureless cylinder. However, atomically detailed simulations demonstrate that such packing considerations alone are insufficient for defining secondary structure; both excluded volume and hydrogen bonding must be explicitly modeled for helix formation. These results have fundamental implications for the construction and interpretation of realistic and meaningful biomacromolecular models.
q-bio.BM:Structure predictions of helical membrane proteins have been designed to take advantage of the structural autonomy of secondary structure elements, as postulated by the two-stage model of Engelman and Popot. In this context, we investigate structure calculation strategies for two membrane proteins with different functions, sizes, aminoacid compositions, and topologies: the glycophorin A homodimer (a paradigm for close inter-helical packing in membrane proteins) and aquaporin (a channel protein). Our structure calculations are based on two alternative folding schemes: a one-step simulated annealing from an extended chain conformation, and a two-step procedure inspired by the grid-search methods traditionally used in membrane protein predictions. In this framework, we investigate rationales for the utilization of sparse NMR data such as distance-based restraints and residual dipolar couplings in structure calculations of helical membrane proteins.
q-bio.BM:Proteins created by combinatorial methods in vitro are an important source of information for understanding sequence-structure-function relationships. Alignments of folded proteins from combinatorial libraries can be analyzed using methods developed for naturally occurring proteins, but this neglects the information contained in the unfolded sequences of the library. We introduce two algorithms, logistic regression and excess information analysis, that use both the folded and unfolded sequences and compare them against contingency table and statistical coupling analysis, which only use the former. The test set for this benchmark study is a library of fictitious proteins that fold according to a hypothetical energy model. Of the four methods studied, only logistic regression is able to correctly recapitulate the energy model from the sequence alignment. The other algorithms predict spurious interactions between alignment positions with strong but individual influences on protein stability. When present in the same protein, stabilizing amino acids tend to lower the energy below the threshold needed for folding. As a result, their frequencies in the alignment can be correlated even if the positions do not interact. We believe any algorithm that neglects the nonlinear relationship between folding and energy is susceptible to this error.
q-bio.BM:In order to extend the results obtained with minimal lattice models to more realistic systems, we study a model where proteins are described as a chain of 20 kinds of structureless amino acids moving in a continuum space and interacting through a contact potential controlled by a 20x20 quenched random matrix. The goal of the present work is to design and characterize amino acid sequences folding to the SH3 conformation, a 60-residues recognition domain common to many regulatory proteins. We show that a number of sequences can fold, starting from a random conformation, to within a distance root mean square deviation (dRMSD) of 2.6A from the native state. Good folders are those sequences displaying in the native conformation an energy lower than a sequence--independent threshold energy.
q-bio.BM:We recently introduced a physical model [Hoang et al., P. Natl. Acad. Sci. USA (2004), Banavar et al., Phys. Rev. E (2004)] for proteins which incorporates, in an approximate manner, several key features such as the inherent anisotropy of a chain molecule, the geometrical and energetic constraints placed by the hydrogen bonds and sterics, and the role played by hydrophobicity. Within this framework, marginally compact conformations resembling the native state folds of proteins emerge as broad competing minima in the free energy landscape even for a homopolymer. Here we show how the introduction of sequence heterogeneity using a simple scheme of just two types of amino acids, hydrophobic (H) and polar (P), and sequence design allows a selected putative native fold to become the free energy minimum at low temperature. The folding transition exhibits thermodynamic cooperativity, if one neglects the degeneracy between two different low energy conformations sharing the same fold topology.
q-bio.BM:In eukaryote nucleosome, DNA wraps around a histone octamer in a left-handed way. We study the process of chirality formation of nucleosome with Brownian dynamics simulation. We model the histone octamer with a quantitatively adjustable chirality: left-handed, right-handed or non-chiral, and simulate the dynamical wrapping process of a DNA molecule on it. We find that the chirality of a nucleosome formed is strongly dependent on that of the histone octamer, and different chiralities of the histone octamer induce its different rotation directions in the wrapping process of DNA. In addition, a very weak chirality of the histone octamer is quite enough for sustaining the correct chirality of the nucleosome formed. We also show that the chirality of a nucleosome may be broken at elevated temperature.
q-bio.BM:Trypsin and chymotrypsin are both serine proteases with high sequence and structural similarities, but with different substrate specificity. Previous experiments have demonstrated the critical role of the two loops outside the binding pocket in controlling the specificity of the two enzymes. To understand the mechanism of such a control of specificity by distant loops, we have used the Gaussian Network Model to study the dynamic properties of trypsin and chymotrypsin and the roles played by the two loops. A clustering method was introduced to analyze the correlated motions of residues. We have found that trypsin and chymotrypsin have distinct dynamic signatures in the two loop regions which are in turn highly correlated with motions of certain residues in the binding pockets. Interestingly, replacing the two loops of trypsin with those of chymotrypsin changes the motion style of trypsin to chymotrypsin-like, whereas the same experimental replacement was shown necessary to make trypsin have chymotrypsin's enzyme specificity and activity. These results suggest that the cooperative motions of the two loops and the substrate-binding sites contribute to the activity and substrate specificity of trypsin and chymotrypsin.
q-bio.BM:The emergence and spreading of chirality on the early Earth is considered by studying a set of reaction-diffusion equations based on a polymerization model. It is found that effective mixing of the early oceans is necessary to reach the present homochiral state. The possibility of introducing mass extinctions and modifying the emergence rate of life is discussed.
q-bio.BM:The differences between uni-directional and bi-directional polymerization are considered. The uni-directional case is discussed in the framework of the RNA world. Similar to earlier models of this type, where polymerization was assumed to proceed in a bi-directional fashion (presumed to be relevant to peptide nucleic acids), left-handed and right-handed monomers are produced via an autocatalysis from an achiral substrate. The details of the bifurcation from a racemic solution to a homochiral state of either handedness is shown to be remarkably independent of whether the polymerization in uni-directional or bi-directional. Slightly larger differences are seen when dissociation is allowed and the dissociation fragments are being recycled into the achiral substrate.
q-bio.BM:A variety of viruses tightly pack their genetic material into protein capsids that are barely large enough to enclose the genome. In particular, in bacteriophages, forces as high as 60 pN are encountered during packaging and ejection, produced by DNA bending elasticity and self-interactions. The high forces are believed to be important for the ejection process, though the extent of their involvement is not yet clear. As a result, there is a need for quantitative models and experiments that reveal the nature of the forces relevant to DNA ejection. Here we report measurements of the ejection forces for two different mutants of bacteriophage lambda, lambda b221cI26 and lambda cI60, which differ in genome length by ~30%. As expected for a force-driven ejection mechanism, the osmotic pressure at which DNA release is completely inhibited varies with the genome length: we find inhibition pressures of 15 atm and 25 atm, respectively, values that are in agreement with our theoretical calculations.
q-bio.BM:The correlations of primary and secondary structures were analyzed using proteins with known structure from Protein Data Bank. The correlation values of amino acid type and the eight secondary structure types at distant position were calculated for distances between -25 and 25. Shapes of the diagrams indicate that amino acids polarity and capability for hydrogen bonding have influence on the secondary structure at some distances. Clear preference of most of the amino acids towards certain secondary structure type classifies amino acids into four groups: alpha-helix admirers, strand admirers, turn and bend admirers and the others. Group four consists of His and Cis, the amino acids that do not show clear preference for any secondary structure. Amino acids from a group have similar physicochemical properties, and the same structural characteristics. The results suggest that amino acid preference for secondary structure type is based on the structural characteristics at Cb and Cg atoms of amino acid. alpha-helix admirers do not have polar heteroatoms on Cb and Cg atoms, nor branching or aromatic group on Cb atom. Amino acids that have aromatic groups or branching on Cb atom are strand admirers. Turn and bend admirers have polar heteroatom on Cb or Cg atoms or do not have Cb atom at all. Our results indicate that polarity and capability for hydrogen bonding have influence on the secondary structure at some distance, and that amino acid preference for secondary structure is caused by structural properties at Cb or Cg atoms.
q-bio.BM:The functionality of proteins is related to their structure in the native state. Protein structures are made up of emergent building blocks of helices and almost planar sheets. A simple coarse-grained geometrical model of a flexible tube barely subject to compaction provides a unified framework for understanding the common character of globular proteins.We argue that a recent critique of the tube idea is not well founded.
q-bio.BM:To gain a deeper insight into cellular processes such as transcription and translation, one needs to uncover the mechanisms controlling the configurational changes of nucleic acids. As a step toward this aim, we present here a novel mesoscopic-level computational model that provides a new window into nucleic acid dynamics. We model a single-stranded nucleic as a polymer chain whose monomers are the nucleosides. Each monomer comprises a bead representing the sugar molecule and a pin representing the base. The bead-pin complex can rotate about the backbone of the chain. We consider pairwise stacking and hydrogen-bonding interactions. We use a modified Monte Carlo dynamics that splits the dynamics into translational bead motion and rotational pin motion. By performing a number of tests we first show that our model is physically sound. We then focus on the study of a the kinetics of a DNA hairpin--a single-stranded molecule comprising two complementary segments joined by a non-complementary loop--studied experimentally. We find that results from our simulations agree with experimental observations, demonstrating that our model is a suitable tool for the investigation of the hybridization of single strands.
q-bio.BM:We investigate the folding behavior of protein sequences by numerically studying all sequences with maximally compact lattice model through exhaustive enumeration. We get the prion-like behavior of protein folding. Individual proteins remaining stable in the isolated native state may change their conformations when they aggregate. We observe the folding properties as the interfacial interaction strength changes, and find that the strength must be strong enough before the propagation of the most stable structures happens.
q-bio.BM:We review some of our recent results obtained within the scope of simple lattice models and Monte Carlo simulations that illustrate the role of native geometry in the folding kinetics of two state folders.
q-bio.BM:Ion channels are proteins with a hole down the middle embedded in cell membranes. Membranes form insulating structures and the channels through them allow and control the movement of charged particles, spherical ions, mostly Na+, K+, Ca++, and Cl-. Membranes contain hundreds or thousands of types of channels, fluctuating between open conducting, and closed insulating states. Channels control an enormous range of biological function by opening and closing in response to specific stimuli using mechanisms that are not yet understood in physical language. Open channels conduct current of charged particles following laws of Brownian movement of charged spheres rather like the laws of electrodiffusion of quasi-particles in semiconductors. Open channels select between similar ions using a combination of electrostatic and 'crowded charge' (Lennard-Jones) forces. The specific location of atoms and the exact atomic structure of the channel protein seems much less important than certain properties of the structure, namely the volume accessible to ions and the effective density of fixed and polarization charge. There is no sign of other chemical effects like delocalization of electron orbitals between ions and the channel protein. Channels play a role in biology as important as transistors in computers, and they use rather similar physics to perform part of that role. Understanding their fluctuations awaits physical insight into the source of the variance and mathematical analysis of the coupling of the fluctuations to the other components and forces of the system.
q-bio.BM:We propose a criterion for optimal parameter selection in coarse-grained models of proteins, and develop a refined elastic network model (ENM) of bovine trypsinogen. The unimodal density-of-states distribution of the trypsinogen ENM disagrees with the bimodal distribution obtained from an all-atom model; however, the bimodal distribution is recovered by strengthening interactions between atoms that are backbone neighbors. We use the backbone-enhanced model to analyze allosteric mechanisms of trypsinogen, and find relatively strong communication between the regulatory and active sites.
q-bio.BM:The protonation of N2 bound to the active center of nitrogenase has been investigated using state-of-the-art DFT calculations. Dinitrogen in the bridging mode is activated by forming two bonds to Fe sites, which results in a reduction of the energy for the first hydrogen transfer by 123 kJ/mol. The axial binding mode with open sulfur bridge is less reactive by 30 kJ/mol and the energetic ordering of the axial and bridged binding mode is reversed in favor of the bridging dinitrogen during the first protonation. Protonation of the central ligand is thermodynamically favorable but kinetically hindered. If the central ligand is protonated, the proton is transferred to dinitrogen following the second protonation. Protonation of dinitrogen at the Mo site does not lead to low-energy intermediates.
q-bio.BM:The ejection of DNA from a bacterial virus (``phage'') into its host cell is a biologically important example of the translocation of a macromolecular chain along its length through a membrane. The simplest mechanism for this motion is diffusion, but in the case of phage ejection a significant driving force derives from the high degree of stress to which the DNA is subjected in the viral capsid. The translocation is further sped up by the ratcheting and entropic forces associated with proteins that bind to the viral DNA in the host cell cytoplasm. We formulate a generalized diffusion equation that includes these various pushing and pulling effects and make estimates of the corresponding speed-ups in the overall translocation process. Stress in the capsid is the dominant factor throughout early ejection, with the pull due to binding particles taking over at later stages. Confinement effects are also investigated, in the case where the phage injects its DNA into a volume comparable to the capsid size. Our results suggest a series of in vitro experiments involving the ejection of DNA into vesicles filled with varying amounts of binding proteins from phage whose state of stress is controlled by ambient salt conditions or by tuning genome length.
q-bio.BM:We present a base-pairing model of oligonuleotide duplex formation and show in detail its equivalence to the Nearest-Neighbour dimer methods from fits to free energy of duplex formation data for short DNA-DNA and DNA-RNA hybrids containing only Watson Crick pairs. In this approach the connection between rank-deficient polymer and rank-determinant oligonucleotide parameter, sets for DNA duplexes is transparent. The method is generalised to include RNA/DNA hybrids where the rank-deficient model with 11 dimer parameters in fact provides marginally improved predictions relative to the standard method with 16 independent dimer parameters ($\Delta G$ mean errors of 4.5 and 5.4 % respectively).
q-bio.BM:A formalism is developed which allows to determine the locations of all local symmetry axes of three-dimensional particles with overall icosahedral symmetry. It relies on the fact that the root system of the non-crystallographic Coxeter group H_3 encodes the locations of the planes of reflection that generate the discrete rotational symmetries of the particles. Via an appropriate extension of the root system, new planes of reflection are introduced which determine local axes of rotational symmetry. An easy-to-implement formalism is derived that allows to compute the surface structure of any three-dimensional icosahedral particle with local symmetries. It can be used also for particles with overall octahedral and tetrahedral symmetry in conjunction with the root systems of the corresponding reflection groups.   Applications to viruses are discussed explicitly. It is shown that the concept of quasi-equivalence in Caspar-Klug Theory corresponds to the special case of local six-fold symmetry axes contained in the theory developed here, and the corresponding geometries can hence be obtained with this formalism based on the root system of H_3.   Moreover, as a by-product, the theory answers the long-standing open question why only certain types of capsomeres, i.e. clusters of protein subunits, are observed in the surface structures of viruses. Since the types of the capsomeres are determined by the orders of the local symmetry axes on which they are located, the possible types of capsomeres are restricted by the spectrum of local symmetry axes allowed by the theory. Based on this we determine the spectrum of all capsomere types that may occur in viral capsids and give explicit examples for the lower-order cases.
q-bio.BM:The tethered-particle method is a single-molecule technique that has been used to explore the dynamics of a variety of macromolecules of biological interest. We give a theoretical analysis of the particle motions in such experiments. Our analysis reveals that the proximity of the tethered bead to a nearby surface (the microscope slide) gives rise to a volume-exclusion effect, resulting in an entropic force on the molecule. This force stretches the molecule, changing its statistical properties. In particular, the proximity of bead and surface brings about intriguing scaling relations between key observables (statistical moments of the bead) and parameters such as the bead size and contour length of the molecule. We present both approximate analytic solutions and numerical results for these effects in both flexible and semiflexible tethers. Finally, our results give a precise, experimentally-testable prediction for the probability distribution of the distance between the polymer attachment point and the center of the mobile bead.
q-bio.BM:The distribution of inequivalent geometries occurring during self-assembly of the major capsid protein in thermodynamic equilibrium is determined based on a master equation approach. These results are implemented to characterize the assembly of SV40 virus and to obtain information on the putative pathways controlling the progressive build-up of the SV40 capsid. The experimental testability of the predictions is assessed and an analysis of the geometries of the assembly intermediates on the dominant pathways is used to identify targets for antiviral drug design.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Assembly models are developed for viral capsids built from protein building blocks that can assume different local bonding structures in the capsid. This situation occurs, for example, for viruses in the family of Papovaviridae, which are linked to cancer and are hence of particular interest for the health sector. More specifically, the viral capsids of the (pseudo-) T=7 particles in this family consist of pentamers that exhibit two different types of bonding structures. While this scenario cannot be described mathematically in terms of Caspar-Klug Theory (Caspar and Klug 1962), it can be modelled via tiling theory (Twarock 2004). The latter is used to encode the local bonding environment of the building blocks in a combinatorial structure, called the assembly tree, which is a basic ingredient in the derivation of assembly models for Papovaviridae along the lines of the equilibrium approach of Zlotnick (Zlotnick 1994). A phase space formalism is introduced to characterize the changes in the assembly pathways and intermediates triggered by the variations in the association energies characterizing the bonds between the building blocks in the capsid. Furthermore, the assembly pathways and concentrations of the statistically dominant assembly intermediates are determined. The example of Simian Virus 40 is discussed in detail.
q-bio.BM:We calculate the equation of state of DNA under tension for the case that the DNA features loops. Such loops occur transiently during DNA condensation in the presence of multivalent ions or sliding cationic protein linkers. The force-extension relation of such looped DNA modelled as a wormlike chain is calculated via path integration in the semiclassical limit. This allows us to determine rigorously the high stretching asymptotics. Notably the functional form of the force-extension curve resembles that of straight DNA, yet with a strongly renormalized apparent persistence length. That means that the experimentally extracted single molecule elasticity does not necessarily reflect the bare DNA stiffness only, but can also contain additional contributions that depend on the overall chain conformation and length.
q-bio.BM:A generalized computational method for folding proteins with a fully transferable potential and geometrically realistic all-atom model is presented and tested on seven different helix bundle proteins. The protocol, which includes graph-theoretical analysis of the ensemble of resulting folded conformations, was systematically applied and consistently produced structure predictions of approximately 3 Angstroms without any knowledge of the native state. To measure and understand the significance of the results, extensive control simulations were conducted. Graph theoretic analysis provides a means for systematically identifying the native fold and provides physical insight, conceptually linking the results to modern theoretical views of protein folding. In addition to presenting a method for prediction of structure and folding mechanism, our model suggests that a accurate all-atom amino acid representation coupled with a physically reasonable atomic interaction potential (that does not require optimization to the test set) and hydrogen bonding are essential features for a realistic protein model.
q-bio.BM:Being the HIV-1 Protease (HIV-1-PR) an essential enzyme in the viral life cycle, its inhibition can control AIDS. The folding of single domain proteins, like each of the monomers forming the HIV-1-PR homodimer, is controlled by local elementary structures (LES, folding units stabilized by strongly interacting, highly conserved, as a rule hydrophobic, amino acids). These LES have evolved over myriad of generations to recognize and strongly attract each other, so as to make the protein fold fast and be stable in its native conformation. Consequently, peptides displaying a sequence identical to those segments of the monomers associated with LES are expected to act as competitive inhibitors and thus destabilize the native structure of the enzyme. These inhibitors are unlikely to lead to escape mutants as they bind to the protease monomers through highly conserved amino acids which play an essential role in the folding process. The properties of one of the most promising inhibitors of the folding of the HIV-1-PR monomers found among these peptides is demonstrated with the help of spectrophotometric assays and CD spectroscopy.
q-bio.BM:It is shown that a small subset of modes which are likely to be involved in protein functional motions of large amplitude can be determined by retaining the most robust normal modes obtained using different protein models. This result should prove helpful in the context of several applications proposed recently, like for solving difficult molecular replacement problems or for fitting atomic structures into low-resolution electron density maps. Moreover, it may also pave the way for the development of methods allowing to predict such motions accurately.
q-bio.BM:A new formalism for calculation of the partition function of single stranded nucleic acids is presented. Secondary structures and the topology of structure elements are the level of resolution that is used. The folding model deals with matches, mismatches, symmetric and asymmetric interior loops, stacked pairs in loop and dangling end regions, multi-branched loops, bulges and single base stacking that might exist at duplex ends or at the ends of helices. Calculations on short and long sequences show, that for short oligonucleotides, a duplex formation often displays a two-state transition. However, for longer oligonucleotides, the thermodynamic properties of the single self-folding transition affects the transition nature of the duplex formation, resulting in a population of intermediate hairpin species in the solution. The role of intermediate hairpin species is analyzed in the case when a short oligonucleotides (molecular beacons) have to reliably identify and hybridize to accessible nucleotides within their targeted mRNA sequences. It is shown that the enhanced specificity of the molecular beacons is a result of their constrained conformational flexibility and the all-or-none mechanism of their hybridization to the target sequence.
q-bio.BM:A vital constituent of a virus is its protein shell, called the viral capsid, that encapsulates and hence provides protection for the viral genome. Viral capsids are usually spherical, and for a significant number of viruses exhibit overall icosahedral symmetry. The corresponding surface lattices, that encode the locations of the capsid proteins and intersubunit bonds, can be modelled by Viral Tiling Theory.   It has been shown in vitro that under a variation of the experimental boundary conditions, such as the pH value and salt concentration, tubular particles may appear instead of, or in addition to, spherical ones. In order to develop models that describe the simultaneous assembly of both spherical and tubular variants, and hence study the possibility of triggering tubular malformations as a means of interference with the replication mechanism, Viral Tiling Theory has to be extended to include tubular lattices with end caps. This is done here for the case of Papovaviridae, which play a distinguished role from the viral structural point of view as they correspond to all pentamer lattices, i.e. lattices formed from clusters of five protein subunits throughout. These results pave the way for a generalisation of recently developed assembly models.
q-bio.BM:Experimental investigations of the biosynthesis of a number of proteins have pointed out that part of the native structure can be acquired already during translation. We carried out a comprehensive statistical analysis of some average structural properties of proteins that have been put forward as possible signatures of this progressive buildup process. Contrary to a widespread belief, it is found that there is no major propensity of the amino acids to form contacts with residues that are closer to the N terminus. Moreover, it is found that the C terminus is significantly more compact and locally-organized than the N one. Also this bias, though, is unlikely to be related to vectorial effects, since it correlates with subtle differences in the primary sequence. These findings indicate that even if proteins aquire their structure vectorially no signature of this seems to be detectable in their average structural properties.
q-bio.BM:The aim of this article is to present a developed method that decomposes the autofluorescence spectrum into the spectra of naturally occurring biochemical components of biotissue. It requires knowledge of detailed spectrum behaviour of different endogenous fluorophores. We have studied the main bio-markers in human tissue and proposed a simple modelling algorithm for their spectra shapes. The empirical method was tested theoretically by quantum-mechanical calculations of the spectra in the unharmonic Morse potential approach.
q-bio.BM:Experimental evidence suggests that the folding and aggregation of the amyloid $\beta$-protein (A$\beta$) into oligomers is a key pathogenetic event in Alzheimer's disease (AD). Inhibiting the pathologic folding and oligomerization of A$\beta$ could be effective in the prevention and treatment of AD. Here, using all-atom molecular dynamics simulations in explicit solvent, we probe the initial stages of folding of a decapeptide segment of A$\beta$, A$\beta_{21-30}$, shown experimentally to nucleate the folding process. In addition, we examine the folding of a homologous decapeptide containing an amino acid substitution linked to hereditary cerebral hemorrhage with amyloidosis--Dutch type, [Gln22]A$\beta_{21-30}$. We find that: (i) when the decapeptide is in water, hydrophobic interactions and transient salt bridges between Lys28 and either Glu22 or Asp23 are important in the formation of a loop in the Val24--Lys28 region of the wild type decapeptide; (ii) in the presence of salt ions, salt bridges play a more prominent role in the stabilization of the loop; (iii) in water with a reduced density, the decapeptide forms a helix, indicating the sensitivity of folding to different aqueous environments; (iv) the ``Dutch'' peptide in water, in contrast to the wild type peptide, fails to form a long-lived Val24--Lys28 loop, suggesting that loop stability is a critical factor in determining whether A$\beta$ folds into pathologic structures. Our results are relevant to understand the mechanism of A$\beta$ peptide folding in different environments, such as intra- and extracellular milieus or cell membranes, and how amino acid substitutions linked to familial forms of amyloidosis cause disease.
q-bio.BM:This paper was withdrawn by the authors.
q-bio.BM:We develop a class of models with which we simulate the assembly of particles into T1 capsid-like objects using Newtonian dynamics. By simulating assembly for many different values of system parameters, we vary the forces that drive assembly. For some ranges of parameters, assembly is facile, while for others, assembly is dynamically frustrated by kinetic traps corresponding to malformed or incompletely formed capsids. Our simulations sample many independent trajectories at various capsomer concentrations, allowing for statistically meaningful conclusions. Depending on subunit (i.e., capsomer) geometries, successful assembly proceeds by several mechanisms involving binding of intermediates of various sizes. We discuss the relationship between these mechanisms and experimental evaluations of capsid assembly processes.
q-bio.BM:In this paper the heat transport in microtubules (MT) is investigated. When the dimension of the structure is of the order of the de Broglie wave length the transport phenomena must be analyzed within quantum mechanics. In this paper we developed the Dirac type thermal equation for MT .The solution of the equation-the temperature fields for electrons can be wave type or diffusion type depending on the dynamics of the scattering. Key words: Microtubules ultrashort laser pulses, Dirac thermal equation, temperature fields.
q-bio.BM:Mouse prion protein PrP106-126 is a peptide corresponding to the residues 107-127 of human prion protein. It has been shown that PrP106-126 can reproduce the main neuropathological features of prionrelated transmissible spongiform encephalopathies and can form amyloid-like fibrils in vitro. The conformational characteristics of PrP106-126 fibril have been investigated by electron microscopy, CD spectroscopy, NMR and molecular dynamics simulations. Recent researches have found out that PrP106-126 in water assumes a stable structure consisting of two parallel beta-sheets that are tightly packed against each other. In this work we perform molecular dynamics simulation to reveal the elongation mechanism of PrP106-126 fibril. Influenced by the edge strands of the fibril which already adopt beta-sheets conformation, single PrP106-126 peptide forms beta-structure and becomes a new element of the fibril. Under acidic condition, single PrP106-126 peptide adopts a much larger variety of conformations than it does under neural condition, which makes a peptide easier to be influenced by the edge strands of the fibril. However, acidic condition dose not largely affect the stability of PrP106-126 peptide fibril. Thus, the speed of fibril elongation can be dramatically increased by lowering the pH value of the solution. The pH value was adjusted by either changing the protonation state of the residues or adding hydronium ions (acidic solution) or hydroxyl ions (alkaline solution). The differences between these two approaches are analyzed here.
q-bio.BM:The thermodynamics of the small SH3 protein domain is studied by means of a simplified model where each bead-like amino acid interacts with the others through a contact potential controlled by a 20x20 random matrix. Good folding sequences, characterized by a low native energy, display three main thermodynamical phases, namely a coil-like phase, an unfolded globule and a folded phase (plus other two phases, namely frozen and random coil, populated only at extremes temperatures). Interestingly, the unfolded globule has some regions already structured. Poorly designed sequences, on the other hand, display a wide transition from the random coil to a frozen state. The comparison with the analytic theory of heteropolymers is discussed.
q-bio.BM:In a seminal paper Caspar and Klug established a theory that provides a family of polyhedra as blueprints for the structural organisation of viral capsids. In particular, they encode the locations of the proteins in the shells that encapsulate, and hence provide protection for, the viral genome. Despite of its huge success and numerous applications in virology experimental results have provided evidence for the fact that the theory is too restrictive to describe all known viruses. Especially, the family of Papovaviridae, which contains cancer-causing viruses, falls out of the scope of this theory.   In a recent paper we have shown that certain members of the family of Papovaviridae can be described via tilings. In this paper, we develop a comprehensive mathematical framework for the derivation of all surface structures of viral particles in this family. We show that this formalism fixes the structure and relative sizes of all particles collectively so that there exists only one scaling factor that relates the sizes of all particles with their biological counterparts.   The series of polyhedra derived here complements the Caspar-Klug family of polyhedra. It is the first mathematical result that provides a common organisational principle for different types of viral particles in the family of Papovaviridae and paves the way for an understanding of Papovaviridae polymorphism. Moreover, it provides crucial input for the construction of assembly models.
q-bio.BM:The amino acid sequences of proteins provide rich information for inferring distant phylogenetic relationships and for predicting protein functions. Estimating the rate matrix of residue substitutions from amino acid sequences is also important because the rate matrix can be used to develop scoring matrices for sequence alignment. Here we use a continuous time Markov process to model the substitution rates of residues and develop a Bayesian Markov chain Monte Carlo method for rate estimation. We validate our method using simulated artificial protein sequences. Because different local regions such as binding surfaces and the protein interior core experience different selection pressures due to functional or stability constraints, we use our method to estimate the substitution rates of local regions. Our results show that the substitution rates are very different for residues in the buried core and residues on the solvent exposed surfaces. In addition, the rest of the proteins on the binding surfaces also have very different substitution rates from residues. Based on these findings, we further develop a method for protein function prediction by surface matching using scoring matrices derived from estimated substitution rates for residues located on the binding surfaces. We show with examples that our method is effective in identifying functionally related proteins that have overall low sequence identity, a task known to be very challenging.
q-bio.BM:This chapter discusses geometric models of biomolecules and geometric constructs, including the union of ball model, the weigthed Voronoi diagram, the weighted Delaunay triangulation, and the alpha shapes. These geometric constructs enable fast and analytical computaton of shapes of biomoleculres (including features such as voids and pockets) and metric properties (such as area and volume). The algorithms of Delaunay triangulation, computation of voids and pockets, as well volume/area computation are also described. In addition, applications in packing analysis of protein structures and protein function prediction are also discussed.
q-bio.BM:This chapter discusses theoretical framework and methods for developing knowledge-based potential functions essential for protein structure prediction, protein-protein interaction, and protein sequence design. We discuss in some details about the Miyazawa-Jernigan contact statistical potential, distance-dependent statistical potentials, as well as geometric statistical potentials. We also describe a geometric model for developing both linear and non-linear potential functions by optimization. Applications of knowledge-based potential functions in protein-decoy discrimination, in protein-protein interactions, and in protein design are then described. Several issues of knowledge-based potential functions are finally discussed.
q-bio.BM:$\beta$-barrel membrane proteins are found in the outer membrane of gram-negative bacteria, mitochondria, and chloroplasts. We have developed probabilistic models to quantify propensities of residues for different spatial locations and for interstrand pairwise contact interactions involving strong H-bonds, side-chain interactions, and weak H-bonds. The propensity values and p-values measuring statistical significance are calculated exactly by analytical formulae we have developed. Contrary to the ``positive-inside'' rule for helical membrane proteins, $\beta$-barrel membrane proteins follow a significant albeit weaker ``positive-outside'' rule, in that the basic residues Arg and Lys are disproportionately favored in the extracellular cap region and disfavored in the periplasmic cap region. Different residue pairs prefer strong backbone H-bonded interstrand pairings (e.g. Gly-Aromatic) or non-H-bonded pairings (e.g. Aromatic-Aromatic). In addition, Tyr and Phe participate in aromatic rescue by shielding Gly from polar environments. These propensities can be used to predict the registration of strand pairs, an important task for the structure prediction of $\beta$-barrel membrane proteins. Our accuracy of 44% is considerably better than random (7%) and other studies. Our results imply several experiments that can help to elucidate the mechanisms of in vitro and in vivo folding of $\beta$-barrel membrane proteins. See supplementary material after the bibliography for detailed techniques.
q-bio.BM:An effective potential function is critical for protein structure prediction and folding simulation. Simplified protein models such as those requiring only $C_\alpha$ or backbone atoms are attractive because they enable efficient search of the conformational space. We show residue specific reduced discrete state models can represent the backbone conformations of proteins with small RMSD values. However, no potential functions exist that are designed for such simplified protein models. In this study, we develop optimal potential functions by combining contact interaction descriptors and local sequence-structure descriptors. The form of the potential function is a weighted linear sum of all descriptors, and the optimal weight coefficients are obtained through optimization using both native and decoy structures. The performance of the potential function in test of discriminating native protein structures from decoys is evaluated using several benchmark decoy sets. Our potential function requiring only backbone atoms or $C_\alpha$ atoms have comparable or better performance than several residue-based potential functions that require additional coordinates of side chain centers or coordinates of all side chain atoms. By reducing the residue alphabets down to size 5 for local structure-sequence relationship, the performance of the potential function can be further improved. Our results also suggest that local sequence-structure correlation may play important role in reducing the entropic cost of protein folding.
q-bio.BM:Without invoking the Markov approximation, we derive formulas for vibrational energy relaxation (VER) and dephasing for an anharmonic system oscillator using a time-dependent perturbation theory. The system-bath Hamiltonian contains more than the third order coupling terms since we take a normal mode picture as a zeroth order approximation. When we invoke the Markov approximation, our theory reduces to the Maradudin-Fein formula which is used to describe VER properties of glass and proteins. When the system anharmonicity and the renormalization effect due to the environment vanishes, our formulas reduce to those derived by Mikami and Okazaki invoking the path-integral influence functional method [J. Chem. Phys. 121 (2004) 10052]. We apply our formulas to VER of the amide I mode of a small amino-acide like molecule, N-methylacetamide, in heavy water.
q-bio.BM:Simplified Go models, where only native contacts interact favorably, have proven useful to characterize some aspects of the folding of small proteins. The success of these models is limited by the fact that all residues interact in the same way, so that the folding features of a protein are determined only by the geometry of its native conformation. We present an extended version of a C-alpha based Go model where different residues interact with different energies. The model is used to calculate the thermodynamics of three small proteins (Protein G, SrcSH3 and CI2) and the effect of mutations on the wildtype sequence. The model allows to investigate some of the most controversial areas in protein folding such as its earliest stages, a subject which has lately received particular attention. The picture which emerges for the three proteins under study is that of a hierarchical process, where local elementary structures (LES) (not necessarily coincident with elements of secondary structure) are formed at the early stages of the folding and drive the protein, through the transition state and the postcritical folding nucleus (FN), resulting from the docking of the LES, to the native conformation.
q-bio.BM:The chiral nature of DNA plays a crucial role in cellular processes. Here we use magnetic tweezers to explore one of the signatures of this chirality, the coupling between stretch and twist deformations. We show that the extension of a stretched DNA molecule increases linearly by 0.42 nm per excess turn applied to the double helix. This result contradicts the intuition that DNA should lengthen as it is unwound and get shorter with overwinding. We then present numerical results of energy minimizations of torsionally restrained DNA that display a behaviour similar to the experimental data and shed light on the molecular details of this surprising effect.
q-bio.BM:PCR (Polymerase Chain Reaction), a method which replicates a selected sequence of DNA, has revolutionized the study of genomic material, but mathematical study of the process has been limited to simple deterministic models or descriptions relying on stochastic processes. In this paper we develop a suite of deterministic models for the reactions of quantitative PCR (Polymerase Chain Reaction) based on the law of mass action. Maps are created from DNA copy number in one cycle to the next, with ordinary differential equations describing the evolution of difference molecular species during each cycle. Qualitative analysis is preformed at each stage and parameters are estimated by fitting each model to data from Roche LightCycler (TM) runs.
q-bio.BM:Kinetics of folding of a protein held in a force-clamp are compared to an unconstrained folding. The comparison is made within a simple topology-based dynamical model of ubiquitin. We demonstrate that the experimentally observed variations in the end-to-end distance reflect microscopic events during folding. However, the folding scenarios in and out of the force-clamp are distinct.
q-bio.BM:In the template-assistance model, normal prion protein (PrPC), the pathogenic cause of prion diseases such as Creutzfeldt-Jakob (CJD) in human, Bovine Spongiform Encephalopathy (BSE) in cow, and scrapie in sheep, converts to infectious prion (PrPSc) through an autocatalytic process triggered by a transient interaction between PrPC and PrPSc. Conventional studies suggest the S1-H1-S2 region in PrPC to be the template of S1-S2 $\beta$-sheet in PrPSc, and the conformational conversion of PrPC into PrPSc may involve an unfolding of H1 in PrPC and its refolding into the $\beta$-sheet in PrPSc. Here we conduct a series of simulation experiments to test the idea of transient interaction of the template-assistance model. We find that the integrity of H1 in PrPC is vulnerable to a transient interaction that alters the native dihedral angles at residue Asn$^{143}$, which connects the S1 flank to H1, but not to interactions that alter the internal structure of the S1 flank, nor to those that alter the relative orientation between H1 and the S2 flank.
q-bio.BM:The Yakushevich model of DNA torsion dynamics supports soliton solutions, which are supposed to be of special interest for DNA transcription. In the discussion of the model, one usually adopts the approximation $\ell_0 \to 0$, where $\ell_0$ is a parameter related to the equilibrium distance between bases in a Watson-Crick pair. Here we analyze the Yakushevich model without $\ell_0 \to 0$. The model still supports soliton solutions indexed by two winding numbers $(n,m)$; we discuss in detail the fundamental solitons, corresponding to winding numbers (1,0) and (0,1) respectively.
q-bio.BM:The Yakushevich (Y) model provides a very simple pictures of DNA torsion dynamics, yet yields remarkably correct predictions on certain physical characteristics of the dynamics. In the standard Y model, the interaction between bases of a pair is modelled by a harmonic potential, which becomes anharmonic when described in terms of the rotation angles; here we substitute to this different types of improved potentials, providing a more physical description of the H-bond mediated interactions between the bases. We focus in particular on soliton solutions; the Y model predicts the correct size of the nonlinear excitations supposed to model the ``transcription bubbles'', and this is essentially unchanged with the improved potential. Other features of soliton dynamics, in particular curvature of soliton field configurations and the Peierls-Nabarro barrier, are instead significantly changed.
q-bio.BM:Simple coarse-grained models, such as the Gaussian Network Model, have been shown to capture some of the features of equilibrium protein dynamics. We extend this model by using atomic contacts to define residue interactions and introducing more than one interaction parameter between residues. We use B-factors from 98 ultra-high resolution X-ray crystal structures to optimize the interaction parameters. The average correlation between GNM fluctuation predictions and the B-factors is 0.64 for the data set, consistent with a previous large-scale study. By separating residue interactions into covalent and noncovalent, we achieve an average correlation of 0.74, and addition of ligands and cofactors further improves the correlation to 0.75. However, further separating the noncovalent interactions into nonpolar, polar, and mixed yields no significant improvement. The addition of simple chemical information results in better prediction quality without increasing the size of the coarse-grained model.
q-bio.BM:Background: One-dimensional protein structures such as secondary structures or contact numbers are useful for three-dimensional structure prediction and helpful for intuitive understanding of the sequence-structure relationship. Accurate prediction methods will serve as a basis for these and other purposes. Results: We implemented a program CRNPRED which predicts secondary structures, contact numbers and residue-wise contact orders. This program is based on a novel machine learning scheme called critical random networks. Unlike most conventional one-dimensional structure prediction methods which are based on local windows of an amino acid sequence, CRNPRED takes into account the whole sequence. CRNPRED achieves, on average per chain, Q3 = 81% for secondary structure prediction, and correlation coefficients of 0.75 and 0.61 for contact number and residue-wise contact order predictions, respectively. Conclusion: CRNPRED will be a useful tool for computational as well as experimental biologists who need accurate one-dimensional protein structure predictions.
q-bio.BM:To confer high specificity and affinity in binding, contacts at interfaces between two interacting macromolecules are expected to exhibit pair preferences for types of atoms or residues. Here we quantify these preferences by measuring the mutual information of contacts for 895 protein-protein interfaces. The information content is significant and is highest at the atomic resolution. A simple phenomenological theory reveals a connection between information at interfaces and the free energy spectrum of association. The connection is presented in the form of a relation between mutual information and the energy gap of the native bound state to off-target bound states. Measurement of information content in designed lattice interfaces show the predicted scaling behavior to the energy gap. Our theory also suggests that mutual information in contacts emerges by a selection mechanism, and that strong selection, or high conservation, of residues should lead to correspondingly high mutual information. Amino acids which contribute more heavily to information content are then expected to be more conserved. We verify this by showing a statistically significant correlation between the conservation of each of the twenty amino acids and their individual contribution to the information content at protein-protein interfaces
q-bio.BM:It was first suggested by Englander et al to model the nonlinear dynamics of DNA relevant to the transcription process in terms of a chain of coupled pendulums. In a related paper [q-bio.BM/0604014] we argued for the advantages of an extension of this approach based on considering a chain of double pendulums with certain characteristics. Here we study a simplified model of this kind, focusing on its general features and nonlinear travelling wave excitations; in particular, we show that some of the degrees of freedom are actually slaved to others, allowing for an effective reduction of the relevant equations.
q-bio.BM:The Fast Fourier Transform (FFT) correlation approach to protein-protein docking can evaluate the energies of billions of docked conformations on a grid if the energy is described in the form of a correlation function. Here, this restriction is removed, and the approach is efficiently used with pairwise interactions potentials that substantially improve the docking results. The basic idea is approximating the interaction matrix by its eigenvectors corresponding to the few dominant eigenvalues, resulting in an energy expression written as the sum of a few correlation functions, and solving the problem by repeated FFT calculations. In addition to describing how the method is implemented, we present a novel class of structure based pairwise intermolecular potentials. The DARS (Decoys As the Reference State) potentials are extracted from structures of protein-protein complexes and use large sets of docked conformations as decoys to derive atom pair distributions in the reference state. The current version of the DARS potential works well for enzyme-inhibitor complexes. With the new FFT-based program, DARS provides much better docking results than the earlier approaches, in many cases generating 50\% more near-native docked conformations. Although the potential is far from optimal for antibody-antigen pairs, the results are still slightly better than those given by an earlier FFT method. The docking program PIPER is freely available for non-commercial applications.
q-bio.BM:We investigated the structural relaxation of myosin motor domain from the pre-power stroke state to the near-rigor state using molecular dynamics simulation of a coarse-grained protein model. To describe the structural change, we propose a "dual Go-model," a variant of the Go-like model that has two reference structures. The nucleotide dissociation process is also studied by introducing a coarse-grained nucleotide in the simulation. We found that the myosin structural relaxation toward the near-rigor conformation cannot be completed before the nucleotide dissociation. Moreover, the relaxation and the dissociation occurred cooperatively when the nucleotide was tightly bound to the myosin head. The result suggested that the primary role of the nucleotide is to suppress the structural relaxation.
q-bio.BM:Self-similar properties of the ribosome in terms of the mass fractal dimension are investigated. We find that both the 30S subunit and the 16S rRNA have fractal dimensions of 2.58 and 2.82, respectively; while the 50S subunit as well as the 23S rRNA has the mass fractal dimension close to 3, implying a compact three dimensional macromolecule. This finding supports the dynamic and active role of the 30S subunit in the protein synthesis, in contrast to the pass role of the 50S subunit.
q-bio.BM:We present an extremely simplified model of multiple-domains polymer stretching in an atomic force microscopy experiment. We portray each module as a binary set of contacts and decompose the system energy into a harmonic term (the cantilever) and long-range interactions terms inside each domain. Exact equilibrium computations and Monte Carlo simulations qualitatively reproduce the experimental saw-tooth pattern of force-extension profiles, corresponding (in our model) to first-order phase transitions. We study the influence of the coupling induced by the cantilever and the pulling speed on the relative heights of the force peaks. The results suggest that the increasing height of the critical force for subsequent unfolding events is an out-of-equilibrium effect due to a finite pulling speed. The dependence of the average unfolding force on the pulling speed is shown to reproduce the experimental logarithmic law.
q-bio.BM:Phi-values are experimental measures of the effects of mutations on the folding kinetics of a protein. A central question is which structural information Phi-values contain about the transition state of folding. Traditionally, a Phi-value is interpreted as the 'nativeness' of a mutated residue in the transition state. However, this interpretation is often problematic because it assumes a linear relation between the nativeness of the residue and its free-energy contribution. We present here a better structural interpretation of Phi-values for mutations within a given helix. Our interpretation is based on a simple physical model that distinguishes between secondary and tertiary free-energy contributions of helical residues. From a linear fit of our model to the experimental data, we obtain two structural parameters: the extent of helix formation in the transition state, and the nativeness of tertiary interactions in the transition state. We apply our model to all proteins with well-characterized helices for which more than 10 Phi-values are available: protein A, CI2, and protein L. The model captures nonclassical Phi-values <0 or >1 in these helices, and explains how different mutations at a given site can lead to different Phi-values.
q-bio.BM:A model for the unidirectional movement of dynein is presented based on structural observations and biochemical experimental results available. In this model, the binding affinity of dynein for microtubule is independent of its nucleotide state and the change between strong and weak microtubule-binding is determined naturally by the variation of relative orientation between the stalk and microtubule as the stalk rotates following nucleotide-state transition. Thus the enigmatic communication from the ATP binding site in the globular domain to the far MT-binding site in the tip of the stalk, which is prerequisite in conventional models, is not required. Using the present model, the previous experimental results such as the effect of ATP and ADP bindings on dissociation of dynein from microtubule, the processive movement of single-headed axonemal dyneins at saturating ATP concentration, the load dependence of step size for the processive movement of two-headed cytoplasmic dyneins and the dependence of stall force on ATP concentration can be well explained.
q-bio.BM:Over the last 10-15 years a general understanding of the chemical reaction of protein folding has emerged from statistical mechanics. The lessons learned from protein folding kinetics based on energy landscape ideas have benefited protein structure prediction, in particular the development of coarse grained models. We survey results from blind structure prediction. We explore how second generation prediction energy functions can be developed by introducing information from an ensemble of previously simulated structures. This procedure relies on the assumption of a funnelled energy landscape keeping with the principle of minimal frustration. First generation simulated structures provide an improved input for associative memory energy functions in comparison to the experimental protein structures chosen on the basis of sequence alignment.
q-bio.BM:The precise details of how myosin-V coordinates the biochemical reactions and mechanical motions of its two head elements to engineer effective processive molecular motion along actin filaments remain unresolved. We compare a quantitative kinetic model of the myosin-V walk, consisting of five basic states augmented by two further states to allow for futile hydrolysis and detachments, with experimental results for run lengths, velocities, and dwell times and their dependence on bulk nucleotide concentrations and external loads in both directions. The model reveals how myosin-V can use the internal strain in the molecule to synchronise the motion of the head elements. Estimates for the rate constants in the reaction cycle and the internal strain energy are obtained by a computational comparison scheme involving an extensive exploration of the large parameter space. This scheme exploits the fact that we have obtained analytic results for our reaction network, e.g. for the velocity but also the run length, diffusion constant and fraction of backward steps. The agreement with experiment is often reasonable but some open problems are highlighted, in particular the inability of such a general model to reproduce the reported dependence of run length on ADP. The novel way that our approach explores parameter space means that any confirmed discrepancies should give new insights into the reaction network model.
q-bio.BM:The prion protein (PrP) binds Cu2+ ions in the octarepeat domain of the N-terminal tail up to full occupancy at pH=7.4. Recent experiments show that the HGGG octarepeat subdomain is responsible for holding the metal bound in a square planar coordination. By using first principle ab initio molecular dynamics simulations of the Car-Parrinello type, the Cu coordination mode to the binding sites of the PrP octarepeat region is investigated. Simulations are carried out for a number of structured binding sites. Results for the complexes Cu(HGGGW)+(wat), Cu(HGGG) and the 2[Cu(HGGG)] dimer are presented. While the presence of a Trp residue and a H2O molecule does not seem to affect the nature of the Cu coordination, high stability of the bond between Cu and the amide Nitrogens of deprotonated Gly's is confirmed in the case of the Cu(HGGG) system. For the more interesting 2[Cu(HGGG)] dimer a dynamically entangled arrangement of the two monomers, with intertwined N-Cu bonds, emerges. This observation is consistent with the highly packed structure seen in experiments at full Cu occupancy.
q-bio.BM:We formulate a simple solvation potential based on a coarsed-grain representation of amino acids with two spheres modeling the $C_\alpha$ atom and an effective side-chain centroid. The potential relies on a new method for estimating the buried area of residues, based on counting the effective number of burying neighbours in a suitable way. This latter quantity shows a good correlation with the buried area of residues computed from all atom crystallographic structures. We check the discriminatory power of the solvation potential alone to identify the native fold of a protein from a set of decoys and show the potential to be considerably selective.
q-bio.BM:The aim of this work is to elucidate how physical principles of protein design are reflected in natural sequences that evolved in response to the thermal conditions of the environment. Using an exactly solvable lattice model, we design sequences with selected thermal properties. Compositional analysis of designed model sequences and natural proteomes reveals a specific trend in amino acid compositions in response to the requirement of stability at elevated environmental temperature, i.e. the increase of fractions of hydrophobic and charged amino acid residues at the expense of polar ones. We show that this from both ends of hydrophobicity scale trend is due to positive (to stabilize the native state) and negative (to destabilize misfolded states) components of protein design. Negative design strengthens specific repulsive nonnative interactions that appear in misfolded structures. A pressure to preserve specific repulsive interactions in non-native conformations may result in correlated mutations between amino acids which are far apart in the native state but may be in contact in misfolded conformations. Such correlated mutations are indeed found in TIM barrel and other proteins.
q-bio.BM:F-actin bundles constitute principal components of a multitude of cytoskeletal processes including stereocilia, filopodia, microvilli, neurosensory bristles, cytoskeletal stress fibers, and the sperm acrosome. The bending, buckling, and stretching behaviors of these processes play key roles in cellular functions ranging from locomotion to mechanotransduction and fertilization. Despite their central importance to cellular function, F-actin bundle mechanics remain poorly understood. Here, we demonstrate that bundle bending stiffness is a state-dependent quantity with three distinct regimes that are mediated by bundle dimensions in addition to crosslink properties. We calculate the complete state-dependence of the bending stiffness and elucidate the mechanical origin of each. A generic set of design parameters delineating the regimes in state-space is derived and used to predict the bending stiffness of a variety of F-actin bundles found in cells. Finally, the broad and direct implications that the isolated state-dependence of F-actin bundle stiffness has on the interpretation of the bending, buckling, and stretching behavior of cytoskeletal bundles is addressed.
q-bio.BM:In this work we develop a theory of interaction of randomly patterned surfaces as a generic prototype model of protein-protein interactions. The theory predicts that pairs of randomly superimposed identical (homodimeric) random patterns have always twice as large magnitude of the energy fluctuations with respect to their mutual orientation, as compared with pairs of different (heterodimeric) random patterns. The amplitude of the energy fluctuations is proportional to the square of the average pattern density, to the square of the amplitude of the potential and its characteristic length, and scales linearly with the area of surfaces. The greater dispersion of interaction energies in the ensemble of homodimers implies that strongly attractive complexes of random surfaces are much more likely to be homodimers, rather than heterodimers. Our findings suggest a plausible physical reason for the anomalously high fraction of homodimers observed in real protein interaction networks.
q-bio.BM:We extend our previously developed general approach (1) to study a phenomenological model in which the simulated packing of hard, attractive spheres on a prolate spheroid surface with convexity constraints produces structures identical to those of prolate virus capsid structures. Our simulation approach combines the traditional Monte Carlo method with the method of random sampling on an ellipsoidal surface and a convex hull searching algorithm. Using this approach we study the assembly and structural origin of non-icosahedral, elongated virus capsids, such as two aberrant flock house virus (FHV) particles and the prolate prohead of bacteriophage phi29, and discuss the implication of our simulation results in the context of recent experimental findings.
q-bio.BM:The need to understand the assembly kinetics of fibril formation has become urgent because of the realization that soluble oligomers of amyloidogenic peptides may be even more neurotoxic than the end product, namely, the amyloid fibrils. In order to fully understand the routes to fibril formation one has to characterize the major species in the assembly pathways. The characterization of the energetics and dynamics of oligomers (dimers, trimers etc) is difficult using experiments alone because they undergo large conformational fluctuations. In this context, carefully planned molecular dynamics simulation studies, computations using coarse-grained models, and bioinformatic analysis have given considerable insights into the early events in the route to fibril formation. Here, we describe progress along this direction using examples taken largely from our own work. In this chapter, we focus on aspects of protein aggregation using Abeta-peptides and prion proteins as examples.
q-bio.BM:The native three dimensional structure of a single protein is determined by the physico chemical nature of its constituent amino acids. The twenty different types of amino acids, depending on their physico chemical properties, can be grouped into three major classes - hydrophobic, hydrophilic and charged. We have studied the anatomy of the weighted and unweighted networks of hydrophobic, hydrophilic and charged residues separately for a large number of proteins. Our results show that the average degree of the hydrophobic networks has significantly larger value than that of hydrophilic and charged networks. The average degree of the hydrophilic networks is slightly higher than that of charged networks. The average strength of the nodes of hydrophobic networks is nearly equal to that of the charged network; whereas that of hydrophilic networks has smaller value than that of hydrophobic and charged networks. The average strength for each of the three types of networks varies with its degree. The average strength of a node in charged networks increases more sharply than that of the hydrophobic and hydrophilic networks. Each of the three types of networks exhibits the 'small-world' property. Our results further indicate that the all amino acids' networks and hydrophobic networks are of assortative type. While maximum of the hydrophilic and charged networks are of assortative type, few others have the characteristics of disassortative mixing of the nodes. We have further observed that all amino acids' networks and hydrophobic networks bear the signature of hierarchy; whereas the hydrophilic and charged networks do not have any hierarchical signature.
q-bio.BM:We study statistical properties of interacting protein-like surfaces and predict two strong, related effects: (i) statistically enhanced self-attraction of proteins; (ii) statistically enhanced attraction of proteins with similar structures. The effects originate in the fact that the probability to find a pattern self-match between two identical, even randomly organized interacting protein surfaces is always higher compared with the probability for a pattern match between two different, promiscuous protein surfaces. This theoretical finding explains statistical prevalence of homodimers in protein-protein interaction networks reported earlier. Further, our findings are confirmed by the analysis of curated database of protein complexes that showed highly statistically significant overrepresentation of dimers formed by structurally similar proteins with highly divergent sequences (superfamily heterodimers). We predict that significant fraction of heterodimers evolved from homodimers with the negative design evolutionary pressure applied against promiscuous homodimer formation. This is achieved through the formation of highly specific contacts formed by charged residues as demonstrated both in model and real superfamily heterodimers
q-bio.BM:Stretching of a protein by a fluid flow is compared to that in a force-clamp apparatus. The comparison is made within a simple topology-based dynamical model of a protein in which the effects of the flow are implemented using Langevin dynamics. We demonstrate that unfolding induced by a uniform flow shows a richer behavior than that in the force clamp. The dynamics of unfolding is found to depend strongly on the selection of the amino acid, usually one of the termini, which is anchored. These features offer potentially wider diagnostic tools to investigate structure of proteins compared to experiments based on the atomic force microscopy.
q-bio.BM:Secretion and role of autotaxin and lysophosphatidic acid in adipose tissue In obesity, adipocyte hypertrophy is often associated with recrutement of new fat cells (adipogenesis) under the control of circulating and local regulatory factors. Among the different lipids released in the extracellular compartment of adipocytes, our group found the presence of lysophosphatidic acid (LPA). LPA is a bioactive phospholipid able to regulate several cell responses via the activation of specific G-protein coupled membrane receptors. Our group found that LPA increases preadipocyte proliferation and inhibits adipogenesis via the activation of LPA1 receptor subtype. Extracellular LPA-synthesis is catalyzed by a lysophospholipase D secreted by adipocytes : autotaxin (ATX). Adipocyte ATX expression strongly increases with adipogenesis as well as in individuals exhibiting type 2 diabetes associated with massive obesity. A possible contribution of ATX and LPA as paracrine regulators of adipogenesis and obesity associated diabetes is proposed.
q-bio.BM:A recently proposed model of non-autocatalytic reactions in dipeptide reactions leading to spontaneous symmetry breaking and homochirality is examined. The model is governed by activation, polymerization, epimerization and depolymerization of amino acids. Symmetry breaking is primarily a consequence of the fact that the rates of reactions involving homodimers and heterodimers are different, i.e., stereoselective, and on the fact that epimerization can only occur on the N-terminal residue and not on the Cterminal residue. This corresponds to an auto-inductive cyclic process that works only in one sense. It is argued that epimerization mimics both autocatalytic behavior as well as mutual antagonism - both of which were known to be crucial for producing full homochirality.
q-bio.BM:The structural organisation of the viral genome within its protein container, called the viral capsid, is an important aspect of virus architecture. Many single-stranded (ss) RNA viruses organise a significant part of their genome in a dodecahedral cage as a RNA duplex structure that mirrors the symmetry of the capsid. Bruinsma and Rudnick have suggested a model for the structural organisation of the RNA in these cages. It is the purpose of this paper to further develop their approach based on results from the areas of graph theory and DNA network engineering. We start by suggesting a scenario for pariacoto virus, a representative of this class of viruses, that is energetically more favorable than those derived previously. We then show that it is a representative of a whole family of cage structures that abide to the same construction principle, and then derive the energetically optimal configuration for a second family of cage structures along similar lines. Finally, we give reasons for the conjecture that these two families are more likely to occur in nature than other scenarios.
q-bio.BM:The sequence-dependent elasticity of double-helical DNA on a nm length scale can be captured by the rigid base-pair model, whose strains are the relative position and orientation of adjacent base-pairs. Corresponding elastic potentials have been obtained from all-atom MD simulation and from high-resolution structural data. On the scale of a hundred nm, DNA is successfully described by a continuous worm-like chain model with homogeneous elastic properties characterized by a set of four elastic constants, which have been directly measured in single-molecule experiments. We present here a theory that links these experiments on different scales, by systematically coarse-graining the rigid base-pair model for random sequence DNA to an effective worm-like chain description. The average helical geometry of the molecule is exactly taken into account in our approach. We find that the available microscopic parameters sets predict qualitatively similar mesoscopic parameters. The thermal bending and twisting persistence lengths computed from MD data are 42 and 48 nm, respectively. The static persistence lengths are generally much higher, in agreement with cyclization experiments. All microscopic parameter sets predict negative twist-stretch coupling. The variability and anisotropy of bending stiffness in short random chains lead to non-Gaussian bend angle distributions, but become unimportant after two helical turns.
q-bio.BM:Processive molecular motors take more-or-less uniformly sized steps, along spatially periodic tracks, mostly forwards but increasingly backwards under loads. Experimentally, the major steps can be resolved clearly within the noise but one knows biochemically that one or more mechanochemical substeps remain hidden in each enzymatic cycle. In order to properly interpret experimental data for back/forward step ratios, mean conditional step-to-step dwell times, etc., a first-passage analysis has been developed that takes account of hidden substeps in $N$-state sequential models. The explicit, general results differ significantly from previous treatments that identify the observed steps with complete mechanochemical cycles; e.g., the mean dwell times $\tau_+$ and $\tau_-$ prior to forward and back steps, respectively, are normally {\it unequal} although the dwell times $\tau_{++}$ and $\tau_{--}$ between {\it successive} forward and back steps are equal. Illustrative (N=2)-state examples display a wide range of behavior. The formulation extends to the case of two or more detectable transitions in a multistate cycle with hidden substeps.
q-bio.BM:For the vast majority of naturally occurring, small, single domain proteins folding is often described as a two-state process that lacks detectable intermediates. This observation has often been rationalized on the basis of a nucleation mechanism for protein folding whose basic premise is the idea that after completion of a specific set of contacts forming the so-called folding nucleus the native state is achieved promptly. Here we propose a methodology to identify folding nuclei in small lattice polymers and apply it to the study of protein molecules with chain length N=48. To investigate the extent to which protein topology is a robust determinant of the nucleation mechanism we compare the nucleation scenario of a native-centric model with that of a sequence specific model sharing the same native fold. To evaluate the impact of the sequence's finner details in the nucleation mechanism we consider the folding of two non- homologous sequences. We conclude that in a sequence-specific model the folding nucleus is, to some extent, formed by the most stable contacts in the protein and that the less stable linkages in the folding nucleus are solely determined by the fold's topology. We have also found that independently of protein sequence the folding nucleus performs the same `topological' function. This unifying feature of the nucleation mechanism results from the residues forming the folding nucleus being distributed along the protein chain in a similar and well-defined manner that is determined by the fold's topological features.
q-bio.BM:The folding of naturally occurring, single domain proteins is usually well-described as a simple, single exponential process lacking significant trapped states. Here we further explore the hypothesis that the smooth energy landscape this implies, and the rapid kinetics it engenders, arises due to the extraordinary thermodynamic cooperativity of protein folding. Studying Miyazawa-Jernigan lattice polymers we find that, even under conditions where the folding energy landscape is relatively optimized (designed sequences folding at their temperature of maximum folding rate), the folding of protein-like heteropolymers is accelerated when their thermodynamic cooperativity enhanced by enhancing the non-additivity of their energy potentials. At lower temperatures, where kinetic traps presumably play a more significant role in defining folding rates, we observe still greater cooperativity-induced acceleration. Consistent with these observations, we find that the folding kinetics of our computational models more closely approximate single-exponential behavior as their cooperativity approaches optimal levels. These observations suggest that the rapid folding of naturally occurring proteins is, at least in part, consequences of their remarkably cooperative folding.
q-bio.BM:An increasing number of proteins are being discovered with a remarkable and somewhat surprising feature, a knot in their native structures. How the polypeptide chain is able to knot itself during the folding process to form these highly intricate protein topologies is not known. Here, we perform a computational study on the 160-amino acid homodimeric protein YibK which, like other proteins in the SpoU family of MTases, contains a deep trefoil knot in its C-terminal region. In this study, we use a coarse-grained C-alpha-chain representation and Langevin dynamics to study folding kinetics. We find that specific, attractive nonnative interactions are critical for knot formation. In the absence of these interactions, i.e. in an energetics driven entirely by native interactions, knot formation is exceedingly unlikely. Further, we find, in concert with recent experimental data on YibK, two parallel folding pathways which we attribute to an early and a late formation of the trefoil knot, respectively. For both pathways, knot formation occurs before dimerization. A bioinformatics analysis of the SpoU family of proteins reveals further that the critical nonnative interactions may originate from evolutionary conserved hydrophobic segments around the knotted region.
q-bio.BM:The structure of the self-cleaving hairpin ribozyme is well characterized, and its folding has been examined in bulk and by single-molecule fluorescence, establishing the importance of cations, especially magnesium in the stability of the native fold. Here we describe the first all-atom folding simulations of the hairpin ribozyme, using a version of a Go potential with separate secondary and tertiary structure energetic contributions. The ratio of tertiary/secondary interaction energies serves as a proxy for non-specific cation binding: a high ratio corresponds to a high concentration, while a low one mimics low concentration. By studying the unfolding behavior of the RNA over a range of temperature and tertiary/secondary energies, a three-state phase diagram emerges, with folded, unfolded (coil) and transient folding/unfolding tertiary structure species. The thermodynamics were verified by paired folding simulations in each region of the phase diagram. The three phase behaviors correspond with experimentally observed states, so this simple model captures the essential aspect of thermodynamics in RNA folding.
q-bio.BM:The refolding from stretched initial conformations of ubiquitin (PDB ID: 1ubq) under the quenched force is studied using the Go model and the Langevin dynamics. It is shown that the refolding decouples the collapse and folding kinetics. The force quench refolding times scale as tau_F ~ exp(f_q*x_F/k_B*T), where f_q is the quench force and x_F = 0.96 nm is the location of the average transition state along the reaction coordinate given by the end-to-end distance. This value is close to x_F = 0.8 nm obtained from the force-clamp experiments. The mechanical and thermal unfolding pathways are studied and compared with the experimental and all-atom simulation results in detail. The sequencing of thermal unfolding was found to be markedly different from the mechanical one. It is found that fixing the N-terminus of ubiquitin changes its mechanical unfolding pathways much more drastically compared to the case when the C-end is anchored. We obtained the distance between the native state and the transition state x_UF=0.24 nm which is in reasonable agreement with the experimental data.
q-bio.BM:Natural proteins fold to a unique, thermodynamically dominant state. Modeling of the folding process and prediction of the native fold of proteins are two major unsolved problems in biophysics. Here, we show successful all-atom ab initio folding of a representative diverse set of proteins, using a minimalist transferable energy model that consists of two-body atom-atom interactions, hydrogen-bonding, and a local sequence energy term that models sequence-specific chain stiffness. Starting from a random coil, the native-like structure was observed during replica exchange Monte Carlo (REMC) simulation for most proteins regardless of their structural classes; the lowest energy structure was close to native- in the range of 2-6 A root-mean-square deviation (RMSD). Our results demonstrate that the successful all-atom folding of a protein chain to its native state is governed by only a few crucial energetic terms.
q-bio.BM:Protein-DNA interactions are vital for many processes in living cells, especially transcriptional regulation and DNA modification. To further our understanding of these important processes on the microscopic level, it is necessary that theoretical models describe the macromolecular interaction energetics accurately. While several methods have been proposed, there has not been a careful comparison of how well the different methods are able to predict biologically important quantities such as the correct DNA binding sequence, total binding free energy, and free energy changes caused by DNA mutation. In addition to carrying out the comparison, we present two important theoretical models developed initially in protein folding that have not yet been tried on protein-DNA interactions. In the process, we find that the results of these knowledge-based potentials show a strong dependence on the interaction distance and the derivation method. Finally, we present a knowledge-based potential that gives comparable or superior results to the best of the other methods, including the molecular mechanics force field AMBER99.
q-bio.BM:The folding of the alpha-helice domain hbSBD of the mammalian mitochondrial branched-chain alpha-ketoacid dehydrogenase (BCKD) complex is studied by the circular dichroism technique in absence of urea. Thermal denaturation is used to evaluate various thermodynamic parameters defining the equilibrium unfolding, which is well described by the two-state model with the folding temperature T_f = 317.8 K and the enthalpy change Delta H_g = 19.67 kcal/mol. The folding is also studied numerically using the off-lattice coarse-grained Go model and the Langevin dynamics. The obtained results, including the population of the native basin, the free energy landscape as a function of the number of native contacts and the folding kinetics, also suggest that the hbSBD domain is a two-state folder. These results are consistent with the biological function of hbSBD in BCKD.
q-bio.BM:We analyze the dependence of cooperativity of the thermal denaturation transition and folding rates of globular proteins on the number of amino acid residues, $N$, using lattice models with side chains,off-lattice Go models and the available experimental data. A dimensionless measure of cooperativity, $\Omega_c$ ($0 < \Omega_c < \infty$), scales as $\Omega_c \sim N^{\zeta}$. The results of simulations and the analysis of experimental data further confirm the earlier prediction that $\zeta$ is universal with $\zeta = 1 +\gamma$, where exponent $\gamma$ characterizes the susceptibility of a self-avoiding walk. This finding suggests that the structural characteristics in the denaturated state are manifested in the folding cooperativity at the transition temperature. The folding rates $k_F$ for the Go models and a dataset of 69 proteins can be fit using $k_F = k_F^0 \exp(-cN^\beta)$. Both $\beta = 1/2$ and 2/3 provide a good fit of the data. We find that $k_F = k_F^0 \exp(-cN^{{1/2}})$, with the average (over the dataset of proteins) $k_F^0 \approx (0.2\mu s)^{-1}$ and $c \approx 1.1$, can be used to estimate folding rates to within an order of magnitude in most cases. The minimal models give identical $N$ dependence with $c \approx 1$. The prefactor for off-lattice Go models is nearly four orders of magnitude larger than the experimental value.
q-bio.BM:The didemnins represent a versatile class of depsipeptides of marine origin and hold a great deal of potential for biomedical application. The biological and geographical origins of the didemnins are reviewed in addition to the chemical structures of the major didemnins. The biological mechanisms behind the antiviral and anticancer effects of selected didemnins are summarized and the special case of dehydrodidemnin B (Aplidin) is expounded upon including structural characteristics, synthesis, pharmacological mechanism and a discussion of its current clinical trials as an anticancer agent.
q-bio.BM:TThe paper had many errors.
q-bio.BM:Pathological folding and oligomer formation of the amyloid beta-protein (Abeta) are widely perceived as central to Alzheimer's disease (AD). Experimental approaches to study Abeta self-assembly are problematic, because most relevant aggregates are quasi-stable and inhomogeneous. We apply a discrete molecular dynamics (DMD) approach combined with a four-bead protein model to study oligomer formation of the amyloid beta-protein (Abeta). We address the differences between the two most common Abeta alloforms, Abeta40 and Abeta42, which oligomerize differently in vitro. We study how the presence of electrostatic interactions (EIs) between pairs of charged amino acids affects Abeta40 and Abeta42 oligomer formation. Our results indicate that EIs promote formation of larger oligomers in both Abeta40 and Abeta42. The Abeta40 size distribution remains unimodal, whereas the Abeta42 distribution is trimodal, as observed experimentally. Abeta42 folded structure is characterized by a turn in the C-terminus that is not present in Abeta40. We show that the same C-terminal region is also responsible for the strongest intermolecular contacts in Abeta42 pentamers and larger oligomers. Our results suggest that this C-terminal region plays a key role in the formation of Abeta42 oligomers and the relative importance of this region increases in the presence of EIs. These results suggest that inhibitors targeting the C-terminal region of Abeta42 oligomers may be able to prevent oligomer formation or structurally modify the assemblies to reduce their toxicity.
q-bio.BM:The results of Brownian dynamics simulations of a single DNA molecule in shear flow are presented taking into account the effect of internal viscosity. The dissipative mechanism of internal viscosity is proved necessary in the research of DNA dynamics. A stochastic model is derived on the basis of the balance equation for forces acting on the chain. The Euler method is applied to the solution of the model. The extensions of DNA molecules for different Weissenberg numbers are analyzed. Comparison with the experimental results available in the literature is carried out to estimate the contribution of the effect of internal viscosity.
q-bio.BM:The network paradigm is increasingly used to describe the topology and dynamics of complex systems. Here we review the results of the topological analysis of protein structures as molecular networks describing their small-world character, and the role of hubs and central network elements in governing enzyme activity, allosteric regulation, protein motor function, signal transduction and protein stability. We summarize available data how central network elements are enriched in active centers and ligand binding sites directing the dynamics of the entire protein. We assess the feasibility of conformational and energy networks to simplify the vast complexity of rugged energy landscapes and to predict protein folding and dynamics. Finally, we suggest that modular analysis, novel centrality measures, hierarchical representation of networks and the analysis of network dynamics will soon lead to an expansion of this field.
q-bio.BM:Annealed importance sampling is a means to assign equilibrium weights to a nonequilibrium sample that was generated by a simulated annealing protocol. The weights may then be used to calculate equilibrium averages, and also serve as an ``adiabatic signature'' of the chosen cooling schedule. In this paper we demonstrate the method on the 50-atom dileucine peptide, showing that equilibrium distributions are attained for manageable cooling schedules. For this system, as naively implemented here, the method is modestly more efficient than constant temperature simulation. However, the method is worth considering whenever any simulated heating or cooling is performed (as is often done at the beginning of a simulation project, or during an NMR structure calculation), as it is simple to implement and requires minimal additional CPU expense. Furthermore, the naive implementation presented here can be improved.
q-bio.BM:Conformational transitions in macromolecular complexes often involve the reorientation of lever-like structures. Using a simple theoretical model, we show that the rate of such transitions is drastically enhanced if the lever is bendable, e.g. at a localized "hinge''. Surprisingly, the transition is fastest with an intermediate flexibility of the hinge. In this intermediate regime, the transition rate is also least sensitive to the amount of "cargo'' attached to the lever arm, which could be exploited by molecular motors. To explain this effect, we generalize the Kramers-Langer theory for multi-dimensional barrier crossing to configuration dependent mobility matrices.
q-bio.BM:We report 10 successfully folding events of trpzip2 by molecular dynamics simulation. It is found that the trizip2 can fold into its native state through different zipper pathways, depending on the ways of forming hydrophobic core. We also find a very fast non-zipper pathway. This indicates that there may be no inconsistencies in the current pictures of beta-hairpin folding mechanisms. These pathways occur with different probabilities. zip-out is the most probable one. This may explain the recent experiment that the turn formation is the rate-limiting step for beta-hairpin folding.
q-bio.BM:Structural fluctuations in the thermal equilibrium of the kinesin motor domain are studied using a lattice protein model with Go interactions. By means of the multi-self-overlap ensemble (MSOE) Monte Carlo method and the principal component analysis (PCA), the free-energy landscape is obtained. It is shown that kinesins have two subdomains that exhibit partial folding/unfolding at functionally important regions: one is located around the nucleotide binding site and the other includes the main microtubule binding site. These subdomains are consistent with structural variability that was reported recently based on experimentally-obtained structures. On the other hand, such large structural fluctuations have not been captured by B-factor or normal mode analyses. Thus, they are beyond the elastic regime, and it is essential to take into account chain connectivity for studying the function of kinesins.
q-bio.BM:We introduce a topology-based nonlinear network model of protein dynamics with the aim of investigating the interplay of spatial disorder and nonlinearity. We show that spontaneous localization of energy occurs generically and is a site-dependent process. Localized modes of nonlinear origin form spontaneously in the stiffest parts of the structure and display site-dependent activation energies. Our results provide a straightforward way for understanding the recently discovered link between protein local stiffness and enzymatic activity. They strongly suggest that nonlinear phenomena may play an important role in enzyme function, allowing for energy storage during the catalytic process.
q-bio.BM:We incorporate hydrodynamic interactions in a structure-based model of ubiquitin and demonstrate that the hydrodynamic coupling may reduce the peak force when stretching the protein at constant speed, especially at larger speeds. Hydrodynamic interactions are also shown to facilitate unfolding at constant force and inhibit stretching by fluid flows.
q-bio.BM:We demonstrate a new algorithm for finding protein conformations that minimize a non-bonded energy function. The new algorithm, called the difference map, seeks to find an atomic configuration that is simultaneously in two constraint spaces. The first constraint space is the space of atomic configurations that have a valid peptide geometry, while the second is the space of configurations that have a non-bonded energy below a given target. These two constraint spaces are used to define a deterministic dynamical system, whose fixed points produce atomic configurations in the intersection of the two constraint spaces. The rate at which the difference map produces low energy protein conformations is compared with that of a contemporary search algorithm, parallel tempering. The results indicate the difference map finds low energy protein conformations at a significantly higher rate then parallel tempering.
q-bio.BM:Vibrational energy transfer of the amide I mode of N-methylacetamide (NMA) is studied theoretically using the vibrational configuration interaction method. A quartic force field of NMA is constructed at the B3LYP/6-31G+(d) level of theory and its accuarcy is checked by comparing the resulting anharmonic frequencies with available theoretical and experimental values. Quantum dynamics calculations for the amide I mode excitation clarify the dominant energy transfer pathways, which sensitively depend on the anharmonic couplings among vibrational modes. A ratio of the anharmonic coupling to the frequency mismatch is employed to predict and interpret the dominant energy flow pathways.
q-bio.BM:It previously has been discovered that visible light irradiation of crystalline substrates can lead to enhancement of subsequent enzymatic reaction rates as sharply peaked oscillatory functions of irradiation time. The particular activating irradiation times can vary with source of a given enzyme and thus, presumably, its molecular structure. The experiments reported here demonstrate that the potential for this anomalous enzyme reaction rate enhancement can be transferred from one bacterial species to another coincident with transfer of the genetic determinant for the relevant enzyme. In particular, the effect of crystal-irradiated chloramphenicol on growth of bacterial strains in which a transferable R-factor DNA plasmid coding for chloramphenicol resistance was or was not present (S. panama R+, E. coli R+, and E. coli R-) was determined. Chloramphenicol samples irradiated 10, 35 and 60 sec produced increased growth rates (diminished inhibition) for the resistant S. panama and E. coli strains, while having no such effect on growth rate of the sensitive E. coli strain. Consistent with past findings, chloramphenicol samples irradiated 5, 30 and 55 sec produced decreased growth rates (increased inhibition) for all three strains.
q-bio.BM:Inherent structure theory is used to discover strong connections between simple characteristics of protein structure and the energy landscape of a Go model. The potential energies and vibrational free energies of inherent structures are highly correlated, and both reflect simple measures of networks of native contacts. These connections have important consequences for models of protein dynamics and thermodynamics.
q-bio.BM:The free-energy landscape of the alpha-helix of protein G is studied by means of metadynamics coupled with a solute tempering algorithm. Metadynamics allows to overcome large energy barriers, whereas solute tempering improves the sampling with an affordable computational effort. From the sampled free-energy surface we are able to reproduce a number of experimental observations, such as the fact that the lowest minimum corresponds to a globular conformation displaying some degree of beta-structure, that the helical state is metastable and involves only 65% of the chain. The calculations also show that the system populates consistently a pi-helix state and that the hydrophobic staple motif is present only in the free-energy minimum associated with the helices, and contributes to their stabilization. The use of metadynamics coupled with solute tempering results then particularly suitable to provide the thermodynamics of a short peptide, and its computational efficiency is promising to deal with larger proteins.
q-bio.BM:Using a time-dependent perturbation theory, vibrational energy relaxation (VER) of isotopically labeled amide I modes in cytochrome c solvated with water is investigated. Contributions to the VER are decomposed into two contributions from the protein and water. The VER pathways are visualized using radial and angular excitation functions for resonant normal modes. Key differences of VER among different amide I modes are demonstrated, leading to a detailed picture of the spatial anisotropy of the VER. The results support the experimental observation that amide I modes in proteins relax with sub picosecond timescales, while the relaxation mechanism turns out to be sensitive to the environment of the amide I mode.
q-bio.BM:Local minima and the saddle points separating them in the energy landscape are known to dominate the dynamics of biopolymer folding. Here we introduce a notion of a "folding funnel" that is concisely defined in terms of energy minima and saddle points, while at the same time conforming to a notion of a "folding funnel" as it is discussed in the protein folding literature.
q-bio.BM:Using magnetic tweezers to investigate the mechanical response of single chromatin fibers, we show that fibers submitted to large positive torsion transiently trap positive turns, at a rate of one turn per nucleosome. A comparison with the response of fibers of tetrasomes (the (H3-H4)2 tetramer bound with ~50 bp of DNA) obtained by depletion of H2A-H2B dimers, suggests that the trapping reflects a nucleosome chiral transition to a metastable form built on the previously documented righthanded tetrasome. In view of its low energy, <8 kT, we propose this transition is physiologically relevant and serves to break the docking of the dimers on the tetramer which in the absence of other factors exerts a strong block against elongation of transcription by the main RNA polymerase.
q-bio.BM:We perform extensive Monte Carlo simulations of a lattice model and the Go potential to investigate the existence of folding pathways at the level of contact cluster formation for two native structures with markedly different geometries. Our analysis of folding pathways revealed a common underlying folding mechanism, based on nucleation phenomena, for both protein models. However, folding to the more complex geometry (i.e. that with more non-local contacts) is driven by a folding nucleus whose geometric traits more closely resemble those of the native fold. For this geometry folding is clearly a more cooperative process.
q-bio.BM:In this study we evaluate, at full atomic detail, the folding processes of two small helical proteins, the B domain of protein A and the Villin headpiece. Folding kinetics are studied by performing a large number of ab initio Monte Carlo folding simulations using a single transferable all-atom potential. Using these trajectories, we examine the relaxation behavior, secondary structure formation, and transition-state ensembles (TSEs) of the two proteins and compare our results with experimental data and previous computational studies. To obtain a detailed structural information on the folding dynamics viewed as an ensemble process, we perform a clustering analysis procedure based on graph theory. Moreover, rigorous pfold analysis is used to obtain representative samples of the TSEs and a good quantitative agreement between experimental and simulated Fi-values is obtained for protein A. Fi-values for Villin are also obtained and left as predictions to be tested by future experiments. Our analysis shows that two-helix hairpin is a common partially stable structural motif that gets formed prior to entering the TSE in the studied proteins. These results together with our earlier study of Engrailed Homeodomain and recent experimental studies provide a comprehensive, atomic-level picture of folding mechanics of three-helix bundle proteins.
q-bio.BM:Strong experimental and theoretical evidence shows that transcription factors and other specific DNA-binding proteins find their sites using a two-mode search: alternating between 3D diffusion through the cell and 1D sliding along the DNA. We consider the role spatial effects in the mechanism on two different scales. First, we reconcile recent experimental findings by showing that the 3D diffusion of the transcription factor is often local, i.e. the transcription factor lands quite near its dissociation site. Second, we discriminate between two types of searches: global searches and local searches. We show that these searches differ significantly in average search time and the variability of search time. Using experimentally measured parameter values, we also show that 1D and 3D search is not optimally balanced, leading to much larger estimates of search time. Together, these results lead to a number of biological implications including suggestions of how prokaryotes and eukaryotes achieve rapid gene regulation and the relationship between the search mechanism and noise in gene expression.
q-bio.BM:Thermal shape fluctuations of grafted microtubules were studied using high resolution particle tracking of attached fluorescent beads. First mode relaxation times were extracted from the mean square displacement in the transverse coordinate. For microtubules shorter than 10 um, the relaxation times were found to follow an L^2 dependence instead of L^4 as expected from the standard wormlike chain model. This length dependence is shown to result from a complex length dependence of the bending stiffness which can be understood as a result of the molecular architecture of microtubules. For microtubules shorter than 5 um, high drag coefficients indicate contributions from internal friction to the fluctuation dynamics.
q-bio.BM:E. Coli. dihydrofolate reductase (DHFR) undergoes conformational transitions between the closed (CS) and occluded (OS) states which, respectively, describe whether the active site is closed or occluded by the Met20 loop. A sequence-based approach is used to identify a network of residues that represents the allostery wiring diagram. We also use a self-organized polymer model to monitor the kinetics of the CS->OS and the reverse transitions. a sliding motion of Met20 loop is observed. The residues that facilitate the Met20 loop motion are part of the network of residues that transmit allosteric signals during the CS->OS transition.
q-bio.BM:A model is presented to describe the nucleotide and repeat addition processivity by the telomerase. In the model, the processive nucleotide addition is implemented on the basis of two requirements: One is that stem IV loop stimulates the chemical reaction of nucleotide incorporation, and the other one is the existence of an ssRNA-binding site adjacent to the polymerase site that has a high affinity for the unpaired base of the template. The unpairing of DNA:RNA hybrid after the incorporation of the nucleotide paired with the last base on the template, which is the prerequisite for repeat addition processivity, is caused by a force acting on the primer. The force is resulted from the unfolding of stem III pseudoknot that is induced by the swinging of stem IV loop towards the nucleotide-bound polymerase site. Based on the model, the dynamics of processive nucleotide and repeat additions by Tetrahymena telomerase are quantitatively studied, which give good explanations to the previous experimental results. Moreover, some predictions are presented. In particular, it is predicted that the repeat addition processivity is mainly determined by the difference between the free energy required to disrupt the DNA:RNA hybrid and that required to unfold the stem III pseudoknot, with the large difference corresponding to a low repeat addition processivity while the small one corresponding to a high repeat addition processivity.
q-bio.BM:Small single-domain proteins often exhibit only a single free-energy barrier, or transition state, between the denatured and the native state. The folding kinetics of these proteins is usually explored via mutational analysis. A central question is which structural information on the transition state can be derived from the mutational data. In this article, we model and structurally interpret mutational Phi-values for two small beta-sheet proteins, the PIN and the FBP WW domain. The native structure of these WW domains comprises two beta-hairpins that form a three-stranded beta-sheet. In our model, we assume that the transition state consists of two conformations in which either one of the hairpins is formed. Such a transition state has been recently observed in Molecular Dynamics folding-unfolding simulations of a small designed three-stranded beta-sheet protein. We obtain good agreement with the experimental data (i) by splitting up the mutation-induced free-energy changes into terms for the two hairpins and for the small hydrophobic core of the proteins, and (ii) by fitting a single parameter, the relative degree to which hairpin 1 and 2 are formed in the transition state. The model helps to understand how mutations affect the folding kinetics of WW domains, and captures also negative Phi-values that have been difficult to interpret.
q-bio.BM:Simple theoretical concepts and models have been helpful to understand the folding rates and routes of single-domain proteins. As reviewed in this article, a physical principle that appears to underly these models is loop closure.
q-bio.BM:The isotopic composition, for example, 14C/12C, 13C/12C, 2H/1H, 15N/14N and 18O/16O, of the elements of matter is heterogeneous. It is ruled by physical, chemical and biological mechanisms. Isotopes can be employed to follow the fate of mineral and organic compounds during biogeochemical transformations. The determination of the isotopic composition of organic substances occurring at trace level in very complex mixtures such as sediments, soils and blood, has been made possible during the last 20 years due to the rapid development of molecular level isotopic techniques. After a brief glance at pioneering studies revealing isotopic breakthroughs at the molecular and intramolecular levels, this paper reviews selected applications of compound-specific isotope analysis in various scientific fields.
q-bio.BM:The conformational dynamics of a single protein molecule in a shear flow is investigated using Brownian dynamics simulations. A structure-based coarse grained model of a protein is used. We consider two proteins, ubiquitin and integrin, and find that at moderate shear rates they unfold through a sequence of metastable states - a pattern which is distinct from a smooth unraveling found in homopolymers. Full unfolding occurs only at very large shear rates. Furthermore, the hydrodynamic interactions between the amino acids are shown to hinder the shear flow unfolding. The characteristics of the unfolding process depend on whether a protein is anchored or not, and if it is, on the choice of an anchoring point.
q-bio.BM:We demonstrate that a common-line method can assemble a 3D oversampled diffracted intensity distribution suitable for high-resolution structure solution from a set of measured 2D diffraction patterns, as proposed in experiments with an X-ray free electron laser (XFEL) (Neutze {\it et al.}, 2000). Even for a flat Ewald sphere, we show how the ambiguities due to Friedel's Law may be overcome. The method breaks down for photon counts below about 10 per detector pixel, almost 3 orders of magnitude higher than expected for scattering by a 500 kDa protein with an XFEL beam focused to a 0.1 micron diameter spot. Even if 10**3 orientationally similar diffraction patterns could be identified and added to reach the requisite photon count per pixel, the need for about 10**6 orientational classes for high-resolution structure determination suggests that about ~ 10**9 diffraction patterns must be recorded. Assuming pulse and read-out rates of 100 Hz, such measurements would require ~ 10**7 seconds, i.e. several months of continuous beam time.
q-bio.BM:Background. All-atom crystallographic refinement of proteins is a laborious manually driven procedure, as a result of which, alternative and multiconformer interpretations are not routinely investigated.   Results. We describe efficient loop sampling procedures in Rappertk and demonstrate that single loops in proteins can be automatically and accurately modelled with few positional restraints. Loops constructed with a composite CNS/Rappertk protocol consistently have better Rfree than those with CNS alone. This approach is extended to a more realistic scenario where there are often large positional uncertainties in loops along with small imperfections in the secondary structural framework. Both ensemble and collection methods are used to estimate the structural heterogeneity of loop regions.   Conclusion. Apart from benchmarking Rappertk for the all-atom protein refinement task, this work also demonstrates its utility in both aspects of loop modelling - building a single conformer and estimating structural heterogeneity the loops can exhibit.
q-bio.BM:Background. Dramatic increases in RNA structural data have made it possible to recognize its conformational preferences much better than a decade ago. This has created an opportunity to use discrete restraint-based conformational sampling for modelling RNA and automating its crystallographic refinement. Results. All-atom sampling of entire RNA chains, termini and loops is achieved using the Richardson RNA backbone rotamer library and an unbiased distribution for glycosidic dihedral angle. Sampling behaviour of Rappertk on a diverse dataset of RNA chains under varying spatial restraints is benchmarked. The iterative composite crystallographic refinement protocol developed here is demonstrated to outperform CNS-only refinement on parts of tRNA(Asp) structure. Conclusion. This work opens exciting possibilities for further work in RNA modelling and crystallography.
q-bio.BM:DNA torsion dynamics is essential in the transcription process; simple models for it have been proposed by several authors, in particular Yakushevich (Y model). These are strongly related to models of DNA separation dynamics such as the one first proposed by Peyrard and Bishop (and developed by Dauxois, Barbi, Cocco and Monasson among others), but support topological solitons. We recently developed a ``composite'' version of the Y model, in which the sugar-phosphate group and the base are described by separate degrees of freedom. This at the same time fits experimental data better than the simple Y model, and shows dynamical phenomena, which are of interest beyond DNA dynamics. Of particular relevance are the mechanism for selecting the speed of solitons by tuning the physical parameters of the non linear medium and the hierarchal separation of the relevant degrees of freedom in ``master'' and ``slave''. These mechanisms apply not only do DNA, but also to more general macromolecules, as we show concretely by considering polyethylene.
q-bio.BM:The Caspar-Klug classification of viruses whose protein shell, called viral capsid, exhibits icosahedral symmetry, has recently been extended to incorporate viruses whose capsid proteins are exclusively organised in pentamers. The approach, named `Viral Tiling Theory', is inspired by the theory of quasicrystals, where aperiodic Penrose tilings enjoy 5-fold and 10-fold local symmetries. This paper analyzes the extent to which this classification approach informs dynamical properties of the viral capsids, in particular the pattern of Raman active modes of vibrations, which can be observed experimentally.
q-bio.BM:Position-specific scoring matrices (PSSMs) are useful for detecting weak homology in protein sequence analysis, and they are thought to contain some essential signatures of the protein families. In order to elucidate what kind of ingredients constitute such family-specific signatures, we apply singular value decomposition to a set of PSSMs and examine the properties of dominant right and left singular vectors. The first right singular vectors were correlated with various amino acid indices including relative mutability, amino acid composition in protein interior, hydropathy, or turn propensity, depending on proteins. A significant correlation between the first left singular vector and a measure of site conservation was observed. It is shown that the contribution of the first singular component to the PSSMs act to disfavor potentially but falsely functionally important residues at conserved sites. The second right singular vectors were highly correlated with hydrophobicity scales, and the corresponding left singular vectors with contact numbers of protein structures. It is suggested that sequence alignment with a PSSM is essentially equivalent to threading supplemented with functional information. The presented method may be used to separate functionally important sites from structurally important ones, and thus it may be a useful tool for predicting protein functions.
q-bio.BM:A method to search for local structural similarities in proteins at atomic resolution is presented. It is demonstrated that a huge amount of structural data can be handled within a reasonable CPU time by using a conventional relational database management system with appropriate indexing of geometric data. This method, which we call geometric indexing, can enumerate ligand binding sites that are structurally similar to sub-structures of a query protein among more than 160,000 possible candidates within a few hours of CPU time on an ordinary desktop computer. After detecting a set of high scoring ligand binding sites by the geometric indexing search, structural alignments at atomic resolution are constructed by iteratively applying the Hungarian algorithm, and the statistical significance of the final score is estimated from an empirical model based on a gamma distribution. Applications of this method to several protein structures clearly shows that significant similarities can be detected between local structures of non-homologous as well as homologous proteins.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum stack-length. We show that the numbers of $k$-noncrossing structures without isolated base pairs are significantly smaller than the number of all $k$-noncrossing structures. In particular we prove that the number of 3- and 4-noncrossing RNA structures with stack-length $\ge 2$ is for large $n$ given by $311.2470 \frac{4!}{n(n-1)...(n-4)}2.5881^n$ and $1.217\cdot 10^{7} n^{-{21/2}} 3.0382^n$, respectively. We furthermore show that for $k$-noncrossing RNA structures the drop in exponential growth rates between the number of all structures and the number of all structures with stack-size $\ge 2$ increases significantly. Our results are of importance for prediction algorithms for pseudoknot-RNA and provide evidence that there exist neutral networks of RNA pseudoknot structures.
q-bio.BM:Network science is already making an impact on the study of complex systems and offers a promising variety of tools to understand their formation and evolution (1-4) in many disparate fields from large communication networks (5,6), transportation infrastructures (7) and social communities (8,9) to biological systems (1,10,11). Even though new highthroughput technologies have rapidly been generating large amounts of genomic data, drug design has not followed the same development, and it is still complicated and expensive to develop new single-target drugs. Nevertheless, recent approaches suggest that multi-target drug design combined with a network-dependent approach and large-scale systems-oriented strategies (12-14) create a promising framework to combat complex multigenetic disorders like cancer or diabetes. Here, we investigate the human network corresponding to the interactions between all US approved drugs and human therapies, defined by known drug-therapy relationships. Our results show that the key paths in this network are shorter than three steps, indicating that distant therapies are separated by a surprisingly low number of chemical compounds. We also identify a sub-network composed by drugs with high centrality measures (15), which represent the structural back-bone of the drug-therapy system and act as hubs routing information between distant parts of the network. These findings provide for the first time a global map of the largescale organization of all known drugs and associated therapies, bringing new insights on possible strategies for future drug development. Special attention should be given to drugs which combine the two properties of (a) having a high centrality value and (b) acting on multiple targets.
q-bio.BM:In this paper we enumerate $k$-noncrossing RNA pseudoknot structures with given minimum arc- and stack-length. That is, we study the numbers of RNA pseudoknot structures with arc-length $\ge 3$, stack-length $\ge \sigma$ and in which there are at most $k-1$ mutually crossing bonds, denoted by ${\sf T}_{k,\sigma}^{[3]}(n)$. In particular we prove that the numbers of 3, 4 and 5-noncrossing RNA structures with arc-length $\ge 3$ and stack-length $\ge 2$ satisfy ${\sf T}_{3,2}^{[3]}(n)^{}\sim K_3 n^{-5} 2.5723^n$, ${\sf T}^{[3]}_{4,2}(n)\sim K_4 n^{-{21/2}} 3.0306^n$, and ${\sf T}^{[3]}_{5,2}(n)\sim K_5 n^{-18} 3.4092^n$, respectively, where $K_3,K_4,K_5$ are constants. Our results are of importance for prediction algorithms for RNA pseudoknot structures.
q-bio.BM:We have developed a new extended replica exchange method to study thermodynamics of a system in the presence of external force. Our idea is based on the exchange between different force replicas to accelerate the equilibrium process. We have shown that the refolding pathways of single ubiquitin depend on which terminus is fixed. If the N-end is fixed then the folding pathways are different compared to the case when both termini are free, but fixing the C-terminal does not change them. Surprisingly, we have found that the anchoring terminal does not affect the pathways of individual secondary structures of three-domain ubiquitin, indicating the important role of the multi-domain construction. Therefore, force-clamp experiments, in which one end of a protein is kept fixed, can probe the refolding pathways of a single free-end ubiquitin if one uses either the poly-ubiquitin or a single domain with the C-terminus anchored. However, it is shown that anchoring one end does not affect refolding pathways of the titin domain I27, and the force-clamp spectroscopy is always capable to predict folding sequencing of this protein. We have obtained the reasonable estimate for unfolding barrier of ubiqutin. The linkage between residue Lys48 and the C-terminal of ubiquitin is found to have the dramatic effect on the location of the transition state along the end-to-end distance reaction coordinate, but the multi-domain construction leaves the transition state almost unchanged. We have found that the maximum force in the force-extension profile from constant velocity force pulling simulations depends on temperature nonlinearly. However, for some narrow temperature interval this dependence becomes linear, as have been observed in recent experiments.
q-bio.BM:A construction method for duplex cage structures with icosahedral sym- metry made out of single-stranded DNA molecules is presented and applied to an icosidodecahedral cage. It is shown via a mixture of analytic and computer techniques that there exist realisations of this graph in terms of two circular DNA molecules. These blueprints for the organisation of a cage structure with a noncrystallographic symmetry may assist in the design of containers made from DNA for applications in nanotechnology.
q-bio.BM:We analyzed folding routes predicted by a variational model in terms of a generalized formalism of the capillarity scaling theory for 28 two-state proteins. The scaling exponent ranged from 0.2 to 0.45 with an average of 0.33. This average value corresponds to packing of rigid objects.That is, on average the folded core of the nucleus is found to be relatively diffuse. We also studied the growth of the folding nucleus and interface along the folding route in terms of the density or packing fraction. The evolution of the folded core and interface regions can be classified into three patterns of growth depending on how the growth of the folded core is balanced by changes in density of the interface. Finally, we quantified the diffuse versus polarized structure of the critical nucleus through direct calculation of the packing fraction of the folded core and interface regions. Our results support the general picture of describing protein folding as the capillarity-like growth of folding nuclei.
q-bio.BM:Biomolecular structures are assemblies of emergent anisotropic building modules such as uniaxial helices or biaxial strands. We provide an approach to understanding a marginally compact phase of matter that is occupied by proteins and DNA. This phase, which is in some respects analogous to the liquid crystal phase for chain molecules, stabilizes a range of shapes that can be obtained by sequence-independent interactions occurring intra- and intermolecularly between polymeric molecules. We present a singularityfree self-interaction for a tube in the continuum limit and show that this results in the tube being positioned in the marginally compact phase. Our work provides a unified framework for understanding the building blocks of biomolecules.
q-bio.BM:The mean time required by a transcription factor (TF) or an enzyme to find a target in the nucleus is of prime importance for the initialization of transcription, gene activation or the start of DNA repair. We obtain new estimates for the mean search time when the TF or enzyme, confined to the cell nucleus, can switch from a one dimensional motion along the DNA and a free Brownian regime inside the crowded nucleus. We give analytical expressions for the mean time the particle stays bound to the DNA, $\tau_{DNA}$, and the mean time it diffuses freely, $\tau_{free}$. Contrary to previous results but in agreement with experimental data, we find a factor $\tau_{DNA} \approx 3.7 \tau_{free}$ for the Lac-I TF. The formula obtained for the time required to bind to a target site is found to be coherent with observed data. We also conclude that a higher DNA density leads to a more efficient search process.
q-bio.BM:We develop coarse-grained models that describe the dynamic encapsidation of functionalized nanoparticles by viral capsid proteins. We find that some forms of cooperative interactions between protein subunits and nanoparticles can dramatically enhance rates and robustness of assembly, as compared to the spontaneous assembly of subunits into empty capsids. For large core-subunit interactions, subunits adsorb onto core surfaces en masse in a disordered manner, and then undergo a cooperative rearrangement into an ordered capsid structure. These assembly pathways are unlike any identified for empty capsid formation. Our models can be directly applied to recent experiments in which viral capsid proteins assemble around the functionalized inorganic nanoparticles [Sun et al., Proc. Natl. Acad. Sci (2007) 104, 1354]. In addition, we discuss broader implications for understanding the dynamic encapsidation of single-stranded genomic molecules during viral replication and for developing multicomponent nanostructured materials.
q-bio.BM:The total conformational energy is assumed to consist of pairwise interaction energies between atoms or residues, each of which is expressed as a product of a conformation-dependent function (an element of a contact matrix, C-matrix) and a sequence-dependent energy parameter (an element of a contact energy matrix, E-matrix). Such pairwise interactions in proteins force native C-matrices to be in a relationship as if the interactions are a Go-like potential [N. Go, Annu. Rev. Biophys. Bioeng. 12. 183 (1983)] for the native C-matrix, because the lowest bound of the total energy function is equal to the total energy of the native conformation interacting in a Go-like pairwise potential. This relationship between C- and E-matrices corresponds to (a) a parallel relationship between the eigenvectors of the C- and E-matrices and a linear relationship between their eigenvalues, and (b) a parallel relationship between a contact number vector and the principal eigenvectors of the C- and E-matrices; the E-matrix is expanded in a series of eigenspaces with an additional constant term, which corresponds to a threshold of contact energy that approximately separates native contacts from non-native ones. These relationships are confirmed in 182 representatives from each family of the SCOP database by examining inner products between the principal eigenvector of the C-matrix, that of the E-matrix evaluated with a statistical contact potential, and a contact number vector. In addition, the spectral representation of C- and E-matrices reveals that pairwise residue-residue interactions, which depends only on the types of interacting amino acids but not on other residues in a protein, are insufficient and other interactions including residue connectivities and steric hindrance are needed to make native structures the unique lowest energy conformations.
q-bio.BM:We consider an elastic rod model for twisted DNA in the plectonemic regime. The molecule is treated as an impenetrable tube with an effective, adjustable radius. The model is solved analytically and we derive formulas for the contact pressure, twisting moment and geometrical parameters of the supercoiled region. We apply our model to magnetic tweezer experiments of a DNA molecule subjected to a tensile force and a torque, and extract mechanical and geometrical quantities from the linear part of the experimental response curve. These reconstructed values are derived in a self-contained manner, and are found to be consistent with those available in the literature.
q-bio.BM:Nicodemi and Prisco recently proposed a model for X-chromosome inactivation in mammals, explaining this phenomenon in terms of a spontaneous symmetry-breaking mechanism [{\it Phys. Rev. Lett.} 99 (2007), 108104]. Here we provide a mean-field version of their model.
q-bio.BM:We present a theoretical investigation on possible selection of olfactory receptors (ORs) as sensing components of nanobiosensors. Accordingly, we generate the impedance spectra of the rat OR I7 in the native and activated state and analyze their differences. In this way, we connect the protein morphological transformation, caused by the sensing action, with its change of electrical impedance. The results are compared with those obtained by studying the best known protein of the GPCR family: bovine rhodopsin. Our investigations indicate that a change in morphology goes with a change in impedance spectrum mostly associated with a decrease of the static impedance up to about 60 % of the initial value, in qualitative agreement with existing experiments on rat OR I7. The predictiveness of the model is tested successfully for the case of recent experiments on bacteriorhodopsin. The present results point to a promising development of a new class of nanobiosensors based on the electrical properties of GPCR and other sensing proteins.
q-bio.BM:How molecular motors like Kinesin regulates the affinity to the rail protein in the process of ATP hydrolysis remains to be uncovered. To understand the regulation mechanism, we investigate the structural fluctuation of KIF1A in different nucleotide states that are realized in the ATP hydrolysis process by molecular dynamics simulations of Go-like model. We found that "alpha4 helix", which is a part of the microtubule (MT) binding site, changes its fluctuation systematically according to the nucleotide states. In particular, the frequency of large fluctuations of alpha4 strongly correlates with the affinity of KIF1A for microtubule. We also show how the strength of the thermal fluctuation and the interaction with the nucleotide affect the dynamics of microtubule binding site. These results suggest that KIF1A regulates the affinity to MT by changing the flexibility of alpha4 helix according to the nucleotide states.
q-bio.BM:A theoretical framework is developed to study the dynamics of protein folding. The key insight is that the search for the native protein conformation is influenced by the rate r at which external parameters, such as temperature, chemical denaturant or pH, are adjusted to induce folding. A theory based on this insight predicts that (1) proteins with non-funneled energy landscapes can fold reliably to their native state, (2) reliable folding can occur as an equilibrium or out-of-equilibrium process, and (3) reliable folding only occurs when the rate r is below a limiting value, which can be calculated from measurements of the free energy. We test these predictions against numerical simulations of model proteins with a single energy scale.
q-bio.BM:Despite the spontaneity of some in vitro protein folding reactions, native folding in vivo often requires the participation of barrel-shaped multimeric complexes known as chaperonins. Although it has long been known that chaperonin substrates fold upon sequestration inside the chaperonin barrel, the precise mechanism by which confinement within this space facilitates folding remains unknown. In this study, we examine the possibility that the chaperonin mediates a favorable reorganization of the solvent for the folding reaction. We begin by discussing the effect of electrostatic charge on solvent-mediated hydrophobic forces in an aqueous environment. Based on these initial physical arguments, we construct a simple, phenomenological theory for the thermodynamics of density and hydrogen bond order fluctuations in liquid water. Within the framework of this model, we investigate the effect of confinement within a chaperonin-like cavity on the configurational free energy of water by calculating solvent free energies for cavities corresponding to the different conformational states in the ATP- driven catalytic cycle of the prokaryotic chaperonin GroEL. Our findings suggest that one function of chaperonins may be to trap unfolded proteins and subsequently expose them to a micro-environment in which the hydrophobic effect, a crucial thermodynamic driving force for folding, is enhanced.
q-bio.BM:We present a top-down approach to the study of the dynamics of icosahedral virus capsids, in which each protein is approximated by a point mass. Although this represents a rather crude coarse-graining, we argue that it highlights several generic features of vibrational spectra which have been overlooked so far. We furthermore discuss the consequences of approximate inversion symmetry as well as the role played by Viral Tiling Theory in the study of virus capsid vibrations.
q-bio.BM:We study the coupled dynamics of primary and secondary structure formation (i.e. slow genetic sequence selection and fast folding) in the context of a solvable microscopic model that includes both short-range steric forces and and long-range polarity-driven forces. Our solution is based on the diagonalization of replicated transfer matrices, and leads in the thermodynamic limit to explicit predictions regarding phase transitions and phase diagrams at genetic equilibrium. The predicted phenomenology allows for natural physical interpretations, and finds satisfactory support in numerical simulations.
q-bio.BM:We analyze the thermodynamic properties of a simplified model for folded RNA molecules recently studied by G. Vernizzi, H. Orland, A. Zee (in {\it Phys. Rev. Lett.} {\bf 94} (2005) 168103). The model consists of a chain of one-flavor base molecules with a flexible backbone and all possible pairing interactions equally allowed. The spatial pseudoknot structure of the model can be efficiently studied by introducing a $N \times N$ hermitian random matrix model at each chain site, and associating Feynman diagrams of these models to spatial configurations of the molecules. We obtain an exact expression for the topological expansion of the partition function of the system. We calculate exact and asymptotic expressions for the free energy, specific heat, entanglement and chemical potential and study their behavior as a function of temperature. Our results are consistent with the interpretation of $1/N$ as being a measure of the concentration of $\rm{Mg}^{++}$ in solution.
q-bio.BM:Blueprints of polyhedral cages with icosahedral symmetry made of circular DNA molecules are provided. The basic rule is that every edge of the cage is met twice in opposite directions by the DNA strand, and vertex junctions are realised by a set of admissible junction types. As nanocontainers for cargo storage and delivery, the icosidodecahedral cages are of special interest as they have the largest volume per surface ratio of all cages discussed here.
q-bio.BM:Group theoretical arguments combined with normal mode analysis techniques are applied to a coarse-grained approximation of icosahedral viral capsids which incorporates areas of variable flexibility. This highlights a remarkable structure of the low-frequency spectrum in this approximation, namely the existence of a plateau of 24 near zero-modes with universal group theory content.
q-bio.BM:The low-frequency Raman spectra of Na-and Cs-DNA water solutions have been studied to determine the mode of counterion vibrations with respect to phosphate groups of the DNA double helix. The obtained spectra are characterized by the water band near 180 cm-1 and by several DNA bands near 100 cm-1. The main difference between Na- and Cs-DNA spectra is observed in case of the band 100 cm-1. In Cs-DNA spectra this band has about twice higher intensity than in Na-DNA spectra. The comparison of obtained spectra with the calculated frequencies of Na- and Cs-DNA conformational vibrations [Perepelytsya S.M., Volkov S.N. Eur. Phys. J. E. 24, 261 (2007)] show that the band 100 cm-1 in the spectra of Cs-DNA is formed by the modes of both H-bond stretching vibrations and vibrations of caesium counterions, while in Na-DNA spectra the band 100 cm-1 is formed by the mode of H-bond stretching vibrations only. The modes of sodium counterion vibrations have a frequency 180 cm-1, and they do not rise above the water band. Thus, the increase in intensity of the band 100 cm-1 in the spectra of Cs-DNA as compared with Na-DNA is caused by the mode of ion-phosphate vibrations.
q-bio.BM:The Ca-sensitive regulatory switch of cardiac muscle is a paradigmatic example of protein assemblies that communicate ligand binding through allosteric change. The switch is a dimeric complex of troponin C (TnC), an allosteric sensor for Ca, and troponin I (TnI), an allosteric reporter. Time-resolved equilibrium FRET measurements suggest that the switch activates in two steps: a TnI-independent Ca-priming step followed by TnI-dependent opening. To resolve the mechanistic role of TnI in activation we performed stopped-flow FRET measurements of activation following rapid addition of a lacking component (Ca or TnI) and deactivation following rapid chelation of Ca. The time-resolved measurements, stopped-flow measurements, and Ca-titration measurements were globally analyzed in terms of a new quantitative dynamic model of TnC-TnI allostery. The analysis provided a mesoscopic parameterization of distance changes, free energy changes, and transition rates among the accessible coarse-grained states of the system. The results reveal (i) the Ca-induced priming step, which precedes opening, is the rate limiting step in activation, (ii) closing is the rate limiting step in deactivation, (iii) TnI induces opening, (iv) an incompletely deactivated population when regulatory Ca is not bound, which generates an accessory pathway of activation, and (v) incomplete activation by Ca--when regulatory Ca is bound, a 3:2 mixture of dynamically inter-converting open (active) and primed-closed (partially active) conformers is observed (15 C). Temperature-dependent stopped-flow FRET experiments provide a near complete thermo-kinetic parametrization of opening. <Abstract Truncated>
q-bio.BM:We explore the use of a top-down approach to analyse the dynamics of icosahedral virus capsids and complement the information obtained from bottom-up studies of viral vibrations available in the literature. A normal mode analysis based on protein association energies is used to study the frequency spectrum, in which we reveal a universal plateau of low-frequency modes shared by a large class of Caspar-Klug capsids. These modes break icosahedral symmetry and are potentially relevant to the genome release mechanism. We comment on the role of viral tiling theory in such dynamical considerations.
q-bio.BM:In many cases, transcriptional regulation involves the binding of transcription factors at sites on the DNA that are not immediately adjacent to the promoter of interest. This action at a distance is often mediated by the formation of DNA loops: Binding at two or more sites on the DNA results in the formation of a loop, which can bring the transcription factor into the immediate neighborhood of the relevant promoter. Though there have been a variety of insights into the combinatorial aspects of transcriptional control, the mechanism of DNA looping as an agent of combinatorial control in both prokaryotes and eukaryotes remains unclear. We use single-molecule techniques to dissect DNA looping in the lac operon. In particular, we measure the propensity for DNA looping by the Lac repressor as a function of the concentration of repressor protein and as a function of the distance between repressor binding sites. As with earlier single-molecule studies, we find (at least) two distinct looped states and demonstrate that the presence of these two states depends both upon the concentration of repressor protein and the distance between the two repressor binding sites. We find that loops form even at interoperator spacings considerably shorter than the DNA persistence length, without the intervention of any other proteins to prebend the DNA. The concentration measurements also permit us to use a simple statistical mechanical model of DNA loop formation to determine the free energy of DNA looping, or equivalently, the J-factor for looping.
q-bio.BM:We apply a simulational proxy of the phi-value analysis and perform extensive mutagenesis experiments to identify the nucleating residues in the folding reactions of two small lattice Go polymers with different native geometries. These results are compared with those obtained from an accurate analysis based on the reaction coordinate folding probability Pfold, and on structural clustering methods. For both protein models, the transition state ensemble is rather heterogeneous and splits-up into structurally different populations. For the more complex geometry the identified subpopulations are actually structurally disjoint. For the less complex native geometry we found a broad transition state with microscopic heterogeneity. For both geometries, the identification of the folding nucleus via the Pfold analysis agrees with the identification of the folding nucleus carried out with the phi-value analysis. For the most complex geometry, however, the apllied methodologies give more consistent results than for the more local geometry. The study of the transition state' structure reveals that the nucleus residues are not necessarily fully native in the transition state. Indeed, it is only for the more complex geometry that two of the five critical residues show a considerably high probability of having all its native bonds formed in the transition state. Therefore, one concludes that in general the phi-value correlates with the acceleration/deceleration of folding induced by mutation, rather than with the degree of nativeness of the transition state, and that the traditional interpretation of phi-values may provide a more realistic picture of the structure of the transition state only for more complex native geometries.
q-bio.BM:Associative memory Hamiltonian structure prediction potentials are not overly rugged, thereby suggesting their landscapes are like those of actual proteins. In the present contribution we show how basin-hopping global optimization can identify low-lying minima for the corresponding mildly frustrated energy landscapes. For small systems the basin-hopping algorithm succeeds in locating both lower minima and conformations closer to the experimental structure than does molecular dynamics with simulated annealing. For large systems the efficiency of basin-hopping decreases for our initial implementation, where the steps consist of random perturbations to the Cartesian coordinates. We implemented umbrella sampling using basin-hopping to further confirm when the global minima are reached. We have also improved the energy surface by employing bioinformatic techniques for reducing the roughness or variance of the energy surface. Finally, the basin-hopping calculations have guided improvements in the excluded volume of the Hamiltonian, producing better structures. These results suggest a novel and transferable optimization scheme for future energy function development.
q-bio.BM:The performance of single folding predictors and combination scores is critically evaluated. We test mean packing, mean pairwise energy and the new index gVSL2 on a dataset of 743 folded proteins and 81 natively unfolded proteins. These predictors have an individual performance comparable or even better than other proposed methods. We introduce here a strictly unanimous score S_{SU} that combines them but leaves undecided those sequences differently classified by two single predictors. The performance of the single predictors on a dataset purged from the proteins left unclassified by S_{SU}, significantly increases, indicating that unclassified proteins are mainly false predictions. Amino acid composition is the main determinant considered by these predictors, therefore unclassified proteins have a composition compatible with both folded and unfolded status. This is why purging a dataset from these ambiguous proteins increases the performance of single predictors. The percentage of proteins predicted as natively unfolded by S_{SU} in the three kingdoms are: 4.1% for Bacteria, 1.0% for Archaea and 20.0% for Eukarya; compatible with previous determinations. Evidence is given of a scaling law relating the number of natively unfolded proteins with the total number of proteins in a genome; a first estimate of the critical exponent is 1.95 +- 0.21
q-bio.BM:The sequence-dependent structural variability and conformational dynamics of DNA play pivotal roles in many biological milieus, such as in the site-specific binding of transcription factors to target regulatory elements. To better understand DNA structure, function, and dynamics in general, and protein-DNA recognition in the 'kB' family of genetic regulatory elements in particular, we performed molecular dynamics simulations of a 20-base pair DNA encompassing a cognate kB site recognized by the proto-oncogenic 'c-Rel' subfamily of NF-kB transcription factors. Simulations of the kB DNA in explicit water were extended to microsecond duration, providing a broad, atomically-detailed glimpse into the structural and dynamical behavior of double helical DNA over many timescales. Of particular note, novel (and structurally plausible) conformations of DNA developed only at the long times sampled in this simulation -- including a peculiar state arising at ~ 0.7 us and characterized by cross-strand intercalative stacking of nucleotides within a longitudinally-sheared base pair, followed (at ~ 1 us) by spontaneous base flipping of a neighboring thymine within the A-rich duplex. Results and predictions from the us-scale simulation include implications for a dynamical NF-kB recognition motif, and are amenable to testing and further exploration via specific experimental approaches that are suggested herein.
q-bio.BM:The interaction cutoff contribution to the ruggedness of protein-protein energy landscape (the artificial ruggedness) is studied in terms of relative energy fluctuations for 1/r^n potentials based on a simplistic model of a protein complex. Contradicting the principle of minimal frustration, the artificial ruggedness exists for short cutoffs and gradually disappears with the cutoff increase. The critical values of the cutoff were calculated for each of eleven popular power-type potentials with n=0-9, 12 and for two thresholds of 5% and 10%. The artificial ruggedness decreases to tolerable thresholds for cutoffs longer than the critical ones. The results showed that for both thresholds the critical cutoff is a non-monotonic function of the potential power n. The functions reach the maximum at n=3-4 and then decrease with the increase of the potential power. The difference between two cutoffs for 5% and 10% artificial ruggedness becomes negligible for potentials decreasing faster than 1/r^12. The results suggest that cutoffs longer than critical ones can be recommended for protein-protein potentials.
q-bio.BM:Mechanical characterization of protein molecules has played a role on gaining insight into the biological functions of proteins, since some proteins perform the mechanical function. Here, we present the mesoscopic model of biological protein materials composed of protein crystals prescribed by Go potential for characterization of elastic behavior of protein materials. Specifically, we consider the representative volume element (RVE) containing the protein crystals represented by alpha-carbon atoms, prescribed by Go potential, with application of constant normal strain to RVE. The stress-strain relationship computed from virial stress theory provides the nonlinear elastic behavior of protein materials and their mechanical properties such as Young's modulus, quantitatively and/or qualitatively comparable to mechanical properties of biological protein materials obtained from experiments and/or atomistic simulations. Further, we discuss the role of native topology on the mechanical properties of protein crystals. It is shown that parallel strands (hydrogen bonds in parallel) enhances the mechanical resilience of protein materials.
q-bio.BM:Unfolded proteins may contain native or non-native residual structure, which has important implications for the thermodynamics and kinetics of folding as well as for misfolding and aggregation diseases. However, it has been universally accepted that residual structure should not affect the global size scaling of the denatured chain, which obeys the statistics of random coil polymers. Here we use a single-molecule optical technique, fluorescence correlation spectroscopy, to probe the denatured state of set of repeat proteins containing an increasing number of identical domains, from two to twenty. The availability of this set allows us to obtain the scaling law for the unfolded state of these proteins, which turns out to be unusually compact, strongly deviating from random-coil statistics. The origin of this unexpected behavior is traced to the presence of extensive non-native polyproline II helical structure, which we localize to specific segments of the polypeptide chain. We show that the experimentally observed effects of PPII on the size scaling of the denatured state can be well-described by simple polymer models. Our findings suggest an hitherto unforeseen potential of non-native structure to induce significant compaction of denatured proteins, affecting significantly folding pathways and kinetics.
q-bio.BM:Renaturation and hybridization reactions lead to the pairing of complementary single-stranded nucleic acids. We present here a theoretical investigation of the mechanism of these reactions in vitro under thermal conditions (dilute solutions of single-stranded chains, in the presence of molar concentrations of monovalent salts and at elevated temperatures). The mechanism follows a Kramers' process, whereby the complementary chains overcome a potential barrier through Brownian motion. The barrier originates from a single rate-limiting nucleation event in which the first complementary base pairs are formed. The reaction then proceeds through a fast growth of the double helix. For the DNA of bacteriophages T7, T4 and $\phi$X174 as well as for Escherichia coli DNA, the bimolecular rate $k_2$ of the reaction increases as a power law of the average degree of polymerization $<N>$ of the reacting single- strands: $k_2 \prop <N>^\alpha$. This relationship holds for $100 \leq <N> \leq 50 000$ with an experimentally determined exponent $\alpha = 0.51 \pm 0.01$. The length dependence results from a thermodynamic excluded-volume effect. The reacting single-stranded chains are predicted to be in universal good solvent conditions, and the scaling law is determined by the relevant equilibrium monomer contact probability. The value theoretically predicted for the exponent is $\alpha = 1-\nu \theta_2$, where $\nu$ is Flory's swelling exponent ($nu approx 0.588$) and $\theta_2$ is a critical exponent introduced by des Cloizeaux ($\theta_2 \approx 0.82$), yielding $\alpha = 0.52 \pm 0.01$, in agreement with the experimental results.
q-bio.BM:This article is interested in the origin of the genetic code, it puts forward a scenario of a simultaneous selection of the bases and amino acids and setting up of a correlation between them. Each amino acid is associated with a pair of its own kind, called the binding pair and each binding pair is associated with the codon(s) corresponding to the same amino acid. An explanation is also proposed about the origin of the start and stop codons.
q-bio.BM:We develop equilibrium and kinetic theories that describe the assembly of viral capsid proteins on a charged central core, as seen in recent experiments in which brome mosaic virus (BMV) capsids assemble around nanoparticles functionalized with polyelectrolyte. We model interactions between capsid proteins and nanoparticle surfaces as the interaction of polyelectrolyte brushes with opposite charge, using the nonlinear Poisson Boltzmann equation. The models predict that there is a threshold density of functionalized charge, above which capsids efficiently assemble around nanoparticles, and that light scatter intensity increases rapidly at early times, without the lag phase characteristic of empty capsid assembly. These predictions are consistent with, and enable interpretation of, preliminary experimental data. However, the models predict a stronger dependence of nanoparticle incorporation efficiency on functionalized charge density than measured in experiments, and do not completely capture a logarithmic growth phase seen in experimental light scatter. These discrepancies may suggest the presence of metastable disordered states in the experimental system. In addition to discussing future experiments for nanoparticle-capsid systems, we discuss broader implications for understanding assembly around charged cores such as nucleic acids.
q-bio.BM:The binding of a ligand molecule to a protein is often accompanied by conformational changes of the protein. A central question is whether the ligand induces the conformational change (induced-fit), or rather selects and stabilizes a complementary conformation from a pre-existing equilibrium of ground and excited states of the protein (selected-fit). We consider here the binding kinetics in a simple four-state model of ligand-protein binding. In this model, the protein has two conformations, which can both bind the ligand. The first conformation is the ground state of the protein when the ligand is off, and the second conformation is the ground state when the ligand is bound. The induced-fit mechanism corresponds to ligand binding in the unbound ground state, and the selected-fit mechanism to ligand binding in the excited state. We find a simple, characteristic difference between the on- and off-rates in the two mechanisms if the conformational relaxation into the ground states is fast. In the case of selected-fit binding, the on-rate depends on the conformational equilibrium constant, while the off-rate is independent. In the case of induced-fit binding, in contrast, the off-rate depends on the conformational equilibrium, while the on-rate is independent. Whether a protein binds a ligand via selected-fit or induced-fit thus may be revealed by mutations far from the protein's binding pocket, or other "perturbations" that only affect the conformational equilibrium. In the case of selected-fit, such mutations will only change the on-rate, and in the case of induced-fit, only the off-rate.
q-bio.BM:We study a matrix model of RNA in which an external perturbation acts on n nucleotides of the polymer chain. The effect of the perturbation appears in the exponential generating function of the partition function as a factor $(1-\frac{n\alpha}{L})$ [where $\alpha$ is the ratio of strengths of the original to the perturbed term and L is length of the chain]. The asymptotic behaviour of the genus distribution functions for the extended matrix model are analyzed numerically when (i) $n=L$ and (ii) $n=1$. In these matrix models of RNA, as $n\alpha/L$ is increased from 0 to 1, it is found that the universality of the number of diagrams $a_{L, g}$ at a fixed length L and genus g changes from $3^{L}$ to $(3-\frac{n\alpha}{L})^{L}$ ($2^{L}$ when $n\alpha/L=1$) and the asymptotic expression of the total number of diagrams $\cal N$ at a fixed length L but independent of genus g, changes in the factor $\exp^{\sqrt{L}}$ to $\exp^{(1-\frac{n\alpha}{L})\sqrt{L}}$ ($exp^{0}=1$ when $n\alpha/L=1$)
q-bio.BM:In protein folding the term plasticity refers to the number of alternative folding pathways encountered in response to free energy perturbations such as those induced by mutation. Here we explore the relation between folding plasticity and a gross, generic feature of the native geometry, namely, the relative number of local and non-local native contacts. The results from our study, which is based on Monte Carlo simulations of simple lattice proteins, show that folding to a structure that is rich in local contacts is considerably more plastic than folding to a native geometry characterized by having a very large number of long-range contacts (i.e., contacts between amino acids that are separated by more than 12 units of backbone distance). The smaller folding plasticity of `non-local' native geometries is probably a direct consequence of their higher folding cooperativity that renders the folding reaction more robust against single- and multiple-point mutations.
q-bio.BM:Significant overweight represents a major health problem in industrialized countries. Besides its known metabolic origins, this condition may also have an infectious cause, as recently postulated. Here, it is surmised that the potentially causative adenovirus 36 contributes to such disorder by inactivating the retinoblastoma tumor suppressor protein (RB) in a manner reminiscent of a mechanism employed by both another pathogenic adenoviral agent and insulin. The present insight additionally suggests novel modes of interfering with obesity-associated pathology.
q-bio.BM:Biological forces govern essential cellular and molecular processes in all living organisms. Many cellular forces, e.g. those generated in cyclic conformational changes of biological machines, have repetitive components. However, little is known about how proteins process repetitive mechanical stresses. To obtain first insights into dynamic protein mechanics, we probed the mechanical stability of single and multimeric ubiquitins perturbed by periodic forces. Using coarse-grained molecular dynamics simulations, we were able to model repetitive forces with periods about two orders of magnitude longer than the relaxation time of folded ubiquitins. We found that even a small periodic force weakened the protein and shifted its unfolding pathways in a frequency- and amplitude-dependent manner. Our results also showed that the dynamic response of even a small protein can be complex with transient refolding of secondary structures and an increasing importance of local interactions in asymmetric protein stability. These observations were qualitatively and quantitatively explained using an energy landscape model and discussed in the light of dynamic single-molecule measurements and physiological forces. We believe that our approach and results provide first steps towards a framework to better understand dynamic protein biomechanics and biological force generation.
q-bio.BM:It is a standard exercise in mechanical engineering to infer the external forces and torques on a body from its static shape and known elastic properties. Here we apply this kind of analysis to distorted double-helical DNA in complexes with proteins. We extract the local mean forces and torques acting on each base-pair of bound DNA from high-resolution complex structures. Our method relies on known elastic potentials and a careful choice of coordinates of the well-established rigid base-pair model of DNA. The results are robust with respect to parameter and conformation uncertainty. They reveal the complex nano-mechanical patterns of interaction between proteins and DNA. Being non-trivially and non-locally related to observed DNA conformations, base-pair forces and torques provide a new view on DNA-protein binding that complements structural analysis.
q-bio.BM:Molecular dynamics studies within a coarse-grained structure based model were used on two similar proteins belonging to the transcarbamylase family to probe the effects in the native structure of a knot. The first protein, N-acetylornithine transcarbamylase, contains no knot whereas human ormithine transcarbamylase contains a trefoil knot located deep within the sequence. In addition, we also analyzed a modified transferase with the knot removed by the appropriate change of a knot-making crossing of the protein chain. The studies of thermally- and mechanically-induced unfolding processes suggest a larger intrinsic stability of the protein with the knot.
q-bio.BM:Comprehensive knowledge of protein-ligand interactions should provide a useful basis for annotating protein functions, studying protein evolution, engineering enzymatic activity, and designing drugs. To investigate the diversity and universality of ligand binding sites in protein structures, we conducted the all-against-all atomic-level structural comparison of over 180,000 ligand binding sites found in all the known structures in the Protein Data Bank by using a recently developed database search and alignment algorithm. By applying a hybrid top-down-bottom-up clustering analysis to the comparison results, we determined approximately 3000 well-defined structural motifs of ligand binding sites. Apart from a handful of exceptions, most structural motifs were found to be confined within single families or superfamilies, and to be associated with particular ligands. Furthermore, we analyzed the components of the similarity network and enumerated more than 4000 pairs of ligand binding sites that were shared across different protein folds.
q-bio.BM:Protein electrostatic states have been demonstrated to play crucial roles in catalysis, ligand binding, protein stability, and in the modulation of allosteric effects. Electrostatic states are demonstrated to appear conserved among DEAD-box motifs and evidence is presented that the structural changes that occur to DEAD box proteins upon ligand binding alter the DEAD-box motif electrostatics in a way the facilitates the catalytic role of the DEAD-box glutatmate.
q-bio.BM:ESPSim is an open source JAVA program that enables the comparisons of protein electrostatic potential maps via the computation of an electrostatic similarity measure. This program has been utilized to demonstrate a high degree of electrostatic similarity among the potential maps of lysozyme proteins, suggesting that protein electrostatic states are conserved within lysozyme proteins. ESPSim is freely available under the AGPL License from http://www.bioinformatics.org/project/?group_id=830
q-bio.BM:Protein electrostatics have been demonstrated to play a vital role in protein functionality, with many functionally important amino acid residues exhibiting an electrostatic state that is altered from that of a normal amino acid residue. Residues with altered electrostatic states can be identified by the presence of a pKa value that is perturbed by 2 or more pK units, and such residues have been demonstrated to play critical roles in catalysis, ligand binding, and protein stability. Within the HCV helicase and polymerase, as well as the HIV reverse transcriptase, highly conserved regions were demonstrated to possess a greater number and magnitude of perturbations than lesser conserved regions, suggesting that there is an interrelationship present between protein electrostatics and evolution.
q-bio.BM:Protein dynamics in cells may be different from that in dilute solutions in vitro since the environment in cells is highly concentrated with other macromolecules. This volume exclusion due to macromolecular crowding is predicted to affect both equilibrium and kinetic processes involving protein conformational changes. To quantify macromolecular crowding effects on protein folding mechanisms, here we have investigated the folding energy landscape of an alpha/beta protein, apoflavodoxin, in the presence of inert macromolecular crowding agents using in silico and in vitro approaches. By coarse-grained molecular simulations and topology-based potential interactions, we probed the effects of increased volume fraction of crowding agents (phi_c) as well as of crowding agent geometry (sphere or spherocylinder) at high phi_c. Parallel kinetic folding experiments with purified Desulfovibro desulfuricans apoflavodoxin in vitro were performed in the presence of Ficoll (sphere) and Dextran (spherocylinder) synthetic crowding agents. In conclusion, we have identified in silico crowding conditions that best enhance protein stability and discovered that upon manipulation of the crowding conditions, folding routes experiencing topological frustrations can be either enhanced or relieved. The test-tube experiments confirmed that apoflavodoxin's time-resolved folding path is modulated by crowding agent geometry. We propose that macromolecular crowding effects may be a tool for manipulation of protein folding and function in living cells.
q-bio.BM:The force generated between actin and myosin acts predominantly along the direction of the actin filament, resulting in relative sliding of the thick and thin filaments in muscle or transport of myosin cargos along actin tracks. Previous studies have also detected lateral forces or torques that are generated between actin and myosin, but the origin and biological role of these sideways forces is not known. Here we adapt an actin gliding filament assay in order to measure the rotation of an actin filament about its axis (twirling) as it is translocated by myosin. We quantify the rotation by determining the orientation of sparsely incorporated rhodamine-labeled actin monomers, using polarized total internal reflection (polTIRF) microscopy. In order to determine the handedness of the filament rotation, linear incident polarizations in between the standard s- and p-polarizations were generated, decreasing the ambiguity of our probe orientation measurement four-fold. We found that whole myosin II and myosin V both twirl actin with a relatively long (micron), left-handed pitch that is insensitive to myosin concentration, filament length and filament velocity.
q-bio.BM:Most of the theoretical models describing the translocation of a polymer chain through a nanopore use the hypothesis that the polymer is always relaxed during the complete process. In other words, models generally assume that the characteristic relaxation time of the chain is small enough compared to the translocation time that non-equilibrium molecular conformations can be ignored. In this paper, we use Molecular Dynamics simulations to directly test this hypothesis by looking at the escape time of unbiased polymer chains starting with different initial conditions. We find that the translocation process is not quite in equilibrium for the systems studied, even though the translocation time tau is about 10 times larger than the relaxation time tau_r. Our most striking result is the observation that the last half of the chain escapes in less than ~12% of the total escape time, which implies that there is a large acceleration of the chain at the end of its escape from the channel.
q-bio.BM:We present a self-contained theory for the mechanical response of DNA in single molecule experiments. Our model is based on a 1D continuum description of the DNA molecule and accounts both for its elasticity and for DNA-DNA electrostatic interactions. We consider the classical loading geometry used in experiments where one end of the molecule is attached to a substrate and the other one is pulled by a tensile force and twisted by a given number of turns. We focus on configurations relevant to the limit of a large number of turns, which are made up of two phases, one with linear DNA and the other one with superhelical DNA. The model takes into account thermal fluctuations in the linear phase and electrostatic interactions in the superhelical phase. The values of the torsional stress, of the supercoiling radius and angle, and key features of the experimental extension-rotation curves, namely the slope of the linear region and thermal buckling threshold, are predicted. They are found in good agreement with experimental data.
q-bio.BM:While slowly turning the ends of a single molecule of DNA at constant applied force, a discontinuity was recently observed at the supercoiling transition, when a small plectoneme is suddenly formed. This can be understood as an abrupt transition into a state in which stretched and plectonemic DNA coexist. We argue that there should be discontinuities in both the extension and the torque at the transition, and provide experimental evidence for both. To predict the sizes of these discontinuities and how they change with the overall length of DNA, we organize a theory for the coexisting plectonemic state in terms of four length-independent parameters. We also test plectoneme theories, including our own elastic rod simulation, finding discrepancies with experiment that can be understood in terms of the four coexisting state parameters.
q-bio.BM:We introduce a simple "patchy particle" model to study the thermodynamics and dynamics of self-assembly of homomeric protein complexes. Our calculations allow us to rationalize recent results for dihedral complexes. Namely, why evolution of such complexes naturally takes the system into a region of interaction space where (i) the evolutionarily newer interactions are weaker, (ii) subcomplexes involving the stronger interactions are observed to be thermodynamically stable on destabilization of the protein-protein interactions and (iii) the self-assembly dynamics are hierarchical with these same subcomplexes acting as kinetic intermediates.
q-bio.BM:We perform molecular dynamics simulations for a simple coarse-grained model of crambin placed inside of a softly repulsive sphere of radius R. The confinement makes folding at the optimal temperature slower and affects the folding scenarios, but both effects are not dramatic. The influence of crowding on folding are studied by placing several identical proteins within the sphere, denaturing them, and then by monitoring refolding. If the interactions between the proteins are dominated by the excluded volume effects, the net folding times are essentially like for a single protein. An introduction of inter-proteinic attractive contacts hinders folding when the strength of the attraction exceeds about a half of the value of the strength of the single protein contacts. The bigger the strength of the attraction, the more likely is the occurrence of aggregation and misfolding.
q-bio.BM:We have developed a new simulation method to estimate the distance between the native state and the first transition state, and the distance between the intermediate state and the second transition state of a protein which mechanically unfolds via intermediates. Assuming that the end-to-end extension $\Delta R$ is a good reaction coordinate to describe the free energy landscape of proteins subjected to an external force, we define the midpoint extension $\Delta R^*$ between two transition states from either constant-force or constant loading rate pulling simulations. In the former case, $\Delta R^*$ is defined as a middle point between two plateaus in the time-dependent curve of $\Delta R$, while, in the latter one, it is a middle point between two peaks in the force-extension curve. Having determined $\Delta R^*$, one can compute times needed to cross two transition state barriers starting from the native state. With the help of the Bell and microscopic kinetic theory, force dependencies of these unfolding times can be used to locate the intermediate state and to extract unfolding barriers. We have applied our method to the titin domain I27 and the fourth domain of {\em Dictyostelium discoideum} filamin (DDFLN4), and obtained reasonable agreement with experiments, using the C$_{\alpha}$-Go model.
q-bio.BM:The protective effect exerted by polyamines (Put and Spd) against cadmium (Cd) stress was investigated in Brassica juncea plants. Treatment with CdCl2 (75 micro-Mole) resulted in a rise of Cd accumulation, a decrease of fresh and dry weights in every plant organ, an increase of free polyamine content at limb and stem levels as well as a decrease at root level. On the other hand, the total conjugated polyamine levels in the stem tissues were unaffected by Cd. In the leaf tissues, this metal caused a reduction of chlorophyll a content, a rise of guaiacol peroxidase (GPOX) activity and an increase of malondialdehyde (MDA), soluble glucide, proline and amino acid contents. Exogenous application, by spraying, of putrescine (Put) and spermidine (Spd) to leaf tissues reduced CdCl2-induced stress. These polyamines proved to exert a partial, though significant, protection of the foliar fresh weight and to alleviate the oxidative stress generated by Cd through reductions of MDA amounts and GPOX (E.C.1.11.1.7) activity. The enhancement of chlorophyll a content in plants by Put and those of Chl a and Chl b by Spd both constitute evidences of their efficacy against the Cd2+-induced loss of pigments. Conversely to Put, Spd caused a decrease of Cd content in leave tissues and a rise in the stems and roots; these findings are in favour of a stimulation of Cd uptake by Spd. The proline stimulation observed with Cd was reduced further to the spraying of Put onto tissues, but the decrease induced by Spd was more limited. In the plants treated with Cd, the amino acid contents in the leaves were unaffected by Put and Spd spraying; on the other hand, Cd2+ disturbed polyamine levels (free and acido-soluble conjugated-forms); we notice the rise of total free PAs and the decrease of their conjugated-ones.
q-bio.BM:Three coarse-grained models of the double-stranded DNA are proposed and compared in the context of mechanical manipulation such as twisting and various schemes of stretching. The models differ in the number of effective beads (between two and five) representing each nucleotide. They all show similar behavior and, in particular, lead to a torque-force phase diagrams qualitatively consistent with experiments and all-atom simulations.
q-bio.BM:We incorporate hydrodynamic interactions (HI) in a coarse-grained and structure-based model of proteins by employing the Rotne-Prager hydrodynamic tensor. We study several small proteins and demonstrate that HI facilitate folding. We also study HIV-1 protease and show that HI make the flap closing dynamics faster. The HI are found to affect time correlation functions in the vicinity of the native state even though they have no impact on same time characteristics of the structure fluctuations around the native state.
q-bio.BM:Peptides and proteins exhibit a common tendency to assemble into highly ordered fibrillar aggregates, whose formation proceeds in a nucleation-dependent manner that is often preceded by the formation of disordered oligomeric assemblies. This process has received much attention because disordered oligomeric aggregates have been associated with neurodegenerative disorders such as Alzheimer's and Parkinson's diseases. Here we describe a self-templated nucleation mechanism that determines the transition between the initial condensation of polypeptide chains into disordered assemblies and their reordering into fibrillar structures. The results that we present show that at the molecular level this transition is due to the ability of polypeptide chains to reorder within oligomers into fibrillar assemblies whose surfaces act as templates that stabilise the disordered assemblies.
q-bio.BM:The presence of oligomeric aggregates, which is often observed during the process of amyloid formation, has recently attracted much attention since it has been associated with neurodegenerative conditions such as Alzheimer's and Parkinson's diseases. We provide a description of a sequence-indepedent mechanism by which polypeptide chains aggregate by forming metastable oligomeric intermediate states prior to converting into fibrillar structures. Our results illustrate how the formation of ordered arrays of hydrogen bonds drives the formation of beta-sheets within the disordered oligomeric aggregates that form early under the effect of hydrophobic forces. Initially individual beta-sheets form with random orientations, which subsequently tend to align into protofilaments as their lengths increases. Our results suggest that amyloid aggregation represents an example of the Ostwald step rule of first order phase transitions by showing that ordered cross-beta structures emerge preferentially from disordered compact dynamical intermediate assemblies.
q-bio.BM:It has been recently argued that the depletion attraction may play an important role in different aspects of the cellular organization, ranging from the organization of transcriptional activity in transcription factories to the formation of the nuclear bodies. In this paper we suggest a new application of these ideas in the context of the splicing process, a crucial step of messanger RNA maturation in Eukaryotes. We shall show that entropy effects and the resulting depletion attraction may explain the relevance of the aspecific intron length variable in the choice of the splice-site recognition modality. On top of that, some qualitative features of the genome architecture of higher Eukaryotes can find an evolutionary realistic motivation in the light of our model.
q-bio.BM:Translocation through a nanopore is a new experimental technique to probe physical properties of biomolecules. A bulk of theoretical and computational work exists on the dependence of the time to translocate a single unstructured molecule on the length of the molecule. Here, we study the same problem but for RNA molecules for which the breaking of the secondary structure is the main barrier for translocation. To this end, we calculate the mean translocation time of single-stranded RNA through a nanopore of zero thickness and at zero voltage for many randomly chosen RNA sequences. We find the translocation time to depend on the length of the RNA molecule with a power law. The exponent changes as a function of temperature and exceeds the naively expected exponent of two for purely diffusive transport at all temperatures. We interpret the power law scaling in terms of diffusion in a one-dimensional energy landscape with a logarithmic barrier.
q-bio.BM:Cellular cargo can be bound to cytoskeletal filaments by one or multiple active or passive molecular motors. Recent experiments have shown that the presence of auxiliary, nondriving motors, results in an enhanced processivity of the cargo, compared to the case of a single active motor alone. We model the observed cooperative transport process using a stochastic model that describes the dynamics of two molecular motors, an active one that moves cargo unidirectionally along a filament track and a passive one that acts as a tether. Analytical expressions obtained from our analysis are fit to experimental data to estimate the microscopic kinetic parameters of our model. Our analysis reveals two qualitatively distinct processivity-enhancing mechanisms: the passive tether can decrease the typical detachment rate of the active motor from the filament track or it can increase the corresponding reattachment rate. Our estimates unambiguously show that in the case of microtubular transport, a higher average run length arises mainly from the ability of the passive motor to keep the cargo close to the filament, enhancing the reattachment rate of an active kinesin motor that has recently detached. Instead, for myosin-driven transport along actin, the passive motor tightly tethers the cargo to the filament, suppressing the detachment rate of the active myosin.
q-bio.BM:FoF1-ATP synthase is the enzyme that provides the 'chemical energy currency' adenosine triphosphate, ATP, for living cells. The formation of ATP is accomplished by a stepwise internal rotation of subunits within the enzyme. Briefly, proton translocation through the membrane-bound Fo part of ATP synthase drives a 10-step rotary motion of the ring of c subunits with respect to the non-rotating subunits a and b. This rotation is transmitted to the gamma and epsilon subunits of the F1 sector resulting in 120 degree steps. In order to unravel this symmetry mismatch we monitor subunit rotation by a single-molecule fluorescence resonance energy transfer (FRET) approach using three fluorophores specifically attached to the enzyme: one attached to the F1 motor, another one to the Fo motor, and the third one to a non-rotating subunit. To reduce photophysical artifacts due to spectral fluctuations of the single fluorophores, a duty cycle-optimized alternating three-laser scheme (DCO-ALEX) has been developed. Simultaneous observation of the stepsizes for both motors allows the detection of reversible elastic deformations between the rotor parts of Fo and F1.
q-bio.BM:In this paper, we model the mechanics of a collagen pair in the connective tissue extracellular matrix that exists in abundance throughout animals, including the human body. This connective tissue comprises repeated units of two main structures, namely collagens as well as axial, parallel and regular anionic glycosaminoglycan between collagens. The collagen fibril can be modeled by Hooke's law whereas anionic glycosaminoglycan behaves more like a rubber-band rod and as such can be better modeled by the worm-like chain model. While both computer simulations and continuum mechanics models have been investigated the behavior of this connective tissue typically, authors either assume a simple form of the molecular potential energy or entirely ignore the microscopic structure of the connective tissue. Here, we apply basic physical methodologies and simple applied mathematical modeling techniques to describe the collagen pair quantitatively. We find that the growth of fibrils is intimately related to the maximum length of the anionic glycosaminoglycan and the relative displacement of two adjacent fibrils, which in return is closely related to the effectiveness of anionic glycosaminoglycan in transmitting forces between fibrils. These reveal the importance of the anionic glycosaminoglycan in maintaining the structural shape of the connective tissue extracellular matrix and eventually the shape modulus of human tissues. We also find that some macroscopic properties, like the maximum molecular energy and the breaking fraction of the collagen, are also related to the microscopic characteristics of the anionic glycosaminoglycan.
q-bio.BM:The wobble hypothesis does not discriminate between uracil and thymine. Methylation could favor further stabilization of uracil in the keto form. Thymine is present in keto form only and can pair up but with adenine. Uracil can easily construct the enol form; that is why it forms the U-G pair.
q-bio.BM:In this paper we analyze the vibrational spectra of a large ensemble of non-homologous protein structures by means of a novel tool, that we coin the Hierarchical Network Model (HNM). Our coarse-grained scheme accounts for the intrinsic heterogeneity of force constants displayed by protein arrangements and also incorporates side-chain degrees of freedom.   Our analysis shows that vibrational entropy per unit residue correlates with the content of secondary structure. Furthermore, we assess the individual contribution to vibrational entropy of the novel features of our scheme as compared with the predictions of state-of-the-art network models. This analysis highlights the importance of properly accounting for the intrinsic hierarchy in force strengths typical of the different atomic bonds that build up and stabilize protein scaffolds.   Finally, we discuss possible implications of our findings in the context of protein aggregation phenomena.
q-bio.BM:Because of the double-helical structure of DNA, in which two strands of complementary nucleotides intertwine around each other, a covalently closed DNA molecule with no interruptions in either strand can be viewed as two interlocked single-stranded rings. Two closed space curves have long been known by mathematicians to exhibit a property called the linking number, a topologically invariant integer, expressible as the sum of two other quantities, the twist of one of the curves about the other, and the writhing number, or writhe, a measure of the chiral distortion from planarity of one of the two closed curves. We here derive expressions for the twist of supercoiled DNA and the writhe of a closed molecule consistent with the modern view of DNA as a sequence of base-pair steps. Structural biologists commonly characterize the spatial disposition of each step in terms of six rigid-body parameters, one of which, coincidentally, is also called the twist. Of interest is the difference in the mathematical properties between this step-parameter twist and the twist of supercoiling associated with a given base-pair step. For example, it turns out that the latter twist, unlike the former, is sensitive to certain translational shearing distortions of the molecule that are chiral in nature. Thus, by comparing the values for the two twists for each step of a high-resolution structure of a protein-DNA complex, the nucleosome considered here, for example, we may be able to determine how the binding of various proteins contributes to chiral structural changes of the DNA.
q-bio.BM:The AGBNP2 implicit solvent model, an evolution of the Analytical Generalized Born plus Non-Polar (AGBNP) model we have previously reported, is presented with the aim of modeling hydration effects beyond those described by conventional continuum dielectric representations. A new empirical hydration free energy component based on a procedure to locate and score hydration sites on the solute surface is introduced to model first solvation shell effects, such as hydrogen bonding, which are poorly described by continuum dielectric models. This new component is added to the Generalized Born and non-polar AGBNP models which have been improved with respect to the description of the solute volume description. We have introduced an analytical Solvent Excluding Volume (SEV) model which reduces the effect of spurious high-dielectric interstitial spaces present in conventional van der Waals representations of the solute volume. The AGBNP2 model is parametrized and tested with respect to experimental hydration free energies of small molecules and the results of explicit solvent simulations. Modeling the granularity of water is one of the main design principles employed for the the first shell solvation function and the SEV model, by requiring that water locations have a minimum available volume based on the size of a water molecule. We show that the new volumetric model produces Born radii and surface areas in good agreement with accurate numerical evaluations. The results of Molecular Dynamics simulations of a series of mini-proteins show that the new model produces conformational ensembles in substantially better agreement with reference explicit solvent ensembles than the original AGBNP model with respect to both structural and energetics measures.
q-bio.BM:It is presented that the positions of amino acids within Genetic Code Table follow from strict their physical and chemical properties as well as from a pure formal determination by the Golden mean.
q-bio.BM:Although DNA is often bent in vivo, it is unclear how DNA-bending forces modulate DNA-protein binding affinity. Here, we report how a range of DNA-bending forces modulates the binding of the Integration Host Factor (IHF) protein to various DNAs. Using solution fluorimetry and electrophoretic mobility shift assays, we measured the affinity of IHF for DNAs with different bending forces and sequence mutations. Bending force was adjusted by varying the fraction of double-stranded DNA in a circular substrate, or by changing the overall size of the circle (1). DNA constructs contained a pair of Forster Resonance Energy Transfer dyes that served as probes for affinity assays, and read out bending forces measured by optical force sensors (2). Small bending forces significantly increased binding affinity; this effect saturated beyond ~3 pN. Surprisingly, when DNA sequences that bound IHF only weakly were mechanically bent by circularization, they bound IHF more tightly than the linear "high-affinity" binding sequence. These findings demonstrate that small bending forces can greatly augment binding at sites that deviate from a protein's consensus binding sequence. Since cellular DNA is subject to mechanical deformation and condensation, affinities of architectural proteins determined in vitro using short linear DNAs may not reflect in vivo affinities.
q-bio.BM:A statistical model of protein families, called profile conditional random fields (CRFs), is proposed. This model may be regarded as an integration of the profile hidden Markov model (HMM) and the Finkelstein-Reva (FR) theory of protein folding. While the model structure of the profile CRF is almost identical to the profile HMM, it can incorporate arbitrary correlations in the sequences to be aligned to the model. In addition, like in the FR theory, the profile CRF can incorporate long-range pairwise interactions between model states via mean-field-like approximations. We give the detailed formulation of the model, self-consistent approximations for treating long-range interactions, and algorithms for computing partition functions and marginal probabilities. We also outline the methods for the global optimization of model parameters as well as a Bayesian framework for parameter learning and selection of optimal alignments.
q-bio.BM:Mathew-Fenn et al. (Science (2008) 322, 446-9) measured end-to-end distances of short DNA and concluded that stretching fluctuations in several consecutive turns of the double helix should be strongly correlated. I argue that this conclusion is based on incorrect assumptions, notably, on a simplistic treatment of the excluded volume effect of reporter labels. Contrary to the author's claim, their conclusion is not supported by other data.
q-bio.BM:The folding dynamics of small single-domain proteins is a current focus of simulations and experiments. Many of these proteins are 'two-state folders', i.e. proteins that fold rather directly from the denatured state to the native state, without populating metastable intermediate states. A central question is how to characterize the instable, partially folded conformations of two-state proteins, in particular the rate-limiting transition-state conformations between the denatured and the native state. These partially folded conformations are short-lived and cannot be observed directly in experiments. However, experimental data from detailed mutational analyses of the folding dynamics provide indirect access to transition states. The interpretation of these data, in particular the reconstruction of transition-state conformations, requires simulation and modeling. The traditional interpretation of the mutational data aims to reconstruct the degree of structure formation of individual residues in the transition state, while a novel interpretation aims at degrees of structure formation of cooperative substructures such as alpha-helices and beta-hairpins. By splitting up mutation-induced free energy changes into secondary and tertiary structural components, the novel interpretation resolves some of the inconsistencies of the traditional interpretation.
q-bio.BM:In Sphingomonas CHY-1, a single ring-hydroxylating dioxygenase is responsible for the initial attack of a range of polycyclic aromatic hydrocarbons (PAHs) composed of up to five rings. The components of this enzyme were separately purified and characterized. The oxygenase component (ht-PhnI) was shown to contain one Rieske-type [2Fe-2S] cluster and one mononuclear Fe center per alpha subunit, based on EPR measurements and iron assay. Steady-state kinetic measurements revealed that the enzyme had a relatively low apparent Michaelis constant for naphthalene (Km= 0.92 $\pm$ 0.15 $\mu$M), and an apparent specificity constant of 2.0 $\pm$ 0.3 $\mu$M-1 s-1. Naphthalene was converted to the corresponding 1,2-dihydrodiol with stoichiometric oxidation of NADH. On the other hand, the oxidation of eight other PAHs occurred at slower rates, and with coupling efficiencies that decreased with the enzyme reaction rate. Uncoupling was associated with hydrogen peroxide formation, which is potentially deleterious to cells and might inhibit PAH degradation. In single turnover reactions, ht-PhnI alone catalyzed PAH hydroxylation at a faster rate in the presence of organic solvent, suggesting that the transfer of substrate to the active site is a limiting factor. The four-ring PAHs chrysene and benz[a]anthracene were subjected to a double ring-dihydroxylation, giving rise to the formation of a significant proportion of bis-cis-dihydrodiols. In addition, the dihydroxylation of benz[a]anthracene yielded three dihydrodiols, the enzyme showing a preference for carbons in positions 1,2 and 10,11. This is the first characterization of a dioxygenase able to dihydroxylate PAHs made up of four and five rings.
q-bio.BM:Initial reactions involved in the bacterial degradation of polycyclic aromatic hydrocarbons (PAHs) include a ring-dihydroxylation catalyzed by a dioxygenase and a subsequent oxidation of the dihydrodiol products by a dehydrogenase. In this study, the dihydrodiol dehydrogenase from the PAH-degrading Sphingomonas strain CHY-1 has been characterized. The bphB gene encoding PAH dihydrodiol dehydrogenase (PDDH) was cloned and overexpressed as a His-tagged protein. The recombinant protein was purified as a homotetramer with an apparent Mr of 110,000. PDDH oxidized the cis-dihydrodiols derived from biphenyl and eight polycyclic hydrocarbons, including chrysene, benz[a]anthracene, and benzo[a]pyene, to corresponding catechols. Remarkably, the enzyme oxidized pyrene 4,5-dihydrodiol, whereas pyrene is not metabolized by strain CHY-1. The PAH catechols produced by PDDH rapidly auto-oxidized in air but were regenerated upon reaction of the o-quinones formed with NADH. Kinetic analyses performed under anoxic conditions revealed that the enzyme efficiently utilized two- to four-ring dihydrodiols, with Km values in the range of 1.4 to 7.1 $\mu$M, and exhibited a much higher Michaelis constant for NAD+ (Km of 160 $\mu$M). At pH 7.0, the specificity constant ranged from (1.3 $\pm$ 0.1) x 106 M?1 s?1 with benz[a]anthracene 1,2-dihydrodiol to (20.0 $\pm$ 0.8) x 106 M?1 s?1 with naphthalene 1,2-dihydrodiol. The catalytic activity of the enzyme was 13-fold higher at pH 9.5. PDDH was subjected to inhibition by NADH and by 3,4-dihydroxyphenanthrene, and the inhibition patterns suggested that the mechanism of the reaction was ordered Bi Bi. The regulation of PDDH activity appears as a means to prevent the accumulation of PAH catechols in bacterial cells.
q-bio.BM:2-Ethyhexyl nitrate (2-EHN) is a major additive of fuel which is used to comply with the cetane number of diesel. Because of its wide use and possible accidental release, 2-EHN is a potential pollutant of the environment. In this study, Mycobacterium austroafricanum IFP 2173 was selected among several strains as the best 2-EHN degrader. The 2-EHN biodegradation rate was increased in biphasic cultures where the hydrocarbon was dissolved in an inert non-aqueous phase liquid (NAPL), suggesting that the transfer of the hydrophobic substrate to the cells was a growth-limiting factor. Carbon balance calculation as well as organic carbon measurement indicated a release of metabolites in the culture medium. Further analysis by gas chromatography revealed that a single metabolite accumulated during growth. This metabolite had a molecular mass of 114 Da as determined by GC/MS and was provisionally identified as 4-ethyldihydrofuran-2(3H)-one by LC-MS/MS analysis. Identification was confirmed by analysis of the chemically synthesized lactone. Based on these results, a plausible catabolic pathway is proposed whereby 2-EHN is converted to 4-ethyldihydrofuran-2(3H)-one, which cannot be metabolised further by strain IFP 2173. This putative pathway provides an explanation for the low energetic efficiency of 2-EHN degradation and its poor biodegradability.
q-bio.BM:In this study, the genes involved in the initial attack on fluorene by Sphingomonas sp. LB126 were investigated. The ? and ? subunits of a dioxygenase complex (FlnA1A2), showing 63% and 51% sequence identity respectively, with the subunits of an angular dioxygenase from Gram-positive Terrabacter sp. DBF63, were identified. When overexpressed in E. coli, FlnA1A2 was responsible for the angular oxidation of fluorene, fluorenol, fluorenone, dibenzofuran and dibenzo-p-dioxin. Moreover, FlnA1A2 was able to oxidize polycyclic aromatic hydrocarbons and heteroaromatics, some of which were not oxidized by the dioxygenase from Terrabacter sp. DBF63. Quantification of resulting oxidation products showed that fluorene and phenanthrene were preferred substrates.
q-bio.BM:We study a simplified model of the RNA molecule proposed by G. Vernizzi, H. Orland and A. Zee in the regime of strong concentration of positive ions in solution. The model considers a flexible chain of equal bases that can pairwise interact with any other one along the chain, while preserving the property of saturation of the interactions. In the regime considered, we observe the emergence of a critical temperature T_c separating two phases that can be characterized by the topology of the predominant configurations: in the large temperature regime, the dominant configurations of the molecule have very large genera (of the order of the size of the molecule), corresponding to a complex topology, whereas in the opposite regime of low temperatures, the dominant configurations are simple and have the topology of a sphere. We determine that this topological phase transition is of first order and provide an analytic expression for T_c. The regime studied for this model exhibits analogies with that for the dense polymer systems studied by de Gennes
q-bio.BM:The kinetics for the assembly of viral proteins into a population of capsids can be measured in vitro with size exclusion chromatography or dynamic light scattering, but extracting mechanistic information from these studies is challenging. For example, it is not straightforward to determine the critical nucleus size or the elongation time (the time required for a nucleated partial capsid to grow completion). We show that, for two theoretical models of capsid assembly, the critical nucleus size can be determined from the concentration dependence of the assembly reaction half-life and the elongation time is revealed by the length of the lag phase. Furthermore, we find that the system becomes kinetically trapped when nucleation becomes fast compared to elongation. Implications of this constraint for determining elongation mechanisms from experimental assembly data are discussed.
q-bio.BM:Single molecule Forster resonance energy transfer (FRET) experiments are used to infer the properties of the denatured state ensemble (DSE) of proteins. From the measured average FRET efficiency, <E>, the distance distribution P(R) is inferred by assuming that the DSE can be described as a polymer. The single parameter in the appropriate polymer model (Gaussian chain, Wormlike chain, or Self-avoiding walk) for P(R) is determined by equating the calculated and measured <E>. In order to assess the accuracy of this "standard procedure," we consider the generalized Rouse model (GRM), whose properties [<E> and P(R)] can be analytically computed, and the Molecular Transfer Model for protein L for which accurate simulations can be carried out as a function of guanadinium hydrochloride (GdmCl) concentration. Using the precisely computed <E> for the GRM and protein L, we infer P(R) using the standard procedure. We find that the mean end-to-end distance can be accurately inferred (less than 10% relative error) using <E> and polymer models for P(R). However, the value extracted for the radius of gyration (Rg) and the persistence length (lp) are less accurate. The relative error in the inferred R-g and lp, with respect to the exact values, can be as large as 25% at the highest GdmCl concentration. We propose a self-consistency test, requiring measurements of <E> by attaching dyes to different residues in the protein, to assess the validity of describing DSE using the Gaussian model. Application of the self-consistency test to the GRM shows that even for this simple model the Gaussian P(R) is inadequate. Analysis of experimental data of FRET efficiencies for the cold shock protein shows that at there are significant deviations in the DSE P(R) from the Gaussian model.
q-bio.BM:The high computational cost of carrying out molecular dynamics simulations of even small-size proteins is a major obstacle in the study, at atomic detail and in explicit solvent, of the physical mechanism which is at the basis of the folding of proteins. Making use of a biasing algorithm, based on the principle of the ratchet-and-pawl, we have been able to calculate eight folding trajectories (to an RMSD between 1.2A and 2.5A) of the B1 domain of protein G in explicit solvent without the need of high-performance computing. The simulations show that in the denatured state there is a complex network of cause-effect relationships among contacts, which results in a rather hierarchical folding mechanism. The network displays few local and nonlocal native contacts which are cause of most of the others, in agreement with the NOE signals obtained in mildly-denatured conditions. Also nonnative contacts play an active role in the folding kinetics. The set of conformations corresponding to the transition state display phi-values with a correlation coefficient of 0.69 with the experimental ones. They are structurally quite homogeneous and topologically native-like, although some of the side chains and most of the hydrogen bonds are not in place.
q-bio.BM:Rigidity analysis using the "pebble game" has been applied to protein crystal structures to obtain information on protein folding, assembly and t he structure-function relationship. However, previous work using this technique has not made clear how the set of hydrogen-bond constraints included in the rigidity analysis should be chosen, nor how sensitive the results of rigidity analysis are to small structural variations. We present a comparative study in which "pebble game" rigidity analysis is applied to multiple protein crystal structures, for each of six differen t protein families. We find that the mainchain rigidity of a protein structure at a given hydrogen-bond energy cutoff is quite sensitive to small structural variations, and conclude that the hydrogen bond constraints in rigidity analysis should be chosen so as to form and test specific hypotheses about the rigidity o f a particular protein. Our comparative approach highlights two different characteristic patterns ("sudden" or "gradual") for protein rigidity loss as constraints are re moved, in line with recent results on the rigidity transitions of glassy networks.
q-bio.BM:We carry out a theoretical study on the isotropic-nematic phase transition and phase separation in amyloid fibril solutions. Borrowing the thermodynamic model employed in the study of cylindrical micelles, we investigate the variations in the fibril length distribution and phase behavior with respect to changes in the protein concentration, fibril's rigidity, and binding energy. We then relate our theoretical findings to the nematic ordering observed in Hen Lysozyme fibril solution.
q-bio.BM:Homeodomain containing proteins are a broad class of DNA binding proteins that are believed to primarily function as transcription factors. Electrostatics interactions have been demonstrated to be critical for the binding of the homeodomain to DNA. An examination of the electrostatic state of homeodomain residues involved in DNA phosphate binding has demonstrated the conserved presence of upward shifted pKa values among the basic residue of lysine and arginine. It is believed that these pKa perturbations work to facilitate binding to DNA since they ensure that the basic residues always retain a positive charge.
q-bio.BM:2D display is a fast and economical way of visualizing polymorphism and comparing genomes, which is based on the separation of DNA fragments in two steps, according first to their size and then to their sequence composition. In this paper, we present an exhaustive study of the numerical issues associated with a model aimed at predicting the final absolute locations of DNA fragments in 2D display experiments. We show that simple expressions for the mobility of DNA fragments in both dimensions allow one to reproduce experimental final absolute locations to better than experimental uncertainties. On the other hand, our simulations also point out that the results of 2D display experiments are not sufficient to determine the best set of parameters for the modeling of fragments separation in the second dimension and that additional detailed measurements of the mobility of a few sequences are necessary to achieve this goal. We hope that this work will help in establishing simulations as a powerful tool to optimize experimental conditions without having to perform a large number of preliminary experiments and to estimate whether 2D DNA display is suited to identify a mutation or a genetic difference that is expected to exist between the genomes of closely related organisms.
q-bio.BM:Proteins are large and complex molecular machines. In order to perform their function, most of them need energy, e.g. either in the form of a photon, like in the case of the visual pigment rhodopsin, or through the breaking of a chemical bond, as in the presence of adenosine triphosphate (ATP). Such energy, in turn, has to be transmitted to specific locations, often several tens of Angstroms away from where it is initially released. Here we show, within the framework of a coarse-grained nonlinear network model, that energy in a protein can jump from site to site with high yields, covering in many instances remarkably large distances. Following single-site excitations, few specific sites are targeted, systematically within the stiffest regions. Such energy transfers mark the spontaneous formation of a localized mode of nonlinear origin at the destination site, which acts as an efficient energy-accumulating centre. Interestingly, yields are found to be optimum for excitation energies in the range of biologically relevant ones.
q-bio.BM:A protein undergoes conformational dynamics with multiple time scales, which results in fluctuating enzyme activities. Recent studies in single molecule enzymology have observe this "age-old" dynamic disorder phenomenon directly. However, the single molecule technique has its limitation. To be able to observe this molecular effect with real biochemical functions {\it in situ}, we propose to couple the fluctuations in enzymatic activity to noise propagations in small protein interaction networks such as zeroth order ultra-sensitive phosphorylation-dephosphorylation cycle. We showed that enzyme fluctuations could indeed be amplified by orders of magnitude into fluctuations in the level of substrate phosphorylation | a quantity widely interested in cellular biology. Enzyme conformational fluctuations sufficiently slower than the catalytic reaction turn over rate result in a bimodal concentration distribution of the phosphorylated substrate. In return, this network amplified single enzyme fluctuation can be used as a novel biochemical "reporter" for measuring single enzyme conformational fluctuation rates.
q-bio.BM:The microtubule assembly process has been extensively studied, but the underlying molecular mechanism remains poorly understood. The structure of an artificially generated sheet polymer that alternates two types of lateral contacts and that directly converts into microtubules, has been proposed to correspond to the intermediate sheet structure observed during microtubule assembly. We have studied the self-assembly process of GMPCPP tubulins into sheet and microtubule structures using thermodynamic analysis and stochastic simulations. With the novel assumptions that tubulins can laterally interact in two different forms, and allosterically affect neighboring lateral interactions, we can explain existing experimental observations. At low temperature, the allosteric effect results in the observed sheet structure with alternating lateral interactions as the thermodynamically most stable form. At normal microtubule assembly temperature, our work indicates that a class of sheet structures resembling those observed at low temperature is transiently trapped as an intermediate during the assembly process. This work may shed light on the tubulin molecular interactions, and the role of sheet formation during microtubule assembly.
q-bio.BM:Metamorphic proteins like Lymphotactin are a notable exception of the empirical principle that structured natural proteins possess a unique three dimensional structure. In particular, the human chemokine lymphotactin protein (Ltn) exists in two distinct conformations (one monomeric and one dimeric) under physiological conditions. In this work we use a Ca Go-model to show how this very peculiar behavior can be reproduced. From the study of the thermodynamics and of the kinetics we characterize the interconversion mechanism. In particular, this takes place through the docking of the two chains living in a third monomeric, partially unfolded, state which shows a residual structure involving a set of local contacts common to the two native conformations. The main feature of two-fold proteins appears to be the sharing of a common set of local contacts between the two distinct folds as confirmed by the study of two designed two-fold proteins. Metamorphic proteins may be more common than expected.
q-bio.BM:Computing the similarity between two protein structures is a crucial task in molecular biology, and has been extensively investigated. Many protein structure comparison methods can be modeled as maximum clique problems in specific k-partite graphs, referred here as alignment graphs. In this paper, we propose a new protein structure comparison method based on internal distances (DAST) which is posed as a maximum clique problem in an alignment graph. We also design an algorithm (ACF) for solving such maximum clique problems. ACF is first applied in the context of VAST, a software largely used in the National Center for Biotechnology Information, and then in the context of DAST. The obtained results on real protein alignment instances show that our algorithm is more than 37000 times faster than the original VAST clique solver which is based on Bron & Kerbosch algorithm. We furthermore compare ACF with one of the fastest clique finder, recently conceived by Ostergard. On a popular benchmark (the Skolnick set) we observe that ACF is about 20 times faster in average than the Ostergard's algorithm.
q-bio.BM:We use computer simulations to study a model, first proposed by Wales [1], for the reversible and monodisperse self-assembly of simple icosahedral virus capsid structures. The success and efficiency of assembly as a function of thermodynamic and geometric factors can be qualitatively related to the potential energy landscape structure of the assembling system. Even though the model is strongly coarse-grained, it exhibits a number of features also observed in experiments, such as sigmoidal assembly dynamics, hysteresis in capsid formation and numerous kinetic traps. We also investigate the effect of macromolecular crowding on the assembly dynamics. Crowding agents generally reduce capsid yields at optimal conditions for non-crowded assembly, but may increase yields for parameter regimes away from the optimum. Finally, we generalize the model to a larger triangulation number T = 3, and observe more complex assembly dynamics than that seen for the original T = 1 model.
q-bio.BM:Why reaction rate constants for enzymatic reactions are typically inversely proportional to fractional power exponents of solvent viscosity remains to be already a thirty years old puzzle. Available interpretations of the phenomenon invoke to either a modification of 1. the conventional Kramers' theory or that of 2. the Stokes law. We show that there is an alternative interpretation of the phenomenon at which neither of these modifications is in fact indispensable. We reconcile 1. and 2. with the experimentally observable dependence. We assume that an enzyme solution in solvent with or without cosolvent molecules is an ensemble of samples with different values of the viscosity for the movement of the system along the reaction coordinate. We assume that this viscosity consists of the contribution with the weight $q$ from cosolvent molecules and that with the weight $1-q$ from protein matrix and solvent molecules. We introduce heterogeneity in our system with the help of a distribution over the weight $q$. We verify the obtained solution of the integral equation for the unknown function of the distribution by direct substitution. All parameters of the model are related to experimentally observable values. General formalism is exemplified by the analysis of literature experimental data for oxygen escape from hemerythin.
q-bio.BM:A simple explanation for the symmetry and degeneracy of the genetic code has been suggested. An alternative to the wobble hypothesis has been proposed. This hypothesis offers explanations for: i) the difference between thymine and uracil, ii) encoding of tryptophan by only one codon, iii) why E. coli have no inosine in isoleucine tRNA, but isoleucine is encoded by three codons. The facts revealed in this study offer a new insight into physical mechanisms of the functioning of the genetic code.
q-bio.BM:A new theoretical survey of proteins' resistance to constant speed stretching is performed for a set of 17 134 proteins as described by a structure-based model. The proteins selected have no gaps in their structure determination and consist of no more than 250 amino acids. Our previous studies have dealt with 7510 proteins of no more than 150 amino acids. The proteins are ranked according to the strength of the resistance. Most of the predicted top-strength proteins have not yet been studied experimentally. Architectures and folds which are likely to yield large forces are identified. New types of potent force clamps are discovered. They involve disulphide bridges and, in particular, cysteine slipknots. An effective energy parameter of the model is estimated by comparing the theoretical data on characteristic forces to the corresponding experimental values combined with an extrapolation of the theoretical data to the experimental pulling speeds. These studies provide guidance for future experiments on single molecule manipulation and should lead to selection of proteins for applications. A new class of proteins, involving cystein slipknots, is identified as one that is expected to lead to the strongest force clamps known. This class is characterized through molecular dynamics simulations.
q-bio.BM:We have developed a generalized semi-analytic approach for efficiently computing cyclization and looping $J$ factors of DNA under arbitrary binding constraints. Many biological systems involving DNA-protein interactions impose precise boundary conditions on DNA, which necessitates a treatment beyond the Shimada-Yamakawa model for ring cyclization. Our model allows for DNA to be treated as a heteropolymer with sequence-dependent intrinsic curvature and stiffness. In this framework, we independently compute enthlapic and entropic contributions to the $J$ factor and show that even at small length scales $(\sim \ell_{p})$ entropic effects are significant. We propose a simple analytic formula to describe our numerical results for a homogenous DNA in planar loops, which can be used to predict experimental cyclization and loop formation rates as a function of loop size and binding geometry. We also introduce an effective torsional persistence length that describes the coupling between twist and bending of DNA when looped.
q-bio.BM:We use the Dominant Reaction Pathway (DRP) approach to study the dynamics of the folding of a beta-hairpin, within a model which accounts for both native and non-native interactions. We compare the most probable folding pathways calculated with the DRP method with those obtained directly from molecular dynamics (MD) simulations. We find that the two approaches give completely consistent results. We investigate the effects of the non-native hydrophobic interactions on the folding dynamics found them to be small.
q-bio.BM:Mechanical unfolding of the fourth domain of Distyostelium discoideum filamin (DDFLN4) was studied in detail using the C$_{\alpha}$-Go model. We show that unfolding pathways of this protein depend on the pulling speed. The agreement between theoretical and experimental results on the sequencing of unfolding events is achieved at low loading rates. The unfolding free energy landscape is also constructed using dependencies of unfolding forces on pulling speeds.
q-bio.BM:Mechanical unfolding of the fourth domain of Distyostelium discoideum filamin (DDFLN4) was studied by all-atom molecular dynamics simulations, using the GROMOS96 force field 43a1 and the simple point charge explicit water solvent. Our study reveals an important role of non-native interactions in the unfolding process. Namely, the existence of a peak centered at the end-to-end extension 22 nm in the force-extension curve, is associated with breaking of non-native hydrogen bonds. Such a peak has been observed in experiments but not in Go models, where non-native interactions are neglected. We predict that an additional peak occurs at 2 nm using not only GROMOS96 force field 43a1 but also Amber 94 and OPLS force fields. This result would stimulate further experimental studies on elastic properties of DDFLN4.
q-bio.BM:With the aid of quantum mechanical calculations we investigate the electronic structure of the full length (FL) potassium channel protein, FL-KcsA, in its closed conformation, and the electronic structure of the ClC chloride channel. The results indicate that both ion channels are strongly polarized towards the extracellular region with respect to the membrane mean plane. FL-KcsA possesses an electric dipole moment of magnitude 403 Debye while ClC has a macrodipole whose magnitude is about five times larger, 1983 Debye, thereby contributing to differentiate their membrane electric barriers. The dipole vectors of both proteins are aligned along the corresponding selectivity filters. This result suggests that potassium and chloride ion channels are not passive with respect to the movement of ions across the membrane and the ionic motion might be partially driven by the electric field of the protein in conjunction with the electrochemical potential of the membrane.
q-bio.BM:We show that minuscule entropic forces, on the order of 100 fN, can prevent the formation of DNA loops--a ubiquitous means of regulating the expression of genes. We observe a tenfold decrease in the rate of LacI-mediated DNA loop formation when a tension of 200 fN is applied to the substrate DNA, biasing the thermal fluctuations that drive loop formation and breakdown events. Conversely, once looped, the DNA-protein complex is insensitive to applied force. Our measurements are in excellent agreement with a simple polymer model of loop formation in DNA, and show that an anti-parallel topology is the preferred LacI-DNA loop conformation for a generic loop-forming construct.
q-bio.BM:The Electron Microscopy Data Bank (EMDB) is a rapidly growing repository for the dissemination of structural data from single-particle reconstructions of supramolecular protein assemblies including motors, chaperones, cytoskeletal assemblies, and viral capsids. While the static structure of these assemblies provides essential insight into their biological function, their conformational dynamics and mechanics provide additional important information regarding the mechanism of their biological function. Here, we present an unsupervised computational framework to analyze and store for public access the conformational dynamics of supramolecular protein assemblies deposited in the EMDB. Conformational dynamics are analyzed using normal mode analysis in the finite element framework, which is used to compute equilibrium thermal fluctuations, cross-correlations in molecular motions, and strain energy distributions for 452 of the 681 entries stored in the EMDB at present. Results for the viral capsid of hepatitis B, ribosome-bound termination factor RF2, and GroEL are presented in detail and validated with all-atom based models. The conformational dynamics of protein assemblies in the EMDB may be useful in the interpretation of their biological function, as well as in the classification and refinement of EM-based structures.
q-bio.BM:The dynamical characterization of proteins is crucial to understand protein function. From a microscopic point of view, protein dynamics is governed by the local atomic interactions that, in turn, trigger the functional conformational changes. Unfortunately, the relationship between local atomic fluctuations and global protein rearrangements is still elusive. Here, atomistic molecular dynamics simulations in conjunction with complex network analysis show that fast peptide relaxations effectively build the backbone of the global free-energy landscape, providing a connection between local and global atomic rearrangements. A minimum-spanning-tree representation, built on the base of transition gradients networks, results in a high resolution mapping of the system dynamics and thermodynamics without requiring any a priori knowledge of the relevant degrees of freedom. These results suggest the presence of a local mechanism for the high communication efficiency generally observed in complex systems.
q-bio.BM:The functional correlation of missense mutations which cause disease remains a challenge to understanding the basis of genetic diseases. This is particularly true for proteins related to diseases for which there are no available three dimensional structures. One such disease is Shwachman Diamond syndrome SDS OMIM 260400, a multi system disease arising from loss of functional mutations. The Homo sapiens Shwachman Bodian Diamond Syndrome gene hSBDS is responsible for SDS. hSBDS is expressed in all tissues and encodes a protein of 250 amino acids SwissProt accession code Q9Y3A5. Sequence analysis of disease associated alleles has identified more than 20 different mutations in affected individuals. While a number of these mutations have been described as leading to the loss of protein function due to truncation, translation or surface epitope association, the structural basis for these mutations has yet to be determined due to the lack of a three-dimensional structure for SBDS.
q-bio.BM:Using the perturbation-response scanning (PRS) technique, we study a set of 23 proteins that display a variety of conformational motions upon ligand binding (e.g. shear, hinge, allosteric). In most cases, PRS determines residues that may be manipulated to achieve the resulting conformational change. PRS reveals that for some proteins, binding induced conformational change may be achieved through the perturbation of residues scattered throughout the protein, whereas in others, perturbation of specific residues confined to a highly specific region are necessary. Correlations between the experimental and calculated atomic displacements are always better or equivalent to those obtained from a modal analysis of elastic network models. Furthermore, best correlations obtained by the latter approach do not always appear in the most collective modes. We show that success of the modal analysis depends on the lack of redundant paths that exist in the protein. PRS thus demonstrates that several relevant modes may simultaneously be induced by perturbing a single select residue on the protein. We also illustrate the biological relevance of applying PRS on the GroEL and ADK structures in detail, where we show that the residues whose perturbation lead to the precise conformational changes usually correspond to those experimentally determined to be functionally important.
q-bio.BM:Living cells provide a fluctuating, out-of-equilibrium environment in which genes must coordinate cellular function. DNA looping, which is a common means of regulating transcription, is very much a stochastic process; the loops arise from the thermal motion of the DNA and other fluctuations of the cellular environment. We present single-molecule measurements of DNA loop formation and breakdown when an artificial fluctuating force, applied to mimic a fluctuating cellular environment, is imposed on the DNA. We show that loop formation is greatly enhanced in the presence of noise of only a fraction of $k_B T$, yet find that hypothetical regulatory schemes that employ mechanical tension in the DNA--as a sensitive switch to control transcription--can be surprisingly robust due to a fortuitous cancellation of noise effects.
q-bio.BM:Capsids of many viruses assemble around nucleic acids or other polymers. Understanding how the properties of the packaged polymer affect the assembly process could promote biomedical efforts to prevent viral assembly or nanomaterials applications that exploit assembly. To this end, we simulate on a lattice the dynamical assembly of closed, hollow shells composed of several hundred to 1000 subunits, around a flexible polymer. We find that assembly is most efficient at an optimum polymer length that scales with the surface area of the capsid; significantly longer than optimal polymers often lead to partial-capsids with unpackaged polymer `tails' or a competition between multiple partial-capsids attached to a single polymer. These predictions can be tested with bulk experiments in which capsid proteins assemble around homopolymeric RNA or synthetic polyelectrolytes. We also find that the polymer can increase the net rate of subunit accretion to a growing capsid both by stabilizing the addition of new subunits and by enhancing the incoming flux of subunits; the effects of these processes may be distinguishable with experiments that monitor the assembly of individual capsids.
q-bio.BM:We study the control parameters that govern the dynamics of in vitro DNA ejection in bacteriophage lambda. Past work has demonstrated that bacteriophage DNA is highly pressurized; this pressure has been hypothesized to help drive DNA ejection. Ions influence this process by screening charges on DNA; however, a systematic variation of salt concentrations to explore these effects has not been undertaken. To study the nature of the forces driving DNA ejection, we performed in vitro measurements of DNA ejection in bulk and at the single-phage level. We present measurements on the dynamics of ejection and on the self-repulsion force driving ejection. We examine the role of ion concentration and identity in both measurements, and show that the charge of counter-ions is an important control parameter. These measurements show that the frictional force acting on the ejecting DNA is subtly dependent on ionic concentrations for a given amount of DNA in the capsid. We also present evidence that phage DNA forms loops during ejection; we confirm that this effect occurs using optical tweezers. We speculate this facilitates circularization of the genome in the cytoplasm.
q-bio.BM:A relationship between the preexponent of the rate constant and the distribution over activation barrier energies for enzymatic/protein reactions is revealed. We consider an enzyme solution as an ensemble of individual molecules with different values of the activation barrier energy described by the distribution. From the solvent viscosity effect on the preexponent we derive the integral equation for the distribution and find its approximate solution. Our approach enables us to attain a twofold purpose. On the one hand it yields a simple interpretation of the solvent viscosity dependence for enzymatic/protein reactions that requires neither a modification of the Kramers' theory nor that of the Stokes law. On the other hand our approach enables us to deduce the form of the distribution over activation barrier energies. The obtained function has a familiar bell-shaped form and is in qualitative agreement with the results of single enzyme kinetics measurements. General formalism is exemplified by the analysis of literature experimental data.
q-bio.BM:A molecular-level model is used to study the mechanical response of empty cowpea chlorotic mottle virus (CCMV) and cowpea mosaic virus (CPMV) capsids. The model is based on the native structure of the proteins that consitute the capsids and is described in terms of the C-alpha atoms. Nanoindentation by a large tip is modeled as compression between parallel plates. Plots of the compressive force versus plate separation for CCMV are qualitatively consistent with continuum models and experiments, showing an elastic region followed by an irreversible drop in force. The mechanical response of CPMV has not been studied, but the molecular model predicts an order of magnitude higher stiffness and a much shorter elastic region than for CCMV. These large changes result from small structural changes that increase the number of bonds by only 30% and would be difficult to capture in continuum models. Direct comparison of local deformations in continuum and molecular models of CCMV shows that the molecular model undergoes a gradual symmetry breaking rotation and accommodates more strain near the walls than the continuum model. The irreversible drop in force at small separations is associated with rupturing nearly all of the bonds between capsid proteins in the molecular model while a buckling transition is observed in continuum models.
q-bio.BM:Using lattice models we explore the factors that determine the tendencies of polypeptide chains to aggregate by exhaustively sampling the sequence and conformational space. The morphologies of the fibril-like structures and the time scales ($\tau_{fib}$) for their formation depend on a subtle balance between hydrophobic and coulomb interactions. The extent of population of a fibril-prone structure in the spectrum of monomer conformations is the major determinant of $\tau_{fib}$. This observation is used to determine the aggregation-prone consensus sequences by exhaustively exploring the sequence space. Our results provide a basis for genome wide search of fragments that are aggregation prone.
q-bio.BM:Cancer is a proliferation disease affecting a genetically unstable cell population, in which molecular alterations can be somatically inherited by genetic, epigenetic or extragenetic transmission processes, leading to a cooperation of neoplastic cells within tumoral tissue. The efflux protein P-glycoprotein (P gp) is overexpressed in many cancer cells and has known capacity to confer multidrug resistance to cytotoxic therapies. Recently, cell-to-cell P-gp transfers have been shown. Herein, we combine experimental evidence and a mathematical model to examine the consequences of an intercellular P-gp trafficking in the extragenetic transfer of multidrug resistance from resistant to sensitive cell subpopulations. We report cell-to-cell transfers of functional P-gp in co-cultures of a P-gp overexpressing human breast cancer MCF-7 cell variant, selected for its resistance towards doxorubicin, with the parental sensitive cell line. We found that P-gp as well as efflux activity distribution are progressively reorganized over time in co-cultures analyzed by flow cytometry. A mathematical model based on a Boltzmann type integro-partial differential equation structured by a continuum variable corresponding to P-gp activity describes the cell populations in co-culture. The mathematical model elucidates the population elements in the experimental data, specifically, the initial proportions, the proliferative growth rates, and the transfer rates of P-gp in the sensitive and resistant subpopulations. We confirmed cell-to-cell transfer of functional P-gp. The transfer process depends on the gradient of P-gp expression in the donor-recipient cell interactions, as they evolve over time. Extragenetically acquired drug resistance is an additional aptitude of neoplastic cells which has implications in the diagnostic value of P-gp expression and in the design of chemotherapy regimens
q-bio.BM:We discuss the appropriate techniques for modelling the geometry of open ended elastic polymer molecules. The molecule is assumed to have fixed endpoints on a boundary surface. In particular we discuss the concept of the winding number, a directional measure of the linking of two curves, which can be shown to be invariant to the set of continuous deformations vanishing at the polymer's end-point and which forbid it from passing through itself. This measure is shown to be the appropriate constraint required to evaluate the geometrical properties of a constrained DNA molecule. Using the net winding measure we define a model of an open ended constrained DNA molecule which combines the necessary constraint of self-avoidance with being analytically tractable. This model builds upon the local models of Bouchiat and Mezard (2000). In particular, we present a new derivation of the polar writhe expression, which detects both the local winding of the curve and non local winding between different sections of the curve. We then show that this expression correctly tracks the net twisting of a DNA molecule subject to rotation at the endpoints, unlike other definitions used in the literature.
q-bio.BM:The electronic structure of charybdotoxin (ChTX), a scorpion venom peptide that is known to act as a potassium channel blocker, is investigated with the aid of quantum mechanical calculations. The dipole moment vector (145 D) of ChTX can be stirred by the full length KcsA potassium channel's macrodipole (403 D) thereby assuming the proper orientation before binding the ion channel on the cell surface. The localization of the frontier orbitals of ChTX has been revealed for the first time. HOMO is localized on Trp14 while the three lowest-energy MOs (LUMO, LUMO+1, and LUMO+2) are localized on the three disulfide bonds that characterize this pepetide. An effective way to engineer the HOMO-LUMO (H-L) gap of ChTX is that of replacing its Trp14 residue with Ala14 whereas deletion of the LUMO-associated disulfide bond with the insertion of a pair of L-alpha-aminobutyric acid residues does not affect the H-L energy gap.
q-bio.BM:We propose a mechanism that explains in a simple and natural form the l-homochiralization of prebiotic aminoacids in a volume of water where a geothermal gradient exists.
q-bio.BM:The coat proteins of many viruses spontaneously form icosahedral capsids around nucleic acids or other polymers. Elucidating the role of the packaged polymer in capsid formation could promote biomedical efforts to block viral replication and enable use of capsids in nanomaterials applications. To this end, we perform Brownian dynamics on a coarse-grained model that describes the dynamics of icosahedral capsid assembly around a flexible polymer. We identify several mechanisms by which the polymer plays an active role in its encapsulation, including cooperative polymer-protein motions. These mechanisms are related to experimentally controllable parameters such as polymer length, protein concentration, and solution conditions. Furthermore, the simulations demonstrate that assembly mechanisms are correlated to encapsulation efficiency, and we present a phase diagram that predicts assembly outcomes as a function of experimental parameters. We anticipate that our simulation results will provide a framework for designing in vitro assembly experiments on single-stranded RNA virus capsids.
q-bio.BM:The assumption of linear response of protein molecules to thermal noise or structural perturbations, such as ligand binding or detachment, is broadly used in the studies of protein dynamics. Conformational motions in proteins are traditionally analyzed in terms of normal modes and experimental data on thermal fluctuations in such macromolecules is also usually interpreted in terms of the excitation of normal modes. We have chosen two important protein motors - myosin V and kinesin KIF1A - and performed numerical investigations of their conformational relaxation properties within the coarse-grained elastic network approximation. We have found that the linearity assumption is deficient for ligand-induced conformational motions and can even be violated for characteristic thermal fluctuations. The deficiency is particularly pronounced in KIF1A where the normal mode description fails completely in describing functional mechanochemical motions. These results indicate that important assumptions of the theory of protein dynamics may need to be reconsidered. Neither a single normal mode, nor a superposition of such modes yield an approximation of strongly nonlinear dynamics.
q-bio.BM:We perform a quantum mechanical study of the peptides that are part of the LH2 complex from Rhodopseudomonas acidophila, a non-sulfur purple bacteria that has the ability of producing chemical energy from photosynthesis. The electronic structure calculations indicate that the transmembrane helices of these peptides are characterized by dipole moments with a magnitude of ~150 D. When the full nonamer assembly made of eighteen peptides is considered, then a macrodipole of magnitude 704 D is built up from the vector sum of each monomer dipole. The macrodipole is oriented normal to the membrane plane and with the positive tip toward the cytoplasm thereby indicating that the electronic charge of the protein scaffold is polarized toward the periplasm. The results obtained here suggest that the asymmetric charge distribution of the protein scaffold contributes an anisotropic electrostatic environment which differentiates the absorption properties of the bacteriochlorophyll pigments, B800 and B850, embedded in the LH2 complex.
q-bio.BM:The approach for calculation of the mode intensities of DNA conformational vibrations in the Raman spectra is developed. It is based on the valence-optic theory and the model for description of conformational vibrations of DNA with counterions. The calculations for Na- and Cs-DNA low-frequency Raman spectra show that the vibrations of DNA backbone chains near 15 cm-1 have the greatest intensity. In the spectrum of Na-DNA at frequency range upper than 40 cm-1 the modes of H-bond stretching in base pairs have the greatest intensities, while the modes of ion-phosphate vibrations have the lowest intensity. In Cs-DNA spectra at this frequency range the mode of ion-phosphate vibrations is prominent. Its intensity is much higher than the intensities of Na-DNA modes of this spectra range. Other modes of Cs-DNA have much lower intensities than in the case of Na-DNA. The comparison of our calculations with the experimental data shows that developed approach gives the understanding of the sensitivity of DNA low-frequency Raman bands to the neutralization of the double helix by light and heavy counterions.
q-bio.BM:The protein folding is regarded as a quantum transition between torsion states on polypeptide chain. The deduction of the folding rate formula in our previous studies is reviewed. The rate formula is generalized to the case of frequency variation in folding. Then the following problems about the application of the rate theory are discussed: 1) The unified theory on the two-state and multi-state protein folding is given based on the concept of quantum transition. 2) The relationship of folding and unfolding rates vs denaturant concentration is studied. 3) The temperature dependence of folding rate is deduced and the non-Arrhenius behaviors of temperature dependence are interpreted in a natural way. 4) The inertial moment dependence of folding rate is calculated based on the model of dynamical contact order and consistent results are obtained by comparison with one-hundred-protein experimental dataset. 5) The exergonic and endergonic foldings are distinguished through the comparison between theoretical and experimental rates for each protein. The ultrafast folding problem is viewed from the point of quantum folding theory and a new folding speed limit is deduced from quantum uncertainty relation. And finally, 6) since only the torsion-accessible states are manageable in the present formulation of quantum transition how the set of torsion-accessible states can be expanded by using statistical energy landscape approach is discussed. All above discussions support the view that the protein folding is essentially a quantum transition between conformational states.
q-bio.BM:Homologous recombination plays a key role in generating genetic diversity, while maintaining protein functionality. The mechanisms by which RecA enables a single-stranded segment of DNA to recognize a homologous tract within a whole genome are poorly understood. The scale by which homology recognition takes place is of a few tens of base pairs, after which the quest for homology is over. To study the mechanism of homology recognition, RecA-promoted homologous recombination between short DNA oligomers with different degrees of heterology was studied in vitro, using fluorescence resonant energy transfer. RecA can detect single mismatches at the initial stages of recombination, and the efficiency of recombination is strongly dependent on the location and distribution of mismatches. Mismatches near the 5' end of the incoming strand have a minute effect, whereas mismatches near the 3' end hinder strand exchange dramatically. There is a characteristic DNA length above which the sensitivity to heterology decreases sharply. Experiments with competitor sequences with varying degrees of homology yield information about the process of homology search and synapse lifetime. The exquisite sensitivity to mismatches and the directionality in the exchange process support a mechanism for homology recognition that can be modeled as a kinetic proofreading cascade.
q-bio.BM:Cell-penetrating peptides (CPPs) such as HIV's trans-activating transcriptional activator (TAT) and polyarginine rapidly pass through the plasma membranes of mammalian cells by an unknown mechanism called transduction. They may be medically useful when fused to well-chosen chains of fewer than about 35 amino acids. I offer a simple model of transduction in which phosphatidylserines and CPPs effectively form two plates of a capacitor with a voltage sufficient to cause the formation of transient pores (electroporation). The model is consistent with experimental data on the transduction of oligoarginine into mouse C2-C12 myoblasts and makes three testable predictions.
q-bio.BM:In recent years, single molecule force techniques have opened a new avenue to decipher the folding landscapes of biopolymers by allowing us to watch and manipulate the dynamics of individual proteins and nucleic acids. In single molecule force experiments, quantitative analyses of measurements employing sound theoretical models and molecular simulations play central role more than any other field. With a brief description of basic theories for force mechanics and molecular simulation technique using self-organized polymer (SOP) model, this chapter will discuss various issues in single molecule force spectroscopy (SMFS) experiments, which include pulling speed dependent unfolding pathway, measurement of energy landscape roughness, the in uence of molecular handles in optical tweezers on measurement and molecular motion, and folding dynamics of biopolymers under force quench condition.
q-bio.BM:We explore in detail the structural, mechanical and thermodynamic properties of a coarse-grained model of DNA similar to that introduced in Thomas E. Ouldridge, Ard A. Louis, Jonathan P.K. Doye, Phys. Rev. Lett. 104 178101 (2010). Effective interactions are used to represent chain connectivity, excluded volume, base stacking and hydrogen bonding, naturally reproducing a range of DNA behaviour. We quantify the relation to experiment of the thermodynamics of single-stranded stacking, duplex hybridization and hairpin formation, as well as structural properties such as the persistence length of single strands and duplexes, and the torsional and stretching stiffness of double helices. We also explore the model's representation of more complex motifs involving dangling ends, bulged bases and internal loops, and the effect of stacking and fraying on the thermodynamics of the duplex formation transition.
q-bio.BM:In this and the associated article 'BioBlender: A Software for Intuitive Representation of Surface Properties of Biomolecules', (Andrei et al) we present BioBlender as a complete instrument for the elaboration of motion (here) and the visualization (Andrei et al) of proteins and other macromolecules, using instruments of computer graphics. A vast number of protein (if not most) exert their function through some extent of motion. Despite recent advances in higly performant methods, it is very difficult to obtain direct information on conformational changes of molecules. However, several systems exist that can shed some light on the variability of conformations of a single peptide chain; among them, NMR methods provide collections of a number of static 'shots' of a moving protein. Starting from this data, and assuming that if a protein exists in more than 1 conformation it must be able to transit between the different states, we have elaborated a system that makes ample use of the computational power of 3D computer graphics technology. Considering information of all (heavy) atoms, we use animation and game engine of Blender to obtain transition states. The model we chose to elaborate our system is Calmodulin, a protein favorite among structural and dynamic studies due to its (relative) simplicity of structure and small dimension. Using Calmodulin we show a procedure that enables the building of a 'navigation map' of NMR models, that can help in the identification of movements. In the process, a number of intermediate conformations is generated, all of which respond to strict bio-physical and bio-chemical criteria. The BioBlender system is available for download from the website www.bioblender.net, together with examples, tutorial and other useful material.
q-bio.BM:Nested sampling is a Bayesian sampling technique developed to explore probability distributions lo- calised in an exponentially small area of the parameter space. The algorithm provides both posterior samples and an estimate of the evidence (marginal likelihood) of the model. The nested sampling algo- rithm also provides an efficient way to calculate free energies and the expectation value of thermodynamic observables at any temperature, through a simple post-processing of the output. Previous applications of the algorithm have yielded large efficiency gains over other sampling techniques, including parallel tempering (replica exchange). In this paper we describe a parallel implementation of the nested sampling algorithm and its application to the problem of protein folding in a Go-type force field of empirical potentials that were designed to stabilize secondary structure elements in room-temperature simulations. We demonstrate the method by conducting folding simulations on a number of small proteins which are commonly used for testing protein folding procedures: protein G, the SH3 domain of Src tyrosine kinase and chymotrypsin inhibitor 2. A topological analysis of the posterior samples is performed to produce energy landscape charts, which give a high level description of the potential energy surface for the protein folding simulations. These charts provide qualitative insights into both the folding process and the nature of the model and force field used.
q-bio.BM:The electrochemical behaviour of the biomedical and metallic alloys, especially in the orthopaedic implants fields, raises many questions. This study is dedicated for studying the Ti-6Al-4V alloy, by electrochemical impedance spectroscopy, EIS, in various physiological media,: Ringer solution, phosphate buffered solution (PBS), PBS solution and albumin, PBS solution with calf serum and PBS solution with calf serum and an antioxidant (sodium azide). Moreover, the desionised water was considered as the reference solution. The tests reproducibility was investigated. The time-frequency-Module graphs highlighted that the desionised water is the most protective for the Ti-6Al-4V alloy. This biomedical alloy is the less protected in the solution constituted by PBS and albumin. The time-frequency graph allows pointing out the graphic signatures of adsorption for organic and inorganic species (differences between the modules means in studied solution and the modules mean in the reference solution). --- Le comportement \'electrochimique des alliages m\'etalliques biom\'edicaux, notamment dans le domaine des implants orthop\'ediques, pose encore de nombreuses questions. Ce travail propose d'\'etudier l'alliage de titane Ti-6Al-4V, par spectroscopie d'imp\'edance \'electrochimique, SIE, dans diff\'erents milieux physiologiques : solution de Ringer, solution \`a base d'un tampon phosphate (PBS), solution PBS avec de l'albumine, solution PBS avec du s\'erum bovin et une solution PBS avec du s\'erum bovin et un antioxydant (azoture de sodium). De plus, une solution d'eau ultra-pure servira de r\'ef\'erence. La reproductibilit\'e des tests a \'et\'e \'etudi\'ee. Les repr\'esentations temps-fr\'equence des modules ont mis en \'evidence que l'eau d\'esionis\'ee est la solution qui pr\'esente le caract\`ere le plus protecteur pour le Ti-6Al-4V. Cet alliage de titane est le moins prot\'eg\'e dans la solution de PBS contenant de l'albumine. Cette repr\'esentation permet de mettre en \'evidence des signatures graphiques d'adsorption des esp\`eces inorganiques et organiques (diff\'erences entre les moyennes des modules dans les solutions \'etudi\'ees et la moyenne des modules dans la solution de r\'ef\'erence).
q-bio.BM:We present a rigidity analysis on a large number of X-ray crystal structures of the enzyme HIV-1 protease using the 'pebble game' algorithm of the software FIRST. We find that although the rigidity profile remains similar across a comprehensive set of high resolution structures, the profile changes significantly in the presence of an inhibitor. Our study shows that the action of the inhibitors is to restrict the flexibility of the beta-hairpin flaps which allow access to the active site. The results are discussed in the context of full molecular dynamics simulations as well as data from NMR experiments.
q-bio.BM:Conventional kinesin is a two-headed homodimeric motor protein, which is able to walk along microtubules processively by hydrolyzing ATP. Its neck linkers, which connect the two motor domains and can undergo a docking/undocking transition, are widely believed to play the key role in the coordination of the chemical cycles of the two motor domains and, consequently, in force production and directional stepping. Although many experiments, often complemented with partial kinetic modeling of specific pathways, support this idea, the ultimate test of the viability of this hypothesis requires the construction of a complete kinetic model. Considering the two neck linkers as entropic springs that are allowed to dock to their head domains and incorporating only the few most relevant kinetic and structural properties of the individual heads, here we develop the first detailed, thermodynamically consistent model of kinesin that can (i) explain the cooperation of the heads (including their gating mechanisms) during walking and (ii) reproduce much of the available experimental data (speed, dwell time distribution, randomness, processivity, hydrolysis rate, etc.) under a wide range of conditions (nucleotide concentrations, loading force, neck linker length and composition, etc.). Besides revealing the mechanism by which kinesin operates, our model also makes it possible to look into the experimentally inaccessible details of the mechanochemical cycle and predict how certain changes in the protein affect its motion.
q-bio.BM:Fifteen years ago Monique Tirion showed that the low-frequency normal modes of a protein are not significantly altered when non-bonded interactions are replaced by Hookean springs, for all atom pairs whose distance is smaller than a given cutoff value. Since then, it has been shown that coarse-grained versions of Tirion's model are able to provide fair insights on many dynamical properties of biological macromolecules. In this text, theoretical tools required for studying these so-called Elastic Network Models are described, focusing on practical issues and, in particular, on possible artifacts. Then, an overview of some typical results that have been obtained by studying such models is given.
q-bio.BM:A recent survey of 17 134 proteins has identified a new class of proteins which are expected to yield stretching induced force-peaks in the range of 1 nN. Such high force peaks should be due to forcing of a slip-loop through a cystine ring, i.e. by generating a cystine slipknot. The survey has been performed in a simple coarse grained model. Here, we perform all-atom steered molecular dynamics simulations on 15 cystine knot proteins and determine their resistance to stretching. In agreement with previous studies within a coarse grained structure based model, the level of resistance is found to be substantially higher than in proteins in which the mechanical clamp operates through shear. The large stretching forces arise through formation of the cystine slipknot mechanical clamp and the resulting steric jamming. We elucidate the workings of such a clamp in an atomic detail. We also study the behavior of five top strength proteins with the shear-based mechanostability in which no jamming is involved. We show that in the atomic model, the jamming state is relieved by moving one amino acid at a time and there is a choice in the selection of the amino acid that advances the first. In contrast, the coarse grained model also allows for a simultaneous passage of two amino acids.
q-bio.BM:A quantum theory on conformation-electron system is presented. Protein folding is regarded as the quantum transition between torsion states on polypeptide chain, and the folding rate is calculated by nonadiabatic operator method. The theory is used to study the temperature dependences of folding rate of 15 proteins and their non-Arrhenius behavior can all be deduced in a natural way. A general formula on the rate-temperature dependence has been deduced which is in good accordance with experimental data. These temperature dependences are further analyzed in terms of torsion potential parameters. Our results show it is necessary to move outside the realm of classical physics when the temperature dependence of protein folding is studied quantitatively.
q-bio.BM:Lasso peptides constitute a class of bioactive peptides sharing a knotted structure where the C-terminal tail of the peptide is threaded through and trapped within an N-terminalmacrolactamring. The structural characterization of lasso structures and differentiation from their unthreaded topoisomers is not trivial and generally requires the use of complementary biochemical and spectroscopic methods. Here we investigated two antimicrobial peptides belonging to the class II lasso peptide family and their corresponding unthreaded topoisomers: microcin J25 (MccJ25), which is known to yield two-peptide product ions specific of the lasso structure under collisioninduced dissociation (CID), and capistruin, for which CID does not permit to unambiguously assign the lasso structure. The two pairs of topoisomers were analyzed by electrospray ionization Fourier transform ion cyclotron resonance mass spectrometry (ESI-FTICR MS) upon CID, infrared multiple photon dissociation (IRMPD), and electron capture dissociation (ECD). CID and ECDspectra clearly permitted to differentiate MccJ25 from its non-lasso topoisomer MccJ25-Icm, while for capistruin, only ECD was informative and showed different extent of hydrogen migration (formation of c\bullet/z from c/z\bullet) for the threaded and unthreaded topoisomers. The ECD spectra of the triply-charged MccJ25 and MccJ25-lcm showed a series of radical b-type product ions {\eth}b0In{\TH}. We proposed that these ions are specific of cyclic-branched peptides and result from a dual c/z\bullet and y/b dissociation, in the ring and in the tail, respectively. This work shows the potentiality of ECD for structural characterization of peptide topoisomers, as well as the effect of conformation on hydrogen migration subsequent to electron capture.
q-bio.BM:Nanospray and collisionally-induced dissociation are used to evaluate the presence and absence of interstrand co-chelation of zinc ions in dimers of metallothionein. As was reported in a previous publication from this laboratory, co-chelation stabilizes the dimer to collisional activation, and facilitates asymmetrical zinc ion transfers during fragmentation. In the case of metallothionein, dimers of the holoprotein are found to share zinc ions, while dimers of metallothionein, in which one domain has been denatured, do not. Zinc ions are silent to most physicochemical probes, e.g., NMR and Mossbauer spectroscopies, and the capability of mass spectrometry to provide information on zinc complexes has widespread potential application in biochemistry.
q-bio.BM:Polymers can be modeled as open polygonal paths and their closure generates knots. Knotted proteins detection is currently achieved via high-throughput methods based on a common framework insensitive to the handedness of knots. Here we propose a topological framework for the computation of the HOMFLY polynomial, an handedness-sensitive invariant. Our approach couples a multi-component reduction scheme with the polynomial computation. After validation on tabulated knots and links the framework was applied to the entire Protein Data Bank along with a set of selected topological checks that allowed to discard artificially entangled structures. This led to an up-to-date table of knotted proteins that also includes two newly detected right-handed trefoil knots in recently deposited protein structures. The application range of our framework is not limited to proteins and it can be extended to the topological analysis of biological and synthetic polymers and more generally to arbitrary polygonal paths.
q-bio.BM:Atomic-accuracy structure prediction of macromolecules is a long-sought goal of computational biophysics. Accurate modeling should be achievable by optimizing a physically realistic energy function but is presently precluded by incomplete sampling of a biopolymer's many degrees of freedom. We present herein a working hypothesis, called the "stepwise ansatz", for recursively constructing well-packed atomic-detail models in small steps, enumerating several million conformations for each monomer and covering all build-up paths. By implementing the strategy in Rosetta and making use of high-performance computing, we provide first tests of this hypothesis on a benchmark of fifteen RNA loop modeling problems drawn from riboswitches, ribozymes, and the ribosome, including ten cases that were not solvable by prior knowledge based modeling approaches. For each loop problem, this deterministic stepwise assembly (SWA) method either reaches atomic accuracy or exposes flaws in Rosetta's all-atom energy function, indicating the resolution of the conformational sampling bottleneck. To our knowledge, SWA is the first enumerative, ab initio build-up method to systematically outperform existing Monte Carlo and knowledge-based methods for 3D structure prediction. As a rigorous experimental test, we have applied SWA to a small RNA motif of previously unknown structure, the C7.2 tetraloop/tetraloop-receptor, and stringently tested this blind prediction with nucleotide-resolution structure mapping data.
q-bio.BM:The surface accessibility of {\alpha}-bungarotoxin has been investigated by using Gd2L7, a newly designed paramagnetic NMR probe. Signal attenuations induced by Gd2L7 on {\alpha}-bungarotoxin C{\alpha}H peaks of 1H-13C HSQC spectra have been analyzed and compared with the ones previously obtained in the presence of GdDTPA-BMA. In spite of the different molecular size and shape, for the two probes a common pathway of approach to the {\alpha}-bungarotoxin surface can be observed with an equally enhanced access of both GdDTPA-BMA and Gd2L7 towards the protein surface side where the binding site is located. Molecular dynamics simulations suggest that protein backbone flexibility and surface hydration contribute to the observed preferential approach of both gadolinium complexes specifically to the part of the {\alpha}-bungarotoxin surface which is involved in the interaction with its physiological target, the nicotinic acetylcholine receptor.
q-bio.BM:The rates of protein folding with photon absorption or emission and the cross section of photon -protein inelastic scattering are calculated from the quantum folding theory by use of standard field-theoretical method. All these protein photo-folding processes are compared with common protein folding without interaction of photons (nonradiative folding). It is demonstrated that there exists a common factor (thermo-averaged overlap integral of vibration wave function, TAOI) for protein folding and protein photo-folding. Based on this finding it is predicted that: 1) the stimulated photo-folding rates show the same temperature dependence as protein folding; 2) the spectral line of electronic transition is broadened to a band which includes abundant vibration spectrum without and with conformational transition and the width of the vibration spectral line is largely reduced; 3) the resonance fluorescence cross section changes with temperature obeying the same law (Luo-Lu's law). The particular form of the folding rate - temperature relation and the abundant spectral structure imply the existence of a set of quantum oscillators in the transition process and these oscillators are mainly of torsion type of low frequency, imply the quantum tunneling between protein conformations does exist in folding and photo-folding processes and the tunneling is rooted deeply in the coherent motion of the conformational-electronic system.
q-bio.BM:In this paper we will review various aspects of the biology of prions and focus on what is currently known about the mammalian PrP prion. Also we briefly describe the prions of yeast and other fungi. Prions are infectious proteins behaving like genes, i.e. proteins that not only contain genetic information in its tertiary structure, i.e. its shape, but are also able to transmit and replicate in a manner analogous to genes but through very different mechanisms. The term prion is derived from "proteinaceous infectious particle" and arose from the Prusiner hypothesis that the infectious agent of certain neurodegenerative diseases was only in a protein, without the participation of nucleic acids. Currently there are several known types of prion, in addition to the originally described, which are pathogens of mammals, yeast and other fungi. Prion proteins are ubiquitous and not always detrimental to their hosts. This vision of the prion as a causative agent of disease is changing, finding more and more evidence that they could have important roles in cells and contribute to the phenotypic plasticity of organisms through the mechanisms of evolution.
q-bio.BM:Genomic DNA is constantly subjected to various mechanical stresses arising from its biological functions and cell packaging. If the local mechanical properties of DNA change under torsional and tensional stress, the activity of DNA-modifying proteins and transcription factors can be affected and regulated allosterically. To check this possibility, appropriate steady forces and torques were applied in the course of all-atom molecular dynamics simulations of DNA with AT- and GC-alternating sequences. It is found that the stretching rigidity grows with tension as well as twisting. The torsional rigidity is not affected by stretching, but it varies with twisting very strongly, and differently for the two sequences. Surprisingly, for AT-alternating DNA it passes through a minimum with the average twist close to the experimental value in solution. For this fragment, but not for the GC-alternating sequence, the bending rigidity noticeably changes with both twisting and stretching. The results have important biological implications and shed light upon earlier experimental observations.
q-bio.BM:The 3'-monofunctional adduct of cisplatin and d(CTCTG*G*TCTC)2 duplex DNA in solvent with explicit counter ions and water molecules were subjected to MD- simulation with AMBER force field on a nanosecond time scale. In order to simulate the closure of the bond between the Pt and 5'-guanine-N7 atoms, the forces acting between them were gradually increased during MD. After 500-800 ps the transformation of the mono-adduct (straight DNA with the cisplatin residue linked to one guanine-N7) to the bus-adduct (bent DNA where Pt atom is connected through the N7 atoms of neighboring guanines) was observed. A cavity between palatinate guanines is formed and filled with solvent molecules. The rapid inclination of the center base pairs initiates a slow transition of the whole molecule from the linear to the bent conformation. After about 1000-1300 ps a stable structure was reached, which is very similar to the one described experimentally. The attractive force between the Pt- atom and the N7 of the second guanine plays the main role in the large conformational changes induced by formation of the adduct-adduct. X-N-Pt-N-torsions accelerate the bending but a torsion force constant greater than 0.2 Kcal/mol lead to the breaking of the H-bonds within the base pairs. The present study is the first dynamical simulation that demonstrates in real time scale such a large conformational perturbation of DNA.
q-bio.BM:Despite the recognized importance of the multi-scale spatio-temporal organization of proteins, most computational tools can only access a limited spectrum of time and spatial scales, thereby ignoring the effects on protein behavior of the intricate coupling between the different scales. Starting from a physico-chemical atomistic network of interactions that encodes the structure of the protein, we introduce a methodology based on multi-scale graph partitioning that can uncover partitions and levels of organization of proteins that span the whole range of scales, revealing biological features occurring at different levels of organization and tracking their effect across scales. Additionally, we introduce a measure of robustness to quantify the relevance of the partitions through the generation of biochemically-motivated surrogate random graph models. We apply the method to four distinct conformations of myosin tail interacting protein, a protein from the molecular motor of the malaria parasite, and study properties that have been experimentally addressed such as the closing mechanism, the presence of conserved clusters, and the identification through computational mutational analysis of key residues for binding.
q-bio.BM:In this work two archaea microorganisms (Haloferax volcanii and Natrialba magadii) used as biocatalyst at a microbial fuel cell (MFC) anode were evaluated. Both archaea are able to grow at high salt concentrations. By increasing the media conductivity, the internal resistance was diminished, improving the MFCs performance. Without any added redox mediator, maximum power (Pmax) and current at Pmax were 11.87 / 4.57 / 0.12 {\mu}W cm-2 and 49.67 / 22.03 / 0.59 {\mu}A cm-2 for H. volcanii, N. magadii and E. coli, respectively. When neutral red was used as redox mediator, Pmax was 50.98 and 5.39 {\mu}W cm-2 for H. volcanii and N. magadii respectively. In this paper an archaea MFC is described and compared with other MFC systems; the high salt concentration assayed here, comparable with that used in Pt-catalyzed alkaline hydrogen fuel cells will open new options when MFC scaling-up is the objective, necessary for practical applications.
q-bio.BM:Three-dimensional RNA models fitted into crystallographic density maps exhibit pervasive conformational ambiguities, geometric errors and steric clashes. To address these problems, we present enumerative real-space refinement assisted by electron density under Rosetta (ERRASER), coupled to Python-based hierarchical environment for integrated 'xtallography' (PHENIX) diffraction-based refinement. On 24 data sets, ERRASER automatically corrects the majority of MolProbity-assessed errors, improves the average Rfree factor, resolves functionally important discrepancies in noncanonical structure and refines low-resolution models to better match higher-resolution models.
q-bio.BM:This paper has been withdrawn by the author due to a missing figure
q-bio.BM:We develop a theory of aggregation using statistical mechanical methods. An example of a complicated aggregation system with several levels of structures is peptide/protein self-assembly. The problem of protein aggregation is important for the understanding and treatment of neurodegenerative diseases and also for the development of bio-macromolecules as new materials. We write the effective Hamiltonian in terms of interaction energies between protein monomers, protein and solvent, as well as between protein filaments. The grand partition function can be expressed in terms of a Zimm-Bragg-like transfer matrix, which is calculated exactly and all thermodynamic properties can be obtained. We start with two-state and three-state descriptions of protein monomers using Potts models that can be generalized to include q-states, for which the exactly solvable feature of the model remains. We focus on n X N lattice systems, corresponding to the ordered structures observed in some real fibrils. We have obtained results on nucleation processes and phase diagrams, in which a protein property such as the sheet content of aggregates is expressed as a function of the number of proteins on the lattice and inter-protein or interfacial interaction energies. We have applied our methods to A{\beta}(1-40) and Curli fibrils and obtained results in good agreement with experiments.
q-bio.BM:By studying the literature about Tetracyclines (TCs), it becomes clearly evident that TCs are very dynamic molecules. In some cases, their structure-activity-relationship (SAR) are known, especially against bacteria, while against other targets, they are virtually unknown. In other diverse yields of research, such as neurology, oncology and virology the utility and activity of the tetracyclines are being discovered and are also emerging as new technological fronts. The first aim of this paper is classify the compounds already used in therapy and prepare the schematic structure in which include the next generation of TCs. The aim of this work is introduce a new framework for the classification of old and new TCs, using a medicinal chemistry approach to the structure of that drugs. A fully documented Structure-Activity-Relationship (SAR) is presented with the analysis data of antibacterial and nonantibacterial (antifungal, antiviral and anticancer) tetracyclines. Lipophilicity of functional groups and conformations interchangeably are determining rules in biological activities of TCs.
q-bio.BM:The consequences of recent experimental finding that hydrogen bonds of the anti-parallel $\beta $-sheet in nonspecific binding site of serine proteases become significantly shorter and stronger synchronously with the catalytic act are examined. We investigate the effect of the transformation of an ordinary hydrogen bond into a low-barrier one on the crankshaft motion a peptide group in the anti-parallel $\beta $-sheet. For this purpose we make use of a realistic model of the peptide chain with stringent microscopically derived coupling interaction potential and effective on-site potential. The coupling interaction characterizing the peptide chain rigidity is found to be surprisingly weak and repulsive in character. The effective on-site potential is found to be a hard one, i.e., goes more steep than a harmonic one. At transformation of the ordinary hydrogen bond into the low-barrier one the frequency of crankshaft motion of the corresponding peptide group in the anti-parallel $\beta $-sheet is roughly doubled.
q-bio.BM:During this era of new drug designing, medicinal plants had become a very interesting object of further research. Pharmacology screening of active compound of medicinal plants would be time consuming and costly. Molecular docking is one of the in silico method which is more efficient compare to in vitro or in vivo method for its capability of finding the active compound in medicinal plants. In this method, three-dimensional structure becomes very important in the molecular docking methods, so we need a database that provides information on three-dimensional structures of chemical compounds from medicinal plants in Indonesia. Therefore, this study will prepare a database which provides information of the three dimensional structures of chemical compounds of medicinal plants. The database will be prepared by using MySQL format and is designed to be placed in http://herbaldb.farmasi.ui.ac.id website so that eventually this database can be accessed quickly and easily by users via the Internet.
q-bio.BM:We investigate the possibility that prebiotic homochirality can be achieved exclusively through chiral-selective reaction rate parameters without any other explicit mechanism for chiral bias. Specifically, we examine an open network of polymerization reactions, where the reaction rates can have chiral-selective values. The reactions are neither autocatalytic nor do they contain explicit enantiomeric cross-inhibition terms. We are thus investigating how rare a set of chiral-selective reaction rates needs to be in order to generate a reasonable amount of chiral bias. We quantify our results adopting a statistical approach: varying both the mean value and the rms dispersion of the relevant reaction rates, we show that moderate to high levels of chiral excess can be achieved with fairly small chiral bias, below 10%. Considering the various unknowns related to prebiotic chemical networks in early Earth and the dependence of reaction rates to environmental properties such as temperature and pressure variations, we argue that homochirality could have been achieved from moderate amounts of chiral selectivity in the reaction rates.
q-bio.BM:Channels are Catalysts for Diffusion and in that sense are Enzymes. This idea is useful for the design and interpretation of experiments.
q-bio.BM:Ahmad et al. recently presented an NMR-based model for a bacterial DnaJ J domain:DnaK(Hsp70):ADP complex(1) that differs significantly from the crystal structure of a disulfide linked mammalian auxilin J domain:Hsc70 complex that we previously published(2). They claimed that their model could better account for existing mutational data, was in better agreement with previous NMR studies, and that the presence of a cross-link in our structure made it irrelevant to understanding J:Hsp70 interactions. Here we detail extensive NMR and mutational data relevant to understanding J:Hsp70 function and show that, in fact, our structure is much better able to account for the mutational data and is in much better agreement with a previous NMR study of a mammalian polyoma virus T-ag J domain:Hsc70 complex than is the Ahmad et al. complex, and that our structure is predictive and provides insight into J:Hsp70 interactions and mechanism of ATPase activation.
q-bio.BM:Protein function frequently involves conformational changes with large amplitude on timescales which are difficult and computationally expensive to access using molecular dynamics. In this paper, we report on the combination of three computationally inexpensive simulation methods-normal mode analysis using the elastic network model, rigidity analysis using the pebble game algorithm, and geometric simulation of protein motion-to explore conformational change along normal mode eigenvectors. Using a combination of ELNEMO and FIRST/FRODA software, large-amplitude motions in proteins with hundreds or thousands of residues can be rapidly explored within minutes using desktop computing resources. We apply the method to a representative set of six proteins covering a range of sizes and structural characteristics and show that the method identifies specific types of motion in each case and determines their amplitude limits.
q-bio.BM:The enzyme FoF1-ATP synthase provides the 'chemical energy currency' adenosine triphosphate (ATP) for living cells. Catalysis is driven by mechanochemical coupling of subunit rotation within the enzyme with conformational changes in the three ATP binding sites. Proton translocation through the membrane-bound Fo part of ATP synthase powers a 10-step rotary motion of the ring of c subunits. This rotation is transmitted to the gamma and epsilon subunits of the F1 part. Because gamma and epsilon subunits rotate in 120 deg steps, we aim to unravel this symmetry mismatch by real time monitoring subunit rotation using single-molecule Forster resonance energy transfer (FRET). One fluorophore is attached specifically to the F1 motor, another one to the Fo motor of the liposome-reconstituted enzyme. Photophysical artifacts due to spectral fluctuations of the single fluorophores are minimized by a previously developed duty cycle-optimized alternating laser excitation scheme (DCO-ALEX). We report the detection of reversible elastic deformations between the rotor parts of Fo and F1 and estimate the maximum angular displacement during the load-free rotation using Monte Carlo simulations
q-bio.BM:A strain of Halomonas bacteria, GFAJ-1, has been reported to be able to use arsenate as a nutrient when phosphate is limiting, and to specifically incorporate arsenic into its DNA in place of phosphorus. However, we have found that arsenate does not contribute to growth of GFAJ-1 when phosphate is limiting and that DNA purified from cells grown with limiting phosphate and abundant arsenate does not exhibit the spontaneous hydrolysis expected of arsenate ester bonds. Furthermore, mass spectrometry showed that this DNA contains only trace amounts of free arsenate and no detectable covalently bound arsenate.
q-bio.BM:The fundamental law for protein folding is the Thermodynamic Principle: the amino acid sequence of a protein determines its native structure and the native structure has the minimum Gibbs free energy. If all chemical problems can be answered by quantum mechanics, there should be a quantum mechanics derivation of Gibbs free energy formula G(X) for every possible conformation X of the protein. We apply quantum statistics to derive such a formula. For simplicity, only monomeric self folding globular proteins are covered. We point out some immediate applications of the formula. We show that the formula explains the observed phenomena very well. It gives a unified explanation to both folding and denaturation; it explains why hydrophobic effect is the driving force of protein folding and clarifies the role played by hydrogen bonding; it explains the successes and deficients of various surface area models. The formula also gives a clear kinetic force of the folding: Fi(X) = - \nablaxi G(X). This also gives a natural way to perform the ab initio prediction of protein structure, minimizing G(X) by Newton's fastest desciending method.
q-bio.BM:Learning how proteins fold will hardly have any impact in the way conventional -- active site centered -- drugs are designed. On the other hand, this knowledge is proving instrumental in defining a new paradigm for the identification of drugs against any target protein: folding inhibition. Targeting folding renders drugs less prone to elicit spontaneous genetic mutations which in many cases, notably in connection with viruses like the Human Immunodeficiency Virus (HIV), can block therapeutic action. From the progress which has taken place during the last years in the understanding of the becoming of a protein, and how to read from the corresponding sequences the associated three-dimensional, biologically active, native structure, the idea of non-conventional (folding) inhibitors and thus of leads to eventual drugs to fight disease, arguably, without creating resistance, emerges as a distinct possibility.
q-bio.BM:Ever since the disorder of proteins is the main cause for many diseases. As compared with other disorders, the major reason that causes disease is of structural inability of many proteins. The potentially imminent availability of recent datasets helps one to discover the protein disorders, however in majority of cases, the stability of proteins depend on the carbon content. Addressing this distinct feature, it is possible to hit upon the carbon distribution along the sequence and can easily recognize the stable nature of protein. There are certain reported mental disorders which fall in to this category. Regardless, such kind of disorder prone protein FMR1p (Fragile X mental retardation 1 protein) is identified as the main cause for the disease Fragile X syndrome. This paper deals with the identification of defects in the FMR1 protein sequence considering the carbon contents along the sequence. This attempt is to evaluate the stability of proteins, accordingly the protein disorders in order to improvise the certain Biological functions of proteins to prevent disease. The transition of the disorder to order protein involves careful considerations and can be achieved by detecting the unstable region that lacks hydrophobicity. This work focuses the low carbon content in the FMR1 protein so as to attain the stable status in future to reduce the morbidity rate caused by Fragile X syndrome for the society.
q-bio.BM:Influenza virus evolves to escape from immune system antibodies that bind to it. We used free energy calculations with Einstein crystals as reference states to calculate the difference of antibody binding free energy ($\Delta\Delta G$) induced by amino acid substitution at each position in epitope B of the H3N2 influenza hemagglutinin, the key target for antibody. A substitution with positive $\Delta\Delta G$ value decreases the antibody binding constant. On average an uncharged to charged amino acid substitution generates the highest $\Delta\Delta G$ values. Also on average, substitutions between small amino acids generate $\Delta\Delta G$ values near to zero. The 21 sites in epitope B have varying expected free energy differences for a random substitution. Historical amino acid substitutions in epitope B for the A/Aichi/2/1968 strain of influenza A show that most fixed and temporarily circulating substitutions generate positive $\Delta\Delta G$ values. We propose that the observed pattern of H3N2 virus evolution is affected by the free energy landscape, the mapping from the free energy landscape to virus fitness landscape, and random genetic drift of the virus. Monte Carlo simulations of virus evolution are presented to support this view.
q-bio.BM:Force field and first principles molecular dynamics simulations on complexes of pig liver esterase (pig liver isoenzymes and a mutant) and selected substrates (1-phenyl-1-ethyl acetate, 1- phenyl-2-butylacetate, proline-{\beta}-naphthylamide and methyl butyrate) are presented. By restrained force field simulations the access of the substrate to the hidden active site was probed. For a few substrates spontaneous access to the active site via a well defined entrance channel was found. The structure of the tetrahedral intermediate was simulated for several substrates and our previous assignment of GLU 452 instead of GLU 336 was confirmed. It was shown that the active site readily adapts to the embedded substrate involving a varying number of hydrophobic residues in the neighborhood. This puts into question key-lock models for enantioselectivity. Ab initio molecular dynamics showed that the structures we found for the tetrahedral intermediate in force field simulations are consistent with the presumed mechanism of ester cleavage. Product release from the active site as final step of the enzymatic reaction revealed to be very slow and took already more than 20ns for the smallest product, methanol.
q-bio.BM:Protein function often involves changes between different conformations. Central questions are how these conformational changes are coupled to the binding or catalytic processes during which they occur, and how they affect the catalytic rates of enzymes. An important model system is the enzyme dihydrofolate reductase (DHFR) from E. coli, which exhibits characteristic conformational changes of the active-site loop during the catalytic step and during unbinding of the product. In this article, we present a general kinetic framework that can be used (1) to identify the ordering of events in the coupling of conformational changes, binding and catalysis and (2) to determine the rates of the substeps of coupled processes from a combined analysis of NMR R2 relaxation dispersion experiments and traditional enzyme kinetics measurements. We apply this framework to E. coli DHFR and find that the conformational change during product unbinding follows a conformational-selection mechanism, i.e. the conformational change occurs predominantly prior to unbinding. The conformational change during the catalytic step, in contrast, is an induced change, i.e. the change occurs after the chemical reaction. We propose that the reason for these conformational changes, which are absent in human and other vertebrate DHFRs, is robustness of the catalytic rate against large pH variations and changes to substrate/product concentrations in E. coli.
q-bio.BM:Ion channels are proteins with holes down their middle that control the flow of ions and electric current across otherwise impermeable biological membranes. The flow of sodium, potassium, calcium (divalent), and chloride ions have been central issues in biology for more than a century. The flow of current is responsible for the signals of the nervous system that propagate over long distances (meters). The concentration of divalent calcium ions is a 'universal' signal that controls many different systems inside cells. The concentration of divalent calcium and other messenger ions has a role in life rather like the role of the voltage in different wires of a computer. Ion channels also help much larger solutes (e.g., organic acid and bases; perhaps polypeptides) to cross membranes but much less is known about these systems. Ion channels can select and control the movement of different types of ions because the holes in channel proteins are a few times larger than the (crystal radii of the) ions themselves. Biology uses ion channels as selective valves to control flow and thus concentration of crucial chemical signals. For example, the concentration of divalent calcium ions determines whether muscles contract or not. Ion channels have a role in biology similar to the role of transistors in computers and technology. Ion Channels Control Concentrations Important To Life The Way Computers Control Voltages Important To Computers.
q-bio.BM:Chemistry is about chemical reactions. Chemistry is about electrons changing their configurations as atoms and molecules react. Chemistry studies reactions as if they occurred in ideal infinitely dilute solutions. But most reactions occur in nonideal solutions. Then everything (charged) interacts with everything else (charged) through the electric field, which is short and long range extending to boundaries of the system. Mathematics has recently been developed to deal with interacting systems of this sort. The variational theory of complex fluids has spawned the theory of liquid crystals. In my view, ionic solutions should be viewed as complex fluids. In both biology and electrochemistry ionic solutions are mixtures highly concentrated (~10M) where they are most important, near electrodes, nucleic acids, enzymes, and ion channels. Calcium is always involved in biological solutions because its concentration in a particular location is the signal that controls many biological functions. Such interacting systems are not simple fluids, and it is no wonder that analysis of interactions, such as the Hofmeister series, rooted in that tradition, has not succeeded as one would hope. We present a variational treatment of hard spheres in a frictional dielectric. The theory automatically extends to spatially nonuniform boundary conditions and the nonequilibrium systems and flows they produce. The theory is unavoidably self-consistent since differential equations are derived (not assumed) from models of (Helmholtz free) energy and dissipation of the electrolyte. The origin of the Hofmeister series is (in my view) an inverse problem that becomes well posed when enough data from disjoint experimental traditions are interpreted with a self-consistent theory.
q-bio.BM:A quantum mechanical model on histone modification is proposed. Along with the methyl / acetate or other groups bound to the modified residues the torsion angles of the nearby histone chain are supposed to participate in the quantum transition cooperatively. The transition rate W is calculated based on the non-radiative quantum transition theory in adiabatic approximation. By using W's the reaction equations can be written for histone modification and the histone modification level can be calculable from the equations, which is decided by not only the atomic group bound to the modified residue, but also the nearby histone chain. The theory can explain the mechanism for the correlation between a pair of chromatin markers observed in histone modification. The temperature dependence and the coherence-length dependence of histone modification are deduced. Several points for checking the proposed theory and the quantum nature of histone modification are suggested as follows: 1, The relationship between lnW and 1/T is same as usual protein folding. The non-Arhenius temperature dependence of the histone modification level is predicted. 2, The variation of histone modification level through point mutation of some residues on the chain is predicted since the mutation may change the coherence-length of the system. 3, Multi-site modification obeys the quantum superposition law and the comparison between multi-site transition and single modification transition gives an additional clue to the testing of the quantum nature of histone modification.
q-bio.BM:Inverted repeat (IR) sequences in DNA can form non-canonical cruciform structures to relieve torsional stress. We use Monte Carlo simulations of a recently developed coarse-grained model of DNA to demonstrate that the nucleation of a cruciform can proceed through a cooperative mechanism. Firstly, a twist-induced denaturation bubble must diffuse so that its midpoint is near the centre of symmetry of the IR sequence. Secondly, bubble fluctuations must be large enough to allow one of the arms to form a small number of hairpin bonds. Once the first arm is partially formed, the second arm can rapidly grow to a similar size. Because bubbles can twist back on themselves, they need considerably fewer bases to resolve torsional stress than the final cruciform state does. The initially stabilised cruciform therefore continues to grow, which typically proceeds synchronously, reminiscent of the S-type mechanism of cruciform formation. By using umbrella sampling techniques we calculate, for different temperatures and superhelical densities, the free energy as a function of the number of bonds in each cruciform along the correlated but non-synchronous nucleation pathways we observed in direct simulations.
q-bio.BM:Emerging high-throughput technologies have led to a deluge of putative non-coding RNA (ncRNA) sequences identified in a wide variety of organisms. Systematic characterization of these transcripts will be a tremendous challenge. Homology detection is critical to making maximal use of functional information gathered about ncRNAs: identifying homologous sequence allows us to transfer information gathered in one organism to another quickly and with a high degree of confidence. ncRNA presents a challenge for homology detection, as the primary sequence is often poorly conserved and de novo secondary structure prediction and search remains difficult. This protocol introduces methods developed by the Rfam database for identifying "families" of homologous ncRNAs starting from single "seed" sequences using manually curated sequence alignments to build powerful statistical models of sequence and structure conservation known as covariance models (CMs), implemented in the Infernal software package. We provide a step-by-step iterative protocol for identifying ncRNA homologs, then constructing an alignment and corresponding CM. We also work through an example for the bacterial small RNA MicA, discovering a previously unreported family of divergent MicA homologs in genus Xenorhabdus in the process.
q-bio.BM:DNA is subject to large deformations in a wide range of biological processes. Two key examples illustrate how such deformations influence the readout of the genetic information: the sequestering of eukaryotic genes by nucleosomes, and DNA looping in transcriptional regulation in both prokaryotes and eukaryotes. These kinds of regulatory problems are now becoming amenable to systematic quantitative dissection with a powerful dialogue between theory and experiment. Here we use a single-molecule experiment in conjunction with a statistical mechanical model to test quantitative predictions for the behavior of DNA looping at short length scales, and to determine how DNA sequence affects looping at these lengths. We calculate and measure how such looping depends upon four key biological parameters: the strength of the transcription factor binding sites, the concentration of the transcription factor, and the length and sequence of the DNA loop. Our studies lead to the surprising insight that sequences that are thought to be especially favorable for nucleosome formation because of high flexibility lead to no systematically detectable effect of sequence on looping, and begin to provide a picture of the distinctions between the short length scale mechanics of nucleosome formation and looping.
q-bio.BM:For decades, dimethyl sulfate (DMS) mapping has informed manual modeling of RNA structure in vitro and in vivo. Here, we incorporate DMS data into automated secondary structure inference using a pseudo-energy framework developed for 2'-OH acylation (SHAPE) mapping. On six non-coding RNAs with crystallographic models, DMS- guided modeling achieves overall false negative and false discovery rates of 9.5% and 11.6%, comparable or better than SHAPE-guided modeling; and non-parametric bootstrapping provides straightforward confidence estimates. Integrating DMS/SHAPE data and including CMCT reactivities give small additional improvements. These results establish DMS mapping - an already routine technique - as a quantitative tool for unbiased RNA structure modeling.
q-bio.BM:Hydrogen bonds are a common feature in protein folding and aggregation. Due to their chemical peculiarities in terms of strength and directionality, a particular attention must be paid to the definition of the hydrogen bond potential itself. This global target has been tackled through a computational approach based on a minimalist description of the protein and the proper design of algorithms, mainly using Monte Carlo and Kinetic Monte Carlo methods. We have designed a hydrogen bond potential, see J. Chem. Phys. 132, 235102 (2010). We have been performed a complete study of sequenceless peptide systems under different conditions, such as temperature and concentration. To carry out full protein studies, we need additional potentials to describe tertiary interactions. We have discussed two different points of view. The first one is the combination of the hydrogen bond potential with a structure-based one. We have evaluated the implications of a proper definition of hydrogen bonds in the thermodynamic and dynamic aspects of protein folding. See Biophys. J. 101, 1474-1482 (2011). We have undertaken a second strategy, combining a generic hydrophobic model with the hydrogen bond one. The two main factors of folding and aggregation are merged, then, to create a simple but complete potential. Thanks to it, we have re-studied peptide aggregation including sequence. Besides, we have simulated complete proteins with different folded shapes. We have analyzed the competition between folding and aggregation, and how sequence and hydrogen bonds can influence the interplay between them. See J. Chem. Phys. 136, 215103 (2012).
q-bio.BM:Consistently predicting biopolymer structure at atomic resolution from sequence alone remains a difficult problem, even for small sub-segments of large proteins. Such loop prediction challenges, which arise frequently in comparative modeling and protein design, can become intractable as loop lengths exceed 10 residues and if surrounding side-chain conformations are erased. This article introduces a modeling strategy based on a 'stepwise ansatz', recently developed for RNA modeling, which posits that any realistic all-atom molecular conformation can be built up by residue-by-residue stepwise enumeration. When harnessed to a dynamic-programming-like recursion in the Rosetta framework, the resulting stepwise assembly (SWA) protocol enables enumerative sampling of a 12 residue loop at a significant but achievable cost of thousands of CPU-hours. In a previously established benchmark, SWA recovers crystallographic conformations with sub-Angstrom accuracy for 19 of 20 loops, compared to 14 of 20 by KIC modeling with a comparable expenditure of computational power. Furthermore, SWA gives high accuracy results on an additional set of 15 loops highlighted in the biological literature for their irregularity or unusual length. Successes include cis-Pro touch turns, loops that pass through tunnels of other side-chains, and loops of lengths up to 24 residues. Remaining problem cases are traced to inaccuracies in the Rosetta all-atom energy function. In five additional blind tests, SWA achieves sub-Angstrom accuracy models, including the first such success in a protein/RNA binding interface, the YbxF/kink-turn interaction in the fourth RNA-puzzle competition. These results establish all-atom enumeration as a systematic approach to protein structure that can leverage high performance computing and physically realistic energy functions to more consistently achieve atomic resolution.
q-bio.BM:Self-associates of nucleic acid components (stacking trimers and tetramers of the base pairs of nucleic acids) and short fragments of nucleic acids are nanoparticles (linear sizes of these particles are more than 10 A. Modern quantum-mechanical methods and softwares allow one to perform ab initio calculations of the systems consisting of 150-200 atoms with enough large basis sets (for example, 6-31G*). The aim of this work is to reveal the peculiarities of molecular and electronic structures, as well as the energy features of nanoparticles of nucleic acid components.
q-bio.BM:Influenza virus contains two highly variable envelope glycoproteins, hemagglutinin (HA) and neuraminidase (NA). The structure and properties of HA, which is responsible for binding the virus to the cell that is being infected, change significantly when the virus is transmitted from avian or swine species to humans. Here we focus on much smaller human individual evolutionary amino acid mutational changes in NA, which cleaves sialic acid groups and is required for influenza virus replication. We show that very small amino acid changes can be monitored very accurately across many Uniprot and NCBI strains using hydropathicity scales to quantify the roughness of water film packages. Quantitative sequential analysis is most effective with the differential hydropathicity scale based on protein self-organized criticality (SOC). NA exhibits punctuated evolution at the molecular scale, millions of times smaller than the more familiar species scale, and thousands of times smaller than the genomic scale. Our analysis shows that large-scale vaccination programs have been responsible for a very large convergent reduction in influenza severity in the last century, a reduction which is hidden from short-term studies of vaccine effectiveness. Hydropathic analysis is capable of interpreting and even predicting trends of functional changes in mutation prolific viruses.
q-bio.BM:Influenza virus contains two highly variable envelope glycoproteins, hemagglutinin (HA) and neuraminidase (NA). The structure and properties of HA, which is responsible for binding the virus to the cell that is being infected, change significantly when the virus is transmitted from avian or swine species to humans. Previously we identified much smaller human individual evolutionary amino acid mutational changes in NA, which cleaves sialic acid groups and is required for influenza virus replication. We showed that these smaller changes can be monitored very accurately across many Uniprot and NCBI strains using hydropathicity scales to quantify the roughness of water film packages, which increases gradually due to migration, but decreases abruptly under large-scale vaccination pressures. Here we show that, while HA evolution is much more complex, it still shows abrupt punctuation changes linked to those of NA. HA exhibits proteinquakes, which resemble earthquakes and are related to hydropathic shifting of sialic acid binding regions. HA proteinquakes based on sialic acid interactions are required for optimal balance between the receptor-binding and receptor-destroying activities of HA and NA for efficient virus replication. Our comprehensive results present an historical (1945-2011) panorama of HA evolution over thousands of strains, and are consistent with many studies of HA and NA interactions based on a few mutations of a few strains. While the common influenza virus discussed here has been rendered almost harmless by decades of vaccination programs, the sequential decoding lessons learned here are applicable to other viruses that are emerging as powerful weapons for controlling and even curing common organ cancers. Those engineered oncolytic drugs will be discussed in future papers.
q-bio.BM:While the concepts involved in Self-Organized Criticality have stimulated thousands of theoretical models, only recently have these models addressed problems of biological and clinical importance. Here we outline how SOC can be used to engineer hybrid viral proteins whose properties, extrapolated from those of known strains, may be sufficiently effective to cure cancer.
q-bio.BM:Although mechanical properties of DNA are well characterized at the kilo base-pair range, a number of recent experiments have suggested that DNA is more flexible at shorter length scales, which correspond to the regime that is crucial for cellular processes such as DNA packaging and gene regulation. Here, we perform a systematic study of the effective elastic properties of DNA at different length scales by probing the conformation and fluctuations of DNA from single base-pair level up to four helical turns, using trajectories from atomistic simulation. We find evidence that supports cooperative softening of the stretch modulus and identify the essential modes that give rise to this effect. The bend correlation exhibits modulations that reflect the helical periodicity, while it yields a reasonable value for the effective persistence length, and the twist modulus undergoes a smooth crossover---from a relatively smaller value at the single base-pair level to the bulk value---over half a DNA-turn.
q-bio.BM:Mutations and oxidative modification in the protein Cu,Zn superoxide dismutase (SOD1) have been implicated in the death of motor neurons in amyotrophic lateral sclerosis (ALS), a presently incurable, invariably fatal neurodegenerative disease. Here we employ steered, all-atom molecular dynamics simulations in implicit solvent to investigate the significance of either mutations or post-translational modifications (PTMs) to SOD1 on metal affinity, dimer stability, and mechanical malleability. The work required to induce moderate structural deformations as a function of sequence index constitutes a "mechanical fingerprint" measuring structural rigidity in the native basin, from which we are able to unambiguously distinguish wild-type (WT) SOD1 from PTM variants, and measure the severity of a given PTM on structural integrity. The cumulative distribution of work values provided a way to cleanly discriminate between SOD1 variants. Disulfide reduction destabilizes dimer stability more than the removal of either metal, but not moreso than the removal of both metals. Intriguingly, we found that disulfide reduction mechanically stabilizes apo SOD1 monomer, underscoring the differences between native basin mechanical properties and equilibrium thermodynamic stabilities, and elucidating the presence of internal stress in the apo state. All PTMs and ALS-associated mutants studied showed an increased tendency to lose either Cu or Zn, and to monomerize- processes known to be critical in the progression of ALS. The valence of Cu strongly modulates its binding free energy. As well, several mutants were more susceptible to loss of metals and monomerization than the disulfide-reduced or apo forms of SOD1. Distance constraints are required to calculate free energies for metal binding and dimer separation, which are validated using thermodynamic cycles.
q-bio.BM:Availability of high-resolution crystal structures of ribosomal subunits of different species opens a route to investigate about molecular interactions between its constituents and stabilization strategy. Structural analysis of the small ribosomal subunit shows that primary binder proteins are mainly employed in stabilizing the folded ribosomal RNA by their high negative free energy of association, where tertiary binders mainly help to stabilize protein-protein interfaces. Secondary binders perform both the functions. Conformational changes of prokaryotic and eukaryotic ribosomal proteins due to complexation with 16S ribosomal RNA are linearly correlated with their RNA-interface area and free energy of association. The proteins having long extensions buried within ribosomal RNA have more flexible structures than those found on the subunit surface. Thermus thermophilus ribosomal proteins undergo high conformational changes compared to those of Escherichia coli, assuring structural stability at high temperature environment. The general stabilization strategy of ribosomal protein-RNA interfaces is shown, where high interface polarity ensures high surface density of Hydrogen bonds even with low base/backbone ratio. Polarity is regulated in evolutionary strategy of ribosomal proteins. Thus, the habitat environmental conditions of the two species sweet up their ribosomal protein-RNA interfaces to alter its physical parameters in order of stabilization.
q-bio.BM:Motivation: Protein surface roughness is fractal in nature. Mass, hydrophobicity, polarizability distributions of protein interior are fractal too, as are the distributions of dipole moments, aromatic residues, and many other structural determinants. The open-access server ProtFract, presents a reliable way to obtain numerous fractal-dimension and correlation-dimension based results to quantify the symmetry of self-similarity in distributions of various properties of protein interior and exterior.   Results: Fractal dimension based comparative analyses of various biophysical properties of Ras superfamily proteins were conducted. Though the extent of sequence and functional overlapping across Ras superfamily structures is extremely high, results obtained from ProtFract investigation are found to be sensitive to detect differences in the distribution of each of the properties. For example, it was found that the RAN proteins are structurally most stable amongst all Ras superfamily proteins, the ARFs possess maximum extent of unused hydrophobicity in their structures, RAB protein interiors have electrostatically least conducive environment, GEM/REM/RAD proteins possess exceptionally high symmetry in the structural organization of their active chiral centres, neither hydrophobicity nor columbic interactions play significant part in stabilizing the RAS proteins but aromatic interactions do, though cation-pi interactions are found to be more dominant in RAN than in RAS proteins. Ras superfamily proteins can best be classified with respect to their class-specific pi-pi and cation-pi interaction symmetries.   Availability: ProtFract is freely available online at the URL: http://bioinfo.net.in/protfract/index.html
q-bio.BM:Upon studying the B-Factors of all the atoms of all non-redundant proteins belonging to 76 most commonly found structural domains of all four major structural classes, it was found that the residue mobility has decreased during the course of evolution. Though increased residue-flexibility was preferred in the early stages of protein structure evolution, less flexibility is preferred in the medieval and recent stages. GLU is found to be the most flexible residue while VAL recorded to have the least flexibility. General trends in decrement of B-Factors conformed to the general trend in the order of emergence of protein structural domains. Decrement of B-Factor is observed to be most decisive (monotonic and uniform) for VAL, while evolution of CYS and LYS flexibility is found to be most skewed. Barring CYS, flexibility of all the residues is found to have increased during evolution of alpha by beta folds, however flexibility of all the residues (barring CYS) is found to have decreased during evolution of all beta folds. Only in alpha by beta folds the tendency of preferring higher residue mobility could be observed, neither alpha plus beta, nor all alpha nor all beta folds were found to support higher residue-mobility. In all the structural classes, the effect of evolutionary constraint on polar residues is found to follow an exactly identical trend as that on hydrophobic residues, only the extent of these effects are found to be different. Though protein size is found to be decreasing during evolution, residue mobility of proteins belonging to ancient and old structural domains showed strong positive dependency upon protein size, however for medieval and recent domains such dependency vanished. It is found that to optimize residue fluctuations, alpha by beta class of proteins are subjected to more stringent evolutionary constraints.
q-bio.BM:Recent experimental data indicate that the elastic wormlike rod model of DNA that works well on long length scales may break down on shorter scales relevant to biology. According to Noy and Golestanian (Phys. Rev. Lett. 109, 228101, 2012) molecular dynamics (MD) simulations predict DNA rigidity close to experimental data and confirm one scenario of such breakdown, namely, that for lengths of a few helical turns, DNA dynamics exhibit long-range bending and stretching correlations. Earlier studies using similar forcefields concluded that (i) MD systematically overestimate the DNA rigidity, and (ii) no deviations from the WLR model are detectable. Here it is argued that the data analysis in the above mentioned paper was incorrect and that the earlier conclusions are valid.
q-bio.BM:Superoxide dismutase-1 (SOD1) is a ubiquitous, Cu and Zn binding, free radical defense enzyme whose misfolding and aggregation play a potential key role in amyotrophic lateral sclerosis, an invariably fatal neurodegenerative disease. Over 150 mutations in SOD1 have been identified with a familial form of the disease, but it is presently not clear what unifying features, if any, these mutants share to make them pathogenic. Here, we develop several new computational assays for probing the thermo-mechanical properties of both ALS-associated and rationally-designed SOD1 variants. Allosteric interaction free energies between residues and metals are calculated, and a series of atomic force microscopy experiments are simulated with variable tether positions, to quantify mechanical rigidity "fingerprints" for SOD1 variants. Mechanical fingerprinting studies of a series of C-terminally truncated mutants, along with an analysis of equilibrium dynamic fluctuations while varying native constraints, potential energy change upon mutation, frustratometer analysis, and analysis of the coupling between local frustration and metal binding interactions for a glycine scan of 90 residues together reveal that the apo protein is internally frustrated, that these internal stresses are partially relieved by mutation but at the expense of metal-binding affinity, and that the frustration of a residue is directly related to its role in binding metals. This evidence points to apo SOD1 as a strained intermediate with "self-allostery" for high metal-binding affinity. Thus, the prerequisites for the function of SOD1 as an antioxidant compete with apo state thermo-mechanical stability, increasing the susceptibility of the protein to misfold in the apo state.
q-bio.BM:Reply to Comment on 'Length Scale Dependence of DNA Mechanical Properties'
q-bio.BM:The conformational change of biological macromolecule is investigated from the point of quantum transition. A quantum theory on protein folding is proposed. Compared with other dynamical variables such as mobile electrons, chemical bonds and stretching-bending vibrations the molecular torsion has the lowest energy and can be looked as the slow variable of the system. Simultaneously, from the multi-minima property of torsion potential the local conformational states are well defined. Following the idea that the slow variables slave the fast ones and using the nonadiabaticity operator method we deduce the Hamiltonian describing conformational change. It is proved that the influence of fast variables on the macromolecule can fully be taken into account through a phase transformation of slow variable wave function. Starting from the conformation- transition Hamiltonian the nonradiative matrix element is calculated in two important cases: A, only electrons are fast variables and the electronic state does not change in the transition process; B, fast variables are not limited to electrons but the perturbation approximation can be used. Then, the general formulas for protein folding rate are deduced. The analytical form of the formula is utilized to study the temperature dependence of protein folding rate and the curious non-Arrhenius temperature relation is interpreted. The decoherence time of quantum torsion state is estimated and the quantum coherence degree of torsional angles in the protein folding is studied by using temperature dependence data. The proposed folding rate formula gives a unifying approach for the study of a large class problems of biological conformational change.
q-bio.BM:It is established that prion protein is the sole causative agent in a number of diseases in humans and animals. However, the nature of conformational changes that the normal cellular form PrPC undergoes in the conversion process to a self-replicating state is still not fully understood. The ordered C-terminus of PrPC proteins has three helices (H1, H2, and H3). Here, we use the Statistical Coupling Analysis (SCA) to infer co-variations at various locations using a family of evolutionarily related sequences, and the response of mouse and human PrPCs to mechanical force to decipher the initiation sites for transition from PrPC to an aggregation prone PrP* state. The sequence-based SCA predicts that the clustered residues in non-mammals are localized in the stable core (near H1) of PrPC whereas in mammalian PrPC they are localized in the frustrated helices H2 and H3 where most of the pathogenic mutations are found. Force-extension curves and free energy profiles as a function of extension of mouse and human PrPC in the absence of disulfide (SS) bond between residues Cys179 and Cys214, generated by applying mechanical force to the ends of the molecule, show a sequence of unfolding events starting first with rupture of H2 and H3. This is followed by disruption of structure in two strands. Helix H1, stabilized by three salt-bridges, resists substantial force before unfolding. Force extension profiles and the dynamics of rupture of tertiary contacts also show that even in the presence of SS bond the instabilities in most of H3 and parts of H2 still determine the propensity to form the PrP* state. In mouse PrPC with SS bond there are about ten residues that retain their order even at high forces.
q-bio.BM:Antimicrobial peptides are a class of small, usually positively charged amphiphilic peptides that are used by the innate immune system to combat bacterial infection in multicellular eukaryotes. Antimicrobial peptides are known for their broad-spectrum antimicrobial activity and thus can be used as a basis for a development of new antibiotics against multidrug-resistant bacteria. The most challengeous task on the way to a therapeutic use of antimicrobial peptides is a rational design of new peptides with enhanced activity and reduced toxicity. Here we report a molecular dynamics and circular dichroism study of a novel synthetic antimicrobial peptide D51. This peptide was earlier designed by Loose et al. using a linguistic model of natural antimicrobial peptides. Molecular dynamics simulation of the peptide folding in explicit solvent shows fast formation of two antiparallel beta strands connected by a beta-turn that is confirmed by circular dichroism measurements. Obtained from simulation amphipatic conformation of the peptide is analysed and possible mechanism of its interaction with bacterial membranes together with ways to enhance its antibacterial activity are suggested.
physics.optics:A numerical study of the properties of Gaussian pulses propagating in planar waveguide under the combined effect of positive Kerr-type nonlinearity, diffraction in planar waveguides and anomalous or normal dispersion, is presented. It is demonstrated how the relative strength of dispersion and diffraction, the strength of nonlinearity and the initial spatial and temporal pulse chirps effect on the parameters of pulse compression, such as the maximal compression factor and the distance to the point of maximal compression.
physics.optics:We report on a theoretical and numerical investigation of the switching of power in new hybrid models of nonlinear coherent couplers consisting of optical slab waveguides with various orders of nonlinearity. The first model consists of two guides with second-order instead of the usual third-order susceptibilities as typified by the Jensen coupler. This second-order system is shown to have a power self-trapping transition at a critical power greater than the third-order susceptibility coupler. Next, we consider a mixed coupler composed of a second-order guide coupled to a third-order guide and show that, although it does not display a rigorous self-trapping transition, for a particular choice of parameters it does show a fairly abrupt trapping of power at a lower power than in the third-order coupler. By coupling this mixed nonlinear pair to a third, purely linear guide, the power trapping can be brought to even lower levels and in this way a satisfactory switching profile can be achieved at less than one sixth the input power needed in the Jensen coupler.
physics.optics:It is noted that the Jones-matrix formalism for polarization optics is a six-parameter two-by-two representation of the Lorentz group. It is shown that the four independent Stokes parameters form a Minkowskian four-vector, just like the energy-momentum four-vector in special relativity. The optical filters are represented by four-by-four Lorentz-transformation matrices. This four-by-four formalism can deal with partial coherence described by the Stokes parameters. A four-by-four matrix formulation is given for decoherence effects on the Stokes parameters, and a possible experiment is proposed. It is shown also that this Lorentz-group formalism leads to optical filters with a symmetry property corresponding to that of two-dimensional Euclidean transformations.
physics.optics:We study the electromagnetic scattering by multilayered biperiodic aggregates of dielectric layers and gratings of conducting plates. We show that the characteristic lengths of such structures provide a good control of absorption bands. The influence of the physical parameters of the problem (sizes, impedances) is discussed.
physics.optics:The propagation of an electromagnetic pulse in a plasma is studied for pulse durations that are comparable to the plasma period. When the carrier frequency of the incident pulse is much higher than the plasma frequency, the pulse propagates without distortion at its group speed. When the carrier frequency is comparable to the plasma frequency, the pulse is distorted and leaves behind it an electromagnetic wake.
physics.optics:We present scattering from many body systems in a new light. In place of the usual van Hove treatment, (applicable to a wide range of scattering processes using both photons and massive particles) based on plane waves, we calculate the scattering amplitude as a space-time integral over the scattering sample for an incident wave characterized by its correlation function which results from the shaping of the wave field by the apparatus. Instrument resolution effects - seen as due to the loss of correlation caused by the path differences in the different arms of the instrument are automatically included and analytic forms of the resolution function for different instruments are obtained. The intersection of the moving correlation volumes (those regions where the correlation functions are significant) associated with the different elements of the apparatus determines the maximum correlation lengths (times) that can be observed in a sample, and hence, the momentum (energy) resolution of the measurement. This geometrical picture of moving correlation volumes derived by our technique shows how the interaction of the scatterer with the wave field shaped by the apparatus proceeds in space and time. Matching of the correlation volumes so as to maximize the intersection region yields a transparent, graphical method of instrument design. PACS: 03.65.Nk, 3.80 +r, 03.75, 61.12.B
physics.optics:In this paper we extend for the case of Maxwell equations the "X-shaped" solutions previously found in the case of scalar (e.g., acoustic) wave equations. Such solutions are localized in theory, i.e., diffraction-free and particle-like (wavelets), in that they maintain their shape as they propagate. In the electromagnetic case they are particularly interesting, since they are expected to be Superluminal. We address also the problem of their practical, approximate production by finite (dynamic) radiators. Finally, we discuss the appearance of the X-shaped solutions from the purely geometric point of view of the Special Relativity theory.   [PACS nos.: 03.50.De; 1.20.Jb; 03.30.+p; 03.40.Kf; 14.80.-j.   Keywords: X-shaped waves; localized solutions to Maxwell equations; Superluminal waves; Bessel beams; Limited-dispersion beams; electromagnetic wavelets; Special Relativity; Extended Relativity].
physics.optics:This paper has been withdrawn by the authors until some changes are made.
physics.optics:The effect of dispersion or diffraction on zero-velocity solitons is studied for the generalized massive Thirring model describing a nonlinear optical fiber with grating or parallel-coupled planar waveguides with misaligned axes. The Thirring solitons existing at zero dispersion/diffraction are shown numerically to be separated by a finite gap from three isolated soliton branches. Inside the gap, there is an infinity of multi-soliton branches. Thus, the Thirring solitons are structurally unstable. In another parameter region (far from the Thirring limit), solitons exist everywhere.
physics.optics:We theoretically study reflection of light by a phase-conjugating mirror preceded by a partially reflecting normal mirror. The presence of a suitably chosen normal mirror in front of the phase conjugator is found to greatly enhance the total phase-conjugate reflected power, even up to an order of magnitude. Required conditions are that the phase-conjugating mirror itself amplifies upon reflection and that constructive interference of light in the region between the mirrors takes place. We show that the phase-conjugate reflected power then exhibits a maximum as a function of the transmittance of the normal mirror.
physics.optics:Reliable control of the deposition process of optical films and coatings frequently requires monitoring of the refractive index profile throughout the layer. In the present work a simple in situ approach is proposed which uses a WKBJ matrix representation of the optical transfer function of a single thin film on a substrate. Mathematical expressions are developed which represent the minima and maxima envelopes of the curves transmittance-vs-time and reflectance-vs-time. The refractive index and extinction coefficient depth profiles of different films are calculated from simulated spectra as well as from experimental data obtained during PECVD of silicon-compound films. Variation of the deposition rate with time is also evaluated from the position of the spectra extrema as a function of time. The physical and mathematical limitations of the method are discussed.
physics.optics:A new definition for the electromagnetic field velocity is proposed. The velocity depends on the physical fields.
physics.optics:We have fabricated light emitting diodes (LEDs) with Schottky contacts on Si-nanocrystals formed by simple techniques as used for standard Si devices. Orange electroluminescence (EL) from these LEDs could be seen with the naked eye at room temperature when a reverse bias voltage was applied. The EL spectrum has a major peak with a photon energy of 1.9 eV and a minor peak with a photon energy of 2.2 eV. Since the electrons and holes are injected into the radiative recombination centers related to nanocrystals through avalanche breakdown, the voltage needed for a visible light emission is reduced to 4.0 - 4.5 V, which is low enough to be applied by a standard Si transistor.
physics.optics:A general model is presented for coupling of high-$Q$ whispering-gallery modes in optical microsphere resonators with coupler devices possessing discrete and continuous spectrum of propagating modes. By contrast to conventional high-Q optical cavities, in microspheres independence of high intrinsic quality-factor and controllable parameters of coupling via evanescent field offer variety of regimes earlier available in RF devices. The theory is applied to the earlier-reported data on different types of couplers to microsphere resonators and complemented by experimental demonstration of enhanced coupling efficiency (about 80%) and variable loading regimes with Q>10^8 fused silica microspheres.
physics.optics:The mechanism of DC-Electric-Field-Induced Second-Harmonic (EFISH) generation at weakly nonlinear buried Si(001)-SiO$_2$ interfaces is studied experimentally in planar Si(001)-SiO$_2$-Cr MOS structures by optical second-harmonic generation (SHG) spectroscopy with a tunable Ti:sapphire femtosecond laser. The spectral dependence of the EFISH contribution near the direct two-photon $E_1$ transition of silicon is extracted. A systematic phenomenological model of the EFISH phenomenon, including a detailed description of the space charge region (SCR) at the semiconductor-dielectric interface in accumulation, depletion, and inversion regimes, has been developed. The influence of surface quantization effects, interface states, charge traps in the oxide layer, doping concentration and oxide thickness on nonlocal screening of the DC-electric field and on breaking of inversion symmetry in the SCR is considered. The model describes EFISH generation in the SCR using a Green function formalism which takes into account all retardation and absorption effects of the fundamental and second harmonic (SH) waves, optical interference between field-dependent and field-independent contributions to the SH field and multiple reflection interference in the SiO$_2$ layer. Good agreement between the phenomenological model and our recent and new EFISH spectroscopic results is demonstrated. Finally, low-frequency electromodulated EFISH is demonstrated as a useful differential spectroscopic technique for studies of the Si-SiO$_2$ interface in silicon-based MOS structures.
physics.optics:The new mechanism for obtaining a nonlinear phase shift has been proposed and the schemes are described for its implementation. As it is shown, the interference of two waves with intensity-dependent amplitude ratio coming from the second harmonic generation should produce the nonlinear phase shift. The sign and amount of nonlinear distortion of a beam wavefront is dependent of the relative phase of the waves that is introduced by the phase element. Calculated value of $n_2^{eff}$ exceeds that connected with cascaded quadratic nonlinearity, at the same conditions.
physics.optics:We analyze the guiding problem in a realistic photonic crystal fiber using a novel full-vector modal technique, a biorthogonal modal method based on the nonselfadjoint character of the electromagnetic propagation in a fiber. Dispersion curves of guided modes for different fiber structural parameters are calculated along with the 2D transverse intensity distribution of the fundamental mode. Our results match those achieved in recent experiments, where the feasibility of this type of fiber was shown.
physics.optics:A new method for investigation of x-ray propagation in a rough narrow dielectric waveguide is proposed on the basis of the numerical integration of the quazioptical equation. In calculations a model rough surface was used with the given statistical properties. It was shown that the losses in the narrow waveguides strongly depend on the wall roughness and on the input angle. The losses are not zero even at zero input angle if the width of the waveguide is smaller or about 1 mkm. The effect is accounted for as the influence of diffraction. The angular spread of the transmitted X-ray radiation is much more narrow than the Fresnel angle of the total external reflection.
physics.optics:We study a generalized notion of two-mode squeezing for the Stokes and anti-Stokes fields in a model of a cavity Raman laser, which leads to a significant reduction in decoherence or quantum noise. The model comprises a loss-less cavity with classical pump, unsaturated medium and arbitrary homogeneous broadening and dispersion. Allowing for arbitrary linear combinations of the two modes in the definition of quadrature variables, we find that there always exists a combination of the two output modes which exhibits quadrature squeezing with noise reduction below the vacuum level. The number of noise photons for this combination mode is proportional to the square root of the number of Stokes noise photons.
physics.optics:We study the effects of higher order transversal modes in a model of a singly-resonant OPO, using both numerical solutions and mode expansions including up to two radial modes. The numerical and two-mode solutions predict lower threshold and higher conversion than the single-mode solution at negative dispersion. Relative power in the zero order radial mode ranges from about 88% at positive and small negative dispersion to 48% at larger negative dispersion, with most of the higher mode content in the first mode, and less than 2% in higher modes.
physics.optics:This paper presents a detailed numerical study of the effect of focusing on the conversion efficiency of low-loss singly-resonant parametric oscillators with collinear focusing of pump and signal. Results are given for the maximal pump depletion and for pump power levels required for various amounts of depletion, as functions of pump and signal confocal parameters, for kI/kP=0.33 and 0.50. It is found that the ratio of pump depletion to maximal depletion as a function of the ratio of pump power to threshold power agrees with the plane-wave prediction to within 5%, for a wide range of focusing conditions. The observed trends are explained as resulting from intensity and phase dependent mechanisms.
physics.optics:Via solution of appropriate variational problem it is shown that light beams with Gaussian spatial profile and sufficiently short duration provide maximal destruction of global coherence under nonlinear self-modulation.
physics.optics:The dynamics of Fabry-Perot cavity with suspended mirrors is described. The suspended mirrors are nonlinear oscillators interacting with each other through the laser circulating in the cavity. The degrees of freedom decouple in normal coordinates, which are the position of the center of mass and the length of the cavity. We introduce two parameters and study how the dynamics changes with respect to these parameters. The first parameter specifies how strong the radiation pressure is. It determines whether the cavity is multistable or not. The second parameter is the control parameter, which determines location of the cavity equilibrium states. The equilibrium state shows hysteresis if the control parameter varies within a wide range. We analyze stability of the equilibrium states and identify the instability region. The instability is explained in terms of the effective potential: the stable states correspond to local minima of the effective potential and unstable states correspond to local maxima. The minima of the effective potential defines the resonant frequencies for the oscillations of the cavity length. We find the frequencies, and analyze how to tune them. Multistability of the cavity with a feedback control system is analyzed in terms of the servo potential. The results obtained in this paper are general and apply to all Fabry-Perot cavities with suspended mirrors.
physics.optics:We show that an azimuthally-periodically-modulated bright ring "necklace" beam can self-trap in self-focusing Kerr media and can exhibit stable propagation for very large distances. These are the first bright (2+1) D beams to exhibit stable self-trapping in a system described by the cubic (2+1) D Nonlinear Schrodinger Equation (NLSE).
physics.optics:We present a new class of micro lasers based on nanoporous molecular sieve host-guest systems. Organic dye guest molecules of 1-Ethyl-4-(4-(p-Dimethylaminophenyl)-1,3-butadienyl)-pyridinium Perchlorat were inserted into the 0.73-nm-wide channel pores of a zeolite AlPO$_4$-5 host. The zeolitic micro crystal compounds where hydrothermally synthesized according to a particular host-guest chemical process. The dye molecules are found not only to be aligned along the host channel axis, but to be oriented as well. Single mode laser emission at 687 nm was obtained from a whispering gallery mode oscillating in a 8-$\mu$m-diameter monolithic micro resonator, in which the field is confined by total internal reflection at the natural hexagonal boundaries inside the zeolitic microcrystals.
physics.optics:We report a quantum ring-like toroidal cavity naturally formed in a vertical-cavity-like active microdisk plane due to Rayleigh's band of whispering gallery modes. The $\sqrt{T}$-dependent redshift and a square-law property of microampere-range threshold currents down to 2 $\mu$A are consistent with a photonic quantum wire view, due to whispering gallery mode-induced dimensional reduction.
physics.optics:The effect of capture of X-ray beam into narrow submicron capillary was investigated with account for diffraction and decay of coherency by roughness scattering in transitional boundary layer. In contrast to well-known Andronov-Leontovich approach the losses do not vanish at zero gliding angle and scale proportional to the first power of roughness amplitude for small gliding angles. It was shown that for small correlation radius of roughness the scattering decay of coherency can be made of the same order as absorption decay of lower channeling modes to produce angular collimation of X-ray beams. Estimates were given for optimum capillary length at different roughness amplitudes for angular sensitivity of X-ray transmission and chenneling effects that can be usefull for designing of detector systems.
physics.optics:We report the measurement of the photons flux produced in parametric down-conversion, performed in photon counting regime with actively quenched silicon avalanche photodiodes as single photon detectors. Measurements are done with the detector in a well defined geometrical and spectral situation. By comparison of the experimental data with the theory, a value for the second order susceptibilities of the non linear crystal can be inferred.
physics.optics:In a frame of quasi-crystal approximation the dispersion equations are obtained for the wave vector of a coherent electromagnetic wave propagating in a media which contains a random set of parallel dielectric cylinders with possible overlapping. The results are compared with that for the case when a regularity at the cylinder placement exists.
physics.optics:Accurate calculation of internal and surface scattering losses in fused silica microspheres is done. We show that in microspheres internal scattering is partly inhibited as compared to losses in the bulk material. We pay attention on the effect of frozen thermodynamical capillary waves on surface roughness. We calculate also the value of mode splitting due to backscattering and other effects of this backscattering.
physics.optics:We analytically compute a localization criterion in double scattering approximation for a set of dielectric spheres or perfectly conducting disks uniformly distributed in a spatial volume which can be either spherical or layered. For every disordered medium, we numerically investigate a localization criterion, and examine the influence of the system parameters on the wavelength localization domains.
physics.optics:The use of specific symmetry properties of the optical second-harmonic generation (the s,s-exclusion rule) has allowed us to observe high-contrast hyper-Rayleigh interference patterns in a completely diffuse light - an effect having no analog in case of linear (Rayleigh) scattering.
physics.optics:We report detailed measurements of the pump-current dependency of the self-pulsating frequency of semiconductor CD lasers. A distinct kink in this dependence is found and explained using rate-equation model. The kink denotes a transition between a region where the self-pulsations are weakly sustained relaxation oscillations and a region where Q-switching takes place. Simulations show that spontaneous emission noise plays a crucial role for the cross-over.
physics.optics:A new method is proposed to produce population inversion on transitions involving the ground state of atoms. The method is realized experimentally with sodium atoms. Lasing at the frequency corresponding to the sodium D_2 line is achieved in the presence of pump radiation resonant to the D_1 line with helium as a buffer gas.
physics.optics:A moving dielectric appears to light as an effective gravitational field. At low flow velocities the dielectric acts on light in the same way as a magnetic field acts on a charged matter wave. We develop in detail the geometrical optics of moving dispersionless media. We derive a Hamiltonian and a Lagrangian to describe ray propagation. We elucidate how the gravitational and the magnetic model of light propagation are related to each other. Finally, we study light propagation around a vortex flow. The vortex shows an optical Aharonov--Bohm effect at large distances from the core, and, at shorter ranges, the vortex may resemble an optical black hole.
physics.optics:We report the first observation of a nonlinear mode in a cylindrical nonlinear Fabry-Perot cavity. The field enhancement from cavity buildup, as well as the large chi3 optical nonlinearity due to resonantly-excited Rb-85 vapor, allows the nonlinear mode to form at low incident optical powers of less than a milliwatt. The mode is observed to occur for both the self-focusing and self-defocusing nonlinearity.
physics.optics:Optical second harmonic generation (SHG) is used as a noninvasive probe of two-dimensional (2D) ferroelectricity in Langmuir-Blodgett (LB) films of copolymer vinylidene fluoride with trifluorethylene. The surface 2D ferroelectric-paraelectric phase transition in the topmost layer of LB films and a thickness independent (almost 2D) transition in the bulk of these films are observed in temperature studies of SHG.
physics.optics:An all optical set-reset flip flop is presented that is based on two coupled identical laser diodes. The lasers are coupled so that when one of the lasers lases it quenches lasing in the other laser. The state of the flip flop is determined by which laser is currently lasing. Rate equations are used to model the flip flop and obtain steady state characteristics. The flip flop is experimentally demonstrated by use of antireflection coated laser diodes and free space optics.
physics.optics:We report on the fabrication of what we believe is the first example of a two dimensional nonlinear periodic crystal\cite{berger}, where the refractive index is constant but in which the 2nd order nonlinear susceptibility is spatially periodic. Such a crystal allows for efficient quasi-phase matched 2nd harmonic generation using multiple reciprocal lattice vectors of the crystal lattice. External 2nd harmonic conversion efficiencies > 60% were measured with picosecond pulses. The 2nd harmonic light can be simultaneously phase matched by multiple reciprocal lattice vectors, resulting in the generation of multiple coherent beams. The fabrication technique is extremely versatile and allows for the fabrication of a broad range of 2-D crystals including quasi-crystals.
physics.optics:The influence of linearly and circularly polarized laser fields on the dynamics of fast electron-impact excitation in atomic helium is discussed. A detailed analysis is made in the excitation of 2^1S, 3^1S and 3^1D dressed states of helium target.
physics.optics:The nonlinear dynamics of dissipative quantum systems in incoherent laser fields is studied in the framework of master equation with random model describing the laser noise and Markovian approximation for dealing with the system-bath couplings.
physics.optics:We present a theoretical study of strong laser-atom interactions, when the laser field parameters are subjected to random processes. The atom is modelled by a two-level and three-level systems, while the statistical fluctuations of the laser field are described by a pre-Gaussian model.
physics.optics:We consider the effect of spatial correlations on sources of polarized electromagnetic radiation. The sources, assumed to be monochromatic, are constructed out of dipoles aligned along a line such that their orientation is correlated with their position. In one representative example, the dipole orientations are prescribed by a generalized form of the standard von Mises distribution for angular variables such that the azimuthal angle of dipoles is correlated with their position. In another example the tip of the dipole vector traces a helix around the symmetry axis of the source, thereby modelling the DNA molecule. We study the polarization properties of the radiation emitted from such sources in the radiation zone. For certain ranges of the parameters we find a rather striking angular dependence of polarization. This may find useful applications in certain biological systems as well as in astrophysical sources.
physics.optics:We study the dynamics of the reduced density matrix(RDM) of the field in the micromaser. The resonator is pumped by N-atomic clusters of two-level atoms. At each given instant there is only one cluster in the cavity. We find the conditions of the independent evolution of the matrix elements of RDM belonging to a (sub)diagonal of the RDM, i.e. conditions of the diagonal invariance for the case of pumping by N-atomic clusters. We analyze the spectrum of the evolution operator of the RDM and discover the existence of the quasitrapped states of the field mode. These states exist for a wide range of number of atoms in the cluster as well as for a broad range of relaxation rates. We discuss the hierarchy of dynamical processes in the micromaser and find an important property of the field states corresponding to the quasi-equilibrium: these states are close to either Fock states or to a superposition of the Fock states. A possibility to tune the distribution function of photon numbers is discussed.
physics.optics:Temporal and angular correlations in atom-mediated photon-photon scattering are measured. Good agreement is found with the theory presented in Part~I.
physics.optics:The mediated photon-photon interaction due to the resonant Kerr nonlinearity in an inhomogeneously broadened atomic vapor is considered. The time-scale for photon-photon scattering is computed and found to be determined by the inhomogeneous broadening and the magnitude of the momentum transfer. This time can be shorter than the atomic relaxation time. Effects of atom statistics are included and the special case of small-angle scattering is considered. In the latter case the time-scale of the nonlinear response remains fast, even though the linear response slows as the inverse of the momentum transfer.
physics.optics:A simple variation of the traditional Young's double slit experiment can demonstrate several subtleties of interference with polarized light, including Berry and Pancharatnam's phase. Since the position of the fringes depends on the polarization state of the light at the input, the apparatus can also be used to measure the light's polarization without a quarter-wave plate or an accurate measurement of the light's intensity. In principle this technique can be used for any wavelength of photon as long as one can effectively polarize the incoming radiation.
physics.optics:The combination of charge separation induced by the formation of a single photorefractive screening soliton and an applied external bias field in a paraelectric is shown to lead to a family of useful electro-optic guiding patterns and properties.
physics.optics:The nonlinear pulse propagation in an optical fibers with varying parameters is investigated. The capture of moving in the frequency domain femtosecond colored soliton by a dispersive trap formed in an amplifying fiber makes it possible to accumulate an additional energy and to reduce significantly the soliton pulse duration. Nonlinear dynamics of the chirped soliton pulses in the dispersion managed systems is also investigated. The methodology developed does provide a systematic way to generate infinite ``ocean'' of the chirped soliton solutions of the nonlinear Schr\"odinger equation (NSE) with varying coefficients.
physics.optics:The Jaynes-Cummings model describing the interaction of a single linearly- polarized mode of the quantized electromagnetic field with an isolated two- level atom is generalized to the case of atomic levels degenerate in the projections of the angular momenta on the quantization axis, which is a usual case in the experiments. This generalization, like the original model, obtains the explicit solution. The model is applied to calculate the dependence of the atomic level populations on the angle between the polarization of cavity field mode and that of the laser excitation pulse in the experiment with one-atom micromaser.
physics.optics:We deduce the simplest form for an axicon Gaussian laser beam, i.e., one with radial polarization of the electric field.
physics.optics:We consider a linearly polarized electromagnetic wave incident on an opaque screen with square aperture of edge a. An application of Faraday's law to a loop parallel to the screen, on the side away from the source, shows that the wave must have longitudinal components there. The ratio of the longitudinal to transverse field is a measure of the diffraction angle.
physics.optics:We show that a time-reversed formulation of Huygens-Kirchhoff diffraction can be used to deduce the transverse and longitudinal fields of a Gaussian laser beam, starting from a simple assumption of a Gaussian beam profile in the far field. An attempt to apply this technique to the far fields of a Hertzian dipole shows how the laws of diffraction do not permit a wave to be focused to a volume smaller than a cubic wavelength in a charge-free region.
physics.optics:Slow light generated by Electromagnetically Induced Transparency is extremely susceptible with respect to Doppler detuning. Consequently, slow-light gyroscopes should have ultrahigh sensitivity.
physics.optics:The second-harmonic interferometric spectroscopy (SHIS) which combines both amplitude (intensity) and phase spectra of the second-harmonic (SH) radiation is proposed as a new spectroscopic technique being sensitive to the type of critical points (CP's) of combined density of states at semiconductor surfaces. The increased sensitivity of SHIS technique is demonstrated for the buried Si(111)-SiO$_2$ interface for SH photon energies from 3.6 eV to 5 eV and allows to separate the resonant contributions from $E^\prime_0/E_1$, $E_2$ and $E^\prime_1$ CP's of silicon.
physics.optics:The frequency of a 700mW monolithic non-planar Nd:YAG ring laser (NPRO) depends with a large coupling coefficient (some MHz/mW) on the power of its laser-diode pump source. Using this effect we demonstrate the frequency stabilization of an NPRO to a frequency reference by feeding back to the current of its pump diodes. We achieved an error point frequency noise smaller than 1mHz/sqrt(Hz), and simultaneously a reduction of the power noise of the NPRO by 10dB without an additional power stabilization feed-back system.
physics.optics:We deduce the emissivity of radiation from a metallic surface as a function of angle and polarization. This effect has found application in the calibration of detectors for cosmic microwave background radiation.
physics.optics:We present the first experimental observation of modulation instability of partially spatially incoherent light beams in non-instantaneous nonlinear media. We show that even in such a nonlinear partially coherent system (of weakly-correlated particles) patterns can form spontaneously. Incoherent MI occurs above a specific threshold that depends on the beams' coherence properties (correlation distance), and leads to a periodic train of one-dimensional (1D) filaments. At a higher value of nonlinearity, incoherent MI displays a two-dimensional (2D) instability and leads to self-ordered arrays of light spots.
physics.optics:We study the polarization of light emitted by spatially correlated sources. We show that in general polarization acquires nontrivial spectral dependence due to spatial correlations. The spectral dependence is found to be absent only for a special class of sources where the correlation length scales as the wavelength of light. We further study the cross correlations between two spatially distinct points that are generated due to propagation. It is found that such cross correlation leads to sufficiently strong spectral dependence of polarization which can be measured experimentally.
physics.optics:We demonstrate experimentally that in a centrosymmetric paraelectric non-stationary boundary conditions can dynamically halt the intrinsic instability of quasi-steady-state photorefractive self-trapping, driving beam evolution into a stable oscillating two-soliton-state configuration.
physics.optics:Mugnai et al. have reported an experiment in which microwave packets appear to travel in air with a speed substantially greater than c. They calculate the group velocity of their packets and find that it agrees with their experimental result. That calculation is incorrect. A correct calculation gives a group velocity less than c. The reported experimental result cannot be reconciled with the Maxwell equations.
physics.optics:Scalar Bessel beams are derived both via the wave equation and via diffraction theory. While such beams have a group velocity that exceeds the speed of light, this is a manifestation of the "scissors paradox" of special relativty. The signal velocity of a modulated Bessel beam is less than the speed of light. Forms of Bessel beams that satisfy Maxwell's equations are also given.
physics.optics:Various algebraic structures of degenerate four-wave mixing equations of optical phase conjugation are analyzed. Two approaches (the spinorial and the Lax-pair based), complementary to each other, are utilized for a systematic derivation of conserved quantities. Symmetry groups of both the equations and the conserved quantities are determined, and the corresponding generators are written down explicitly. Relation between these two symmetry groups is found. Conserved quantities enable the introduction of new methods for integration of the equations in the cases when the coupling $\Gamma$ is either purely real or purely imaginary. These methods allow for both geometries of the process, namely the transmission and the reflection, to be treated on an equal basis. One approach to introduction of Hamiltonian and Lagrangian structures for the 4WM systems is explored, and the obstacles in successful implementation of that programe are identified. In case of real coupling these obstacles are removable, and full Hamiltonian and Lagrangian formulations of the initial system are possible.
physics.optics:Instead of using frequency dependent refractive index, we propose to use the extinction theorem to describe reflection and transmission of an ultrashort pulse passing through the boundary. When the duration of the pulse is comparable with the relaxation time, the results differ significantly from those given by the traditional method, especially if the carrier frequency is close to an absorbtion line. We compare the two approaches using the data of GaAs in the infrared domain.
physics.optics:The Classification of Polarization elements, the polarization affecting optical devices which have a Jones matrix representation, according to the types of eigenvectors they possess, is given a new visit through the Group-theoretical connection of polarization elements. The diattenuators and retarders are recognized as the elements corresponding to boosts and rotations respectively. The structure of homogeneous elements other than diattenuators and retarders are identified by giving the quaternion corresponding to these elements. The set of degenerate polarization elements is identified with the so called `null' elements of the Lorentz Group. Singular polarization elements are examined in their more illustrative Mueller matrix representation and finally the eigenstructure of a special class of singular Mueller matrices is studied.
physics.optics:Complex photonic band structures (CPBS) of transmission metallic gratings with rectangular slits are shown to exhibit strong discontinuities that are not evidenced in the usual energetic band structures. These discontinuities are located on Wood's anomalies and reveal unambiguously two different types of resonances, which are identified as horizontal and vertical surface-plasmon resonances. Spectral position and width of peaks in the transmission spectrum can be directly extracted from CPBS for both kinds of resonances.
physics.optics:We present a brief classical discussion of a process to reduce the group velocity of an electromagnetic pulse by many orders of magnitude.
physics.optics:The properties of pulse propagation in a nonlinear fiber including linear damped term added in the usual nonlinear Schr\"odinger equation is analyzed analytically. We apply variational modified approach based on the lagrangian that describe the dynamic of system and with a trial function we obtain a solution which is more accuracy when compared with a pertubative solution. As a result, the problem of pulse propagation in a fiber with loss can be described in good agreement with exact results.
physics.optics:The group velocity for pulses in an optical medium can be negative at frequencies between those of a pair of laser-pumped spectral lines. The gain medium then can amplify the leading edge of a pulse resulting in a time advance of the pulse when it exits the medium, as has been recently demonstrated in the laboratory. This effect has been called superluminal, but, as a classical analysis shows, it cannot result in signal propgation at speeds greater than that of light in vacuum.
physics.optics:In two models it is shown that a light pulse propagates from a vacuum into certain media with velocity greater than that of a light in a vacuum (c). By numerical calculation the propagating properties of such a light are given.
physics.optics:The results of the study of ultra-short pulse generation in continuous-wave Kerr-lens mode-locked (KLM) solid-state lasers with semiconductor saturable absorbers are presented. The issues of extremely short pulse generation are addressed in the frames of the theory that accounts for the coherent nature of the absorber-pulse interaction. We developed an analytical model that bases on the coupled generalized Landau-Ginzburg laser equation and Bloch equations for a coherent absorber. We showed, that in the absence of KLM semiconductor absorber produces 2pi - non-sech-pulses of self-induced transparency, while the KLM provides an extremely short sech-shaped pulse generation. 2pi- and pi-sech-shaped solutions and variable-area chirped pulses have been found. It was shown, that the presence of KLM removes the limitation on the minimal modulation depth in absorber. An automudulational stability and self-starting ability were analyzed, too.
physics.optics:Based on self - consistent field theory we study a soliton generation in cw solid-state lasers with semiconductor saturable absorber. Various soliton destabilizations, i.e. the switch from femtosecond to picosecond generation (''picosecond collapse''), an automodulation regime, breakdown of soliton generation and hysteresis behavior, are predicted.
physics.optics:The effect of transmission of x-ray beams through submicron capillaries was investigated with account for diffraction and roughness scattering. Possible explanation of anomalous energy dependence of transmission through thin Cr/C/Cr channeles was given due to effect of periodic deformations.
physics.optics:Nonstationary pulse regimes associated with self modulation of a Kerr-lens modelocked Ti:sapphire laser have been studied experimentally and theoretically. Such laser regimes occur at an intracavity group delay dispersion that is smaller or larger than what is required for stable modelocking and exhibit modulation in pulse amplitude and spectra at frequencies of several hundred kHz. Stabilization of such modulations, leading to an increase in the pulse peak power by a factor of ten, were accomplished by weakly modulating the pump laser with the self-modulation frequency. The main experimental observations can be explained with a round trip model of the fs laser taking into account gain saturation, Kerr lensing, and second- and third-order dispersion.
physics.optics:The theoretical calculation for nonlinear refractive index in Cr: ZnSe - active medium predicts the strong defocusing cascaded second-order nonlinearity within 2000 - 3000 nm spectral range. On the basis of this result the optimal cavity configuration for Kerr-lens mode locking is proposed that allows to achieve a sub-100 fs pulse duration. The numerical simulations testify about strong destabilizing processes in the laser resulting from a strong self-phase modulation. The stabilization of the ultrashort pulse generation is possible due to spectral filtering that increases the pulse duration up to 300 fs.
physics.optics:The influence of nonlinear properties of semiconductor saturable absorbers on ultrashort pulse generation was investigated. It was shown, that linewidth enhancement, quadratic and linear ac Stark effect contribute essentially to the mode locking in cw solid-state lasers, that can increase the pulse stability, decrease pulse duration and reduce the mode locking threshold
physics.optics:We demonstrate that the shift of the stop band position with increasing oblique angle in periodic structures results in a wide transverse exponential field distribution corresponding to strong angular confinement of the radiation. The beam expansion follows an effective diffusive equation depending only upon the spectral mode width. In the presence of gain, the beam cross section is limited only by the size of the gain area. As an example of an active periodic photonic medium, we calculate and measure laser emission from a dye-doped cholesteric liquid crystal film.
physics.optics:The frequency of the Calcium ^3P_1--^1S_0 intercombination line at 657 nm is phase-coherently measured in terms of the output of a primary cesium frequency standard using an optical frequency comb generator comprising a sub-10 fs Kerr-lens mode-locked Ti:Sapphire laser and an external microstructure fiber for self-phase-modulation. The measured frequency of \nu_Ca = 455 986 240 494 276 Hz agrees within its relative uncertainty of 4 10^-13 with the values previously measured with a conceptually different harmonic frequency chain and with the value recommended for the realization of the SI unit of length.
physics.optics:Accurate phase-locked 3:1 division of an optical frequency was achieved, by using a continuous-wave (cw) doubly resonant optical parametric oscillator. A fractional frequency stability of 2*10^(-17) of the division process has been achieved for 100s integration time. The technique developed in this work can be generalized to the accurate phase and frequency control of any cw optical parametric oscillator.
physics.optics:It was usually assumed that the resonator based on a waveguide has the eigen oscillations that are formed by interference of two waves which propagate in different directions and have equal amplitudes. These patterns are usually called standing waves. We have shown that the eigen oscillations of a resonator which is filled by a layered dielectric can be base on the evanescent (non-propagating) waves. In some cases we need only one eigen wave to compose the eigen oscillation of a closed cavity.
physics.optics:The plane-wave dynamics of 3*omega => (2*omega, omega) subharmonic optical parametric oscillators containing a second harmonic generator of the idler wave omega is analyzed analytically by using the meanfield approximation and numerically by taking into account the field propagation inside the media. The resonant Chi(2):Chi(2) cascaded second-order nonlinearities induce a mutual injection-locking of the signal and idler waves that leads to coherent self phase-locking of the pump and subharmonic waves, freezing the phase diffusion noise. In case of signal-and-idler resonant devices, largely detuned sub-threshold states occur due to a subcritical bifurcation, broadening out the self-locking frequency range to a few cavity linewidths.
physics.optics:For the first time an all optical flip-flop is demonstrated based on two coupled Mach-Zehnder interferometers which contain semiconductor optical amplifiers in their arms. The flip-flop operation is discussed and it is demonstrated using commercially available fiber pigtailed devices. Being based on Mach-Zehnder interferometers, the flip-flop has potential for very high speed operation.
physics.optics:The strong asymmetry in charge distribution supporting a single non-interacting spatial needle soliton in a paraelectric photorefractive is directly observed by means of electroholographic readout. Whereas in trapping conditions a quasi-circular wave is supported, the underlying double-dipolar structure can be made to support two distinct propagation modes.
physics.optics:I present a theoretical treatment of parametric scattering in strong coupling semiconductor microcavities to model experiments in which parametric oscillator behaviour has been observed. The model consists of a non-linear excitonic oscillator coupled to a cavity mode which is driven by the external fields, and predicts the output power, below threshold gain and spectral blue shifts of the parametric oscillator. The predictions are found to be in excellent agreement with the experimental data.
physics.optics:A number of factors that influence spectral position of the femtosecond pulse in a Kerr-lens modelocked Cr:LiSGaF laser have been identified: high-order dispersion, gain saturation, reabsorption from the ground state, and stimulated Raman scattering. Using the one-dimensional numerical model for the simulation of the laser cavity, the relative contributions of different factors have been compared. The Raman effect provides the largest self-frequency shift from the gain peak (up to 60 nm), followed by the gain saturation (25 nm), while the high-order dispersion contribution is insignificant (5 nm). Comparison with the experimental data confirm that the stimulated Raman scattering is a main cause of the ultrashort pulse self-frequency shift observed in Cr:LiSGaF and Cr:LiSAF lasers
physics.optics:Simultaneous measurements of the intensity and phase of a probe wave reflected from an interface between silica and elemental alpha-gallium reveal its very strong optical nonlinearity, affecting both these parameters of the reflected wave. The data corroborate with a non-thermal mechanism of optical response which assumes appearance of a homogeneous highly metallic layer, only a few nanometer thick, between the silica and bulk alpha-gallium.
physics.optics:We consider pulse propagation in a linear anomalously dispersive medium where the group velocity exceeds the speed of light in vacuum (c) or even becomes negative. A signal velocity is defined operationally based on the optical signal-to-noise ratio, and is computed for cases appropriate to the recent experiment where such a negative group velocity was observed. It is found that quantum fluctuations limit the signal velocity to values less than c.
physics.optics:Polarization dynamics of femtosecond light pulses propagating in air is studied by computer simulation. A rich variety of dynamics is found that depends on the initial polarization state and power of the pulse. Effects of polarization on the plasma and supercontinuum generation are also discussed.
physics.optics:We report measurements of thermal self-locking of a Fabry-Perot cavity containing a potassium niobate (KNbO3) crystal. We develop a method to determine linear and nonlinear optical absorption coefficients in intracavity crystals by detailed analysis of the transmission lineshapes. These lineshapes are typical of optical bistability in thermally loaded cavities. For our crystal, we determine the one-photon absorption coefficient at 846 nm to be (0.0034 \pm 0.0022) per m and the two-photon absorption coefficient at 846 nm to be (3.2 \pm 0.5) \times 10^{-11} m/W and the one-photon absorption coefficient at 423 nm to be (13 \pm 2) per m. We also address the issue of blue-light-induced-infrared-absorption (BLIIRA), and determine a coefficient for this excited state absorption process. Our method is particularly well suited to bulk absorption measurements where absorption is small compared to scattering. We also report new measurements of the temperature dependence of the index of refraction at 846 nm, and compare to values in the literature.
physics.optics:We review and extend the analogies between Gaussian pulse propagation and Gaussian beam diffraction. In addition to the well-known parallels between pulse dispersion in optical fiber and CW beam diffraction in free space, we review temporal lenses as a way to describe nonlinearities in the propagation equations, and then introduce further concepts that permit the description of pulse evolution in more complicated systems. These include the temporal equivalent of a spherical dielectric interface, which is used by way of example to derive design parameters used in a recent dispersion-mapped soliton transmission experiment. Our formalism offers a quick, concise and powerful approach to analyzing a variety of linear and nonlinear pulse propagation phenomena in optical fibers.
physics.optics:We have measured the frequency of the $6s^2S_{1/2} - 5d^2D_{3/2}$ electric-quadrupole transition of $^{171}$Yb$^+$ with a relative uncertainty of $1\times 10^{-14}$, $\nu_{Yb}$ = 688 358 979 309 312 Hz $\pm$ 6 Hz. A femtosecond frequency comb generator was used to phase-coherently link the optical frequency derived from a single trapped ion to a cesium fountain controlled hydrogen maser. This measurement is one of the most accurate measurements of optical frequencies ever reported, and it represents a contribution to the development of optical clocks based on an $^{171}$Yb$^+$ ion standard.
physics.optics:The time behaviour of microwaves undergoing partial reflection by photonic barriers was measured in the time and in the frequency domain. It was observed that unlike the duration of partial reflection by dielectric layers, the measured reflection duration of barriers is independent of their length. The experimental results point to a nonlocal behaviour of evanescent modes at least over a distance of some ten wavelengths. Evanescent modes correspond to photonic tunnelling in quantum mechanics.
physics.optics:We study the conditions for soliton-like wave propagation in the Photorefractive (PR) and electro-optic (i.e., Pockels) material, by using Nonlinear Schrodinger (NLS) equation. The complete NLS equation is solved analytically and numerically by transforming it into the phase space. Our results clearly show the existence of both the dark and bright solitary solutions for the PR medium. Interestingly, however, we find only one bright solitary solution in the Pockels case and there is no evidence of any dark solitary solution.
physics.optics:In the theory of optical gap solitons, slowly-moving finite-amplitude Lorentzian solutions are found to mediate the transition from bright to coexistent dark-antidark solitary wave pairs when the laser frequency is detuned out of the proper edge of a dynamical photonic bandgap. Catastrophe theory is applied to give a geometrical description of this strongly asymmetrical 'morphing' process.
physics.optics:Harmonic and Intermodulation distortions occur when a physical system is excited with a single or several frequencies and when the relationship between the input and output is non-linear. Working with non-linearities in the Frequency domain is not straightforward specially when the relationship between the input and output is not trivial. We outline the complete derivation of the Harmonic and Intermodulation distortions from basic principles to a general physical system. For illustration, the procedure is applied to the Single Mode laser diode where the relationship of input to output is non-trivial. The distortions terms are extracted directly from the Laser Diode rate equations and the method is tested by comparison to many results cited in the literature. This methodology is general enough to be applied to the extraction of distortion terms to any desired order in many physical systems in a general and systematic way.
physics.optics:We reelaborate on the basic properties of lossless multilayers. We show that the transfer matrices for these multilayers have essentially the same algebraic properties as the Lorentz group SO(2,1) in a (2+1)-dimensional spacetime, as well as the group SL(2,R) underlying the structure of the ABCD law in geometrical optics. By resorting to the Iwasawa decomposition, we represent the action of any multilayer as the product of three matrices of simple interpretation. This group-theoretical structure allows us to introduce bilinear transformations in the complex plane. The concept of multilayer transfer function naturally emerges and its corresponding properties in the unit disc are studied. We show that the Iwasawa decomposition reflects at this geometrical level in three simple actions that can be considered the basic pieces for a deeper undestanding of the multilayer behavior. We use the method to analyze in detail a simple practical example.
physics.optics:A method is presented to investigate diffraction of an electromagnetic plane wave by an infinitely thin infinitely conducting circular cylinder with longitudinal slots. It is based on the use of the combined boundary conditions method that consists on expressing the continuity of the tangential components of both the electric and the magnetic fields in a single equation. This method proves to be very efficient for this kind of problems and leads to fast numerical codes.
physics.optics:We present a reliable, narrow linewidth (100 kHz) continuous-wave optical parametric oscillator (OPO) suitable for high-resolution spectroscopy applications. The OPO is based on a periodically-poled lithium-niobate crystal and features a specially designed intracavity etalon which permits its continuous tuning and stable operation at any desired wavelength in a wide operation range. We demonstrate Doppler-free spectroscopy on a rovibrational transition of methane at 3.39 um.
physics.optics:We describe the action of a plane interface between two semi-infinite media in terms of a transfer matrix. We find a remarkably simple factorization of this matrix, which enables us to express the Fresnel coefficients as a hyperbolic rotation.
physics.optics:We report on a methodology for the evaluation of the DC characteristics, small-signal frequency response and large-signal dynamic response of carrier and photon density responses in semiconductor laser diodes. A single mode laser is considered and described with a pair of rate equations containing a novel non-linear gain compensation term depending on a single parameter that can be chosen arbitrarily. This approach can be applied to any type of solid-state laser as long as it is described by a set of rate equations.
physics.optics:We report the discovery of a "dark area theorem," a new quantum optical relation for propagation of unmatched pulses in thick three-level $\Lambda$-type media. We define dark area and derive the dark area theorem for a coherently prepared and inhomogeneously broadened lambda medium. We also obtain the first equation for the spatial evolution of the dark state amplitude prior to pulse-matching.
physics.optics:We propose experimentally simplified schemes of an optically dispersive interface region between two coupled free electron lasers (FELs), aimed at achieving a much broader gain bandwidth than in a conventional FEL or a conventional optical klystron composed of two separated FELs. The proposed schemes can {\it universally} enhance the gain of FELs, regardless of their design when operated in the short pulsed regime.
physics.optics:As a light beam is produced by an amplification of modes of the zero point field in its source, this field cannot be distinguished; consequently a nonlinear optical effect is a function of the total field. However, we generally prefer to use a conventional field which excludes the zero point field; for a low conventional field, the total field may be developed to the first order, so that the effect appears linear.   This nearly trivial remark allows a correct computation of the signal of a photocell used for photon counting and shows that the "impulsive stimulated Raman scattering" (ISRS), a nonlinear, without threshold effect, which shifts the frequencies, becomes linear at low light levels, so that the shifted spectra are not distorted.
physics.optics:The effect of thermal fluctuations in the resonance fluorescence of a three-level system is studied. The damped three-level system is driven by two strong incident classical fields near resonances frequencies. The simulation of a thermal bath is obtained with a large system of harmonic oscillators that represent the normal modes of the thermal radiation field. The time evolution of the fluorescent light intensities are obtained solving by a iterative method the Heisenberg equations of motion in the integral form. The results show that the time development of the intensity of the fluorescence light is strongly affected by the interaction of the system with the thermal bath.
physics.optics:The dynamical response of a relativistic bunch of electrons injected in a planar magnetic undulator and interacting with a counterpropagating electromagnetic wave is studied. We demonstrate a resonance condition for which the free electron laser (FEL) dynamics is strongly influenced by the presence of the external field. It opens up the possibility of control of short wavelength FEL emission characteristics by changing the parameters of the microwave field without requiring change in the undulator's geometry or configuration. Numerical examples, assuming realistic parameter values analogous to those of the TTF-FEL, currently under development at DESY, are given for possible control of the amplitude or the polarization of the emitted radiation.
physics.optics:We have operated a CW triply resonant OPO using a PPLN crystal pumped by a Nd:YAG laser at 1.06 micron and generating signal and idler modes in the 2-2.3 micron range. The OPO was operated stably in single mode operation over large periods of time with a pump threshold as low as 500 microwatts.
physics.optics:We extend a modal theory of diffraction by a set of parallel fibers to deal with the case of a hard boundary: that is a structure made for instance of air-holes inside a dielectric matrix. Numerical examples are given concerning some resonant phenomena.
physics.optics:We report observation of lasing in the scarred modes in an asymmetrically deformed microcavity made of liquid jet. The observed scarred modes correspond to morphology-dependent resonance of radial mode order 3 with their Q values in the range of 10^6. Emission directionality is also observed, corresponding to a hexagonal unstable periodic orbit.
physics.optics:We have demonstrated an ultrahigh-Q whispering-gallery-mode (WGM) microsphere laser based on the evanescent-wave-coupled gain. Dye molecules outside the sphere near the equator were excited, resulting in WGM lasing in the lowest radial mode order. The loaded quality factor of the lasing WGM was 8(2)\times 10^9, the highest ever achieved in the microlaser.
physics.optics:An extended cavity diode laser operating in the Littrow configuration emitting near 657 nm is stabilized via its injection current to a reference cavity with a finesse of more than 10^5 and a corresponding resonance linewidth of 14 kHz. The laser linewidth is reduced from a few MHz to a value below 30 Hz. The compact and robust setup appears ideal for a portable optical frequency standard using the Calcium intercombination line.
physics.optics:We introduce a novel concept for optical frequency measurement and division which employs a Kerr-lens mode-locked laser as a transfer oscillator whose noise properties do not enter the measurement process. We experimentally demonstrate, that this method opens up the route to phase-link signals with arbitrary frequencies in the optical or microwave range while their frequency stability is preserved.
physics.optics:We investigate the self-phase modulation of intense femtosecond laser pulses propagating in an ionizing gas and its effects on collective properties of high-order harmonics generated in the medium. Plasmas produced in the medium are shown to induce a positive frequency chirp on the leading edge of the propagating laser pulse, which subsequently drives high harmonics to become positively chirped. In certain parameter regimes, the plasma-induced positive chirp can help to generate sharply peaked high harmonics, by compensating for the dynamically-induced negative chirp that is caused by the steep intensity profile of intense short laser pulses.
physics.optics:We developed a novel technique for frequency measurement and synthesis, based on the operation of a femtosecond comb generator as transfer oscillator. The technique can be used to measure frequency ratios of any optical signals throughout the visible and near-infrared part of the spectrum. Relative uncertainties of $10^{-18}$ for averaging times of 100 s are possible. Using a Nd:YAG laser in combination with a nonlinear crystal we measured the frequency ratio of the second harmonic $\nu_{SH}$ at 532 nm to the fundamental $\nu_0$ at 1064 nm, $\nu_{SH}/\nu_0 = 2.000 000 000 000 000 001 \times (1 \pm 7 \times 10^{-19})$.
physics.optics:The mixed crystal of a para-dibromobenzene with a para-chloronitrobenzene is investigated at concentration of components from 0% up to 60% of a para-chloronitrobenzene by the method of Low-Frequency Raman spectroscopy. It is shown, that in range of concentrations from 25% up to 50% of a para-chloronitrobenzene the spectrum of the mixed crystal would consist of the sum of spectrums a and b phases which relation of intensities depends on concentration of components. It is also found, that the single crystal in this range has rod frame.
physics.optics:The stability of polarization, areas, and number of self-induced transparency (SIT)-solitons at the output from the LaF_3:Pr^{3+} crystal is theoretically studied versus the polarization direction and the area of the input linearly polarized laser pulse. For this purpose the Vector Area Theorem is rederived and two-dimensional Vector Area Theorem map is obtained. The map is governed by the crystal symmetry and takes into account directions of the dipole matrix element vectors of the different site subgroups of optically excited ions. The Vector Area Theorem mapping of the time evolution of the laser pulse allows one to highlight soliton polarization properties.
physics.optics:The dynamics of light in Fabry-Perot cavities with varying length and input laser frequency are analyzed and the exact condition for resonance is derived. This dynamic resonance depends on the light transit time in the cavity and the Doppler effect due to the mirror motions. The response of the cavity to length variations is very different from its response to laser frequency variations. If the frequency of these variations is equal to multiples of the cavity free spectral range, the response to length is maximized while the response to the laser frequency is zero. Implications of these results for the detection of gravitational waves using kilometer-scale Fabry-Perot cavities are discussed.
physics.optics:Numerical simulation of the National Ignition Facility (NIF) laser performance and automated control of the laser setup process are crucial to the project's success. These functions will be performed by two closely coupled computer code: the virtual beamline (VBL) and the laser performance operations model (LPOM).
physics.optics:Since its birth, the laser has been extraordinarily effective in the study and applications of laser-matter interaction at the atomic and molecular level and in the nonlinear optics of the bound electron. In its early life, the laser was associated with the physics of electron volts and of the chemical bond. Over the past fifteen years, however, we have seen a surge in our ability to produce high intensities, five to six orders of magnitude higher than was possible before. At these intensities, particles, electrons and protons, acquire kinetic energy in the mega-electron-volt range through interaction with intense laser fields. This opens a new age for the laser, the age of nonlinear relativistic optics coupling even with nuclear physics. We suggest a path to reach an extremely high-intensity level $10^{26-28} $W/cm$^2$ in the coming decade, much beyond the current and near future intensity regime $10^{23} $W/cm$^2$, taking advantage of the megajoule laser facilities. Such a laser at extreme high intensity could accelerate particles to frontiers of high energy, tera-electron-volt and peta-electron-volt, and would become a tool of fundamental physics encompassing particle physics, gravitational physics, nonlinear field theory, ultrahigh-pressure physics, astrophysics, and cosmology. We focus our attention on high-energy applications in particular and the possibility of merged reinforcement of high-energy physics and ultraintense laser.
physics.optics:A simple and intuitive geometical method to analyze Fresnel formulas is presented. It applies to transparent media and is valid for perpendicular and parallel polarizations. The approach gives a graphical characterization particularly simple of the critical and Brewster angles. It also provides an interpretation of the relation between the reflection coefficients for both basic polarizations as a symmetry in the plane.
physics.optics:The tunnel effect is considered here within the framework of electromagnetic propagation. The classical problem of a plane gap of dielectric, surrounded on both sides by a medium with larger refraction index, is studied in the case in which an electromagnetic plane wave impinges into the gap with an incidence angle larger than the critical angle. In this condition (total reflection), the gap acts as a classically forbidden region and behaves like a tunnel. The field inside the forbidden gap consists of two evanescent waves, each one having its wavefronts normal to the interface. In the present paper we study the total field derived as a superposition of two such evanescent waves, its wavefronts, and the directions of propagation of both phase and energy.
physics.optics:The motion of an electromagnetic wave, through a classically-forbidden region, has recently attracted renewed interest because of its implication with regard to the theoretical and experimental problems of superluminality. From an experimental point of view, many papers provide an evidence of superluminality in different physical systems. Theoretically, the problem of a passage through a forbidden gap has been treated by considering plane waves at oblique incidence into a plane parallel layer of a medium with a refractive index smaller than the index of the surrounding medium, and also confined (Gaussian) beams, still at oblique incidence. In the present paper the case of a Bessel beam is examined, at normal incidence into the layer (Secs. II and III), in the scalar approximation (Sec. IV) and by developing also a vectorial treatment (Sec. V). Conclusions are reported in Sic. VI.
physics.optics:The tunneling time is here investigated by means of an electromagnetic model, for a system where a gap, between two parallel planes, acts as a classically-forbidden region for an impinging pulse with incidence angle larger than the critical angle. In all cases of frustrated total reflection we obtain a superluminal behavior both for phase and group delays.
physics.optics:We report an injection-locked cw titanium:sapphire ring laser at 846 nm. It produces 1.00 W in a single frequency when pumped with 5.5 W. Single frequency operation requires only a few milliwatts of injected power.
physics.optics:Accurate knowledge of absorption coefficient of a sample is a prerequisite for measuring the third order optical nonlinearity of materials, which could become a serious limitation for unknown samples. We introduce a new method, which measures both the absorption coefficient and the third order optical nonlinearity of materials with high sensitivity in a single experimental setup. We use a dual-beam pump-probe experiment under different conditions to achieve this goal. We also demonstrate a counterintuitive coupling of the non-interacting probe-beam with the pump-beam in pump-probe z-scan experiment.
physics.optics:We obtain gain of the probe field at multiple frequencies in a closed three-level V-type system using frequency modulated pump field. There is no associated population inversion among the atomic states of the probe transition. We describe both the steady-state and transient dynamics of this system. Under suitable conditions, the system exhibits large gain simultaneously at series of frequencies far removed from resonance. Moreover, the system can be tailored to exhibit multiple frequency regimes where the probe experiences anomalous dispersion accompanied by negligible gain-absorption over a large bandwidth, a desirable feature for obtaining superluminal propagation of pulses with negligible distortion.
physics.optics:An undoped double quantum well (DQW) was driven with a terahertz (THz) electric field of frequency \omega_{THz} polarized in the growth direction, while simultaneously illuminated with a near-infrared (NIR) laser at frequency \omega_{NIR}. The intensity of NIR upconverted sidebands \omega_{sideband}=\omega_{NIR} + \omega_{THz} was maximized when a dc voltage applied in the growth direction tuned the excitonic states into resonance with both the THz and NIR fields. There was no detectable upconversion far from resonance. The results demonstrate the possibility of using gated DQW devices for all-optical wavelength shifting between optical communication channels separated by up to a few THz.
physics.optics:Driving a double-quantum-well excitonic intersubband resonance with a terahertz (THz) electric field of frequency \omega_{THz} generated terahertz optical sidebands \omega=\omega_{THz}+\omega_{NIR} on a weak NIR probe. At high THz intensities, the intersubband dipole energy which coupled two excitons was comparable to the THz photon energy. In this strong-field regime the sideband intensity displayed a non-monotonic dependence on the THz field strength. The oscillating refractive index which gives rise to the sidebands may be understood by the formation of Floquet states, which oscillate with the same periodicity as the driving THz field.
physics.optics:We elaborate on the consequences of the factorization of the transfer matrix of any lossless multilayer in terms of three basic matrices of simple interpretation. By considering the bilinear transformation that this transfer matrix induces in the complex plane, we introduce the concept of multilayer transfer function and study its properties in the unit disk. In this geometrical setting, our factorization translates into three actions that can be viewed as the basic pieces for understanding the multilayer behavior. Additionally, we introduce a simple trace criterion that allows us to classify multilayers in three types with properties closely related to one (and only one) of these three basic matrices. We apply this approach to analyze some practical examples that are representative of these types of matrices.
physics.optics:We consider the problem of radiation into free space from the end-facet of a single-mode photonic crystal fiber (PCF). We calculate the numerical aperture NA=sin theta from the half-divergence angle theta ~ tan^{-1}(lambda/pi w) with pi w^2 being the effective area of the mode in the PCF. For the fiber first presented by Knight et al. we find a numerical aperture NA ~ 0.07 which compares to standard fiber technology. We also study the effect of different hole sizes and demonstrate that the PCF technology provides a large freedom for NA-engineering. Comparing to experiments we find good agreement.
physics.optics:Polarized and azimuthal dependencies of optical second harmonics generation (SHG) at the surface of noncentrosymmetric semiconductor crystals have been measured on polished surfaces of ZnSe(100), using a fundamental wavelength of 1.06$\mu m$. The SHG intensity patterns were analyzed for all four combination of p- and s-polarized incidence and output, considering both the bulk and surface optical nonlinearities in the electric dipole approximation. We found that the measurement using $S_{in}-S_{out}$ is particularly useful in determining the symmetry of the oxdized layer interface, which would lower the effective symmetry of the surface from $C_{4v}$ to $C_{2v}.$ We also have shown that the [011] and [0$\bar{1}$1] directions can be distinguished through the analysis of p-incident and p-output confugration.
physics.optics:Fundamental rules and definitions of Fractional Differintegrals are outlined. Factorizing 1-D and 2-D Helmholtz equations four fractional eigenfunctions are determined. The functions exhibit incident and reflected plane waves as well as diffracted incident and reflected waves of the half-plane edge. They allow to construct the Sommerfeld half-plane diffraction solutions. Parabolic-Wave Equation (PWE, Leontovich-Fock) for paraxial propagation is factorized and differetial fractional solutions of Fresnel-integral type are derived. We arrived at two solutions, which are the mothers of known and new solutions.
physics.optics:In the classical theory, an electromagnetic field obeying Maxwell's equations cannot be absorbed quickly by matter, so that it remains a zero point field. Splitting the total, genuine electromagnetic field into the sum of a conventional field and a zero point field is physically meaningless until a receiver attenuates the genuine field down to the zero point field, or studying the amplification of the zero point field by a source.   In classical optics all optical effects must be written using the genuine field, so that at low light levels the nonlinear effects become linear in relation to the conventional field. The result of the interpretation of all observations, even at low light levels, is exactly the same in quantum electrodynamics and in the semi- classical theory.   The zero point field is stochastic only far from the sources and the receivers; elsewhere, it is shaped by matter, it may be studied through fields visible before an absorption or after an amplification.   A classical study of the reduction of the wave packet extends the domain of equivalence of the classical and quantum zero point field; using both interpretations of this field makes the results more reliable, because the traps are different.
physics.optics:The smaller the size of a light-emitting microcavity, the more important it becomes to understand the effects of the cavity boundary on the optical mode profile. Conventional methods of laser physics, such as the paraxial approximation, become inapplicable in many of the more exotic cavity designs to be discussed here. Cavities in the shape of microdisks, pillars and rings can yield low lasing thresholds in a wide variety of gain media: quantum wells, wires and even dots, as well as quantum cascade superlattices and GaN. An overview of the experimental and theoretical status is provided, with special emphasis on the light extraction problem.
physics.optics:The stationary states of a microlaser are related to the decaying quasibound states of the corresponding passive cavity. These are interpreted classically as originating from sequential escape attempts of an ensemble of rays obeying a curvature-corrected Fresnel formula. Polarization-dependent predictions of this model, and its limitations for stable orbits in partially chaotic systems are discussed.
physics.optics:We measured and calculated transmission spectra of two-dimensional quasiperiodic photonic crystals (PCs) based on a 5-fold (Penrose) or 8-fold (octagonal) symmetric quasiperiodic pattern. The photonic crystal consisted of dielectric cylindrical rods in air placed normal to the basal plane on vertices of tiles composing the quasiperiodic pattern. An isotropic photonic band gap (PBG) appeared in the TM mode, where electric fields were parallel to the rods, even when the real part of a dielectric constant of the rod was as small as 2.4. An isotropic PBG-like dip was seen in tiny Penrose and octagonal PCs with only 6 and 9 rods, respectively. These results indicate that local multiple light scattering within the tiny PC plays an important role in the PBG formation. Besides the isotropic PBG, we found dips depending on the incident angle of the light. This is the first report of anisotropic structures clearly observed in transmission spectra of quasiperiodic PCs. Based on rod-number and rod-arrangement dependence, it is thought that the shapes and positions of the anisotropic dips are determined by global multiple light scattering covering the whole system. In contrast to the isotropic PBG due to local light scattering, we could not find any PBGs due to global light scattering even though we studied transmission spectra of a huge Penrose PC with 466 rods.
physics.optics:A perfect focus telescope is one in which all rays parallel to the axis meet at a point and give equal magnification there. It is shown that these two conditions define the shapes of both primary and secondary mirrors. Apart from scale, the solution depends upon two parameters, $s$, which gives the mirror separation in terms of the effective focal length, and $K$, which gives the relative position of the final focus in that unit. The two conditions ensure that the optical systems have neither spherical aberration nor coma, no matter how fast the $f$ ratio. All known coma--free systems emerge as approximate special cases. In his classical paper, K. Schwarzschild studied all two mirror systems whose profiles were conic sections. We make no such a priori shape conditions but demand a perfect focus and solve for the mirrors' shapes.
physics.optics:The results of experimental testing the existence of intense Lorentzian--like wings with FWHM $\sim 4.5 cm^{-1}$ in the absorption spectra of polyatomic molecules in a gas phase are presented. Two independent experimental methods were used for evaluating the integral intensity of the line wings for a number of substances. In the first case, the cross--section of the far wings of absorption bands in a gas phase spectrum were measured. Then, these band wings were extrapolated inside the contour of absorption band. In the second case, the saturation degree of the linear spectrum of molecules was determined. Radiation of a pulsed $CO_2$--laser was used at low gas pressure ($\sim 16$ mtorr) and averaged excitation level of molecules ${<n>}\sim 0.1$ quanta/molecule. The values obtained by these two independent methods coincide for a variety of molecules. The average relative integral intensity of the line wings varied from $\sim 0.6%$ for $SF_6$ and $SiF_4$ to $\sim 90%$ for $(CF_3)_2O$ and $(CF_3)_2CO$.
physics.optics:It is shown that the direct Fourier synthesization of light beams allows one to create polarity-asymmetric waves, which are able, in the process of nonlinear interaction with a medium, to break its inversion symmetry. As a result, these "polar" waves may show the effect of optical rectification in nonlinear centrosymmetric media by generating light-induced dc electric polarization. At the same time, the waves of this type, due to their unusual symmetry properties, can be used for detecting the direction and sign of a dc electric field applied to the medium. The prospects of application of polar waves to data recording and processing are discussed.
physics.optics:Presented is an analysis of general scaling perturbations in a transmitting fiber. For elliptical perturbations, under some conditions an intermode dispersion parameter characterizing modal PMD is shown to be directly proportional to the mode dispersion.
physics.optics:Extensive Bose-Einstein condensation research activities have recently led to studies of fermionic atoms and optical confinements. Here we present a case of micro-optical fermionic electron phase transition. Optically confined ordering and phase transitions of a fermionic cloud in dynamic steady state are associated with Rayleigh emissions from photonic quantum ring manifold which are generated by nature without any ring lithography. The whispering gallery modes, produced in a semiconductor Rayleigh-Fabry-Perot toroidal cavity at room temperature, exhibit novel properties of ultralow thresholds open to nano-ampere regime, thermal stabilities from square-root-T-dependent spectral shift, and angularly varying intermode spacings. The photonic quantum ring phenomena are associated with a photonic field-driven phase transition of quantum-well-to-quantum-wire and hence the photonic (non-de Broglie) quantum corral effect on the Rayleigh cavity-confined carriers in dynamic steady state. Based upon the intra-cavity fermionic condensation we also offer a prospect for an electrically driven few-quantum dot single photon source from the photonic quantum ring laser for quantum information processors.
physics.optics:Nonlinear optical media that are normally dispersive, support a new type of localized (nondiffractive and nondispersive) wavepackets that are X-shaped in space and time and have slower than exponential decay. High-intensity X-waves, unlike linear ones, can be formed spontaneously through a trigger mechanism of conical emission, thus playing an important role in experiments.
physics.optics:We report three-dimensional laser microfabrication, which enables microstructuring of materials on the scale of 0.2-1 micrometers. The two different types of microfabrication demonstrated and discussed in this work are based on holographic recording, and light-induced damage in transparent dielectric materials. Both techniques use nonlinear optical excitation of materials by ultrashort laser pulses (duration < 1 ps).
physics.optics:We propose a concept for production of high power coherent attosecond pulses in X-ray range. An approach is based on generation of 8th harmonic of radiation in a multistage HGHG FEL (high gain high harmonic free electron laser) configuration starting from shot noise. Single-spike phenomena occurs when electron bunch is passed through the sequence of four relatively short undulators. The first stage is a conventional "long" wavelength (0.8 nm) SASE FEL which operates in the high-gain linear regime. The 0.1 nm wavelength range is reached by successive multiplication (0.8 nm $\to$ 0.4 nm $\to$ 0.2 nm $\to$ 0.1 nm) in a stage sequence. Our study shows that the statistical properties of the high-harmonic radiation from the SASE FEL, operating in linear regime, can be used for selection of radiation pulses with a single spike in time domain. The duration of the spikes is in attosecond range. Selection of single-spike high-harmonic pulses is achieved by using a special trigger in data acquisition system. The potential of X-ray SASE FEL at TESLA at DESY for generating attosecond pulses is demonstrated. Since the design of XFEL laboratory at TESLA is based on the use of long SASE undulators with tunable gap, no special place nor additional FEL undulators are required for attophysics experiments. The use of a 10 GW-level attosecond X-ray pulses at X-ray SASE FEL facility will enable us to track processes inside atoms.
physics.optics:We present an algorithm for the maximization of photonic bandgaps in two-dimensional crystals. Once the translational symmetries of the underlying structure have been imposed, our algorithm finds a global maximal (and complete, if one exists) bandgap. Additionally, we prove two remarkable results related to maximal bandgaps: the so-called `maximum contrast' rule, and about the location in the Brillouin zone of band edges.
physics.optics:We investigate the propagation of electromagnetic waves in finite photonic band gap structures. We analyze the phenomenon of conduction and forbidden bands and we show that two regimes are to be distinguished with respect to the existence of a strong field near the interfaces. We precise the domain for which an effective medium theory is sounded.
physics.optics:The maximum bit-rate of a slab waveguide is ultimately determined by the waveguide dispersion. We show that while the maximum bit rate in a waveguide is inversely proportional to the waveguide's width, bit rate per unit width (i.e., spatial capacity) decreases, and in the limit of a zero-width waveguide it converges to a value, which is independent of the waveguide's refractive indices. This value is qualitatively equivalent to the transmission rate per unit of width in free space. We also show that in a 3D waveguide (e.g., fibers), unlike free space, the spatial capacity vanishes in the same limit.
physics.optics:The photonic band dispersion and density of states (DOS) are calculated for the three-dimensional (3D) hexagonal structure corresponding to a distributed Bragg reflector patterned with a 2D triangular lattice of circular holes. Results for the Si/SiO$_2$ and GaAs/AlGaAs systems determine the optimal parameters for which a gap in the 2D plane occurs and overlaps the 1D gap of the multilayer. The DOS is considerably reduced in correspondence with the overlap of 2D and 1D gaps. Also, the local density of states (i.e., the DOS weighted with the squared electric field at a given point) has strong variations depending on the position. Both results imply substantial changes of spontaneous emission rates and patterns for a local emitter embedded in the structure and make this system attractive for the fabrication of a 3D photonic crystal with controlled radiative properties.
physics.optics:We present a Fourier transform methodology for all-order polarization mode dispersion (PMD) analysis, based on the first Born approximation to the coupled-mode equation solution. Our method predicts wavelength-dependent PMD effects and allows design of filters for their mitigation.
physics.optics:We demonstrate the combination of a hemispherical solid immersion lens with a micro-photoluminescence setup. Two advantages introduced by the SIL, an improved resolution of 0.4 times the wavelength in vacuum and a 5 times enhancement of the collection efficiency, make it an ideal system for spatially resolved spectroscopy applications. The influence of the air gap between the SIL and the sample surface is investigated in detail. We confirm the tolerance of the set--up to an air gap of several micrometers. Such a system is proven to be ideal system in the studies of exciton transport and polarization dependent single quantum dot spectroscopy.
physics.optics:It is assumed, that the clumps of lines do not connected with states mixing and IVR, but they are the result of breaking (destruction) of the process of averaging of momentum of inertia of molecules during the vibration motion of atoms. Rough estimates of the widths of clumps of lines in absorption spectra of some acetylenic derivatives were made with this model. Obtained results are in a satisfactory agreement with the available experimental data. This idea allows also in principle to explain the origin of intensive wings of lines, the existence of which was discussed earlier.
physics.optics:The origin of the Kerr type nonlinearity of the medium as a result of the interaction between photons via the Dirac delta-potential is presented in the formalism adopted from the photon wave function approach. In the view of the result the optical soliton may be treated as a bound state (cluster) of many photons.
physics.optics:The propagation of photon in a dielectric may be described with the help of the scalar and vector potentials of the medium. The main novelty of the paper is that the concept of the vector potential (which is connected with the velocity of the medium) can be extended to relativistic velocities of the medium. The position-dependent photon wave function was used to describe the propagation of the photon. The new concepts of the velocity of photon as particle and the photon mass in the dielectric medium were proposed.
physics.optics:Mathematical aspects of the SU(1,1) group parameter x dynamics governed by Hamiltonians exhibiting some special types of time dependence has been presented on an elementary level from the point of view of Moebius transformation of complex plane. The trajectories of x in continuous and mappings in discrete dynamics are considered. Some simple examples have been examined. Analytical considerations and numerical results have been given.
physics.optics:Propagation of the TE electromagnetic waves in self-focusing medium is governed by the nonlinear Schroedinger equation. In this paper the stationary solutions of this equation have been systematically presented. The phase-plane method, qualitative analysis, and mechanical interpretation of the differential equations are widely used. It is well known that TE waves can be guided by the single interface between two semi-infinite media, providing that one of the media has a self-focusing (Kerr type) nonlinearity. This special solution is called a spatial soliton. In this paper our interests are not restricted to the soliton solutions. In the context of the nonlinear substrate and cladding we have found solutions which could be useful to describe also the incident light in nonlinear medium. This result is the main point of the paper. Some of the presented stationary solutions were already used in similar optical context in literature but we show a little wider class of solutions. In the last section we review and illustrate some results concerning the spatial soliton solution.
physics.optics:We present angle- and polarization-resolved measurements of the optical transmission of a subwavelength hole array. These results give a (far-field) visualization of the corresponding (near-field) propagation of the excited surface plasmons and allow for a simple analysis of their polarization properties.
physics.optics:A monochromatic linear source of light is rotated with certain angular frequency and when such light is analysed after reflection then a change of frequency or wavelength may be observed depending on the location of the observer. This change of frequency or wavelength is different from the classical Doppler effect [1] or relativistic Doppler effect [2]. The reason behind this shift in wavelength is that a certain time interval observed by an observer in the rotating frame is different from that of a stationary observer.
physics.optics:We investigate the spectral response of a Brillouin amplifier in the frequency regime within the SBS bandwidth. This is done by amplitude modulating the pump with a low frequency, and therefore, unlike previous studies, the spectrum of the modulated pump is, in all cases, smaller than the SBS bandwidth. We show both theoretically and experimentally that unlike phase modulation, which was reported in the literature, the amplitude modulation increases the Brillouin amplifier gain, and that this effect has a very narrow bandwidth. Only modulation frequencies that are lower than a certain cut-off frequency increase the gain. This cut-off frequency is inversely proportional to the fiber's length, and can therefore be arbitrarily small.
physics.optics:The Phase Diverse Speckle (PDS) problem is formulated mathematically as Multi Frame Blind Deconvolution (MFBD) together with a set of Linear Equality Constraints (LECs) on the wavefront expansion parameters. This MFBD-LEC formulation is quite general and, in addition to PDS, it allows the same code to handle a variety of different data collection schemes specified as data, the LECs, rather than in the code. It also relieves us from having to derive new expressions for the gradient of the wavefront parameter vector for each type of data set. The idea is first presented with a simple formulation that accommodates Phase Diversity, Phase Diverse Speckle, and Shack-Hartmann wavefront sensing. Then various generalizations are discussed, that allows many other types of data sets to be handled.
physics.optics:A Monte Carlo simulation has been performed to track light rays in cylindrical fibres by ray optics. The trapping efficiencies for skew and meridional rays in active fibres and distributions of characteristic quantities for all trapped light rays have been calculated. The simulation provides new results for curved fibres, where the analytical expressions are too complex to be solved. The light losses due to sharp bending of fibres are presented as a function of the ratio of curvature to fibre radius and bending angle. It is shown that a radius of curvature to fibre radius ratio of greater than 65 results in a loss of less than 10% with the loss occuring in the initial stage of the bend (at bending angles Phi circa pi/8 rad).
physics.optics:We have measured the photonic bandgap in the transmission of microwaves through a two-dimensional photonic crystal slab. The structure was constructed by cementing acrylic rods in a hexagonal closed-packed array to form rectangular stacks. We find a bandgap centered at approximately 11 GHz, whose depth, width and center frequency vary with the number of layers in the slab, angle of incidence and microwave polarization.
physics.optics:We study forward stimulated Raman emission from weakly fluorescent dye 4'-diethylamino-N-methyl-4-stilbazolium tosylate (DEST) in 1,2,dichloroethane solution excited by a 28 ps, 532 nm Nd: YAG laser. Neat 1, 2, dichloroethane emits the first Stokes line at 631 nm with a spectral width of 1.6 nm corresponding to a Raman shift of 2956 per cm. We observe reduction of spectral width with the addition of DEST in 1, 2, dichloroethane solution. The single pass conversion efficiency for forward Raman emission is as high as 20 percent in a 1 cm path length sample. The pulse duration of forward stimulated Raman emission measured by a third order autocorrelation technique is 10 ps in neat 1, 2, dichloroethane, whereas it is nearly 3 ps for 0.04 mM of DEST solution.
physics.optics:Distribution of centrosymmetrical molecules of an impurity (p-diclorobenzene) in monocrystals of solid solutions in two different matrixes with centrosymmetrical (p-dibrombenzene) and noncentrosymmetrical (p-bromchlorbenzene) molecules by the method of a Raman Effect is determined.
physics.optics:We demonstrate that twisting one part of a chiral photonic structure about its helical axis produces a single circularly polarized localized mode that gives rise to an anomalous crossover in propagation. Up to a crossover thickness, this defect results in a peak in transmission and exponential scaling of the linewidth for a circularly polarized wave with the same handedness as structure. Above the crossover, however, the linewidth saturates and the defect mode can be excited only by the oppositely polarized wave, resulting in a peak in reflection instead of transmission.
physics.optics:We study experimentally and theoretically the polarization alternation during the switch-on transient of a quasi-isotropic CO$_2$ laser emitting on the fundamental mode. The observed transient dynamics is well reproduced by means of a model which provides a quantitative discrimination between the intrinsic asymmetry due to the kinetic coupling of molecules with different angular momenta, and the extrinsic anisotropies, due to a tilted intracavity window. Furthermore, the experiment provides a numerical assignment for the decay rate of the coherence term for a CO$_2$ laser.
physics.optics:In this paper we present an analysis of information transfer time based on holomorphism, causality and the classical principle of stationary phase. We also make a preliminary study of the effect of noise on information transfer time, and find that noise tends to increase transfer times. Noise and information signals are both essentially acausal, such that analytic continuation (i.e. prediction) is impossible, which also implies that their frequency spectra cannot be holomorphic. This leads to the paradox of a non-holomorphic information-bearing light signal, yet whose underlying Maxwell equations governing the propagation of the EM wave describe a holomorphic function in spacetime. We find that application of stationary phase and entropy arguments circumvents this difficulty, with stationary phase only suggesting the most likely transfer times of an information signal in the presence of noise. Faster transit times are not excluded, but are highly improbable. Stationary phase solutions, by definition, do not include signal forerunners, whose detection in the presence of noise is also unreliable. Hence a finite information capacity ensues, as expected from Shannon's law, and information cannot be transferred faster than c. We also find that the method of stationary phase implies complex transfer times. However, by considering spacetime to be isomorphic with the complex temporal plane, we find that an imaginary time is equivalent to a real distance, and can be interpreted as the uncertainty in the spatial position of the information pulse. Finally, we apply our theory to a photonic band gap crystal, and find that information transfer speed and tunneling is always subluminal.
physics.optics:Enhancement of optical Kerr nonlinearity for self-action by electro-magnetically induced transparency in a four-level atomic system including dephasing between the ground states is studied in detail by solving the density matrix equations for the atomic levels. We discern three major contributions, from energy shifts of the ground states induced by the probe light, to the third-order susceptibility in the four-level system. In this four-level system with the frequency-degenerate probes, quantum interference amongst the three contributions can, not only enhance the third-order susceptibility more effectively than in the three-level system with the same characteristic parameters, but also make the ratio between its real and imaginary part controllable. Due to dephasing between the two ground states and constructive quantum interference, the most effective enhancement generally occurs at an offset that is determined by the atomic transition frequency difference and the coupling Rabi frequency.
physics.optics:We numerically study supercontinuum (SC) generation in photonic crystal fibers pumped with low-power 30-ps pulses close to the zero dispersion wavelength 647nm. We show how the efficiency is significantly improved by designing the dispersion to allow widely separated spectral lines generated by degenerate four-wave-mixing (FWM) directly from the pump to broaden and merge. By proper modification of the dispersion profile the generation of additional FWM Stokes and anti-Stokes lines results in efficient generation of an 800nm wide SC. Simulations show that the predicted efficient SC generation is more robust and can survive fiber imperfections modelled as random fluctuations of the dispersion coefficients along the fiber length.
physics.optics:We numerically study the possibilities for improved large-mode area endlessly single mode photonic crystal fibers for use in high-power delivery applications. By carefully choosing the optimal hole diameter we find that a triangular core formed by three missing neighboring air holes considerably improves the mode area and loss properties compared to the case with a core formed by one missing air hole. In a realized fiber we demonstrate an enhancement of the mode area by ~30 % without a corresponding increase in the attenuation.
physics.optics:A new type of perturbative expansion is built in order to give a rigorous derivation and to clarify the range of validity of some commonly used model equations.   This model describes the evolution of the modulation of two short and localized pulses, fundamental and second harmonic, propagating together in a bulk uniaxial crystal with non-vanishing second order susceptibility $\chi^(2)$ and interacting through the nonlinear effect known as ``cascading'' in nonlinear optics.   The perturbative method mixes a multi-scale expansion with a power series expansion of the susceptibility, and must be carefully adapted to the physical situation. It allows the determination of the physical conditions under which the model is valid: the order of magnitude of the walk-off, phase-mismatch,and anisotropy must have determined values.
physics.optics:Nonlinear phase noise, often called the Gordon-Mollenauer effect, can be compensated electronically by subtracting from the received phase a correction proportional to the received intensity. The optimal scaling factor is derived analytically and found to be approximately equal to half of the ratio of mean nonlinear phase noise and the mean received intensity. Using optimal compensation, the standard deviation of residual phase noise is halved, doubling the transmission distance in systems limited by nonlinear phase noise.
physics.optics:On the basis of the data given in the works of different authors a criterion of phase-photometric method of measurement of energy angle of divergence has been formulated. Validity of application of the obtained relations for a ray beam with an arbitrary diameter and an arbitrary shape of the wave front has been proved. Advantages of the proposed phase-photometric method in comparison with the focal-spot method have been confirmed. Necessity and possibility of building a standard solid angle has been proved.
physics.optics:This document contains my detailed calculation of the Generalised Few-cycle Envelope Approximation (GFEA) propagation equation reported and used in Phys. Rev. A (submitted) and its associated longer version at arXiv.org. This GFEA propagation equation is intended to be applicable to optical pulses only a few cycles long, a regime where the standard Slowly Varying Envelope Approximation (SVEA) fails.
physics.optics:We present a comprehensive framework for treating the nonlinear interaction of few-cycle pulses using an envelope description that goes beyond the traditional SVEA method. This is applied to a range of simulations that demonstrate how the effect of a $\chi^{(2)}$ nonlinearity differs between the many-cycle and few-cycle cases. Our approach, which includes diffraction, dispersion, multiple fields, and a wide range of nonlinearities, builds upon the work of Brabec and Krausz[1] and Porras[2]. No approximations are made until the final stage when a particular problem is considered.   The original version (v1) of this arXiv paper is close to the published Phys.Rev.A. version, and much smaller in size.
physics.optics:Broadband noise on supercontinuum spectra generated in microstructure fiber is shown to lead to amplitude fluctuations as large as 50 % for certain input laser pulse parameters. We study this noise using both experimental measurements and numerical simulations with a generalized stochastic nonlinear Schroedinger equation, finding good quantitative agreement over a range of input pulse energies and chirp values. This noise is shown to arise from nonlinear amplification of two quantum noise inputs: the input pulse shot noise and the spontaneous Raman scattering down the fiber.
physics.optics:The probability density function of Kerr effect phase noise, often called the Gordon-Mollenauer effect, is derived analytically. The Kerr effect phase noise can be accurately modeled as the summation of a Gaussian random variable and a noncentral chi-square random variable with two degrees of freedom. Using the received intensity to correct for the phase noise, the residual Kerr effect phase noise can be modeled as the summation of a Gaussian random variable and the difference of two noncentral chi-square random variables with two degrees of freedom. The residual phase noise can be approximated by Gaussian distribution better than the Kerr effect phase noise without correction.
physics.optics:Steady-state and dynamics of the self-phase-locked (3\omega ==> 2\omega, \omega) subharmonic optical parametric oscillator are analyzed in the pump-and-signal resonant configuration, using an approximate analytical model and a full propagation model. The upper branch solutions are found always stable, regardless of the degree of pump enhancement. The domain of existence of stationary states is found to critically depend on the phase-mismatch of the competing second-harmonic process.
physics.optics:In the present paper we investigate the transmission and reflection band behavior for a plane electromagnetic wave falling obliquely on an ideal layered structure. The dependence of this behavior on the problem parameters and wave incident angle is considered. It is shown, that in general case the band width is a non-monotonous function of the problem parameters. A condition is found, which defines the possibility of the contact of the transmission bands. This condition has the same form for s and p waves. It is also shown that irrespective of the wave polarization, the transmission coefficient equals to the unit at the contact points.
physics.optics:The problem of determination of the maximum of second harmonic generation in the potential well containing a rectangular barrier is considered. It is shown that, in general, the problem of finding the ensemble of structures with equidistant first three levels has two types of solutions.   For the first type the second and third energy levels are located above a rectangular barrier, and for the second type the third level is located above the barrier only. It is also shown, that generation corresponding to the second type of solution always is less than generation for the first one. Taking into account the effective mass changes the problem of finding the generation maximum for a finite depth well is exactly solved.
physics.optics:We investigate numerically optical properties of novel two-dimensional photonic materials where parallel dielectric rods are randomly placed with the restriction that the distance between rods is larger than a certain value. A large complete photonic gap (PG) is found when rods have sufficient density and dielectric contrast. Our result shows that neither long-range nor short-range order is an essential prerequisite to the formation of PGs. A universal principle is proposed for designing arbitrarily shaped waveguides, where waveguides are fenced with side walls of periodic rods and surrounded by the novel photonic materials. We observe highly efficient transmission of light for various waveguides. Due to structural uniformity, the novel photonic materials are best suited for filling up the outer region of waveguides of arbitrary shape and dimension comparable with the wavelength.
physics.optics:We suggest an effective method for controlling nonlinear switching in arrays of weakly coupled optical waveguides. We demonstrate the digitized switching of a narrow input beam for up to eleven waveguides in the engineered waveguide arrays.
physics.optics:Plane waves in Kerr media spontaneously generate paraxial X-waves (i.e. non-dispersive and non-diffractive pulsed beams) that get amplified along propagation. This effect can be considered a form of conical emission (i.e. spatio-temporal modulational instability), and can be used as a key for the interpretation of the out of axis energy emission in the splitting process of focused pulses in normally dispersive materials. A new class of spatio-temporal localized wave patterns is identified. X-waves instability, and nonlinear X-waves, are also expected in periodical Bose condensed gases.
physics.optics:The Dicke superradiance on vibronic transitions of impurity crystals is considered. It is shown that parameters of the superradiance (duration and intensity of the superradiance pulse and delay times) on each vibronic transition depend on the strength of coupling of electronic states with the intramolecular impurity vibration (responsible for the vibronic structure of the optical spectrum in the form of vibrational replicas of the pure electronic line) and on the crystal temperature through the Debye-Waller factor of the lattice vibrations. Theoretical estimates of the ratios of the time delays, as well as of the superradiance pulse intensities for different vibronic transitions well agree with the results of experimental observations of two-color superradiance in the polar dielectric KCl:O2-. In addition, the theory describes qualitatively correctly the critical temperature dependence of the superradiance effect.
physics.optics:This work is concerned with the propagation of electromagnetic waves in isotropic chiral media and with the effects produced by a plane boundary between two such media. In analogy with the phenomena of reflection and refraction of plane electromagnetic waves in ordinary dielectrics, the kinematical and dynamical aspects of these phenomena are studied, such as the intensity of the various wave components and the change in the polarization of the wave as it crosses the boundary. As a prerequisite of this, we show that the plane wave solution must be written as a suitable superposition of the circularly amplitudes on both sides of the interface, we elucidate which is the appropriate set of conditions that the solution must satisfy at the boundary, and we set down the minimal, and complete, set of equations that must be solved for the coefficient amplitudes in order to satisfy the boundary conditions. The equations are solved explicitly for some particular cases and configurations (e.g., normal incidence), the salient features of those solutions are analyzed in some detail, and the general solution to the equations is given as well.
physics.optics:We study the class of endlessly single-mode all-silica photonic crystal fibers with a triangular air-hole cladding. We consider the sensibility to longitudinal nonuniformities and the consequences and limitations for realizing low-loss large-mode area photonic crystal fibers. We also discuss the dominating scattering mechanism and experimentally we confirm that both macro and micro-bending can be the limiting factor.
physics.optics:Some aspects of lasing at vibronic transitions in impurity crystals are theoretically studied. The threshold conditions for a vibronic laser are shown to be dependent on the strength of interaction of optical centers with a local vibration, which forms the vibronic spectrum, and the crystal lattice temperature. The theory can be easily generalized to the spectrum containing a structureless phonon sideband and well agrees with the experimental temperature dependence of the output power of a Mg2SiO4:Cr4+ forsterite laser.
physics.optics:We investigate the characteristics of guided wave modes in planar coupled waveguides. In particular, we calculate the dispersion relations for TM modes in which one or both of the guiding layers consists of negative index media (NIM)-where the permittivity and permeability are both negative. We find that the Poynting vector within the NIM waveguide axis can change sign and magnitude, a feature that is reflected in the dispersion curves.
physics.optics:It has recently been shown that periodic layered media can reflect strongly for all incident angles and polarizations in a given frequency range. The standard treatment gets these band gaps from an eigenvalue equation for the Bloch factor in an infinite periodic structure. We argue that such a procedure may become meaningless when dealing with structures with not very many periods. We propose an alternative approach based on a factorization of the multilayer transfer matrix in terms of three fundamental matrices of simple interpretation. We show that the trace of the transfer matrix sorts the periodic structures into three types with properties closely related to one (and only one) of the three fundamental matrices. We present the reflectance associated to each one of these types, which can be considered as universal features of the reflection in these media.
physics.optics:We study effects of finite height and surrounding material on photonic crystal slabs of one- and two-dimensional photonic crystals with a pseudo-spectral method and finite difference time domain simulation methods. The band gap is shown to be strongly modified by the boundary material. As an application we suggest reflection and guiding of light by patterning the material on top/below the slab.
physics.optics:The characteristics of an imaging system formed by a slab of a lossy left-handed material (LHM) are studied. The transfer function of the LHM imaging system is written in an appropriate product form with each term having a clear physical interpretation. A tiny loss of the LHM may suppress the transmission of evanescent waves through the LHM slab and this is explained physically. An analytical expression for the resolution of the imaging system is derived. It is shown that it is impossible to make a subwavelength imaging by using a realistic LHM imaging system unless the LHM slab is much thinner than the wavelength.
physics.optics:We observe the formation of an intense optical wavepacket fully localized in all dimensions, i.e. both longitudinally (in time) and in the transverse plane, with an extension of a few tens of fsec and microns, respectively. Our measurements show that the self-trapped wave is a X-shaped light bullet spontaneously generated from a standard laser wavepacket via the nonlinear material response (i.e., second-harmonic generation), which extend the soliton concept to a new realm, where the main hump coexists with conical tails which reflect the symmetry of linear dispersion relationship.
physics.optics:The statistical properties of nonlinear phase noise, often called the Gordon-Mollenauer effect, is studied analytically when the number of fiber spans is very large. The joint characteristic functions of the nonlinear phase noise with electric field, received intensity, and the phase of amplifier noise are all derived analytically. Based on the joint characteristic function of nonlinear phase noise with the phase of amplifier noise, the error probability of signal having nonlinear phase noise is calculated using the Fourier series expansion of the probability density function. The error probability is increased due to the dependence between nonlinear phase noise and the phase of amplifier noise. When the received intensity is used to compensate the nonlinear phase noise, the optimal linear and nonlinear minimum mean-square error compensators are derived analytically using the joint characteristic function of nonlinear phase noise and received intensity. Using the joint probability density of received amplitude and phase, the optimal maximum a posteriori probability detector is derived analytically. The nonlinear compensator always performs better than linear compensator.
physics.optics:The exact Green function for the scalar wave equation in a plane with any set of perfectly reflecting straight mirrors, which may be joined to form corners, is given as a diffraction scattering series. Instances would be slit diffraction in optics, or the Schrodinger equation inside (or outside) a general polygonal enclosure ('quantum polygon billiards'). The method is based on the seminal 1896 Riemann helicoid surface solution by Sommerfeld for optical diffraction by a single corner. It is generalised to account for multiple scatter by adapting the analysis of Stovicek for a closely related problem: a collection of magnetic flux lines (points) in a plane, the multi-flux Aharonov-Bohm effect. The short wavelength limit is shown to yield the 'geometrical theory of diffraction'. For slit diffraction the exact series is shown to coincide with that of Schwarzschild in 1902.
physics.optics:We experimentally demonstrate for the first time that a linearly polarized beam is focussed to an asymmetric spot when using a high-numerical aperture focussing system. This asymmetry was predicted by Richards and Wolf [Proc.R.Soc.London A, 253, 358 (1959)] and can only be measured when a polarization insensitive sensor is placed in the focal region. We used a specially modified photodiode in a knife edge type set up to obtain highly resolved images of the total electric energy density distribution at the focus. The results are in good agreement with the predictions of a vectorial focussing theory.
physics.optics:A simple model is used to estimate the Q factor in numerical simulations of differential phase shift keying (DPSK) with optical delay demodulation and balanced detection. It is found that an alternative definition of Q is needed for DPSK in order to have a more accurate prediction of the bit error ratio (BER).
physics.optics:With using of point-dipole model the theoretical calculations of main refractive indices and orientation of indicatrix of 18 minerals are performed. The feature of studied minerals is the statistically disordered arrangement of CO3, SO4, SO2, PO4 groups and also separate ions. The optical characters of uniaxial minerals and orientation of indicatrix of orthorhombic and monoclinic minerals, obtained by results of calculations, agree with experimental definitions.
physics.optics:The features of a compact, single pass, multi-pixel optical parametric generator are discussed. Several hundreds of independent high spatial-quality tunable ultrashort pulses were produced by pumping a bulk lithium triborate crystal with an array of tightly focussed intense beams. The array of beams was produced by shining a microlenses array with a large pump beam. Overall conversion efficiency to signal and idler up to 30% of the pump beam has been reported. Shot-to-shot energy fluctuation down to 3% was achieved for the generated radiation.
physics.optics:We use a spatially resolved cavity ring-down technique to show that the 2D eigenmode of an unstable optical cavity has a fractal pattern, i.e. it looks the same at different length scales. In agreement with theory, we find that this pattern has the maximum conceivable roughness, i.e., its fractal dimension is 3.01 plus\minus 0.04. This insight in the nature of unstable cavity eigenmodes may lead to better understanding of wave dynamics in open systems, for both light and matter waves.
physics.optics:Numerical Calculations are employed to study the modulation of light by surface acoustic waves (SAWs) in photonic band gap (PBG) structures. The on/off contrast ratio in PBG switch based on optical cavity is determined as a function of the SAW induced dielectric modulation. We show that these structures exhibit high contrast ratios even for moderate acousto-optic coupling
physics.optics:By combining the definition of the Wigner distribution function (WDF) and the matrix method of optical system modeling, we can evaluate the transformation of the former in centered systems with great complexity. The effect of stops and lens diameter are also considered and are shown to be responsible for non-linear clipping of the resulting WDF in the case of coherent illumination and non-linear modulation of the WDF when the illumination is incoherent. As an example, the study of a single lens imaging systems illustrates the applicability of the method.
physics.optics:Free-space propagation can be described as a shearing of the Wigner distribution function in the spatial coordinate; this shearing is linear in paraxial approximation but assumes a more complex shape for wide-angle propagation. Integration in the frequency domain allows the determination of near-field diffraction, leading to the well known Fresnel diffraction when small angles are considered and allowing exact prediction of wide-angle diffraction. The authors use this technique to demonstrate evanescent wave formation and diffraction elimination for very small apertures.
physics.optics:We reelaborate on the basic properties of lossless multilayers by using bilinear transformations. We study some interesting properties of the multilayer transfer function in the unit disk, showing that hyperbolic geometry turns out to be an essential tool for understanding multilayer action. We use a simple trace criterion to classify multilayers into three classes that represent rotations, translations, or parallel displacements. Moreover, we show that these three actions can be decomposed as a product of two reflections in hyperbolic lines. Therefore, we conclude that hyperbolic reflections can be considered as the basic pieces for a deeper understanding of multilayer optics.
physics.optics:In a coherent monoenergetic beam of non-interacting particles, the phase velocity and the particle transport velocity are functions of position, with the strongest variation being in the focal region. These velocities are everywhere parallel to each other, and their product is constant in space. For a coherent monochromatic electromagnetic beam, the energy transport velocity is never greater than the speed of light, and can even be zero. The phase velocities (one each for the non-zero components of the electric and magnetic fields, in general) can be different from each other and from the energy transport velocity, both in direction and in magnitude. The phase velocities at a given point are independent of time, for both particle and electromagnetic beams. The energy velocity is independent of time for the particle beam, but in general oscillates (with angular frequency 2w) in magnitude and direction about its mean value at a given point in the electromagnetic beam. However, there exist electromagnetic steady beams, within which the energy flux, energy density and energy velocity are all independent of time.
physics.optics:The polarization properties of monochromatic light beams are studied. In contrast to the idealization of an electromagnetic plane wave, finite beams which are everywhere linearly polarized in the same direction do not exist. Neither do beams which are everywhere circularly polarized in a fixed plane. It is also shown that transversely finite beams cannot be purely transverse in both their electric and magnetic vectors, and that their electromagnetic energy travels at less than c. The electric and magnetic fields in an electromagnetic beam have different polarization properties in general, but there exists a class of steady beams in which the electric and magnetic polarizations are the same (and in which energy density and energy flux are independent of time). Examples are given of exactly and approximately linearly polarized beams, and of approximately circularly polarized beams.
physics.optics:We suggest a geometrical framework to discuss periodic layered structures in the unit disk. The band gaps appear when the point representing the system approaches the unit circle. We show that the trace of the matrix describing the basic period allows for a classification in three families of orbits with quite different properties. The laws of convergence of the iterates to the unit circle can be then considered as universal features of the reflection.
physics.optics:The effect of the Kerr nonlinearity on linear non-diffractive Bessel beams is investigated analytically and numerically using the nonlinear Schr\"odinger equation. The nonlinearity is shown to primarily affect the central parts of the Bessel beam, giving rise to radial compression or decompression depending on whether the nonlinearity is focusing or defocusing, respectively. The dynamical properties of Gaussian-truncated Bessel beams are also analysed in the presence of a Kerr nonlinearity. It is found that although a condition for width balance in the root-mean-square sense exists, the beam profile becomes strongly deformed during propagation and may exhibit the phenomena of global and partial collapse.
physics.optics:We describe an optical technique based on the statistical analysis of the random intensity distribution due to the interference of the near-field scattered light with the strong transmitted beam. It is shown that, from the study of the two-dimensional power spectrum of the intensity, one derives the scattered intensity as a function of the scattering wave vector. Near-field conditions are specified and discussed. The substantial advantages over traditional scattering technique are pointed out, and is indicated that the technique could be of interest for wave lengths other than visible light.
physics.optics:The usual computation of the spontaneous emission uses a mixture of classical and quantum postulates. A purely classical computation shows that a source of electromagnetic field absorbs light in the eigenmode it is able to emit. Thus in an excitation by an other mode, the component of this mode on the eigenmode is absorbed, while the remainder is scattered. This loss of energy does not apply to the zero point field which has its regular energy in the eigenmode, so that the zero point field seems more effective than the other fields for the stimulation of light emission.
physics.optics:We perform numerical studies of the effect of sidewall imperfections on the resonant state broadening of the optical microdisk cavities for lasing applications. We demonstrate that even small edge roughness causes a drastic degradation of high-Q whispering gallery (WG) mode resonances reducing their Q-values by many orders of magnitude. At the same time, low-Q WG resonances are rather insensitive to the surface roughness. The results of numerical simulation obtained using the scattering matrix technique, are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surface of sections in the classical ray picture.
physics.optics:We consider plane waves propagating in quadratic nonlinear slab waveguides with nonlinear quasi-phase-matching gratings. We predict analytically and verify numerically the complete gain spectrum for transverse modulational instability, including hitherto undescribed higher order gain bands.
physics.optics:Optical near field has been generated by Laguarre-Gaussian doughnut beam on inner surface of "atom funnel". The resulting optical near field has been measured with the help of fiber probe and a consequent effect on cold atoms- released from MOT, has been estimated. Atoms with temperature less than 10 micro_kelvin can be reflected by the optical near field.
physics.optics:We report on an attempt to generate highly stable continuous terahertz (THz) wave by using optical frequency comb (OFC). About 10-nm wide OFC has been generated through a deep phase modulation of a 852 nm laser line in lithium niobate crystal cavity. The multiple optical modes (side bands) of the OFC, which are equally separated from each other by the modulation frequency (=6 GHz) are taken as the frequency reference. When another semiconductor laser is frequency locked, the stability of the difference frequency between the master laser and the second laser is improved on the same order of the RF modulator. An ultra-narrow line and tunable THz radiation source can be achieved by photomixing of this stable difference-frequency optical beat in a photoconductive antenna.
physics.optics:Doughnut shaped light beam has been generated from Gaussian mode ($TEM_{00}$) cw-Ti sapphire laser. After splitting the pump beam into two equal intensity components and introducing unequal convergence and phase delay while they are recombined it results in doughnut mode. Such a beam is tunable and have long propagation length. The evanescent field generated by 360 mW (at 780 nm wavelength) of such a beam creates optical field of 600 nm decay length with a 5.75 neV repulsive dipole potential. Thus cold Rb atoms (at 10{$\mu$}K or less temperature) released from MOT can be reflected by the surface so that the atoms are collected ultimately at the bottom of the prism. By focussing such doughnut beam with 8 cm focal length converging lens, the dark radius reduces to 22{$\mu$}. We also observe such beam to contain azimuthal phase as well as radial phase distribution.
physics.optics:Ferroelectric ordering, the electroclinic effect and chiral smectic C (SmC*) - smectic A (SmA*) phase transitions in thin planar ferroelectric liquid crystal (FLC) cells are studied by means of linear electrooptic and second harmonic generation techniques. The ferroelectric switching is detected in biased FLC cells by measuring azimuthal dependences of linear and nonlinear responses. The applied DC-electric field rotates the FLC symmetry axis with initial and final orientations in the cell plane. Comparative studies of the switching behavior in reflection and transmission allows to distinguish the contributions from the bulk and the sub-surface layers of the cell. The analysis of temperature dependence shows the existence of a strong surface coupling. The temperature dependent nonlinear polarization shows a critical behavior corresponding to the superfluid model.
physics.optics:We have generated high power doughnut beam suitable for atom funnel experiment with the conversion efficiency of about 50 %.
physics.optics:We consider large-mode area photonic crystal fibers for visible applications where micro-deformation induced attenuation becomes a potential problem when the effective area A_eff is sufficiently large compared to lambda^2. We argue how a slight increase in fiber diameter D can be used in screening the high-frequency components of the micro-deformation spectrum mechanically and we confirm this experimentally for both 15 and 20 micron core fibers. For typical bending-radii (R~16 cm) the operating band-width increases by ~3-400 nm to the low-wavelength side.
physics.optics:We investigate general properties of spatial 1-dimensional bright photorefractive solitons and suggest various analytical approximations for the soliton profile and the half width, both depending on an intensity parameter r.
physics.optics:A model for a non-Kerr cylindrical nematic fiber is presented. We use the multiple scales method to show the possibility of constructing different kinds of wavepackets of transverse magnetic (TM) modes propagating through the fiber. This procedure allows us to generate different hierarchies of nonlinear partial differential equations (PDEs) which describe the propagation of optical pulses along the fiber. We go beyond the usual weakly nonlinear limit of a Kerr medium and derive an extended Nonlinear Schrodinger equation (eNLS) with a third order derivative nonlinearity, governing the dynamics for the amplitude of the wavepacket. In this derivation the dispersion, self-focussing and diffraction in the nematic are taken into account. Although the resulting nonlinear $PDE$ may be reduced to the modified Korteweg de Vries equation (mKdV), it also has additional complex solutions which include two-parameter families of bright and dark complex solitons. We show analytically that under certain conditions, the bright solitons are actually double embedded solitons. We explain why these solitons do not radiate at all, even though their wavenumbers are contained in the linear spectrum of the system. Finally, we close the paper by making comments on the advantages as well as the limitations of our approach, and on further generalizations of the model and method presented.
physics.optics:The modal cut-off is investigated experimentally in a series of high quality non-linear photonic crystal fibers. We demonstrate a suitable measurement technique to determine the cut-off wavelength and verify it by inspecting the near field of the modes that may be excited below and above the cut-off. We observe a double peak structure in the cut-off spectra, which is attributed to a splitting of the higher order modes. The cut-off is measured for seven different fiber geometries with different pitches and relative hole size, and a very good agreement with recent theoretical work is found.
physics.optics:We address the long-standing unresolved problem concerning the V-parameter in a photonic crystal fiber (PCF). Formulate the parameter appropriate for a core-defect in a periodic structure we argue that the multi-mode cut-off occurs at a wavelength lambda* which satisfies V_PCF(lambda*)=pi. Comparing to numerics and recent cut-off calculations we confirm this result.
physics.optics:We propose in this article an unambiguous definition of the local density of electromagnetic states (LDOS) in a vacuum near an interface in an equilibrium situation at temperature $T$. We show that the LDOS depends only on the electric field Green function of the system but does not reduce in general to the trace of its imaginary part as often used in the literature. We illustrate this result by a study of the LDOS variations with the distance to an interface and point out deviations from the standard definition. We show nevertheless that this definition remains correct at frequencies close to the material resonances such as surface polaritons. We also study the feasability of detecting such a LDOS with apetureless SNOM techniques. We first show that a thermal near-field emission spectrum above a sample should be detectable and that this measurement could give access to the electromagnetic LDOS. It is further shown that the apertureless SNOM is the optical analog of the scanning tunneling microscope which is known to detect the electronic LDOS. We also discuss some recent SNOM experiments aimed at detecting the electromagnetic LDOS.
physics.optics:The nonlinearity of a transmission fiber may be compensated by a specialty fiber and an optical phase conjugator. Such combination may be used to pre-distort signals before each fiber span so to linearize an entire transmission line.
physics.optics:Two fiber lines may compensate each other for nonlinearity with the help of optical phase conjugation. The pair of fiber lines and the optical signals in them may be either mirror-symmetric or translationally symmetric about the conjugator.
physics.optics:We have developed a scattering-matrix approach for numerical calculation of resonant states and Q-values of a nonideal optical disk cavity of an arbitrary shape and of an arbitrary varying refraction index. The developed method has been applied to study the effect of surface roughness and inhomogeneity of the refraction index on Q-values of microdisk cavities for lasing applications. We demonstrate that even small surface roughness can lead to a drastic degradation of high-Q cavity modes by many orders of magnitude. The results of numerical simulation are analyzed and explained in terms of wave reflection at a curved dielectric interface combined with the examination of Poincare surfaces of section and Husimi distributions.
physics.optics:We analyse theoretically for the first time to our knowledge the perfect phase matching of guided TE and TM modes with a multilayer waveguide composed of linear isotropic dielectric materials. Alongside strict investigation into dispersion relations for multilayer systems, we give an explicit qualitative explanation for the phenomenon of mode matching on the basis of the standard one-dimensional homogenization technique, and discuss the minimum number of layers and the refractive index profile for the proposed device scheme. Direct applications of the scheme include polarization-insensitive, intermodal dispersion-free planar propagation, efficient fibre-to-planar waveguide coupling and, potentially, mode filtering. As a self-sufficient result, we present compact analytical expressions for the mode dispersion in a finite, N-period, three-layer dielectric superlattice.
physics.optics:The characteristic function of soliton phase jitter is found analytically when the soliton is perturbed by amplifier noise. In additional to that from amplitude jitter, the nonlinear phase noise due to frequency and timing jitter is also analyzed. Because the nonlinear phase noise is not Gaussian distributed, the overall phase jitter is also non-Gaussian. For a fixed mean nonlinear phase shift, the contribution of nonlinear phase noise from frequency and timing jitter decreases with distance and signal-to-noise ratio.
physics.optics:The field energy distributions and effective mode areas of silica-based photonic bandgap fibers with a honeycomb airhole structure in the cladding and an extra airhole defining the core are investigated. We present a generalization of the common effective area definition, suitable for the problem at hand, and compare the results for the photonic bandgap fibers with those of index-guiding microstructured fibers. While the majority of the field energy in the honeycomb photonic bandgap fibers is found to reside in the silica, a substantial fraction (up to ~30%) can be located in the airholes. This property may show such fibers particularly interesting for sensor applications, especially those based on nonlinear effects or interaction with other structures (e.g. Bragg gratings) in the glass.
physics.optics:The waveguiding properties of two silica-based airguiding photonic bandgap fiber designs are investigated with special emphasis on material effects. The nonlinear coefficients are found to be 1-2 orders of magnitude smaller than those obtained in index-guiding microstructured fibers with large mode areas. The material dispersion of silica makes a significant contribution to the total chromatic dispersion although less than 10% of the field energy is located in the silica regions of the fibers. These findings suggest that dispersion engineering through the choice of base material may be a possibility in this type of fibers.
physics.optics:Lithium thioindate (LiInS$_{2}$) is a new nonlinear chalcogenide biaxial material transparent from 0.4 to 12 $\mu$m, that has been successfully grown in large sizes and good optical quality. We report on new physical properties that are relevant for laser and nonlinear optics applications. With respect to AgGaS(e)$_2$ ternary chalcopyrite materials, LiInS$_{2}$ displays a nearly-isotropic thermal expansion behavior, a 5-times larger thermal conductivity associated with high optical damage thresholds, and an extremely low intensity-dependent absorption allowing direct high-power downconversion from the near-IR to the deep mid-IR. Continuous-wave difference-frequency generation (5-11$ \mu$m) of Ti:sapphire laser sources is reported for the first time.
physics.optics:We find that the function that describes the surface of spherical aberration free lenses can be used for both positive and negative refractive index media. With the inclusion of negative index, this function assumes the form of all the conic sections and expands the theory of aplanatic optical surfaces. There are two different symmetry centers with respect to the index that create an asymmetric relationship between positive and negative index lens profiles. In the thin lens limit the familiar formulas for image position and magnification hold for any index.
physics.optics:We report the surprising observation of directional tunneling escape from nearly spherical fused-silica optical resonators, in which most of the phase space is filled with nonchaotic regular trajectories. Experimental and theoretical studies of the dependence of the far-field emission pattern on both the degree of deformation and the excitation condition show that nonperturbative phase-space structures in the internal ray dynamics profoundly affect tunneling leakage of the whispering-gallery modes.
physics.optics:Intermodal interactions displayed through the phenomena of mode coupling and conversion in optical systems are treated by means of the Lindstedt-Poincare perturbation method of strained parameters more widely known in classical quantum mechanics and quantum chemistry as the stationary perturbation technique. The focus here is on the mode conversion at the points of virtual phase matching (otherwise called anticrossings or avoided crossings) associated with the maximum conversion efficiency. The method is shown to provide a convenient tool to deal with intermodal interactions at anticrossings -- interactions induced by any kind of perturbation in dielectric index profile of the waveguide, embracing optical inhomogeneity, magnetization of arbitrary orientation, and nonlinearity. Closed-form analytic expressions are derived for the minimum value of mode mismatch and for the length of complete mode conversion (the coupling length, or the beat length) in generic waveguiding systems exhibiting anticrossings. Demonstrating the effectiveness of the method, these general expressions are further applied to the case of TE -- TM mode conversion in (i) a multilayer gyrotropic waveguide under piecewise-constant, arbitrarily oriented magnetization, and (ii) an optically-inhomogeneous planar dielectric waveguide -- an example which the standard coupled-mode theory fails to describe.
physics.optics:We exploit a slightly noncollinear second-harmonic cross-correlation scheme to map the 3D space-time intensity distribution of an unknown complex-shaped ultrashort optical pulse. We show the capability of the technique to reconstruct both the amplitude and the phase of the field through the coherence of the nonlinear interaction down to a resolution of 10 $\mu$m in space and 200 fs in time. This implies that the concept of second-harmonic holography can be employed down to the sub-ps time scale, and used to discuss the features of the technique in terms of the reconstructed fields.
physics.optics:Nonlinear mode coupling in a coaxial waveguide filled with Faraday material has been considered. The picture of mode interaction is shown to resemble Coulomb interaction of charges: higher modes with nonzero angular momentum interact like effective charges via exchange of zero angular momentum quanta of the fundamental mode. Thus, at large distances this interaction becomes the dominant mechanism of mode coupling. The developed model may be used in designing coaxial photonic crystal fibers with strong tailored mode interaction.
physics.optics:We develop a general theory of spatial solitons in a liquid crystalline medium exhibiting a nonlinearity with an arbitrary degree of effective nonlocality. The model accounts the observability of "accessible solitons" and establishes an important link with parametric solitons.
physics.optics:Theoretical analysis is presented on quantum state evolution of polarization light waves at frequencies $\omega_{o}$ and $\omega_{e}$ in a periodically poled nonlinear crystal (PPNC). It is shown that the variances of all the four Stokes parameters can be squeezed.
physics.optics:Counter-propagating light fields have the ability to create self-organized one-dimensional optically bound arrays of microscopic particles, where the light fields adapt to the particle locations and vice versa. We develop a theoretical model to describe this situation and show good agreement with recent experimental data (Phys. Rev. Lett. 89, 128301 (2002)) for two and three particles, if the scattering force is assumed to dominate the axial trapping of the particles. The extension of these ideas to two and three dimensional optically bound states is also discussed.
physics.optics:The inversion of a diffraction pattern offers aberration-free diffraction-limited 3D images without the resolution and depth-of-field limitations of lens-based tomographic systems, the only limitation being radiation damage. We review our experimental results, discuss the fundamental limits of this technique and future plans.
physics.optics:The stability of two-dimensional bright vortex solitons in a media with focusing cubic and defocusing quintic nonlinearities is investigated analytically and numerically. It is proved that above some critical beam powers not only one- and two-charged but also multiple-charged stable vortex solitons do exist. A vortex soliton occurs robust with respect to symmetry-breaking modulational instability in the self-defocusing regime provided that its radial profile becomes flattened, so that a self-trapped wave beam gets a pronounced surface. It is demonstrated that the dynamics of a slightly perturbed stable vortex soliton resembles an oscillation of a liquid stream having a surface tension. Using the idea of sustaining effective surface tension for spatial vortex soliton in a media with competing nonlinearities the explanation of a suppression of the modulational instability is proposed.
physics.optics:Quasi error-free 10 Gbit/s data transmission is demonstrated over a novel type of 50 micron core diameter photonic crystal fiber with as much as 100 m length. Combined with 850$ nm VCSEL sources, this fiber is an attractive alternative to graded-index multi-mode fibers for datacom applications. A comparison to numerical simulations suggests that the high bit-rate may be partly explained by inter-modal diffusion.
physics.optics:The Casimir force between metallic plates made of realistic materials is evaluated for distances in the nanometer range. A spectrum over real frequencies is introduced and shows narrow peaks due to surface resonances (plasmon polaritons or phonon polaritons) that are coupled across the vacuum gap. We demonstrate that the Casimir force originates from the attraction (repulsion) due to the corresponding symmetric (antisymmetric) eigenmodes, respectively. This picture is used to derive a simple analytical estimate of the Casimir force at short distances. We recover the result known for Drude metals without absorption and compute the correction for weakly absorbing materials.
physics.optics:In recent years there has been an explosive development of interest in the measurement of forces at the microscopic level, such as within living cells, as well as the properties of fluids and suspensions on this scale, using optically trapped particles as probes. The next step would be to measure torques and associated rotational motion. This would allow measurement on very small scales since no translational motion is needed. It could also provide an absolute measurement of the forces holding a stationary non-rotating particle in place. The laser-induced torque acting on an optically trapped microscopic birefringent particle can be used for these measurements. Here we present a new method for simple, robust, accurate, simultaneous measurement of the rotation speed of a laser trapped birefringent particle, and the optical torque acting on it, by measuring the change in angular momentum of the light from passing through the particle. This method does not depend on the size or shape of the particle or the laser beam geometry, nor does it depend on the properties of the surrounding medium. This could allow accurate measurement of viscosity on a microscopic scale.
physics.optics:Optical tweezers are widely used for the manipulation of cells and their internal structures. However, the degree of manipulation possible is limited by poor control over the orientation of trapped cells. We show that it is possible to controllably align or rotate disc shaped cells - chloroplasts of Spinacia oleracea - in a plane polarised Gaussian beam trap, using optical torques resulting predominantly from circular polarisation induced in the transmitted beam by the non-spherical shape of the cells.
physics.optics:Optical trapping is a widely used technique, with many important applications in biology and metrology. Complete modelling of trapping requires calculation of optical forces, primarily a scattering problem, and non-optical forces. The T-matrix method is used to calculate forces acting on spheroidal and cylindrical particles.
physics.optics:Optical trapping, where microscopic particles are trapped and manipulated by light is a powerful and widespread technique, with the single-beam gradient trap (also known as optical tweezers) in use for a large number of biological and other applications.   The forces and torques acting on a trapped particle result from the transfer of momentum and angular momentum from the trapping beam to the particle.   Despite the apparent simplicity of a laser trap, with a single particle in a single beam, exact calculation of the optical forces and torques acting on particles is difficult. Calculations can be performed using approximate methods, but are only applicable within their ranges of validity, such as for particles much larger than, or much smaller than, the trapping wavelength, and for spherical isotropic particles.   This leaves unfortunate gaps, since wavelength-scale particles are of great practical interest because they are readily and strongly trapped and are used to probe interesting microscopic and macroscopic phenomena, and non-spherical or anisotropic particles, biological, crystalline, or other, due to their frequent occurance in nature, and the possibility of rotating such objects or controlling or sensing their orientation.   The systematic application of electromagnetic scattering theory can provide a general theory of laser trapping, and render results missing from existing theory. We present here calculations of force and torque on a trapped particle obtained from this theory and discuss the possible applications, including the optical measurement of the force and torque.
physics.optics:Multipole expansion of an incident radiation field - that is, representation of the fields as sums of vector spherical wavefunctions - is essential for theoretical light scattering methods such as the T-matrix method and generalised Lorenz-Mie theory (GLMT). In general, it is theoretically straightforward to find a vector spherical wavefunction representation of an arbitrary radiation field. For example, a simple formula results in the useful case of an incident plane wave. Laser beams present some difficulties. These problems are not a result of any deficiency in the basic process of spherical wavefunction expansion, but are due to the fact that laser beams, in their standard representations, are not radiation fields, but only approximations of radiation fields. This results from the standard laser beam representations being solutions to the paraxial scalar wave equation. We present an efficient method for determining the multipole representation of an arbitrary focussed beam.
physics.optics:The T-matrix method is widely used for the calculation of scattering by particles of sizes on the order of the illuminating wavelength. Although the extended boundary condition method (EBCM) is the most commonly used technique for calculating the T-matrix, a variety of methods can be used.   We consider some general principles of calculating T-matrices, and apply the point-matching method to calculate the T-matrix for particles devoid of symmetry. This method avoids the time-consuming surface integrals required by the EBCM.
physics.optics:Light-induced rotation of absorbing microscopic particles by transfer of angular momentum from light to the material raises the possibility of optically driven micromachines. The phenomenon has been observed using elliptically polarized laser beams or beams with helical phase structure. But it is difficult to develop high power in such experiments because of overheating and unwanted axial forces, limiting the achievable rotation rates to a few hertz. This problem can in principle be overcome by using transparent particles, transferring angular momentum by a mechanism first observed by Beth in 1936, when he reported a tiny torque developed in a quartz waveplate due to the change in polarization of transmitted light. Here we show that an optical torque can be induced on microscopic birefringent particles of calcite held by optical tweezers. Depending on the polarization of the incident beam, the particles either become aligned with the plane of polarization (and thus can be rotated through specified angles) or spin with constant rotation frequency. Because these microscopic particles are transparent, they can be held in three-dimensional optical traps at very high power without heating. We have observed rotation rates in excess of 350 Hz.
physics.optics:This report contains a tutorial introduction to the method of importance sampling. The use of this method is illustrated for simulations of the noise-induced energy jitter of return-to-zero pulses in optical communication systems.
physics.optics:The 3-dimensional coherence matrix is interpreted by emphasising its invariance with respect to spatial rotations. Under these transformations, it naturally decomposes into a real symmetric positive definite matrix, interpreted as the moment of inertia of the ensemble (and the corresponding ellipsoid), and a real axial vector, corresponding to the mean angular momentum of the ensemble. This vector and tensor are related by several inequalities, and the interpretation is compared to those in which unitary invariants of the coherence matrix are studied.
physics.optics:We propose novel multi-phase-matched process that starts with generation of a pair of symmetric second-harmonic waves. Each of them interacts again with the fundamental wave to produce two constructively interfering third harmonic waves collinear to the fundamental input wave.
physics.optics:We numerically calculate the equivalent mode-field radius of the fundamental mode in a photonic crystal fiber (PCF) and show that this is a function of the V-parameter only and not the relative hole size. This dependency is similar to what is found for graded-index standard fibers and we furthermore show that the relation for the PCF can be excellently approximated with the same general mathematical expression. This is to our knowledge the first semi-analytical description of the mode-field radius of a PCF.
physics.optics:The output spectrum of both gas and semiconductor lasers usually contains more than one frequency. Multimode operation in gas versus semiconductor lasers arises from different physics. In gas lasers, slow equilibration of the electron populations at different energies makes each frequency an independent single-mode laser. The slow electron diffusion in semiconductor lasers, combined with the spatially varying optical intensity patterns of the modes, makes each region of space an independent single-mode laser. We develop a rate equation model for the photon number in each mode which captures all these effects. Plotting the photon number versus pumping rate for the competing modes, in both subthreshold and above threshold operation, illustrates the changes in the laser output spectrum due to either slow equilibration or slow diffusion of electrons.
physics.optics:A cascaded iterative Fourier transform (CIFT) algorithm is presented for optical security applications. Two phase-masks are designed and located in the input and the Fourier domains of a 4-f correlator respectively, in order to implement the optical encryption or authenticity verification. Compared with previous methods, the proposed algorithm employs an improved searching strategy: modifying the phase-distributions of both masks synchronously as well as enlarging the searching space. Computer simulations show that the algorithm results in much faster convergence and better image quality for the recovered image. Each of these masks is assigned to different person. Therefore, the decrypted image can be obtained only when all these masks are under authorization. This key-assignment strategy may reduce the risk of being intruded.
physics.optics:Generic wave dislocations (phase singularities, optical vortices) in three dimensions have anisotropic local structure, which is analysed, with emphasis on the twist of surfaces of equal phase along the singular line, and the rotation of the local anisotropy ellipse (twirl). Various measures of twist and twirl are compared in specific examples, and a theorem is found relating the (quantised) topological twist and twirl for a closed dislocation loop with the anisotropy C line index threading the loop.
physics.optics:Diffraction is a fundamental property of light propagation. Owing to this phenomenon,light diffracts out in all directions when it passes through a subwavelength slit.This imposes a fundamental limit on the transverse size of a light beam at a given distance from the aperture. We show that a subwavelength-sized beam propagating without diffractive broadening can be produced in free space by the constructive interference of multiple beams of a Fresnel source of the respective high-refraction-index waveguide. Moreover, it is shown that such a source can be constructed not only for continuous waves, but also for ultra-short (near single-cycle) pulses. The results theoretically demonstrate the feasibility of completely diffraction-free subwavelength-beam optics, for both continuous waves and ultra-short pulses. The approach extends operation of the near-field subwavelength-beam optics, such as near-field scanning optical microscopy and spectroscopy,to the "not-too-distant" field regime (0.5 to about 10 wavelengths).
physics.optics:In recent years the topic of localized wave solutions of the homogeneous scalar wave equation, i.e., the wave fields that propagate without any appreciable spread or drop in intensity, has been discussed in many aspects in numerous publications. In this review the main results of this rather disperse theoretical material are presented in a single mathematical representation - the Fourier decomposition by means of angular spectrum of plane waves. This unified description is shown to lead to a transparent physical understanding of the phenomenon as such and yield the means of optical generation of such wave fields.
physics.optics:In recent experiments, localized and stationary pulses have been generated in second-order nonlinear processes with femtosecond pulses, whose asymptotic features relate with those of nondiffracting and nondispersing polychromatic Bessel beams in linear dispersive media. We investigate on the nature of these linear waves, and show that they can be identified with the X-shaped (O-shaped) modes of the hyperbolic (elliptic) wave equation in media with normal (anomalous) dispersion. Depending on the relative strengths of mode phase mismatch, group velocity mismatch with respect to a plane pulse, and of the defeated group velocity dispersion, these modes can adopt the form of pulsed Bessel beams, focus wave modes, and X-waves (O-waves), respectively.
physics.optics:We explain the main concepts centered around Sharafutdinov's ray transform, its kernel, and the extent to which it can be inverted. It is shown how the ray transform emerges naturally in any attempt to reconstruct optical and stress tensors within a photoelastic medium from measurements on the state of polarization of light beams passing through the strained medium. The problem of reconstruction of stress tensors is crucially related to the fact that the ray transform has a nontrivial kernel; the latter is described by a theorem for which we provide a new proof which is simpler and shorter as in Sharafutdinov's original work, as we limit our scope to tensors which are relevant to Photoelasticity. We explain how the kernel of the ray transform is related to the decomposition of tensor fields into longitudinal and transverse components. The merits of the ray transform as a tool for tensor reconstruction are studied by walking through an explicit example of reconstructing the $\sigma_{33}$-component of the stress tensor in a cylindrical photoelastic specimen. In order to make the paper self-contained we provide a derivation of the basic equations of Integrated Photoelasticity which describe how the presence of stress within a photoelastic medium influences the passage of polarized light through the material.
physics.optics:The controversial term "nondiffracting beam" was introduced into optics by Durnin in 1987. Discussions related to that term revived interest in problems of the light diffraction and resulted in an appearance of the new research direction of the classical optics dealing with the localized transfer of electromagnetic energy. In the paper, the physical concept of the nondiffracting propagation is presented and the basic properties of the nondiffracting beams are reviewed. Attention is also focused to the experimental realization and to applications of the nondiffracting beams.
physics.optics:Theory of the optical parametric amplification at high-frequency pumping in crystals with a regular space modulation of the sign of nonlinear coupling coefficient of interacting waves is developed. By applying the matrix method, the theory is based on a step-by-step approach. It is shown that, in the case where the pumping intensity is less than some critical value, the spatial dynamics of the signal intensity inside a separate layer with the constant nonlinear coefficient has an oscillatory behavior and the change of the signal intensity from layer to layer is defined, in general, by the power function. The same law is valid for the change of variance of signal's quadrature components. At large number of layers, these dependences can be reduced to the well-known ones for homogeneous nonlinear optical crystals.
physics.optics:We show how it is possible to controllably rotate or align microscopic particles of isotropic nonabsorbing material in a TEM00 Gaussian beam trap, with simultaneous measurement of the applied torque using purely optical means. This is a simple and general method of rotation, requiring only that the particle is elongated along one direction. Thus, this method can be used to rotate or align a wide range of naturally occurring particles. The ability to measure the applied torque enables the use of this method as a quantitative tool--the rotational equivalent of optical tweezers based force measurement. As well as being of particular value for the rotation of biological specimens, this method is also suitable for the development of optically-driven micromachines.
physics.optics:The near-field diffraction of fs and sub-fs light pulses by nm-size slit-type apertures and its implication for near-field scanning optical microscopy (NSOM) is analyzed. The amplitude distributions of the diffracted wave-packets having the central wavelengths in the visible spectral region are found by using the Neerhoff and Mur coupled integral equations, which are solved numerically for each Fourier's component of the wave-packet. In the case of fs pulses, the duration and transverse dimensions of the diffracted pulse remain practically the same as that of the input pulse. This demonstrates feasibility of the NSOM in which a fs pulse is used to provide the fs temporal resolution together with nm-scale spatial resolution. In the sub-fs domain, the Fourier spectrum of the transmitted pulse experiences a considerable narrowing that leads to the increase of the pulse duration in a few times. This imposes a limit on the simultaneous resolutions in time and space.
physics.optics:We show theoretically and demonstrate experimentally that highly absorbing particles can be trapped and manipulated in a single highly focused Gaussian beam. Our studies of the effects of polarized light on such particles show that they can be set into rotation by elliptically polarized light and that both the sense and the speed of their rotation can be smoothly controlled.
physics.optics:Wolf discovered how the spatial coherence characteristics of the source affect the spectrum of the radiation in the far zone. In particular the spatial coherence of the source can result either in red or blue shifts in the measured spectrum.His predictions have been verified in a large number of different classes of systems. Wolf and coworkers usually assume a given form of source correlations and study its consequence. In this paper we consider microscopic origin of spatial coherence and radiation from a system of atoms. We discuss how the radiation is different from that produced from an independent system of atoms. We show that the process of radiation itself is responsible for the creation of spatial correlations within the source. We present different features of the spectrum and other statistical properties of the radiation, which show strong dependence on the spatial correlations. We show the existence of a new type of two-photon resonance that arises as a result of such spatial correlations. We further show how the spatial coherence of the field can be used in the context of radiation generated by nonlinear optical processes. We conclude by demonstrating the universality of Wolf shifts and its application in the context of pulse propagation in a dispersive medium.
physics.optics:We experimentally demonstrate for the first time that a radially polarized field can be focussed to a spot size significantly smaller (0.16(1) lambda^2) than for linear polarization (0.26 lambda^2). The effect of the vector properties of light is shown by a comparison of the focal intensity distribution for radially and azimuthally polarized input fields. For strong focusing a radially polarized field leads to a longitudinal electric field component at the focus which is sharp and centered at the optical axis. The relative contribution of this component is enhanced by using an annular aperture.
physics.optics:Two strong simultaneous resonances of scattering--double-resonant extremely asymmetrical scattering (DEAS)--are predicted in two parallel, oblique, periodic Bragg arrays separated by a gap, when the scattered wave propagates parallel to the arrays. One of these resonances is with respect to frequency (which is common to all types of Bragg scattering), and another is with respect to phase variation between the arrays. The diffractional divergence of the scattered wave is shown to be the main physical reason for DEAS in the considered structure. Although the arrays are separated, they are shown to interact by means of the diffractional divergence of the scattered wave across the gap from one array into the other. It is also shown that increasing separation between the two arrays results in a broader and weaker resonance with respect to phase shift. The analysis is based on a recently developed new approach allowing for the diffractional divergence of the scattered wave inside and outside the arrays. Physical interpretations of the predicted features of DEAS in separated arrays are also presented. Applicability conditions for the developed theory are derived.
physics.optics:Radiation pressure forces in a focussed laser beam can be used to trap microscopic absorbing particles against a substrate. Calculations based on momentum transfer considerations show that stable trapping occurs before the beam waist, and that trapping is more effective with doughnut beams. Such doughnut beams can transfer angular momentum leading to rotation of the trapped particles. Energy is also transferred, which can result in heating of the particles to temperatures above the boiling point of the surrounding medium.
physics.optics:Extremely asymmetrical scattering (EAS) is a highly resonant type of Bragg scattering with a strong resonant increase of the scattered wave amplitude inside and outside the grating. EAS is realized when the scattered wave propagates parallel to the grating boundaries. We present a rigorous algorithm for the analysis of non-steady-state EAS, and investigate the relaxation of the incident and scattered wave amplitudes to their steady-state values. Non-steady-state EAS of bulk TE electromagnetic waves is analyzed in narrow and wide, slanted, holographic gratings. Typical relaxation times are determined and compared with previous rough estimations. Physical explanation of the predicted effects is presented.
physics.optics:We demonstrate generation and frequency doubling of unit charge vortices in a linear astigmatic resonator. Topological instability of the double charge harmonic vortices leads to well separated vortex cores that are shown to rotate, and become anisotropic, as the resonator is tuned across resonance.
physics.optics:We have developed a new method based on two cavities containing $\chi^{(2)}$ media to reshape optical pulses by an all-optical technique. The system is entirely passive \emph{i.e.}, all the energy is brought by the incoming pulse and uses two successive optical cavities with independent thresholds. The output pulse is close to a rectangular shape. We show that this technique could be extended to high bit rates and telecommunication wavelength using very small cavities containing current nonlinear materials.
physics.optics:We show that useful non-instantaneous nonlinear phase shifts can be obtained from cascaded quadratic processes in the presence of group velocity mismatch. The two-field nature of the process permits responses that can be effectively advanced or retarded in time with respect to one of the fields. There is an analogy to a generalized Raman-scattering effect, permitting both red and blue shifts of short pulses. We expect this capability to have many applications in short-pulse generation and propagation, such as the compensation of Raman-induced effects and high-quality pulse compression, which we discuss.
physics.optics:A method of formation of the tightly confined distortion-free fs pulses with the step-like decreasing of intensity under the finite-length propagation in free space is described. Such pulses are formed by the Fresnel source of a high refraction-index waveguide. The source reproduces in free space a propagation-invariant (distortion-free) pulse confined by the waveguide. Converse to the case of material waveguides, when the pulse goes out from the Fresnel (virtual) waveguide its shape is not changed, but the intensity immediately drops down to the near-zero level.
physics.optics:Based on a recent formulation of the V-parameter of a photonic crystal fiber we provide numerically based empirical expressions for this quantity only dependent on the two structural parameters - the air hole diameter and the hole-to-hole center spacing. Based on the unique relation between the V-parameter and the equivalent mode field radius we identify how the parameter space for these fibers is restricted in order for the fibers to remain single mode while still having a guided mode confined to the core region.
physics.optics:The alternative to dynamic alignment explanation of experimental results on spatial-asymmetric dissociation of molecules in a laser field is proposed. The concept of geometrical alignment is sufficient for explanation of these results. In this case the spatial anisotropy of interaction of molecules with laser radiation is transferred from one field to another through the ordinary mechanism of nonlinear optical interactions. Thus laser radiation does not create alignment, but only registers it. A physical basis of such nonlinear processes is inequality of forward and reversed optical transitions that corresponds to a concept of time invariance violation in electromagnetic interactions. Directions of the further researches in the field of alignment spectroscopy are discussed.
physics.optics:Periodic layered media can reflect strongly for all incident angles and polarizations in a given frequency range. Quarter-wave stacks at normal incidence are commonplace in the design of such omnidirectional reflectors. We discuss alternative design criteria to optimize these systems.
physics.optics:We study propagation of a pair of oppositely charged and mutually incoherent vortices in anisotropic nonlinear optical media. Mutual interactions retard the delocalization of the vortex core observed for isolated vortices.
physics.optics:The efficiency of evanescent coupling between a silica optical fiber taper and a silicon photonic crystal waveguide is studied. A high reflectivity mirror on the end of the photonic crystal waveguide is used to recollect, in the backwards propagating fiber mode, the optical power that is initially coupled into the photonic crystal waveguide. An outcoupled power in the backward propagating fiber mode of 88% of the input power is measured, corresponding to a lower bound on the coupler efficiency of 94%.
physics.optics:The reversible phase transition induced by femtosecond laser excitation of Gallium has been studied by measuring the dielectric function at 775 nm with ~ 200 fs temporal resolution. The real and imaginary parts of the transient dielectric function were calculated from absolute reflectivity of Gallium layer measured at two different angles of incidence, using Fresnel formulas. The time-dependent electron-phonon effective collision frequency, the heat conduction coefficient and the volume fraction of a new phase were restored directly from the experimental data, and the time and space dependent electron and lattice temperatures in the layer undergoing phase transition were reconstructed without ad hoc assumptions. We converted the temporal dependence of the electron-phonon collision rate into the temperature dependence, and demonstrated, for the first time, that the electron-phonon collision rate has a non-linear character. This temperature dependence converges into the known equilibrium function during the cooling stage. The maximum fraction of a new phase in the laser-excited Gallium layer reached only 60% even when the deposited energy was two times the equilibrium enthalpy of melting. We have also demonstrated that the phase transition pace and a fraction of the transformed material depended strongly on the thickness of the laser-excited Gallium layer, which was of the order of several tens of nanometers for the whole range of the pump laser fluencies up to the damage threshold. The kinetics of the phase transformation after the laser excitation can be understood on the basis of the classical theory of the first-order phase transition while the duration of non-thermal stage appears to be comparable to the sub-picosecond pulse length.
physics.optics:The derivation of a new condition for characterizing isotropic dielectric-magnetic materials exhibiting negative phase velocity, and the equivalence of that condition with previously derived conditions, are presented.
physics.optics:Linear and nonlinear directional couplers are currently used in fiber optics communications. They may also play a role in multiphoton approaches to quantum information processing if accurate control is obtained over the phases and polarizations of the signals at the output of the coupler. With this motivation, the constants of motion of the coupler equation are used to obtain an explicit analytical solution for the nonlinear coupler.
physics.optics:A single-mode all-silica photonic crystal fiber with an effective area of 600 square-micron and low bending loss is demonstrated. The fiber is characterized in terms of attenuation, chromatic dispersion and modal properties.
physics.optics:Based on the Wigner distribution approach, an analysis of the effect of partial incoherence on the transverse instability of soliton structures in nonlinear Kerr media is presented. It is explicitly shown, that for a Lorentzian incoherence spectrum the partial incoherence gives rise to a damping which counteracts, and tends to suppress, the transverse instability growth. However, the general picture is more complicated and it is shown that the effect of the partial incoherence depends crucially on the form of the incoherence spectrum. In fact, for spectra with finite rms-width, the partial incoherence may even increase both the growth rate and the range of unstable, transverse wave numbers.
physics.optics:We study theoretically and experimentally the modulational instability of broad optical beams in photorefractive nonlinear media. We demonstrate the impact of the anisotropy of the nonlinearity on the growth rate of periodic perturbations. Our findings are confirmed by experimental measurements in a strontium barium niobate photorefractive crystal.
physics.optics:Coherent Anti-Stokes Raman Scattering (CARS) processes are ``coherent,'' but the phase of the anti-Stokes radiation is usually lost by most incoherent spectroscopic CARS measurements. We propose a novel Raman microscopy imaging method called Nonlinear Interferometric Vibrational Imaging, which measures Raman spectra by obtaining the temporal anti-Stokes signal through nonlinear interferometry. With a more complete knowledge of the anti-Stokes signal, we show through simulations that a high-resolution Raman spectrum can be obtained of a molecule in a single pulse using broadband radiation. This could be useful for identifying the three-dimensional spatial distribution of molecular species in tissue.
physics.optics:An examination of the propagation of intense 200 fs pulses in water reveals light filaments not sustained by the balance between Kerr-induced self-focusing and plasma-induced defocusing. Their appearance is interpreted as the consequence of a spontaneous reshaping of the wave packet form a gaussian into a conical wave, driven by the requirement of maximum localization, minimum losses and stationarity in the presence of non-linear absorption.
physics.optics:In positive phase-mismatched SHG and normal dispersion, a gaussian spatio-temporal pulse transforms spontaneously into a X-pulse, underlies spatio-temporal compression and eventually leads to stationary 3-D propagation. Experimental and numerical data are provided
physics.optics:It is shown that a system of two coupled planar material sheets possessing surface mode (polariton) resonances can be used for the purpose of evanescent field restoration and, thus, for the sub-wavelength near-field imaging. The sheets are placed in free space so that they are parallel and separated by a certain distance. Due to interaction of the resonating surface modes (polaritons) of the sheets an exponential growth in the amplitude of an evanescent plane wave coming through the system can be achieved. This effect was predicted earlier for backward-wave (double-negative or Veselago) slab lenses. The alternative system considered here is proved to be realizable at microwaves by grids or arrays of resonant particles. The necessary electromagnetic properties of the resonating grids and the particles are investigated and established. Theoretical results are supported by microwave experiments that demonstrate amplification of evanescent modes.
physics.optics:Motivated by recent experimental work by Folkenberg et al. we consider the effect of weak disorder in the air-hole lattice of small-core photonic crystal fibers. We find that the broken symmetry leads to higher-order modes which have generic intensity distributions resembling those found in standard fibers with elliptical cores. This explains why recently reported experimental higher-order mode profiles appear very different from those calculated numerically for ideal photonic crystal fibers with inversion and six-fold rotational symmetry. The splitting of the four higher-order modes into two groups fully correlates with the observation that these modes have different cut-offs.
physics.optics:We present a numerical investigation of the ray dynamics in a paraxial optical cavity when a ray splitting mechanism is present. The cavity is a conventional two-mirror stable resonator and the ray splitting is achieved by inserting an optical beam splitter perpendicular to the cavity axis. We show that depending on the position of the beam splitter the optical resonator can become unstable and the ray dynamics displays a positive Lyapunov exponent.
physics.optics:This paper analyses theoretically and numerically the effect of varying grating amplitude on the extremely asymmetrical scattering (EAS) of bulk and guided optical modes in non-uniform strip-like periodic Bragg arrays with stepwise and gradual variations in the grating amplitude across the array. A recently developed new approach based on allowance for the diffractional divergence of the scattered wave is used for this analysis. It is demonstrated that gradual variations in magnitude of the grating amplitude may change the pattern of EAS noticeably but not radically. On the other hand, phase variations in the grating may result in a radically new type of Bragg scattering - double-resonant EAS (DEAS). In this case, a combination of two strong simultaneous resonances (one with respect to frequency, and another with respect to the phase variation) is predicted to take place in non-uniform arrays with a step-like phase and gradual magnitude variations of the grating amplitude. The tolerances of EAS and DEAS to small gradual variations in the grating amplitude are determined. The main features of these types of scattering in non-uniform arrays are explained by the diffractional divergence of the scattered wave inside and outside the array.
physics.optics:We present a symmetry-based theory of the depolarization induced by subwavelength metal hole arrays. We derive the Mueller matrices of hole arrays with various symmetries (in particular square and hexagonal) when illuminated by a finite-diameter (e.g. gaussian) beam. The depolarization is due to a combination of two factors: (i) propagation of surface plasmons along the surface of the array, (ii) a spread of wave vectors in the incident beam.
physics.optics:We investigate the ray dynamics in an optical cavity when a ray splitting mechanism is present. The cavity is a conventional two-mirror stable resonator and the ray splitting is achieved by inserting an optical beam splitter perpendicular to the cavity axis. Using Hamiltonian optics, we show that such a simple device presents a surprisingly rich chaotic ray dynamics.
physics.optics:We concentrate on the forces and torques exerted on transparent and absorbing particles trapped in laser beams containing optical vortices. We review previous theoretical and experimental work and then present new calculations of the effect of vortex beams on absorbing particles.
physics.optics:We have monitored the space-time transformation of 150-fs pulse, undergoing self-focusing and filamentation in water, by means of the nonlinear gating tech- nique. We have observed that pulse splitting and subsequent recombination apply to axial temporal intensity only, whereas space-integrated pulse profile preserves its original shape.
physics.optics:It is shown that three-dimensional nonparaxial beams are described by the oblate spheroidal exact solutions of the Helmholtz equation. For the first time, their beam behaviour is investigated and their corresponding parameters are defined. Using the fact that the beam width of the family of paraxial Gaussian beams is described by an hyperbola, the connection between the physical parameters of nonparaxial spheroidal beam solutions and those of paraxial beams is formally stablished. These results are also helpful to investigate the exact vector nonparaxial beams.
physics.optics:The images of the silicon test object has been carried out. An image for in-line hard X-ray hologram is presented. The transmission of a hologram image for hard X-ray radiation using Fresnel phase zone plate has been investigated.
physics.optics:We develop a theory of light transmission through an aperture-type near-field optical probe with a dissipative matter in its semiconducting core described by a complex frequency-dependent dielectric function. We evaluate the near-field transmission coefficient of a metallized silicon probe with a large taper angle of in the visible and near-infrared wavelength range. It is shown that in this spectral range the use of a short silicon probe instead of a glass one allows to achieve a strong (up to 10$^2-10^{3}$) enhancement in the transmission efficiency.
physics.optics:A metal nanoparticle plasmon waveguide for electromagnetic energy transport utilizing dispersion engineering to dramatically increase lateral energy confinement via a two-dimensional pattern of Au dots on an optically thin Si membrane is described. Using finite-difference time-domain simulations and coupled-mode theory, we show that phase-matched evanescent excitation from conventional fiber tapers is possible with efficiencies > 90 % for realistic geometries. Energy loss in this waveguide is mainly due to material absorption, allowing for 1/e energy decay distances of about 2 mm for excitation at telecommunication frequencies. This concept can be extended to the visible regime and promises applications in optical energy guiding, optical sensing, and switching.
physics.optics:A two-dimensional photonic crystal microcavity design supporting a wavelength-scale volume resonant mode with a calculated quality factor (Q) insensitive to deviations in the cavity geometry at the level of Q~2x10^4 is presented. The robustness of the cavity design is confirmed by optical fiber-based measurements of passive cavities fabricated in silicon. For microcavities operating in the lambda = 1500 nm wavelength band, quality factors between 1.3-4.0x10^4 are measured for significant variations in cavity geometry and for resonant mode normalized frequencies shifted by as much as 10% of the nominal value.
physics.optics:e investigate both experimentally and theoretically the waveguiding properties of a novel double trench waveguide where a conventional single-mode strip waveguide is embedded in a two dimensional photonic crystal (PhC) slab formed in silicon on insulator (SOI) wafers. We demonstrate that the bandwidth for relatively low-loss (50dB/cm) waveguiding is significantly expanded to 250nm covering almost all the photonic band gap owing to nearly linear dispersion of the TE-like waveguiding mode. The flat transmission spectrum however is interrupted by numerous narrow stop bands. We found that these stop bands can be attributed to anti-crossing between TE-like (positive parity) and TM-like (negative parity) modes. This effect is a direct result of the strong asymmetry of the waveguides that have an upper cladding of air and lower cladding of oxide. To our knowledge this is the first demonstration of the effects of cladding asymmetry on the transmission characteristics of the PhC slab waveguides.
physics.optics:In this paper, we present an approximate expression for determining the effective permittivity describing the coherent propagation of an electromagnetic wave in random media. Under the Quasicrystalline Coherent Potential Approximation (QC-CPA), it is known that multiple scattering theory provided an expression for this effective permittivity. The numerical evaluation of this one is, however, a challenging problem. To find a tractable expression, we add some new approximations to the (QC-CPA) approach. As a result, we obtained an expression for the effective permittivity which contained at the same time the Maxwell-Garnett formula in the low frequency limit, and the Keller formula, which has been recently proved to be in good agreement for particles exceeding the wavelength.
physics.optics:We investigate the electromagnetic propagation in two-dimensional photonic crystals, formed by parallel dielectric cylinders embedded a uniform medium. The frequency band structure is computed using the standard plane-wave expansion method, while the propagation and scattering of the electromagnetic waves are calculated by the multiple scattering theory. It is shown that within partial bandgaps, the waves tend to bend away from the forbidden directions. Such a property may render novel applications in manipulating optical flows. In addition, the relevance with the imaging by flat photonic crystal slabs will also be discussed.
physics.optics:This paper is the result of setting up GRENOUILLE in the Nonlinear Dynamics Laboratory at the University of Maryland at College Park. With the experience acquired in the process of setting up GRENOUILLE, this manual was compiled from literature and from hand-on experience to serve as a quick guide, a step-by-step help to construct GRENOUILLE and to understand some of its basic principles.
physics.optics:Accurate knowledge of absorption coefficient of a sample is a prerequisite for measuring the third order optical nonlinearity of materials, which can be a serious limitation for unknown samples. We introduce a method, which measures both the absorption coefficient and the third order optical nonlinearity of materials with high sensitivity in a single experimental arrangement. We use a dual-beam pump-probe experiment and conventional single-beam z-scan under different conditions to achieve this goal. We also demonstrate a counterintuitive coupling of the non-interacting probe-beam with the pump-beam in pump-probe z-scan experiment.
physics.optics:The concept of a plane scatterer that was developed earlier for scalar waves is generalized so that polarization of light is included. Starting from a Lippmann-Schwinger formalism for vector waves, we show that the Green function has to be regularized before T-matrices can be defined in a consistent way. After the regularization, optical modes and Green functions are determined exactly for finite structures built up of an arbitrary number of parallel planes, at arbitrary positions, and where each plane can have different optical properties. The model is applied to the special case of finite crystals consisting of regularly spaced identical planes, where analytical methods can be taken further and only light numerical tasks remain. The formalism is used to calculate position- and orientation-dependent spontaneous-emission rates inside and near the finite photonic crystals. The results show that emission rates and reflection properties can differ strongly for scalar and for vector waves. The finite size of the crystal influences the emission rates. For parallel dipoles close to a plane, emission into guided modes gives rise to a peak in the frequency-dependent emission rate.
physics.optics:Single-shot ultrafast absorbance spectroscopy based on the frequency encoding of the kinetics is analyzed theoretically and implemented experimentally. The kinetics are sampled in the frequency domain using linearly chirped, amplified 33 fs FWHM pulses derived from a Ti:sapphire laser. A variable length grating pair compressor is used to achieve the time resolution of 500-1000 channels per a 2-to-160 ps window with sensitivity > 5x10-4. In terms of the acquisition time, FDSS has an advantage over the pump-probe spectroscopy in a situation when the "noise" is dominated by amplitude variations of the signal, due to the pump and flow instabilities. The possibilities of FDSS are illustrated with the kinetics obtained in multiphoton ionization of water and aqueous iodide and one-photon excitation of polycrystalline ZnSe and thin-film amorphous Si:H. Unlike other "single-shot" techniques, FDSS can be implemented for fluid samples flowing in a high-speed jet and for thin solid samples that exhibit interference fringes; no a priori knowledge of the excitation profile of the pump across the beam is needed. Another advantage is that due to the interference of quasimonochromatic components of the chirped probe pulse, an oscillation pattern near the origin of the FDSS kinetics emerges. This pattern is unique and can be used to determine the complex dielectric function of the photogenerated species.
physics.optics:Single-shot ultrafast absorbance spectroscopy based on the frequency encoding of the kinetics is analyzed theoretically and implemented experimentally. In Part II of the series, arbitrary thickness sample is analysed theroretically. The model is then used to simulate the results for a-Si:H films.
physics.optics:We present a unifying point of view which allows to understand spectral features reported in recent experiments with two-dimensional arrays of subwavelength holes in metal films. We develop a Fano analysis of the related scattering problem by distinguishing two interfering contributions to the transmission process, namely a non-resonant contribution (direct scattering) and a resonant contribution (surface plasmon excitation). The introduction of a coupling strength between these two contributions naturally induces resonance shifts and asymmetry of profiles which satisfy simple scaling relations. We also report an experiment to confirm this analysis.
physics.optics:We study the dispersion and leakage properties for the recently reported low-loss photonic band-gap fiber by Smith et al. [Nature 424, 657 (2003)]. We find that surface modes have a significant impact on both the dispersion and leakage properties of the fundamental mode. Our dispersion results are in qualitative agreement with the dispersion profile reported recently by Ouzounov et al. [Science 301, 1702 (2003)] though our results suggest that the observed long-wavelength anomalous dispersion is due to an avoided crossing (with surface modes) rather than band-bending caused by the photonic band-gap boundary of the cladding.
physics.optics:Micron-scale optical cavities are produced using a combination of template sphere self-assembly and electrochemical growth. Transmission measurements of the tunable microcavities show sharp resonant modes with a Q-factor>300, and 25-fold local enhancement of light intensity. The presence of transverse optical modes confirms the lateral confinement of photons. Calculations show sub-micron mode volumes are feasible. The small mode volume of these microcavities promises to lead to a wide range of applications in microlasers, atom optics, quantum information, biophotonics and single molecule detection.
physics.optics:We experimentally compare the optical bandwidth of a conventional single-mode fiber (SMF) with 3 different photonic crystal fibers (PCF) all optimized for visible applications. The spectral attenuation, single-turn bend loss, and mode-field diameters (MFD) are measured and the PCF is found to have a significantly larger bandwidth than the SMF for an identical MFD. It is shown how this advantage can be utilized for realizing a larger MFD for the PCF while maintaining a bending resistant fiber.
physics.optics:We consider an air-silica honeycomb lattice and demonstrate a new approach to the formation of a core defect. Typically, a high or low-index core is formed by adding a high-index region or an additional air-hole (or other low-index material) to the lattice, but here we discuss how a core defect can be formed by manipulating the cladding region rather than the core region itself. Germanium-doping of the honeycomb lattice has recently been suggested for the formation of a photonic band-gap guiding silica-core and here we experimentally demonstrate how an index-guiding silica-core can be formed by fluorine-doping of the honeycomb lattice.
physics.optics:Self-similar propagation of ultrashort, parabolic pulses in a laser resonator is observed theoretically and experimentally. This constitutes a new type of pulse-shaping in modelocked lasers: in contrast to the well-known static (soliton-like) and breathing (dispersion-managed soliton) pulse evolutions, asymptotic solutions to the nonlinear wave equation that governs pulse propagation in most of the laser cavity are observed. Stable self-similar pulses exist with energies much greater than can be tolerated in soliton-like pulse shaping, and this will have implications for practical lasers.
physics.optics:The existence of resonant enhanced transmission and collimation of light waves by subwavelength slits in metal films [for example, see T.W. Ebbesen et al., Nature (London) 391, 667 (1998) and H.J. Lezec et al., Science, 297, 820 (2002)] leads to the basic question: Can a light be enhanced and simultaneously localized in space and time by a subwavelength slit? To address this question, the spatial distribution of the energy flux of an ultrashort (femtosecond) wave-packet diffracted by a subwavelength (nanometer-size) slit was analyzed by using the conventional approach based on the Neerhoff and Mur solution of Maxwell's equations. The results show that a light can be enhanced by orders of magnitude and simultaneously localized in the near-field diffraction zone at the nm- and fs-scales. Possible applications in nanophotonics are discussed.
physics.optics:We demonstrate an optical system that can apply and accurately measure the torque exerted by the trapping beam on a rotating birefringent probe particle. This allows the viscosity and surface effects within liquid media to be measured quantitatively on a micron-size scale using a trapped rotating spherical probe particle. We use the system to measure the viscosity inside a prototype cellular structure.
physics.optics:The core theorem on which the above paper is centred - that a perfectly conducting body of revolution absorbs no angular momentum from an axisymmetric electromagnetic wave field - is in fact a special case of a more general result in electromagnetic scattering theory. In addition, the scaling of the efficiency of transfer of angular momentum to an object with the wavelength and object size merits further discussion. Finally, some comments are made on the choice of terminology and the erroneous statement that a circularly polarized plane wave does not carry angular momentum.
physics.optics:The propagation of plane waves in a Faraday chiral medium is investigated. Conditions for the phase velocity to be directed opposite to the direction of power flow are derived for propagation in an arbitrary direction; simplified conditions which apply to propagation parallel to the distinguished axis are also established. These negative phase-velocity conditions are explored numerically using a representative Faraday chiral medium, arising from the homogenization of an isotropic chiral medium and a magnetically biased ferrite. It is demonstrated that the phase velocity may be directed opposite to power flow, provided that the gyrotropic parameter of the ferrite component medium is sufficiently large compared with the corresponding nongyrotropic permeability parameters.
physics.optics:Second harmonic optical coherence tomography, which uses coherence gating of second-order nonlinear optical response of biological tissues for imaging, is described and demonstrated. Femtosecond laser pulses were used to excite second harmonic waves from collagen harvested from rat tail tendon and a reference nonlinear crystal. Second harmonic interference fringe signals were detected and used for image construction. Because of the strong dependence of second harmonic generation on molecular and tissue structures, this technique offers contrast and resolution enhancement to conventional optical coherence tomography.
physics.optics:Considering the diffraction of a plane wave by a periodically corrugated half-space, we show that the transformation of the refracting medium from positive/negative phase-velocity to negative/positive phase-velocity type has an influence on the diffraction efficiencies. This effect increases with increasing corrugation depth, owing to the presence of evanescent waves in the troughs of the corrugated interface.
physics.optics:We have developed a scheme to measure the optical torque, exerted by a laser beam on a phase object, by measuring the orbital angular momentum of the transmitted beam. The experiment is a macroscopic simulation of a situation in optical tweezers, as orbital angular momentum has been widely used to apply torque to microscopic objects. A hologram designed to generate LG02 modes and a CCD camera are used to detect the orbital component of the beam. Experimental results agree with theoretical numerical calculations, and the strength of the orbital component suggest its usefulness in optical tweezers for micromanipulation.
physics.optics:We investigate optical parametric oscillations through four-wave mixing in resonant cavities and photonic crystals. The theoretical analysis underlines the relevant features of the phenomenon and the role of the density of states. Using fully vectorial 3D time-domain simulations, including both dispersion and nonlinear polarization, for the first time we address this process in a face centered cubic lattice and in a photonic crystal slab. The results lead the way to the development of novel parametric sources in isotropic media.
physics.optics:We present a study of orbital angular momentum transfer from pump to down-converted beams in a type-II Optical Parametric Oscillator. Cavity and anisotropy effects are investigated and demostrated to play a central role in the transverse mode dynamics. While the idler beam can oscillate in a Laguerre-Gauss mode, the crystal birefringence induces an astigmatic effect in the signal beam that prevents the resonance of such mode.
physics.optics:We introduce a prototype model for globally-coupled oscillators in which each element is given an oscillation frequency and a preferential oscillation direction (polarization), both randomly distributed. We found two collective transitions: to phase synchronization and to polarization ordering. Introducing a global-phase and a polarization order parameters, we show that the transition to global-phase synchrony is found when the coupling overcomes a critical value and that polarization order enhancement can not take place before global-phase synchrony. We develop a self-consistent theory to determine both order parameters in good agreement with numerical results.
physics.optics:We report on a polarization maintaining large mode area photonic crystal fiber. Unlike, previous work on polarization maintaining photonic crystal fibers, birefringence is introduced using stress applying parts. This has allowed us to realize fibers, which are both single mode at any wavelength and have a practically constant birefringence for any wavelength. The fibers presented in this work have mode field diameters from about 4 to 6.5 micron, and exhibit a typical birefringence of 1.5e-4.
physics.optics:Speden is a computer program that reconstructs the electron density of single particles from their x-ray diffraction patterns, using a single-particle adaptation of the Holographic Method in crystallography. (Szoke, A., Szoke, H., and Somoza, J.R., 1997. Acta Cryst. A53, 291-313.) The method, like its parent, is unique that it does not rely on ``back'' transformation from the diffraction pattern into real space and on interpolation within measured data. It is designed to deal successfully with sparse, irregular, incomplete and noisy data. It is also designed to use prior information for ensuring sensible results and for reliable convergence. This article describes the theoretical basis for the reconstruction algorithm, its implementation and quantitative results of tests on synthetic and experimentally obtained data. The program could be used for determining the structure of radiation tolerant samples and, eventually, of large biological molecular structures without the need for crystallization.
physics.optics:We detect the vortex evolution from the increase of the fractional phase step by interfering two beams of opposite but equal fractional step increment.The interference pattern generated shows evidence of the birth of an additional single extra charge as the fractional phase step increase extends above a half-integer value
physics.optics:We report on a single-mode photonic crystal fiber with attenuation and effective area at 1550 nm of 0.48 dB/km and 130 square-micron, respectively. This is, to our knowledge, the lowest loss reported for a PCF not made from VAD prepared silica and at the same time the largest effective area for a low-loss (< 1 dB/km) PCF. We briefly discuss the future applications of PCFs for data transmission and show for the first time, both numerically and experimentally, how the group velocity dispersion is related to the mode field diameter
physics.optics:We resort to the concept of turns to provide a geometrical representation of the action of any lossless multilayer, which can be considered as the analogous in the unit disk to the sliding vectors in Euclidean geometry. This construction clearly shows the peculiar effects arising in the composition of multilayers. A simple optical experiment revealing the appearance of the Wigner angle is analyzed in this framework.
physics.optics:We examine the Seidel aberrations of thin spherical lenses composed of media with refractive index not restricted to be positive. We find that consideration of this expanded parameter space allows reduction or elimination of more aberrations than is possible with only positive index media. In particular we find that spherical lenses possessing real aplanatic focal points are possible only with negative index. We perform ray tracing, using custom code that relies only on Maxwell's equations and conservation of energy, that confirms the results of the aberration calculations.
physics.optics:We study coupling and decoupling of parallel waveguides in two-dimensional square-lattice photonic crystals. We show that the waveguide coupling is prohibited at some wavelengths when there is an odd number of rows between the waveguides. In contrast, decoupling does not take place when there is even number of rows between the waveguides. Decoupling can be used to avoid cross talk between adjacent waveguides.
physics.optics:We report on an easy-to-evaluate expression for the prediction of the bend-loss for a large mode area photonic crystal fiber (PCF) with a triangular air-hole lattice. The expression is based on a recently proposed formulation of the V-parameter for a PCF and contains no free parameters. The validity of the expression is verified experimentally for varying fiber parameters as well as bend radius. The typical deviation between the position of the measured and the predicted bend loss edge is within measurement uncertainty.
physics.optics:We proposed and realized a two-dimensional (2D) photonic bandedge laser surrounded by the photonic bandgap. The heterogeneous photonic crystal structure consists of two triangular lattices of the same lattice constant with different air hole radii. The photonic crystal laser was realized by room-temperature optical pumping of air-bridge slabs of InGaAsP quantum wells emitting at 1.55 micrometer. The lasing mode was identified from its spectral positions and polarization directions. A low threshold incident pump power of 0.24mW was achieved. The measured characteristics of the photonic crystal lasers closely agree with the results of real space and Fourier space calculations based on the finite-difference time-domain method.
physics.optics:The effect of lattice termination on the surface states in a two-dimensional truncated photonic crystal slab is experimentally studied in a high index contrast silicon-on-insulator system. A single-mode silicon strip waveguide that is separated from the photonic crystal by a trench of variable width is used to evanescently couple to surface states in the surrounding lattice. It is demonstrated that the dispersion of the surface states depends strongly on the specific termination of the lattice.
physics.optics:A new phasing algorithm has been used to determine the phases of diffuse elastic X-ray scattering from a non-periodic array of gold balls of 50 nm diameter. Two-dimensional real-space images, showing the charge-density distribution of the balls, have been reconstructed at 50 nm resolution from transmission diffraction patterns recorded at 550 eV energy. The reconstructed image fits well with scanning electron microscope (SEM) image of the same sample. The algorithm, which uses only the density modification portion of the SIR2002 program, is compared with the results obtained via the Gerchberg-Saxton-Fienup HIO algorithm. In this way the relationship between density modification in crystallography and the HiO algorithm used in signal and image processing is elucidated.
physics.optics:Iterative projection algorithms for phase retrieval are tested on two simple toy models. The result provides useful insights in the behavior of these algorithms.
physics.optics:We demonstrate a technique for shaping current inputs for the direct modulation of a semiconductor laser for digital communication. The introduction of shaped current inputs allows for the suppression of relaxation oscillations and the avoidance of dynamical memory in the physical laser device, i.e., the output will not be influenced by previously communicated information. On the example of time-optimized bits, the possible performance enhancement for high data rate communications is shown numerically.
physics.optics:Phase-stabilized 12-fs, 1-nJ pulses from a commercial Ti:sapphire oscillator are directly amplified in a chirped-pulse optical parametric amplifier and recompressed to yield near-transform-limited 17.3-fs pulses. The amplification process is demonstrated to be phase preserving and leads to 85-uJ, carrier-envelope-offset phase-locked pulses at 1 kHz for 0.9 mJ of pump, corresponding to a single-pass gain of 8.5 x 10^4.
physics.optics:We have observed for the first time a new photonic quantum ring emission of anti-whispering gallery modes from a negative mesa-type toroid cavity due to semiconductor photonic corrals.
physics.optics:We predict that nonlinear left-handed metamaterials can support both TE- and TM-polarized self-trapped localized beams, spatial electromagnetic solitons. Such solitons appear as single- and multi-hump beams, being either symmetric or antisymmetric, and they can exist due to the hysteresis-type magnetic nonlinearity and the effective domains of negative magnetic permeability.
physics.optics:The use of one or more gold nanoballs as reference objects for Fourier Transform holography (FTH) is analysed using experimental soft X-ray diffraction from objects consisting of separated clusters of these balls. The holograms are deconvoluted against ball reference objects to invert to images, in combination with a Wiener filter to control noise. A resolution of ~30nm, smaller than one ball, is obtained even if a large cluster of balls is used as the reference, giving the best resolution yet obtained by X-ray FTH. Methods of dealing with missing data due to a beamstop are discussed. Practical prepared objects which satisfy the FTH condition are suggested, and methods of forming them described.
physics.optics:We study the transmission properties of a nonlinear periodic structure containing alternating slabs of a nonlinear right handed material and a linear left handed material. We find that the transmission associated with the zero averaged- refractive- index gap exhibits a bistable characteristic that is relatively insensitive to incident angle. This is in contrast to the nonlinear behavior of the usual Bragg gap
physics.optics:By use of an imaging spectrometer we map the far-field ($\theta-\lambda$) spectra of 200 fs optical pulses that have undergone beam collapse and filamentation in a Kerr medium. By studying the evolution of the spectra with increasing input power and using a model based on stationary linear asymptotic wave modes, we are able to trace a consistent model of optical beam collapse high-lighting the interplay between conical emission, multiple pulse splitting and other effects such as spatial chirp.
physics.optics:Periodic structures consisting of alternating layers of positive index and negative index materials possess a novel band gap at the frequency at which the average refractive index is zero. We show that in the presence of a Kerr nonlinearity, this zero-n gap can switch from low transmission to a perfectly transmitting state, forming a nonlinear resonance or gap soliton in the process. This zero-n gap soliton is omnidirectional in contrast to the usual Bragg gap soliton of positive index periodic structures
physics.optics:Experimental evidence of mode-selective evanescent power coupling at telecommunication frequencies with efficiencies up to 75 % from a tapered optical fiber to a carefully designed metal nanoparticle plasmon waveguide is presented. The waveguide consists of a two-dimensional square lattice of lithographically defined Au nanoparticles on an optically thin silicon membrane. The dispersion and attenuation properties of the waveguide are analyzed using the fiber taper. The high efficiency of power transfer into these waveguides solves the coupling problem between conventional optics and plasmonic devices and could lead to the development of highly efficient plasmonic sensors and optical switches.
physics.optics:In optical second harmonic generation with normal dispersion, the virtually infinite bandwidth of the unbounded, hyperbolic, modulational instability leads to quenching of spatial multi-soliton formation and to the occurrence of a catastrophic spatio-temporal break-up when an extended beam is let to interact with an extremely weak external noise with coherence time much shorter than that of the pump.
physics.optics:Interfering liquid surface waves are generated by electrically driven vertical oscillations of two or more equispaced pins immersed in a liquid (water). The corresponding intensity distribution, resulting from diffraction of monochromatic light by the reflection phase grating formed on the liquid surface, is calculated theoretically and found to tally with experiments. The curious features of the diffraction pattern and its relation to the interference of waves on the liquid surface are used to measure the amplitude and wavelength of the resultant surface wave along the line joining the two sources of oscillation. Finally, a sample diffraction pattern obtained by optically probing surface regions where interference produces a lattice--like structure is demonstrated and qualitatively explained.
physics.optics:We report polarization tomography experiments on metallic nanohole arrays with square and hexagonal symmetry. As a main result, we find that a fully polarized input beam is partly depolarized after transmission through a nanohole array. This loss of polarization coherence is found to be anisotropic, i.e. it depends on the polarization state of the input beam. The depolarization is ascribed to a combination of two factors: i) the nonlocal response of the array due to surface plasmon propagation, ii) the non-plane wave nature of a practical input beam.
physics.optics:Using analytical modeling and detailed numerical simulations, we investigate properties of hybrid systems of Photonic Crystal micro-cavities which incorporate a highly non-linear Ultra Slow Light medium. We demonstrate that such systems, while being miniature in size (order wavelength), and integrable, could enable ultra-fast non-linear all-optical switching at single photon energy levels.
physics.optics:We fabricated two dimensional photonic crystal structures in zinc oxide films with focused ion beam etching. Lasing is realized in the near ultraviolet frequency at room temperature under optical pumping. From the measurement of lasing frequency and spatial profile of the lasing modes, as well as the photonic band structure calculation, we conclude that lasing occurs in the strongly localized defect modes near the edges of photonic band gap. These defect modes originate from the structure disorder unintentionally introduced during the fabrication process.
physics.optics:One dimensional rectangular metallic gratings enable enhanced transmission of light for specific resonance frequencies. Two kinds of modes participating to enhanced transmission have already been demonstrated : (i) waveguide modes and (ii) surface plasmon polaritons (SPP). Since the original paper of Hessel and Oliner \cite{hessel} pointing out the existence of (i), no progress was made in their understanding. We present here a carefull analysis, and show that the coupling between the light and such resonances can be tremendously improved using an {\it evanescent} wave. This leads to enhanced localisation of light in cavities, yielding, in particular, to a very selective light transmission through these gratings.
physics.optics:We describe an experiment showing a spontaneous symmetry breaking phenomenon between the intensities of the ordinary and extraordinary components of the fundamental field in intracavity type-II harmonic generation. It is based on a triply resonant dual cavity containing a type II phase matched $\chi^{(2)}$ crystal pumped at the fundamental frequency $\omega_0$. The pump beam generates in the cavity a second harmonic mode at frequency $2\omega_0$ which acts as a pump for frequency degenerate type II parametric down conversion. Under operating conditions which are precisely symmetric with respect to the ordinary and extraordinary components of the fundamental wave, we have observed a breaking of the symmetry on the intensities of these two waves in agreement with the theoretical predictions.
physics.optics:Unavoidable variations in size and position of the building blocks of photonic crystals cause light scattering and extinction of coherent beams. We present a new model for both 2 and 3-dimensional photonic crystals that relates the extinction length to the magnitude of the variations. The predicted lengths agree well with our new experiments on high-quality opals and inverse opals, and with literature data analyzed by us. As a result, control over photons is limited to distances up to 50 lattice parameters ($\sim 15 \mu$m) in state-of-the-art structures, thereby impeding large-scale applications such as integrated circuits. Conversely, scattering in photonic crystals may lead to novel physics such as Anderson localization and non-classical diffusion.
physics.optics:A semi-analytical method evaluates the error probability of DPSK signals with intrachannel four-wave-mixing (IFWM) in a highly dispersive fiber link with strong pulse overlap. Depending on initial pulse width, the mean nonlinear phase shift of the system can be from 1 to 2 rad for signal-to-noise ratio (SNR) penalty less than 1 dB. An approximated empirical formula, valid for penalty less than 2 dB, uses the variance of the differential phase of the ghost pulses to estimate the penalty.
physics.optics:The major sources causing deterioration of optical quality in extremely large optical telescopes are misadjustments of the mirrors, deformations of monolithic mirrors, and misalignments of segments in segmented mirrors. For active optics corrections, all three errors, which can partially compensate each other, are measured simultaneously. It is therefore of interest to understand the similarities and differences between the three corresponding types of modes which describe these errors. The first two types are best represented by Zernike polynomials and elastic modes respectively, both of them being continuous and smooth functions. The segment misaligment modes, which are derived by singular value decomposition, are by their nature not smooth and in general discontinuous. However, for mirrors with a large number of segments, the lowest modes become effectively both smooth and continuous. This paper derives analytical expressions for these modes, using differential operators and their adjoints, for the limit case of infinitesimally small segments. For segmented mirrors with approximately 1000 segments, it is shown that these modes agree well with the corresponding lowest singular value decomposition modes. Furthermore, the analytical expressions reveal the nature of the segment misalignment modes and allow for a detailed comparison with the elastic modes of monolithic mirrors. Some mathematical features emerge as identical in the two cases.
physics.optics:We report on remote delivery of 25 pJ broadband near-infrared femtosecond light pulses from a Ti:sapphire laser through 150 meters of single-mode optical fiber. Pulse distortion due to dispersion is overcome with pre-compensation using adaptive pulse shaping techniques, while nonlinearities are mitigated using an SF10 rod for the final stage of pulse compression. Near transform limited pulse duration of 130 fs is measured after the final compression.
physics.optics:High refractive index contrast optical microdisk resonators fabricated from silicon-on-insulator wafers are studied using an external silica fiber taper waveguide as a wafer-scale optical probe. Measurements performed in the 1500 nm wavelength band show that these silicon microdisks can support whispering-gallery modes with quality factors as high as 5.2 x 10^5, limited by Rayleigh scattering from fabrication induced surface roughness. Microdisks with radii as small as 2.5 microns are studied, with measured quality factors as high as 4.7 x 10^5 for an optical mode volume of 5.3 cubic wavelengths in the material.
physics.optics:We develop and demonstrate two numerical methods for solving the class of open cavity problems which involve a curved, cylindrically symmetric conducting mirror facing a planar dielectric stack. Such dome-shaped cavities are useful due to their tight focusing of light onto the flat surface. The first method uses the Bessel wave basis. From this method evolves a two-basis method, which ultimately uses a multipole basis. Each method is developed for both the scalar field and the electromagnetic vector field and explicit ``end user'' formulas are given. All of these methods characterize the arbitrary dielectric stack mirror entirely by its 2\times2 transfer matrices for s- and p-polarization. We explain both theoretical and practical limitations to our method. Non-trivial demonstrations are given, including one of a stack-induced effect (the mixing of near-degenerate Laguerre-Gaussian modes) that may persist arbitrarily far into the paraxial limit. Cavities as large as 50 \lambda are treated, far exceeding any vectorial solutions previously reported.
physics.optics:We recently proposed two-dimensional coupled photonic crystal microcavity arrays as a route to achieve a slow-group velocity of light (flat band) in all crystal directions. In this paper we present the first experimental demonstration of such structures with a measured group velocity below 0.008c and discuss the feasibility of applications such as low-threshold photonic crystal lasers with increased output powers, optical delay components and sensors.
physics.optics:We present the design and fabrication of photonic crystal structures exhibiting electromagnetic bands that are flattened in all crystal directions, i.e., whose frequency variation with wavevector is minimized. Such bands can be used to reduce group velocity of light propagating in arbitrary crystal direction, which is of importance for construction of miniaturized tunable optical delay components, low-threshold photonic crystal lasers, and study of nonlinear optics phenomena.
physics.optics:By direct numerical simulations of the kinoform refractive lens within the quazioptical approach the effects of shape misalinement were investigated. The quazioptical approach was based on numerical integration of parabolic equation for complex wave amplitude by spep-by-step method with "splitting" procedure. The effect of 2pi- shift compensation was calculated for different orders and imperfections. The performance of kinoform lens was compared with the compound refractive lens and thin lens approximation.
physics.optics:We review a novel method for characterizing both the spectral and spatial properties of resonant cavities within two-dimensional photonic crystals (PCs). An optical fiber taper serves as an external waveguide probe whose micron-scale field is used to source and couple light from the cavity modes, which appear as resonant features in the taper's wavelength-dependent transmission spectrum when it is placed within the cavity's near field. Studying the linewidth and depth of these resonances as a function of the taper's position with respect to the resonator produces quantitative measurements of the quality factor Q and modal volume Veff of the resonant cavity modes. Polarization information about the cavity modes can be obtained by studying their depths of coupling when the cavity is probed along different axes by the taper. This fiber-based technique has been used to measure Q ~ 40,000 and Veff ~ 0.9 cubic wavelengths in a graded square lattice PC microcavity fabricated in silicon. The speed and versatility of this fiber-based probe is highlighted, and a discussion of its applicability to other wavelength-scale resonant elements is given.
physics.optics:Micro-domes based on a combination of metallic and dielectric multilayer mirrors are studied using a fully vectorial numerical basis-expansion method that accurately accounts for the effects of an arbitrary Bragg stack and can efficiently cover a large range of dome shapes and sizes. Results are examined from three different viewpoints: (i) the ray-optics limit, (ii) the (semi-) confocal limit for which exact wave solutions are known, and (iii) the paraxial approximation using vectorial Gaussian beams.
physics.optics:An overview is provided over the physics of dielectric microcavities with non-paraxial mode structure; examples are microdroplets and edge-emitting semiconductor microlasers. Particular attention is given to cavities in which two spatial degrees of freedom are coupled via the boundary geometry. This generally necessitates numerical computations to obtain the electromagnetic cavity fields, and hence intuitive understanding becomes difficult. However, as in paraxial optics, the ray picture shows explanatory and predictive strength that can guide the design of microcavities. To understand the ray-wave connection in such asymmetric resonant cavities, methods from chaotic dynamics are required.
physics.optics:We analyze theoretically the propagation of surface plasmon polaritons about a metallic corner with a finite bend radius, using a one-dimensional model analogous to the scattering from a finite-depth potential well. We obtain expressions for the energy reflection and transmission coefficients in the short wavelength limit, as well as an upper bound for the transmittance. In certain cases we find that propagation on non-planar interfaces may result in lower losses than on flat surfaces, contrary to expectation. In addition, we also find that the maximum transmittance depends non-monotonously on the bend radius, allowing increased transmission with decreasing radius.
physics.optics:This book chapter is the first part of a review of nanoporous materials for optical applications. Whereas the second part [J.Sauer, F. Marlow and F.Schueth, pp. 153-172 of the same volume] discusses material properties, this part gives a self-contained discussion of fluorescence and lasing in dielectric microresonators, with special emphasis on the hexagonal morphology found in molecular-sieve-dye compounds.
physics.optics:We address the formation and propagation of multi-spot soliton packets in saturable Kerr nonlinear media with an imprinted harmonic transverse modulation of the refractive index. We show that, in sharp contrast to homogeneous media where stable multi-peaked solitons do not exist, the photonic lattices support stable higher-order structures in the form of soliton packets, or soliton trains. Intuitively, such trains can be viewed as made of several lowest order solitons bound together with appropriate relative phases and their existence as stable objects puts forward the concept of compact manipulation of several solitons as a single entity.
physics.optics:The formation, deformation, and break-up of liquid interfaces are ubiquitous phenomena in nature. In the present article we discuss the deformation of a liquid interface produced by optical radiation forces. Usually, the bending of such an interface by the radiation pressure of a c.w. laser beam is weak. However, the effect can be enhanced significantly if one works with a near-critical phase-separated liquid mixture, whereby the surface tension becomes weak. The bending may in this way become as large as several tenths of micrometers, even with the use of only moderate laser power. This near-criticality is a key element in our experimental investigations as reviewed in the article. The effect is achieved by working with a micellar phase of microemulsions, at room temperature. We give a brief survey of the theory of electromagnetic forces on continuous matter, and survey earlier experiments in this area, such as the Ashkin-Dziedzic optical radiation force experiment on a water/air surface (1973), the Zhang-Chang experiment on the laser-induced deformation of a micrometer-sized spherical water droplet (1988), and the experiment of Sakai et al. measuring surface tensions of interfaces in a non-contact manner (2001). Thereafter, we survey results we obtained in recent years by performing experiments on near-critical interfaces, such as interface bending in the linear regime, stationary large deformations of liquid interfaces, asymmetric pressure effects on interfaces under intense illumination, nonlinear deformations, and laser-sustained liquid columns.
physics.optics:We introduce solitons supported by Bessel photonic lattices in cubic nonlinear media. We show that the cylindrical geometry of the lattice, with several concentric rings, affords unique soliton properties and dynamics. In particular, besides the lowest-order solitons trapped in the center of the lattice, we find soliton families trapped at different lattice rings. Such solitons can be set into controlled rotation inside each ring, thus featuring novel types of in-ring and inter-ring soliton interactions.
physics.optics:We present a novel general framework to deal with forward and backward components of the electromagnetic field in axially-invariant nonlinear optical systems, which include those having any type of linear or nonlinear transverse inhomogeneities. With a minimum amount of approximations, we obtain a system of two first-order equations for forward and backward components explicitly showing the nonlinear couplings among them. The modal approach used allows for an effective reduction of the dimensionality of the original problem from 3+1 (three spatial dimensions plus one time dimension) to 1+1 (one spatial dimension plus one frequency dimension). The new equations can be written in a spinor Dirac-like form, out of which conserved quantities can be calculated in an elegant manner. Finally, these new equations inherently incorporate spatio-temporal couplings, so that they can be easily particularized to deal with purely temporal or purely spatial effects. Nonlinear forward pulse propagation and non-paraxial evolution of spatial structures are analyzed as examples.
physics.optics:Metamaterials--artificially structured materials with tailored electromagnetic response--can be designed to have properties difficult to achieve with existing materials. Here we present a structured metamaterial, based on conducting split ring resonators (SRRs), which has an effective index-of-refraction with a constant spatial gradient. We experimentally confirm the gradient by measuring the deflection of a microwave beam by a planar slab of the composite metamaterial over a broad range of frequencies. The gradient index metamaterial represents an alternative approach to the development of gradient index lenses and similar optics that may be advantageous, especially at higher frequencies. In particular, the gradient index material we propose may be suited for terahertz applications, where the magnetic resonant response of SRRs has recently been demonstrated.
physics.optics:A recent advance in optical coherence tomography (OCT), termed swept-source OCT, is generalized into a new technique, Fourier-domain OCT. It represents a realization of a full-field OCT system in place of the conventional serial image acquisition in transverse directions typically implemented in "flying-spot" mode. To realize the full-field image acquisition, a Fourier holography system illuminated with a swept-source is employed instead of a Michelson interferometer commonly used in OCT. Fourier-domain OCT offers a new leap in signal-to-noise ratio improvement, as compared to flying-spot OCT systems. This paper presents experimental evidence that the signal-to-noise ratio of this new technique is indeed improved.
physics.optics:We show that a slab of meta-material (with $\epsilon=\mu=-1+i\Delta$) possesses a vortex-like surface wave with no ability to transport energy, whose nature is completely different from a localized mode or a standing wave. Through computations based on a rigorous time-dependent Green's function approach, we demonstrate that such a mode inevitably generates characteristic image oscillations in two dimensional focusing with even a monochromatic source, which were observed in many numerical simulations, but such oscillations are weak in three dimensional focusing.
physics.optics:The holographic imaging of rigid objects with diode lasers emitting in many wavelengths in a sillenite Bi12TiO20 photorefractive crystal is both theoretically and experimentally investigated. It is shown that, due to the multi-wavelength emission and the typically large free spectral range of this light source, contour fringes appear on the holographic image corresponding to the surface relief, even in single-exposure recordings. The influence of the number of emitted modes on the fringe width is analysed, and the possible applications of the contour fringes in the field of optical metrology are pointed out.
physics.optics:We develop a point-scattering approach to the plane-wave optical transmission of subwavelength metal hole arrays. We present a real space description instead of the more conventional reciprocal space description; this naturally produces interfering resonant features in the transmission spectra and makes explicit the tensorial properties of the transmission matrix. We give transmission spectra simulations for both square and hexagonal arrays; these can be evaluated at arbitrary angles and polarizations.
physics.optics:We derive phase-matching conditions for four-wave mixing between solitons and linear waves in optical fibres with arbitrary dispersion and demonstrate resonant excitation of new spectral components via this process.
physics.optics:We have experimentally studied polarization properties of the two-dimensional coupled photonic crystal microcavity arrays, and observed a strong polarization dependence of the transmission and reflection of light from the structures - the effects that can be employed in building miniaturized polarizing optical components. Moreover, by combining these properties with a strong sensitivity of the coupled bands on the surrounding refractive index, we have demonstrated a detection of small refractive index changes in the environment, which is useful for construction of bio-chemical sensors.
physics.optics:We demonstrate a new class of hollow-core Bragg fibers that are composed of concentric cylindrical silica rings separated by nanoscale support bridges. We theoretically predict and experimentally observe hollow-core confinement over an octave frequency range. The bandwidth of bandgap guiding in this new class of Bragg fibers exceeds that of other hollow-core fibers reported in the literature. With only three rings of silica cladding layers, these Bragg fibers achieve propagation loss of the order of 1 dB/m.
physics.optics:We analyze, by the finite-difference time-domain numerical methods, several ways to enhance the directional emission from photonic crystal waveguides through the beaming effect recently predicted by Moreno et al. [Phys. Rev. E 69, 121402(R) (2004)], by engineering the surface modes and corrugation of the photonic crystal surface. We demonstrate that the substantial enhancement of the light emission can be achieved by increasing the refractive index of the surface layer. We also measure power of surface modes and reflected power and confirm that the enhancement of the directional emission is related to the manipulation of the photonic crystal surface modes.
physics.optics:It is shown that the monochromatic optical wave propagating through the medium with linear birefringence in presence of a signal electromagnetic wave (whose wavelength is equal to the polarization beats length), displays Faraday rotation having the frequency of the signal wave and unsuppressed by linear birefringence. The effect is resonant with respect to the frequency of a signal wave. The "sharpness" of the resonance is defined by length of the birefringent medium.
physics.optics:Second harmonic generation in a two dimensional nonlinear quasi-crystal is demonstrated for the first time. Temperature and wavelength tuning of the crystal reveal the uniformity of the pattern while angle tuning reveals the dense nature of the crystal's Fourier spectrum. These results compare well with theoretical predictions showing the excellent uniformity of the crystal and suggest that more complicated ``nonlinear holograms'' should be possible.
physics.optics:We show analytically, and numerically that highly-dispersive media can be used to drastically increase lifetimes of high-Q microresonators. In such a resonator, lifetime is limited either by undesired coupling to radiation, or by intrinsic absorption of the constituent materials. The presence of dispersion weakens coupling to the undesired radiation modes and also effectively reduces the material absorption.
physics.optics:The Lorentz transformations for the optical constants (electric permittivity, magnetic permeability and index of refraction) of moving media are considered.
physics.optics:The existence of resonant enhanced transmission and collimation of light waves by subwavelength slits in metal films [for example, see T.W. Ebbesen et al., Nature (London) 391, 667 (1998) and H.J. Lezec et al., Science, 297, 820 (2002)] leads to the basic question: Can a light pulse be enhanced and simultaneously localized in space and time by a subwavelength slit? To address this question, the spatial distribution of the energy flux of an ultrashort (femtosecond) wave-packet diffracted by a subwavelength (nanometer-size) slit was analyzed by using the conventional approach based on the Neerhoff and Mur solution of Maxwell's equations. The results show that a light pulse can be enhanced by orders of magnitude and simultaneously localized in the near-field diffraction zone at the nm- and fs-scales. Possible applications in nanophotonics are discussed.
physics.optics:We address the dynamics of higher-order solitons in optical lattices, and predict their self-splitting into the set of their single-soliton constituents. The splitting is induced by the potential introduced by the lattice, together with the imprinting of a phase tilt onto the initial multisoliton states. The phenomenon allows the controllable generation of several coherent solitons linked via their Zakharov-Shabat eigenvalues. Application of the scheme to the generation of correlated matter waves in Bose-Einstein condensates is discussed.
physics.optics:We report on a high-efficiency 461 nm blue light conversion from an external cavity-enhanced second-harmonic generation of a 922 nm diode laser with a quasi-phase-matched KTP crystal (PPKTP). By choosing a long crystal (LC=20 mm) and twice looser focusing (w0=43 $\mu$m) than the "optimal" one, thermal lensing effects due to the blue power absorption are minimized while still maintaining near-optimal conversion efficiency. A stable blue power of 234 mW with a net conversion efficiency of eta=75% at an input mode-matched power of 310 mW is obtained. The intra-cavity measurements of the conversion efficiency and temperature tuning bandwidth yield an accurate value d33(461 nm)=15 pm/V for KTP and provide a stringent validation of some recently published linear and thermo-optic dispersion data of KTP.
physics.optics:We consider an anisotropic homogenized composite medium (HCM) arising from isotropic particulate component phases based on ellipsoidal geometries. For cubically nonlinear component phases, the corresponding zeroth-order strong-permittivity-fluctuation theory (SPFT) (which is equivalent to the Bruggeman homogenization formalism) and second-order SPFT are established and used to estimate the constitutive properties of the HCM. The relationship between the component phase particulate geometry and the HCM constitutive properties is explored. Significant differences are highlighted between the estimates of the Bruggeman homogenization formalism and the second-order SPFT estimates. The prospects for nonlinearity enhancement are investigated.
physics.optics:Under certain circumstances, Voigt waves can propagate in a biaxial composite medium even though the component material phases individually do not support Voigt wave propagation. This phenomenon is considered within the context of the strong--permittivity--fluctuation theory. A generalized implementation of the theory is developed in order to explore the propagation of Voigt waves in any direction. It is shown that the correlation length--a parameter characterizing the distributional statistics of the component material phases--plays a crucial role in facilitating the propagation of Voigt waves in the homogenized composite medium.
physics.optics:In conventional approaches to the homogenization of random particulate composites, both the distribution and size of the component phase particles are often inadequately taken into account. Commonly, the spatial distributions are characterized by volume fraction alone, while the electromagnetic response of each component particle is represented as a vanishingly small depolarization volume. The strong-permittivity-fluctuation theory (SPFT) provides an alternative approach to homogenization wherein a comprehensive description of distributional statistics of the component phases is accommodated. The bilocally-approximated SPFT is presented here for the anisotropic homogenized composite which arises from component phases comprising ellipsoidal particles. The distribution of the component phases is characterized by a two-point correlation function and its associated correlation length. Each component phase particle is represented as an ellipsoidal depolarization region of nonzero volume. The effects of depolarization volume and correlation length are investigated through considering representative numerical examples. It is demonstrated that both the spatial extent of the component phase particles and their spatial distributions are important factors in estimating coherent scattering losses of the macroscopic field.
physics.optics:The polarization matrix ($2\times2$) obtained from two component eigen-spinors of spherical harmonics help us to evaluate the differential matrix $N$ of the anisotropic optical medium. The geometric phase is realized through {\it helicity} of photon, assuming the transmission of polarized light through the crystal which has been twisted about the normal to its surface over a closed path.
physics.optics:A circularly polarized rotationally symmetric paraxial laser beams carries hbar angular momentum per photon as spin. Focussing the beam with a rotationally symmetric lens cannot change this angular momentum flux, yet the focussed beam must have spin less than hbar per photon. The remainder of the original spin is converted to orbital angular momentum, manifesting itself as a longitudinal optical vortex at the focus. This demonstrates that optical orbital angular momentum can be generated by a rotationally symmetric optical system which preserves the total angular momentum of the beam.
physics.optics:The sign of the refractive index of any medium is soley determined by the requirement that the propagation of an electromagnetic wave obeys Einstein causality. Our analysis shows that this requirement predicts that the real part of the refractive index may be negative in an isotropic medium even if the electric permittivity and the magnetic permeability are both positive. Such a system may be a route to negative index media at optical frequencies. We also demonstrate that the refractive index may be positive in left-handed media that contain two molecular species where one is in its excited state.
physics.optics:This paper presents a novel approach to wave propagation inside the Fabry-Perot framework. It states that the time-averaged Poynting vector modulus could be nonequivalent with the squared-field amplitude modulus. This fact permits the introduction of a new kind of nonlinear medium whose nonlinearity is proportional to the time-averaged Poynting vector modulus. Its transmittance is calculated and found to differ with that obtained for the Kerr medium, whose nonlinearity is proportional to the squared-field amplitude modulus. The latter emphasizes the nonequivalence of these magnitudes. A space-time symmetry analysis shows that the Poynting nonlinearity should be only possible in noncentrosymmetric materials.
physics.optics:The resonant optical modes of a high permittivity dielectric prism with an equilateral triangular cross section are discussed. Eigenmode solutions of the scalar Helmholtz equation with Dirichlet boundary conditions, appropriate to a conducting boundary, are applied for this purpose. The particular plane wave components present in these modes are analyzed for their total internal reflection behavior and implied mode confinement when the conducting boundary is replaced by a sharp dielectric mismatch. Improvement in TIR confinement by adjusting the longitudinal wavevector $k_z$ is also discussed. For two-dimensional electromagnetic solutions ($k_z=0$), TE polarization leads to longer lifetime than TM polarization, assuming that escape of evanescent boundary waves at the corners is the primary decay process.
physics.optics:We present a simple polarizing Mach-Zehnder interferometer that can be used for optimal minimal ellipsometry: Only four intensities are measured to determine the three Stokes parameters, and an optimal choice for the four polarization projections can be achieved for any sufficiently small wavelength range of interest.
physics.optics:Gold micro-mirrors have been formed in silicon in an inverted pyramidal shape. The pyramidal structures are created in the (100) surface of a silicon wafer by anisotropic etching in potassium hydroxide. High quality micro-mirrors are then formed by sputtering gold onto the smooth silicon (111) faces of the pyramids. These mirrors show great promise as high quality optical devices suitable for integration into MOEMS systems.
physics.optics:We demonstrate low-threshold random lasing in random amplifying layered medium via photon localization. Lasing is facilitated by resonant excitation of localized modes at the pump laser wavelength, which are peaked deep within the sample with greatly enhanced intensity. Emission occurs into long-lived localized modes overlapping the localized gain region. This mechanism overcomes a fundamental barrier to reducing lasing thresholds in diffusive random lasers, in which multiple scattering restricts the excitation region to the proximity of the sample surface.
physics.optics:The rate of linear collisionless damping (Landau damping) in a classical electron gas confined to a heated ionized thin film is calculated. The general expression for the imaginary part of the dielectric tensor in terms of the parameters of the single-particle self-consistent electron potential is obtained. For the case of a deep rectangular well, it is explicitly calculated as a function of the electron temperature in the two limiting cases of specular and diffuse reflection of the electrons from the boundary of the self-consistent potential. For realistic experimental parameters, the contribution of Landau damping to the heating of the electron subsystem is estimated. It is shown that for films with a thickness below about 100 nm and for moderate laser intensities it may be comparable with or even dominate over electron-ion collisions and inner ionization.
physics.optics:We propose and analyze a new type of a resonator in an annular geometry which is based on a single defect surrounded by radial Bragg reflectors on both sides. We show that the conditions for efficient mode confinement are different from those of the conventional Bragg waveguiding in a rectangular geometry. A simple and intuitive approach to the design of optimal radial Bragg reflectors is proposed and employed, yielding chirped gratings. Small bending radii and strong control over the resonator dispersion are possible by the Bragg confinement. A design compromise between large Free Spectral Range (FSR) requirements and fabrication tolerances is suggested.
physics.optics:Lasing at telecommunication wavelengths from annular resonators employing radial Bragg reflectors is demonstrated at room temperature under pulsed optical pumping. Sub milliwatt pump threshold levels are observed for resonators with 0.5-1.5 wavelengths wide defects of radii 7-8 mm. The quality factors of the resonator modal fields are estimated to be on the order of a few thousands. The electromagnetic field is shown to be guided by the defect. Good agreement is found between the measured and calculated spectrum.
physics.optics:We address Bessel photonic lattices of radial symmetry imprinted in cubic Kerr-type nonlinear media and show that they support families of stable dipole-mode solitons featuring two out-of-phase light spots located in different lattice rings. We show that the radial symmetry of the Bessel lattices afford a variety of unique soliton dynamics including controlled radiation-free rotation of the dipole-mode solitons.
cs.LG:This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.
cs.LG:Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.
cs.LG:The multiplicative Newton-like method developed by the author et al. is extended to the situation where the dynamics is restricted to the orthogonal group. A general framework is constructed without specifying the cost function. Though the restriction to the orthogonal groups makes the problem somewhat complicated, an explicit expression for the amount of individual jumps is obtained. This algorithm is exactly second-order-convergent. The global instability inherent in the Newton method is remedied by a Levenberg-Marquardt-type variation. The method thus constructed can readily be applied to the independent component analysis. Its remarkable performance is illustrated by a numerical simulation.
cs.LG:We construct new algorithms from scratch, which use the fourth order cumulant of stochastic variables for the cost function. The multiplicative updating rule here constructed is natural from the homogeneous nature of the Lie group and has numerous merits for the rigorous treatment of the dynamics. As one consequence, the second order convergence is shown. For the cost function, functions invariant under the componentwise scaling are choosen. By identifying points which can be transformed to each other by the scaling, we assume that the dynamics is in a coset space. In our method, a point can move toward any direction in this coset. Thus, no prewhitening is required.
cs.LG:Given a reference computer, Kolmogorov complexity is a well defined function on all binary strings. In the standard approach, however, only the asymptotic properties of such functions are considered because they do not depend on the reference computer. We argue that this approach can be more useful if it is refined to include an important practical case of simple binary strings. Kolmogorov complexity calculus may be developed for this case if we restrict the class of available reference computers. The interesting problem is to define a class of computers which is restricted in a {\it natural} way modeling the real-life situation where only a limited class of computers is physically available to us. We give an example of what such a natural restriction might look like mathematically, and show that under such restrictions some error terms, even logarithmic in complexity, can disappear from the standard complexity calculus.   Keywords: Kolmogorov complexity; Algorithmic information theory.
cs.LG:In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.
cs.LG:An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and relational domains.
cs.LG:When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.   Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).   As a case study, we present two alternative implementations of the ILP system Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which loads all data in main memory, and Tilde-LDS, which loads the examples one by one. We experimentally compare the implementations, showing Tilde-LDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.
cs.LG:In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a {\it stigmergic} approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(\lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.
cs.LG:Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.
cs.LG:The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for construction of probabilistic classifiers. This paper presents an empirical comparison of the MBBC algorithm with three other Bayesian classifiers: Naive Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these are implemented using the K2 framework of Cooper and Herskovits. The classifiers are compared in terms of their performance (using simple accuracy measures and ROC curves) and speed, on a range of standard benchmark data sets. It is concluded that MBBC is competitive in terms of speed and accuracy with the other algorithms considered.
cs.LG:In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, we make use of a parametric model of kernel matrices, and estimate missing entries by fitting the model to existing entries. The parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source. For model fitting, we adopt the em algorithm based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.
cs.LG:To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur.   We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree \Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures.   The complexity for searching the optimal Bayesian networks of in-degree \Delta increases only polynomially in the number of random varibales for constant \Delta and the optimal joint measure associated with a given graph can be found by convex optimization.
cs.LG:We make progress on two important problems regarding attribute efficient learnability.   First, we give an algorithm for learning decision lists of length $k$ over $n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time $n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight.   Second, we give an algorithm for learning an unknown parity function on $k$ out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$. For $k=o(\log n)$ this yields a polynomial time algorithm with sample complexity $o(n)$. This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.
cs.LG:Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.
cs.LG:It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipf's distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable "progress in studies".
cs.LG:For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.
cs.LG:We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) := 1/N sum_i (f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when y_i is a noisy version of f*(x_i) for some function f* in H, the output of the algorithm converges to f* as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N -> infinity. For the case where N -> infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we describe the limiting 'non-noisy', 'non-regularized' function f*, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.
cs.LG:We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2.   To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated.   Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.
cs.LG:We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space $[0,1]$ and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.
cs.LG:We consider a general class of forecasting protocols, called "linear protocols", and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecaster's predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptic's capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.
cs.LG:This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.
cs.LG:We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.
cs.LG:We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed "bag of components" representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects' structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.
cs.LG:This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for "reactive" experts problems, which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.
cs.LG:A main problem of "Follow the Perturbed Leader" strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t^(1/2)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t^2 log t) samples in each step.
cs.LG:Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.
cs.LG:We consider the problem of learning unions of rectangles over the domain $[b]^n$, in the uniform distribution membership query learning setting, where both b and n are "large". We obtain poly$(n, \log b)$-time algorithms for the following classes:   - poly$(n \log b)$-way Majority of $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   - Union of poly$(\log(n \log b))$ many $O(\frac{\log^2 (n \log b)} {(\log \log(n \log b) \log \log \log (n \log b))^2})$-dimensional rectangles.   - poly$(n \log b)$-way Majority of poly$(n \log b)$-Or of disjoint $O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.   Our main algorithmic tool is an extension of Jackson's boosting- and Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$, building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to obtain the results stated above are techniques from exact learning [Beimel, Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$ circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean functions as thresholds of parities [Klivans, Servedio 2001].
cs.LG:We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithm's performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is "universal" (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.
cs.LG:The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t - x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, >...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x*. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may impose practical limitations on an agent's performance, as O(eps^-4) queries are made before the queries converge to x* with eps accuracy.
cs.LG:A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).
cs.LG:We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a "regret term" of O(N^(-1/2)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.
cs.LG:Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.
cs.LG:It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.
cs.LG:Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.
cs.LG:Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m->infinity and V for k->infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.
cs.LG:Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.
cs.LG:Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.
cs.LG:A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their $L_1$ distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the $L_1$ metric into a distribution that is good under the KL-divergence.
cs.LG:In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.
cs.LG:In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.
cs.LG:We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as "learning from possibilities" that formalizes these ideas, then we present a more specific learning setting, referred to as "assumption-based learning" that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.
cs.LG:We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learner/classifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. "Nested tree," "embedded tree" and "recursive tree" are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.
cs.LG:We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a "leading prediction strategy", which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.
cs.LG:Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.
cs.LG:We present basic notions of Gold's "learnability in the limit" paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.
cs.LG:An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.
cs.LG:Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictor's goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.
cs.LG:We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon.   In this scenario, we give a poly(n/epsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the ``clustering'' model, where such assumptions are unavoidable.   Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.
cs.LG:Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for "hedging" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.
cs.LG:This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.
cs.LG:Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2^D \sqrt{n}), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.
cs.LG:We propose an axiomatic approach to the concept of an intrinsic dimension of a dataset, based on a viewpoint of geometry of high-dimensional structures. Our first axiom postulates that high values of dimension be indicative of the presence of the curse of dimensionality (in a certain precise mathematical sense). The second axiom requires the dimension to depend smoothly on a distance between datasets (so that the dimension of a dataset and that of an approximating principal manifold would be close to each other). The third axiom is a normalization condition: the dimension of the Euclidean $n$-sphere $\s^n$ is $\Theta(n)$. We give an example of a dimension function satisfying our axioms, even though it is in general computationally unfeasible, and discuss a computationally cheap function satisfying most but not all of our axioms (the ``intrinsic dimensionality'' of Ch\'avez et al.)
cs.LG:This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.
cs.LG:We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.
cs.LG:Speaker identification is a powerful, non-invasive and in-expensive biometric technique. The recognition accuracy, however, deteriorates when noise levels affect a specific band of frequency. In this paper, we present a sub-band based speaker identification that intends to improve the live testing performance. Each frequency sub-band is processed and classified independently. We also compare the linear and non-linear merging techniques for the sub-bands recognizer. Support vector machines and Gaussian Mixture models are the non-linear merging techniques that are investigated. Results showed that the sub-band based method used with linear merging techniques enormously improved the performance of the speaker identification over the performance of wide-band recognizers when tested live. A live testing improvement of 9.78% was achieved
cs.LG:Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such "VC dimensions" exist for models taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations appropriate for the missing case, the one of models with values in R^Q. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.
cs.LG:We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.
cs.LG:Supervised learning deals with the inference of a distribution over an output or label space $\CY$ conditioned on points in an observation space $\CX$, given a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of applications of interest, acquisition of large amounts of observations is easy, while the process of generating labels is time-consuming or costly. One way to deal with this problem is {\em active} learning, where points to be labelled are selected with the aim of creating a model with better performance than that of an model trained on an equal number of randomly sampled points. In this paper, we instead propose to deal with the labelling cost directly: The learning goal is defined as the minimisation of a cost which is a function of the expected model performance and the total cost of the labels used. This allows the development of general strategies and specific algorithms for (a) optimal stopping, where the expected cost dictates whether label acquisition should continue (b) empirical evaluation, where the cost is used as a performance metric for a given combination of inference, stopping and sampling methods. Though the main focus of the paper is optimal stopping, we also aim to provide the background for further developments and discussion in the related field of active learning.
cs.LG:The method of defensive forecasting is applied to the problem of prediction with expert advice for binary outcomes. It turns out that defensive forecasting is not only competitive with the Aggregating Algorithm but also handles the case of "second-guessing" experts, whose advice depends on the learner's prediction; this paper assumes that the dependence on the learner's prediction is continuous.
cs.LG:Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms. There are two known varieties of defensive forecasting: "continuous", in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and "randomized", in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized. This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous.
cs.LG:The purpose of this note is to show how the method of maximum entropy in the mean (MEM) may be used to improve parametric estimation when the measurements are corrupted by large level of noise. The method is developed in the context on a concrete example: that of estimation of the parameter in an exponential distribution. We compare the performance of our method with the bayesian and maximum likelihood approaches.
cs.LG:We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.
cs.LG:Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.
cs.LG:Recent spectral clustering methods are a propular and powerful technique for data clustering. These methods need to solve the eigenproblem whose computational complexity is $O(n^3)$, where $n$ is the number of data samples. In this paper, a non-eigenproblem based clustering method is proposed to deal with the clustering problem. Its performance is comparable to the spectral clustering algorithms but it is more efficient with computational complexity $O(n^2)$. We show that with a transitive distance and an observed property, called K-means duality, our algorithm can be used to handle data sets with complex cluster shapes, multi-scale clusters, and noise. Moreover, no parameters except the number of clusters need to be set in our algorithm.
cs.LG:Covariances from categorical variables are defined using a regular simplex expression for categories. The method follows the variance definition by Gini, and it gives the covariance as a solution of simultaneous equations. The calculated results give reasonable values for test data. A method of principal component analysis (RS-PCA) is also proposed using regular simplex expressions, which allows easy interpretation of the principal components. The proposed methods apply to variable selection problem of categorical data USCensus1990 data. The proposed methods give appropriate criterion for the variable selection problem of categorical
cs.LG:For a classification problem described by the joint density $P(\omega,x)$, models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have been shown to be an optimal similarity measure for nearest neighbor classification. This paper analyzes demonstrates several additional properties of that conditional distribution. The paper first shows that we can reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$ given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class labels, and gives an asymptotically Bayes-optimal classification procedure. It also shows, given such an optimal similarity measure, how to construct a classifier that outperforms the nearest neighbor classifier and achieves Bayes-optimal classification rates. The paper then analyzes Bayesian similarity in a framework where a classifier faces a number of related classification tasks (multitask learning) and illustrates that reconstruction of the class posterior distribution is not possible in general. Finally, the paper identifies a distinct class of classification problems using $P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to solve those problems is the Bayes optimal solution.
cs.LG:Learning machines which have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither the Bayes a posteriori distribution converges to the normal distribution nor the maximum likelihood estimator satisfies asymptotic normality. This is the main reason why it has been difficult to predict their generalization performances from trained states. In this paper, we study four errors, (1) Bayes generalization error, (2) Bayes training error, (3) Gibbs generalization error, and (4) Gibbs training error, and prove that there are mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that Bayes and Gibbs generalization errors are estimated by Bayes and Gibbs training errors, and propose widely applicable information criteria which can be applied to both regular and singular statistical models.
cs.LG:We consider the problem of choosing a density estimate from a set of distributions F, minimizing the L1-distance to an unknown distribution (Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the problem: Scheffe tournament winner and minimum distance estimate. The Scheffe tournament estimate requires fewer computations than the minimum distance estimate, but has strictly weaker guarantees than the latter.   We focus on the computational aspect of density estimation. We present two algorithms, both with the same guarantee as the minimum distance estimate. The first one, a modification of the minimum distance estimate, uses the same number (quadratic in |F|) of computations as the Scheffe tournament. The second one, called ``efficient minimum loss-weight estimate,'' uses only a linear number of computations, assuming that F is preprocessed.   We also give examples showing that the guarantees of the algorithms cannot be improved and explore randomized algorithms for density estimation.
cs.LG:Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.
cs.LG:The cross-entropy method is a simple but efficient method for global optimization. In this paper we provide two online variants of the basic CEM, together with a proof of convergence.
cs.LG:We prove that the optimal assignment kernel, proposed recently as an attempt to embed labeled graphs and more generally tuples of basic data to a Hilbert space, is in fact not always positive definite.
cs.LG:Given R groups of numerical variables X1, ... XR, we assume that each group is the result of one underlying latent variable, and that all latent variables are bound together through a linear equation system. Moreover, we assume that some explanatory latent variables may interact pairwise in one or more equations. We basically consider PLS Path Modelling's algorithm to estimate both latent variables and the model's coefficients. New "external" estimation schemes are proposed that draw latent variables towards strong group structures in a more flexible way. New "internal" estimation schemes are proposed to enable PLSPM to make good use of variable group complementarity and to deal with interactions. Application examples are given.
cs.LG:We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators from "users" to the "objects" they rate. Recent low-rank type matrix completion approaches to CF are shown to be special cases. However, unlike existing regularization based CF methods, our approach can be used to also incorporate information such as attributes of the users or the objects -- a limitation of existing regularization based CF methods. We then provide novel representer theorems that we use to develop new estimation methods. We provide learning algorithms based on low-rank decompositions, and test them on a standard CF dataset. The experiments indicate the advantages of generalizing the existing regularization based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.
cs.LG:We study the problem of learning k-juntas given access to examples drawn from a number of different product distributions. Thus we wish to learn a function f : {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best known algorithms for the general problem of learning a k-junta require running time of n^k * poly(n,2^k), we show that given access to k different product distributions with biases separated by \gamma>0, the functions may be learned in time poly(n,2^k,\gamma^{-k}). More generally, given access to t <= k different product distributions, the functions may be learned in time n^{k/t} * poly(n,2^k,\gamma^{-k}). Our techniques involve novel results in Fourier analysis relating Fourier expansions with respect to different biases and a generalization of Russo's formula.
cs.LG:The use of computational intelligence techniques for classification has been used in numerous applications. This paper compares the use of a Multi Layer Perceptron Neural Network and a new Relational Network on classifying the HIV status of women at ante-natal clinics. The paper discusses the architecture of the relational network and its merits compared to a neural network and most other computational intelligence classifiers. Results gathered from the study indicate comparable classification accuracies as well as revealed relationships between data features in the classification data. Much higher classification accuracies are recommended for future research in the area of HIV classification as well as missing data estimation.
cs.LG:This paper aims to showcase the measure of structural diversity of an ensemble of 9 classifiers and then map a relationship between this structural diversity and accuracy. The structural diversity was induced by having different architectures or structures of the classifiers The Genetical Algorithms (GA) were used to derive the relationship between diversity and the classification accuracy by evolving the classifiers and then picking 9 classifiers out on an ensemble of 60 classifiers. It was found that as the ensemble became diverse the accuracy improved. However at a certain diversity measure the accuracy began to drop. The Kohavi-Wolpert variance method is used to measure the diversity of the ensemble. A method of voting is used to aggregate the results from each classifier. The lowest error was observed at a diversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024 as maximum diversity measured. The parameters that were varied were: the number of hidden nodes, learning rate and the activation function.
cs.LG:Using a support vector machine requires to set two types of hyperparameters: the soft margin parameter C and the parameters of the kernel. To perform this model selection task, the method of choice is cross-validation. Its leave-one-out variant is known to produce an estimator of the generalization error which is almost unbiased. Its major drawback rests in its time requirement. To overcome this difficulty, several upper bounds on the leave-one-out error of the pattern recognition SVM have been derived. Among those bounds, the most popular one is probably the radius-margin bound. It applies to the hard margin pattern recognition SVM, and by extension to the 2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2, as a direct extension of the 2-norm SVM to the multi-class case. For this machine, a generalized radius-margin bound is then established.
cs.LG:This article considers constrained $\ell_1$ minimization methods for the recovery of high dimensional sparse signals in three settings: noiseless, bounded error and Gaussian noise. A unified and elementary treatment is given in these noise settings for two $\ell_1$ minimization methods: the Dantzig selector and $\ell_1$ minimization with an $\ell_2$ constraint. The results of this paper improve the existing results in the literature by weakening the conditions and tightening the error bounds. The improvement on the conditions shows that signals with larger support can be recovered accurately. This paper also establishes connections between restricted isometry property and the mutual incoherence property. Some results of Candes, Romberg and Tao (2006) and Donoho, Elad, and Temlyakov (2006) are extended.
cs.LG:We identify the classical Perceptron algorithm with margin as a member of a broader family of large margin classifiers which we collectively call the Margitron. The Margitron, (despite its) sharing the same update rule with the Perceptron, is shown in an incremental setting to converge in a finite number of updates to solutions possessing any desirable fraction of the maximum margin. Experiments comparing the Margitron with decomposition SVMs on tasks involving linear kernels and 2-norm soft margin are also reported.
cs.LG:This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of distributional stability which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm.
cs.LG:This article describes an approach to designing a distributed and modular neural classifier. This approach introduces a new hierarchical clustering that enables one to determine reliable regions in the representation space by exploiting supervised information. A multilayer perceptron is then associated with each of these detected clusters and charged with recognizing elements of the associated cluster while rejecting all others. The obtained global classifier is comprised of a set of cooperating neural networks and completed by a K-nearest neighbor classifier charged with treating elements rejected by all the neural networks. Experimental results for the handwritten digit recognition problem and comparison with neural and statistical nonmodular classifiers are given.
cs.LG:Nous pr\'esentons dans cette contribution une approche \`a la fois symbolique et probabiliste permettant d'extraire l'information sur la segmentation du signal de parole \`a partir d'information prosodique. Nous utilisons pour ce faire des grammaires probabilistes poss\'edant une structure hi\'erarchique minimale. La phase de construction des grammaires ainsi que leur pouvoir de pr\'ediction sont \'evalu\'es qualitativement ainsi que quantitativement.   -----   Methodologically oriented, the present work sketches an approach for prosodic information retrieval and speech segmentation, based on both symbolic and probabilistic information. We have recourse to probabilistic grammars, within which we implement a minimal hierarchical structure. Both the stages of probabilistic grammar building and its testing in prediction are explored and quantitatively and qualitatively evaluated.
cs.LG:Statistical learning theory chiefly studies restricted hypothesis classes, particularly those with finite Vapnik-Chervonenkis (VC) dimension. The fundamental quantity of interest is the sample complexity: the number of samples required to learn to a specified level of accuracy. Here we consider learning over the set of all computable labeling functions. Since the VC-dimension is infinite and a priori (uniform) bounds on the number of samples are impossible, we let the learning algorithm decide when it has seen sufficient samples to have learned. We first show that learning in this setting is indeed possible, and develop a learning algorithm. We then show, however, that bounding sample complexity independently of the distribution is impossible. Notably, this impossibility is entirely due to the requirement that the learning algorithm be computable, and not due to the statistical nature of the problem.
cs.LG:We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend on an unknown subset of k<<n variables (so-called k-juntas) is agnostically learnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k}, and log(1/delta). In other words, there is an algorithm with the claimed running time that, given epsilon, delta > 0 and access to a random walk on {-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with probability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f, where opt(f) denotes the distance of a closest k-junta to f.
cs.LG:The method of stable random projections is a tool for efficiently computing the $l_\alpha$ distances using low memory, where $0<\alpha \leq 2$ is a tuning parameter. The method boils down to a statistical estimation task and various estimators have been proposed, based on the geometric mean, the harmonic mean, and the fractional power etc.   This study proposes the optimal quantile estimator, whose main operation is selecting, which is considerably less expensive than taking fractional power, the main operation in previous estimators. Our experiments report that the optimal quantile estimator is nearly one order of magnitude more computationally efficient than previous estimators. For large-scale learning tasks in which storing and computing pairwise distances is a serious bottleneck, this estimator should be desirable.   In addition to its computational advantages, the optimal quantile estimator exhibits nice theoretical properties. It is more accurate than previous estimators when $\alpha>1$. We derive its theoretical error bounds and establish the explicit (i.e., no hidden constants) sample complexity bound.
cs.LG:Applications in machine learning and data mining require computing pairwise Lp distances in a data matrix A. For massive high-dimensional data, computing all pairwise distances of A can be infeasible. In fact, even storing A or all pairwise distances of A in the memory may be also infeasible. This paper proposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where p is even) distances into a sum of 2 marginal norms and p-1 ``inner products'' at different orders. Then we apply normal or sub-Gaussian random projections to approximate the resultant ``inner products,'' assuming that the marginal norms can be computed exactly by a linear scan. We propose two strategies for applying random projections. The basic projection strategy requires only one projection matrix but it is more difficult to analyze, while the alternative projection strategy requires p-1 projection matrices but its theoretical analysis is much easier. In terms of the accuracy, at least for p=4, the basic strategy is always more accurate than the alternative strategy if the data are non-negative, which is common in reality.
cs.LG:We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel \citep{GaeFlaWro03,BorOngSchVisetal05}, marginalized graph kernel \citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04}, and geometric kernel on graphs \citep{Gaertner02}. Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels \citep{KonLaf02}, regularization on graphs \citep{SmoKon03}, and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels \citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized to graphs reduce to the random walk graph kernel.
cs.LG:We study probability distributions over free algebras of trees. Probability distributions can be seen as particular (formal power) tree series [Berstel et al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. We argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. First, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. Note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. Second, the algebraic representation can be easily extended to deal with unranked trees (like XML trees where a symbol may have an unbounded number of children). Both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that Hidden Markov Models are equivalent to nondeterministic string automata); nowadays applications for Web Information Extraction, Web Services and document processing consider unranked trees.
cs.LG:We present a novel graphical framework for modeling non-negative sequential data with hierarchical structure. Our model corresponds to a network of coupled non-negative matrix factorization (NMF) modules, which we refer to as a positive factor network (PFN). The data model is linear, subject to non-negativity constraints, so that observation data consisting of an additive combination of individually representable observations is also representable by the network. This is a desirable property for modeling problems in computational auditory scene analysis, since distinct sound sources in the environment are often well-modeled as combining additively in the corresponding magnitude spectrogram. We propose inference and learning algorithms that leverage existing NMF algorithms and that are straightforward to implement. We present a target tracking example and provide results for synthetic observation data which serve to illustrate the interesting properties of PFNs and motivate their potential usefulness in applications such as music transcription, source separation, and speech recognition. We show how a target process characterized by a hierarchical state transition model can be represented as a PFN. Our results illustrate that a PFN which is defined in terms of a single target observation can then be used to effectively track the states of multiple simultaneous targets. Our results show that the quality of the inferred target states degrades gradually as the observation noise is increased. We also present results for an example in which meaningful hierarchical features are extracted from a spectrogram. Such a hierarchical representation could be useful for music transcription and source separation applications. We also propose a network for language modeling.
cs.LG:We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the inner product then the learned vector is a linear combination of the input data. This result, known as the {\em representer theorem}, is at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, thereby completing the characterization of kernel methods based on regularization. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition for these class of matrix regularizers and highlight them with some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing function.
cs.LG:In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.
cs.LG:We consider the task of learning a classifier from the feature space $\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the features can be partitioned into class-conditionally independent feature sets $\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classifier from $\mathcal{X}_2$ to $\mathcal{X}_1$ and 2) learning the class-conditional distribution of the feature set $\mathcal{X}_1$. This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples. We present experimental evaluation of the idea in two real world applications.
cs.LG:Maximum Variance Unfolding (MVU) and its variants have been very successful in embedding data-manifolds in lower dimensional spaces, often revealing the true intrinsic dimension. In this paper we show how to also incorporate supervised class information into an MVU-like method without breaking its convexity. We call this method the Isometric Separation Map and we show that the resulting kernel matrix can be used as a binary/multiclass Support Vector Machine-like method in a semi-supervised (transductive) framework. We also show that the method always finds a kernel matrix that linearly separates the training data exactly without projecting them in infinite dimensional spaces. In traditional SVMs we choose a kernel and hope that the data become linearly separable in the kernel space. In this paper we show how the hyperplane can be chosen ad-hoc and the kernel is trained so that data are always linearly separable. Comparisons with Large Margin SVMs show comparable performance.
cs.LG:In this paper, I expand Shannon's definition of entropy into a new form of entropy that allows integration of information from different random events. Shannon's notion of entropy is a special case of my more general definition of entropy. I define probability using a so-called performance function, which is de facto an exponential distribution. Assuming that my general notion of entropy reflects the true uncertainty about a probabilistic event, I understand that our perceived uncertainty differs. I claim that our perception is the result of two opposing forces similar to the two famous antagonists in Chinese philosophy: Yin and Yang. Based on this idea, I show that our perceived uncertainty matches the true uncertainty in points determined by the golden ratio. I demonstrate that the well-known sigmoid function, which we typically employ in artificial neural networks as a non-linear threshold function, describes the actual performance. Furthermore, I provide a motivation for the time dilation in Einstein's Special Relativity, basically claiming that although time dilation conforms with our perception, it does not correspond to reality. At the end of the paper, I show how to apply this theoretical framework to practical applications. I present recognition rates for a pattern recognition problem, and also propose a network architecture that can take advantage of general entropy to solve complex decision problems.
cs.LG:Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence.   This paper studies the scenario where the observations are drawn from a stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary phi-mixing and beta-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios.   We also illustrate the application of our phi-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios.
cs.LG:Ensemble classification is an emerging approach to land cover mapping whereby the final classification output is a result of a consensus of classifiers. Intuitively, an ensemble system should consist of base classifiers which are diverse i.e. classifiers whose decision boundaries err differently. In this paper ensemble feature selection is used to impose diversity in ensembles. The features of the constituent base classifiers for each ensemble were created through an exhaustive search algorithm using different separability indices. For each ensemble, the classification accuracy was derived as well as a diversity measure purported to give a measure of the inensemble diversity. The correlation between ensemble classification accuracy and diversity measure was determined to establish the interplay between the two variables. From the findings of this paper, diversity measures as currently formulated do not provide an adequate means upon which to constitute ensembles for land cover mapping.
cs.LG:The enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum random walk (QRW) with the problem of data clustering, and develop two clustering algorithms based on the one dimensional QRW. Then, the probability distributions on the positions induced by QRW in these algorithms are investigated, which also indicates the possibility of obtaining better results. Consequently, the experimental results have demonstrated that data points in datasets are clustered reasonably and efficiently, and the clustering algorithms are of fast rates of convergence. Moreover, the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach.
cs.LG:We present a convex formulation of dictionary learning for sparse signal decomposition. Convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm. In particular, our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using a large set of synthetic examples, we compare the estimation abilities of the convex and non-convex approaches, showing that while the convex formulation has a single local minimum, this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation.
cs.LG:We introduce a simple and computationally trivial method for binary classification based on the evaluation of potential functions. We demonstrate that despite the conceptual and computational simplicity of the method its performance can match or exceed that of standard Support Vector Machine methods.
cs.LG:We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an $r$-dimensional random vector $\mathbf{Z} \in \mathbb{R}^r$, where $r \geq 2$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\Theta(r \sqrt{T})$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form $O(r \sqrt{T} \log^{3/2} T)$.
cs.LG:We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.
cs.LG:Cooperative decision making is a vision of future network management and control. Distributed connection preemption is an important example where nodes can make intelligent decisions on allocating resources and controlling traffic flows for multi-class service networks. A challenge is that nodal decisions are spatially dependent as traffic flows trespass multiple nodes in a network. Hence the performance-complexity trade-off becomes important, i.e., how accurate decisions are versus how much information is exchanged among nodes. Connection preemption is known to be NP-complete. Centralized preemption is optimal but computationally intractable. Decentralized preemption is computationally efficient but may result in a poor performance. This work investigates distributed preemption where nodes decide whether and which flows to preempt using only local information exchange with neighbors. We develop, based on the probabilistic graphical models, a near-optimal distributed algorithm. The algorithm is used by each node to make collectively near-optimal preemption decisions. We study trade-offs between near-optimal performance and complexity that corresponds to the amount of information-exchange of the distributed algorithm. The algorithm is validated by both analysis and simulation.
cs.LG:In statistical problems, a set of parameterized probability distributions is used to estimate the true probability distribution. If Fisher information matrix at the true distribution is singular, then it has been left unknown what we can estimate about the true distribution from random samples. In this paper, we study a singular regression problem and prove a limit theorem which shows the relation between the singular regression problem and two birational invariants, a real log canonical threshold and a singular fluctuation. The obtained theorem has an important application to statistics, because it enables us to estimate the generalization error from the training error without any knowledge of the true probability distribution.
cs.LG:Scenarios for the emergence or bootstrap of a lexicon involve the repeated interaction between at least two agents who must reach a consensus on how to name N objects using H words. Here we consider minimal models of two types of learning algorithms: cross-situational learning, in which the individuals determine the meaning of a word by looking for something in common across all observed uses of that word, and supervised operant conditioning learning, in which there is strong feedback between individuals about the intended meaning of the words. Despite the stark differences between these learning schemes, we show that they yield the same communication accuracy in the realistic limits of large N and H, which coincides with the result of the classical occupancy problem of randomly assigning N objects to H words.
cs.LG:In this paper, we propose a technique to extract constrained formal concepts.
cs.LG:Recently, different works proposed a new way to mine patterns in databases with pathological size. For example, experiments in genome biology usually provide databases with thousands of attributes (genes) but only tens of objects (experiments). In this case, mining the "transposed" database runs through a smaller search space, and the Galois connection allows to infer the closed patterns of the original database. We focus here on constrained pattern mining for those unusual databases and give a theoretical framework for database and constraint transposition. We discuss the properties of constraint transposition and look into classical constraints. We then address the problem of generating the closed patterns of the original database satisfying the constraint, starting from those mined in the "transposed" database. Finally, we show how to generate all the patterns satisfying the constraint from the closed ones.
cs.LG:We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -- that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.
cs.LG:This paper formalises the concept of learning symbolic rules from multisource data in a cardiac monitoring context. Our sources, electrocardiograms and arterial blood pressure measures, describe cardiac behaviours from different viewpoints. To learn interpretable rules, we use an Inductive Logic Programming (ILP) method. We develop an original strategy to cope with the dimensionality issues caused by using this ILP technique on a rich multisource language. The results show that our method greatly improves the feasibility and the efficiency of the process while staying accurate. They also confirm the benefits of using multiple sources to improve the diagnosis of cardiac arrhythmias.
cs.LG:The problem of completing a low-rank matrix from a subset of its entries is often encountered in the analysis of incomplete data sets exhibiting an underlying factor model with applications in collaborative filtering, computer vision and control. Most recent work had been focused on constructing efficient algorithms for exact or approximate recovery of the missing matrix entries and proving lower bounds for the number of known entries that guarantee a successful recovery with high probability. A related problem from both the mathematical and algorithmic point of view is the distance geometry problem of realizing points in a Euclidean space from a given subset of their pairwise distances. Rigidity theory answers basic questions regarding the uniqueness of the realization satisfying a given partial set of distances. We observe that basic ideas and tools of rigidity theory can be adapted to determine uniqueness of low-rank matrix completion, where inner products play the role that distances play in rigidity theory. This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the completion matrix, that serves as the analogue of the rigidity matrix.
cs.LG:We introduce a new protocol for prediction with expert advice in which each expert evaluates the learner's and his own performance using a loss function that may change over time and may be different from the loss functions used by the other experts. The learner's goal is to perform better or not much worse than each expert, as evaluated by that expert, for all experts simultaneously. If the loss functions used by the experts are all proper scoring rules and all mixable, we show that the defensive forecasting algorithm enjoys the same performance guarantee as that attainable by the Aggregating Algorithm in the standard setting and known to be optimal. This result is also applied to the case of "specialist" (or "sleeping") experts. In this case, the defensive forecasting algorithm reduces to a simple modification of the Aggregating Algorithm.
cs.LG:We present multiplicative updates for solving hard and soft margin support vector machines (SVM) with non-negative kernels. They follow as a natural extension of the updates for non-negative matrix factorization. No additional param- eter setting, such as choosing learning, rate is required. Ex- periments demonstrate rapid convergence to good classifiers. We analyze the rates of asymptotic convergence of the up- dates and establish tight bounds. We test the performance on several datasets using various non-negative kernels and report equivalent generalization errors to that of a standard SVM.
cs.LG:Collecting large labeled data sets is a laborious and expensive task, whose scaling up requires division of the labeling workload between many teachers. When the number of classes is large, miscorrespondences between the labels given by the different teachers are likely to occur, which, in the extreme case, may reach total inconsistency. In this paper we describe how globally consistent labels can be obtained, despite the absence of teacher coordination, and discuss the possible efficiency of this process in terms of human labor. We define a notion of label efficiency, measuring the ratio between the number of globally consistent labels obtained and the number of labels provided by distributed teachers. We show that the efficiency depends critically on the ratio alpha between the number of data instances seen by a single teacher, and the number of classes. We suggest several algorithms for the distributed labeling problem, and analyze their efficiency as a function of alpha. In addition, we provide an upper bound on label efficiency for the case of completely uncoordinated teachers, and show that efficiency approaches 0 as the ratio between the number of labels each teacher provides and the number of classes drops (i.e. alpha goes to 0).
cs.LG:This paper has been retracted.
cs.LG:A $p$-adic modification of the split-LBG classification method is presented in which first clusterings and then cluster centers are computed which locally minimise an energy function. The outcome for a fixed dataset is independent of the prime number $p$ with finitely many exceptions. The methods are applied to the construction of $p$-adic classifiers in the context of learning.
cs.LG:This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It also shows that a number of widely used transductive regression algorithms are in fact unstable. Finally, it reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, for one of the algorithms, in particular for determining the radius of the local neighborhood used by the algorithm.
cs.LG:Motivation: Several different threads of research have been proposed for modeling and mining temporal data. On the one hand, approaches such as dynamic Bayesian networks (DBNs) provide a formal probabilistic basis to model relationships between time-indexed random variables but these models are intractable to learn in the general case. On the other, algorithms such as frequent episode mining are scalable to large datasets but do not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature.   Results: We present a unification of these two seemingly diverse threads of research, by demonstrating how dynamic (discrete) Bayesian networks can be inferred from the results of frequent episode mining. This helps bridge the modeling emphasis of the former with the counting emphasis of the latter. First, we show how, under reasonable assumptions on data characteristics and on influences of random variables, the optimal DBN structure can be computed using a greedy, local, algorithm. Next, we connect the optimality of the DBN structure with the notion of fixed-delay episodes and their counts of distinct occurrences. Finally, to demonstrate the practical feasibility of our approach, we focus on a specific (but broadly applicable) class of networks, called excitatory networks, and show how the search for the optimal DBN structure can be conducted using just information from frequent episodes. Application on datasets gathered from mathematical models of spiking neurons as well as real neuroscience datasets are presented.   Availability: Algorithmic implementations, simulator codebases, and datasets are available from our website at http://neural-code.cs.vt.edu/dbn
cs.LG:Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).
cs.LG:In this paper, we consider the coherent theory of (epistemic) uncertainty of Walley, in which beliefs are represented through sets of probability distributions, and we focus on the problem of modeling prior ignorance about a categorical random variable. In this setting, it is a known result that a state of prior ignorance is not compatible with learning. To overcome this problem, another state of beliefs, called \emph{near-ignorance}, has been proposed. Near-ignorance resembles ignorance very closely, by satisfying some principles that can arguably be regarded as necessary in a state of ignorance, and allows learning to take place. What this paper does, is to provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is \emph{latent}. We argue that such a setting is by far the most common case in practice, and we provide, for the case of categorical latent variables (and general \emph{manifest} variables) a condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied even in the most common statistical problems. We regard these results as a strong form of evidence against the possibility to adopt a condition of prior near-ignorance in real statistical problems.
cs.LG:Engine assembly is a complex and heavily automated distributed-control process, with large amounts of faults data logged everyday. We describe an application of temporal data mining for analyzing fault logs in an engine assembly plant. Frequent episode discovery framework is a model-free method that can be used to deduce (temporal) correlations among events from the logs in an efficient manner. In addition to being theoretically elegant and computationally efficient, frequent episodes are also easy to interpret in the form actionable recommendations. Incorporation of domain-specific information is critical to successful application of the method for analyzing fault logs in the manufacturing domain. We show how domain-specific knowledge can be incorporated using heuristic rules that act as pre-filters and post-filters to frequent episode discovery. The system described here is currently being used in one of the engine assembly plants of General Motors and is planned for adaptation in other plants. To the best of our knowledge, this paper presents the first real, large-scale application of temporal data mining in the manufacturing domain. We believe that the ideas presented in this paper can help practitioners engineer tools for analysis in other similar or related application domains as well.
cs.LG:This paper presents a new hybrid learning algorithm for unsupervised classification tasks. We combined Fuzzy c-means learning algorithm and a supervised version of Minimerror to develop a hybrid incremental strategy allowing unsupervised classifications. We applied this new approach to a real-world database in order to know if the information contained in unlabeled features of a Geographic Information System (GIS), allows to well classify it. Finally, we compared our results to a classical supervised classification obtained by a multilayer perceptron.
cs.LG:We analyze the expected cost of a greedy active learning algorithm. Our analysis extends previous work to a more general setting in which different queries have different costs. Moreover, queries may have more than two possible responses and the distribution over hypotheses may be non uniform. Specific applications include active learning with label costs, active learning for multiclass and partial label queries, and batch mode active learning. We also discuss an approximate version of interest when there are very many queries.
cs.LG:We present three related ways of using Transfer Learning to improve feature selection. The three methods address different problems, and hence share different kinds of information between tasks or feature classes, but all three are based on the information theoretic Minimum Description Length (MDL) principle and share the same underlying Bayesian interpretation. The first method, MIC, applies when predictive models are to be built simultaneously for multiple tasks (``simultaneous transfer'') that share the same set of features. MIC allows each feature to be added to none, some, or all of the task models and is most beneficial for selecting a small set of predictive features from a large pool of features, as is common in genomic and biological datasets. Our second method, TPC (Three Part Coding), uses a similar methodology for the case when the features can be divided into feature classes. Our third method, Transfer-TPC, addresses the ``sequential transfer'' problem in which the task to which we want to transfer knowledge may not be known in advance and may have different amounts of data than the other tasks. Transfer-TPC is most beneficial when we want to transfer knowledge between tasks which have unequal amounts of labeled data, for example the data for disambiguating the senses of different verbs. We demonstrate the effectiveness of these approaches with experimental results on real world data pertaining to genomics and to Word Sense Disambiguation (WSD).
cs.LG:Many learning machines that have hierarchical structure or hidden variables are now being used in information science, artificial intelligence, and bioinformatics. However, several learning machines used in such fields are not regular but singular statistical models, hence their generalization performance is still left unknown. To overcome these problems, in the previous papers, we proved new equations in statistical learning, by which we can estimate the Bayes generalization loss from the Bayes training loss and the functional variance, on the condition that the true distribution is a singularity contained in a learning machine. In this paper, we prove that the same equations hold even if a true distribution is not contained in a parametric model. Also we prove that, the proposed equations in a regular case are asymptotically equivalent to the Takeuchi information criterion. Therefore, the proposed equations are always applicable without any condition on the unknown true distribution.
cs.LG:The problem of classifying sonar signals from rocks and mines first studied by Gorman and Sejnowski has become a benchmark against which many learning algorithms have been tested. We show that both the training set and the test set of this benchmark are linearly separable, although with different hyperplanes. Moreover, the complete set of learning and test patterns together, is also linearly separable. We give the weights that separate these sets, which may be used to compare results found by other algorithms.
cs.LG:Clusters of genes that have evolved by repeated segmental duplication present difficult challenges throughout genomic analysis, from sequence assembly to functional analysis. Improved understanding of these clusters is of utmost importance, since they have been shown to be the source of evolutionary innovation, and have been linked to multiple diseases, including HIV and a variety of cancers. Previously, Zhang et al. (2008) developed an algorithm for reconstructing parsimonious evolutionary histories of such gene clusters, using only human genomic sequence data. In this paper, we propose a probabilistic model for the evolution of gene clusters on a phylogeny, and an MCMC algorithm for reconstruction of duplication histories from genomic sequences in multiple species. Several projects are underway to obtain high quality BAC-based assemblies of duplicated clusters in multiple species, and we anticipate that our method will be useful in analyzing these valuable new data sets.
cs.LG:In this paper, we present two classes of Bayesian approaches to the two-sample problem. Our first class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as flexible nonparametric priors over the unknown distributions.
cs.LG:In this paper, we present the step by step knowledge acquisition process by choosing a structured method through using a questionnaire as a knowledge acquisition tool. Here we want to depict the problem domain as, how to evaluate teachers performance in higher education through the use of expert system technology. The problem is how to acquire the specific knowledge for a selected problem efficiently and effectively from human experts and encode it in the suitable computer format. Acquiring knowledge from human experts in the process of expert systems development is one of the most common problems cited till yet. This questionnaire was sent to 87 domain experts within all public and private universities in Pakistani. Among them 25 domain experts sent their valuable opinions. Most of the domain experts were highly qualified, well experienced and highly responsible persons. The whole questionnaire was divided into 15 main groups of factors, which were further divided into 99 individual questions. These facts were analyzed further to give a final shape to the questionnaire. This knowledge acquisition technique may be used as a learning tool for further research work.
cs.LG:We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.
cs.LG:This paper has been withdrawn due to an error found by Dana Angluin and Lev Reyzin.
cs.LG:We learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks. We exploit the intuition that for domain adaptation, we wish to share classifier structure, but for multitask learning, we wish to share covariance structure. Our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets.
cs.LG:We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to define distributions over the countably infinite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these "reference types") that are generic across all clusters. Inference in our framework, which requires integrating over infinitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple--but general--parameterization of our model based on a Gaussian assumption. We evaluate this model on one artificial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics.
cs.LG:Dirichlet process (DP) mixture models provide a flexible Bayesian framework for density estimation. Unfortunately, their flexibility comes at a cost: inference in DP mixture models is computationally expensive, even when conjugate distributions are used. In the common case when one seeks only a maximum a posteriori assignment of data points to clusters, we show that search algorithms provide a practical alternative to expensive MCMC and variational techniques. When a true posterior sample is desired, the solution found by search can serve as a good initializer for MCMC. Experimental results show that using these techniques is it possible to apply DP mixture models to very large data sets.
cs.LG:The maze traversal problem (finding the shortest distance to the goal from any position in a maze) has been an interesting challenge in computational intelligence. Recent work has shown that the cellular simultaneous recurrent neural network (CSRN) can solve this problem for simple mazes. This thesis focuses on exploiting relevant information about the maze to improve learning and decrease the training time for the CSRN to solve mazes. Appropriate variables are identified to create useful clusters using relevant information. The CSRN was next modified to allow for an additional external input. With this additional input, several methods were tested and results show that clustering the mazes improves the overall learning of the traversal problem for the CSRN.
cs.LG:We propose a randomized algorithm for training Support vector machines(SVMs) on large datasets. By using ideas from Random projections we show that the combinatorial dimension of SVMs is $O({log} n)$ with high probability. This estimate of combinatorial dimension is used to derive an iterative algorithm, called RandSVM, which at each step calls an existing solver to train SVMs on a randomly chosen subset of size $O({log} n)$. The algorithm has probabilistic guarantees and is capable of training SVMs with Kernels for both classification and regression problems. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up existing SVM learners, without loss of accuracy.
cs.LG:We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.
cs.LG:In Data Mining, the usefulness of association rules is strongly limited by the huge amount of delivered rules. In this paper we propose a new approach to prune and filter discovered rules. Using Domain Ontologies, we strengthen the integration of user knowledge in the post-processing task. Furthermore, an interactive and iterative framework is designed to assist the user along the analyzing task. On the one hand, we represent user domain knowledge using a Domain Ontology over database. On the other hand, a novel technique is suggested to prune and to filter discovered rules. The proposed framework was applied successfully over the client database provided by Nantes Habitat.
cs.LG:Gaussian processes (GPs) provide a probabilistic nonparametric representation of functions in regression, classification, and other problems. Unfortunately, exact learning with GPs is intractable for large datasets. A variety of approximate GP methods have been proposed that essentially map the large dataset into a small set of basis points. The most advanced of these, the variable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have its own length scale. However, VSGP was only derived for regression. We describe how VSGP can be applied to classification and other problems, by deriving it as an expectation propagation algorithm. In this view, sparse GP approximations correspond to a KL-projection of the true posterior onto a compact exponential family of GPs. VSGP constitutes one such family, and we show how to enlarge this family to get additional accuracy. In particular, we show that endowing each basis point with its own full covariance matrix provides a significant increase in approximation power.
cs.LG:In this paper we discuss the techniques involved in the design of the famous statistical spam filters that include Naive Bayes, Term Frequency-Inverse Document Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes Additive Regression Tree. We compare these techniques with each other in terms of accuracy, recall, precision, etc. Further, we discuss the effectiveness and limitations of statistical filters in filtering out various types of spam from legitimate e-mails.
cs.LG:We study the problem of online regression. We prove a theoretical bound on the square loss of Ridge Regression. We do not make any assumptions about input vectors or outcomes. We also show that Bayesian Ridge Regression can be thought of as an online algorithm competing with all the Gaussian linear experts.
cs.LG:We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on $n$-point nominal data. Anomalies are declared whenever the score of a test sample falls below $\alpha$, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, $\alpha$, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.
cs.LG:In this paper, we prove a crucial theorem called Mirroring Theorem which affirms that given a collection of samples with enough information in it such that it can be classified into classes and subclasses then (i) There exists a mapping which classifies and subclassifies these samples (ii) There exists a hierarchical classifier which can be constructed by using Mirroring Neural Networks (MNNs) in combination with a clustering algorithm that can approximate this mapping. Thus, the proof of the Mirroring theorem provides a theoretical basis for the existence and a practical feasibility of constructing hierarchical classifiers, given the maps. Our proposed Mirroring Theorem can also be considered as an extension to Kolmogrovs theorem in providing a realistic solution for unsupervised classification. The techniques we develop, are general in nature and have led to the construction of learning machines which are (i) tree like in structure, (ii) modular (iii) with each module running on a common algorithm (tandem algorithm) and (iv) selfsupervised. We have actually built the architecture, developed the tandem algorithm of such a hierarchical classifier and demonstrated it on an example problem.
cs.LG:This paper describes a methodology for detecting anomalies from sequentially observed and potentially noisy data. The proposed approach consists of two main elements: (1) {\em filtering}, or assigning a belief or likelihood to each successive measurement based upon our ability to predict it from previous noisy observations, and (2) {\em hedging}, or flagging potential anomalies by comparing the current belief against a time-varying and data-adaptive threshold. The threshold is adjusted based on the available feedback from an end user. Our algorithms, which combine universal prediction with recent work on online convex programming, do not require computing posterior distributions given all current observations and involve simple primal-dual parameter updates. At the heart of the proposed approach lie exponential-family models which can be used in a wide variety of contexts and applications, and which yield methods that achieve sublinear per-round regret against both static and slowly varying product distributions with marginals drawn from the same exponential family. Moreover, the regret against static distributions coincides with the minimax value of the corresponding online strongly convex game. We also prove bounds on the number of mistakes made during the hedging step relative to the best offline choice of the threshold with access to all estimated beliefs and feedback signals. We validate the theory on synthetic data drawn from a time-varying distribution over binary vectors of high dimensionality, as well as on the Enron email dataset.
cs.LG:We present in this paper a study on the ability and the benefits of using a keystroke dynamics authentication method for collaborative systems. Authentication is a challenging issue in order to guarantee the security of use of collaborative systems during the access control step. Many solutions exist in the state of the art such as the use of one time passwords or smart-cards. We focus in this paper on biometric based solutions that do not necessitate any additional sensor. Keystroke dynamics is an interesting solution as it uses only the keyboard and is invisible for users. Many methods have been published in this field. We make a comparative study of many of them considering the operational constraints of use for collaborative systems.
cs.LG:This document describes concisely the ubiquitous class of exponential family distributions met in statistics. The first part recalls definitions and summarizes main properties and duality with Bregman divergences (all proofs are skipped). The second part lists decompositions and related formula of common exponential family distributions. We recall the Fisher-Rao-Riemannian geometries and the dual affine connection information geometries of statistical manifolds. It is intended to maintain and update this document and catalog by adding new distribution items.
cs.LG:One of the most popular algorithms for clustering in Euclidean space is the $k$-means algorithm; $k$-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is {\em well-clustered}. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of $k$-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian -- or, in other words, when the input comes from a mixture of Gaussians.   We analyze three aspects of the $k$-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components.   Finally, we study the sample requirement of $k$-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of $k$-means is {\em near-optimal}.
cs.LG:In this paper, we consider delay-optimal power and subcarrier allocation design for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base station. There are $K$ queues at the base station for the downlink traffic to the $K$ mobiles with heterogeneous packet arrivals and delay requirements. We shall model the problem as a $K$-dimensional infinite horizon average reward Markov Decision Problem (MDP) where the control actions are assumed to be a function of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). This problem is challenging because it corresponds to a stochastic Network Utility Maximization (NUM) problem where general solution is still unknown. We propose an {\em online stochastic value iteration} solution using {\em stochastic approximation}. The proposed power control algorithm, which is a function of both the CSI and the QSI, takes the form of multi-level water-filling. We prove that under two mild conditions in Theorem 1 (One is the stepsize condition. The other is the condition on accessibility of the Markov Chain, which can be easily satisfied in most of the cases we are interested.), the proposed solution converges to the optimal solution almost surely (with probability 1) and the proposed framework offers a possible solution to the general stochastic NUM problem. By exploiting the birth-death structure of the queue dynamics, we obtain a reduced complexity decomposed solution with linear $\mathcal{O}(KN_F)$ complexity and $\mathcal{O}(K)$ memory requirement.
cs.LG:Association rule mining plays vital part in knowledge mining. The difficult task is discovering knowledge or useful rules from the large number of rules generated for reduced support. For pruning or grouping rules, several techniques are used such as rule structure cover methods, informative cover methods, rule clustering, etc. Another way of selecting association rules is based on interestingness measures such as support, confidence, correlation, and so on. In this paper, we study how rule clusters of the pattern Xi - Y are distributed over different interestingness measures.
cs.LG:This paper presents a tumor detection algorithm from mammogram. The proposed system focuses on the solution of two problems. One is how to detect tumors as suspicious regions with a very weak contrast to their background and another is how to extract features which categorize tumors. The tumor detection method follows the scheme of (a) mammogram enhancement. (b) The segmentation of the tumor area. (c) The extraction of features from the segmented tumor area. (d) The use of SVM classifier. The enhancement can be defined as conversion of the image quality to a better and more understandable level. The mammogram enhancement procedure includes filtering, top hat operation, DWT. Then the contrast stretching is used to increase the contrast of the image. The segmentation of mammogram images has been playing an important role to improve the detection and diagnosis of breast cancer. The most common segmentation method used is thresholding. The features are extracted from the segmented breast area. Next stage include, which classifies the regions using the SVM classifier. The method was tested on 75 mammographic images, from the mini-MIAS database. The methodology achieved a sensitivity of 88.75%.
cs.LG:Among all the partition based clustering algorithms K-means is the most popular and well known method. It generally shows impressive results even in considerably large data sets. The computational complexity of K-means does not suffer from the size of the data set. The main disadvantage faced in performing this clustering is that the selection of initial means. If the user does not have adequate knowledge about the data set, it may lead to erroneous results. The algorithm Automatic Initialization of Means (AIM), which is an extension to K-means, has been proposed to overcome the problem of initial mean generation. In this paper an attempt has been made to compare the performance of the algorithms through implementation
cs.LG:Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.
cs.LG:In this paper we consider the problem of reconstructing a hidden weighted hypergraph of constant rank using additive queries. We prove the following: Let $G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$ hyperedges. For any $m$ there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log n}{\log m}) $$ additive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal Query Complexity Bounds for Finding Graphs. {\em STOC}, 749--758,~2008].   When the weights of the hypergraph are integers that are less than $O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for unweighted hypergraphs) there exists a non-adaptive algorithm that finds the edges of the graph and their weights using $$ O(\frac{m\log \frac{n^d}{m}}{\log m}). $$ additive queries.   Using the information theoretic bound the above query complexities are tight.
cs.LG:Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.
cs.LG:Discovering latent representations of the observed world has become increasingly more relevant in data analysis. Much of the effort concentrates on building latent variables which can be used in prediction problems, such as classification and regression. A related goal of learning latent structure from data is that of identifying which hidden common causes generate the observations, such as in applications that require predicting the effect of policies. This will be the main problem tackled in our contribution: given a dataset of indicators assumed to be generated by unknown and unmeasured common causes, we wish to discover which hidden common causes are those, and how they generate our data. This is possible under the assumption that observed variables are linear functions of the latent causes with additive noise. Previous results in the literature present solutions for the case where each observed variable is a noisy function of a single latent variable. We show how to extend the existing results for some cases where observed variables measure more than one latent variable.
cs.LG:Bayes statistics and statistical physics have the common mathematical structure, where the log likelihood function corresponds to the random Hamiltonian. Recently, it was discovered that the asymptotic learning curves in Bayes estimation are subject to a universal law, even if the log likelihood function can not be approximated by any quadratic form. However, it is left unknown what mathematical property ensures such a universal law. In this paper, we define a renormalizable condition of the statistical estimation problem, and show that, under such a condition, the asymptotic learning curves are ensured to be subject to the universal law, even if the true distribution is unrealizable and singular for a statistical model. Also we study a nonrenormalizable case, in which the learning curves have the different asymptotic behaviors from the universal law.
cs.LG:Associative Classifier is a novel technique which is the integration of Association Rule Mining and Classification. The difficult task in building Associative Classifier model is the selection of relevant rules from a large number of class association rules (CARs). A very popular method of ordering rules for selection is based on confidence, support and antecedent size (CSA). Other methods are based on hybrid orderings in which CSA method is combined with other measures. In the present work, we study the effect of using different interestingness measures of Association rules in CAR rule ordering and selection for associative classifier.
cs.LG:This paper presents a framework aimed at monitoring the behavior of aircraft in a given airspace. Nominal trajectories are determined and learned using data driven methods. Standard procedures are used by air traffic controllers (ATC) to guide aircraft, ensure the safety of the airspace, and to maximize the runway occupancy. Even though standard procedures are used by ATC, the control of the aircraft remains with the pilots, leading to a large variability in the flight patterns observed. Two methods to identify typical operations and their variability from recorded radar tracks are presented. This knowledge base is then used to monitor the conformance of current operations against operations previously identified as standard. A tool called AirTrajectoryMiner is presented, aiming at monitoring the instantaneous health of the airspace, in real time. The airspace is "healthy" when all aircraft are flying according to the nominal procedures. A measure of complexity is introduced, measuring the conformance of current flight to nominal flight patterns. When an aircraft does not conform, the complexity increases as more attention from ATC is required to ensure a safe separation between aircraft.
cs.LG:The paper deals with on-line regression settings with signals belonging to a Banach lattice. Our algorithms work in a semi-online setting where all the inputs are known in advance and outcomes are unknown and given step by step. We apply the Aggregating Algorithm to construct a prediction method whose cumulative loss over all the input vectors is comparable with the cumulative loss of any linear functional on the Banach lattice. As a by-product we get an algorithm that takes signals from an arbitrary domain. Its cumulative loss is comparable with the cumulative loss of any predictor function from Besov and Triebel-Lizorkin spaces. We describe several applications of our setting.
cs.LG:The performance in higher secondary school education in India is a turning point in the academic lives of all students. As this academic performance is influenced by many factors, it is essential to develop predictive data mining model for students' performance so as to identify the slow learners and study the influence of the dominant factors on their academic performance. In the present investigation, a survey cum experimental methodology was adopted to generate a database and it was constructed from a primary and a secondary source. While the primary data was collected from the regular students, the secondary data was gathered from the school and office of the Chief Educational Officer (CEO). A total of 1000 datasets of the year 2006 from five different schools in three different districts of Tamilnadu were collected. The raw data was preprocessed in terms of filling up missing values, transforming values in one form into another and relevant attribute/ variable selection. As a result, we had 772 student records, which were used for CHAID prediction model construction. A set of prediction rules were extracted from CHIAD prediction model and the efficiency of the generated CHIAD prediction model was found. The accuracy of the present model was compared with other model and it has been found to be satisfactory.
cs.LG:The recent increase in dimensionality of data has thrown a great challenge to the existing dimensionality reduction methods in terms of their effectiveness. Dimensionality reduction has emerged as one of the significant preprocessing steps in machine learning applications and has been effective in removing inappropriate data, increasing learning accuracy, and improving comprehensibility. Feature redundancy exercises great influence on the performance of classification process. Towards the better classification performance, this paper addresses the usefulness of truncating the highly correlated and redundant attributes. Here, an effort has been made to verify the utility of dimensionality reduction by applying LVQ (Learning Vector Quantization) method on two Benchmark datasets of 'Pima Indian Diabetic patients' and 'Lung cancer patients'.
cs.LG:A key problem in sensor networks is to decide which sensors to query when, in order to obtain the most useful information (e.g., for performing accurate prediction), subject to constraints (e.g., on power and bandwidth). In many applications the utility function is not known a priori, must be learned from data, and can even change over time. Furthermore for large sensor networks solving a centralized optimization problem to select sensors is not feasible, and thus we seek a fully distributed solution. In this paper, we present Distributed Online Greedy (DOG), an efficient, distributed algorithm for repeatedly selecting sensors online, only receiving feedback about the utility of the selected sensors. We prove very strong theoretical no-regret guarantees that apply whenever the (unknown) utility function satisfies a natural diminishing returns property called submodularity. Our algorithm has extremely low communication requirements, and scales well to large sensor deployments. We extend DOG to allow observation-dependent sensor selection. We empirically demonstrate the effectiveness of our algorithm on several real-world sensing tasks.
cs.LG:Recently Kutin and Niyogi investigated several notions of algorithmic stability--a property of a learning map conceptually similar to continuity--showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.
cs.LG:We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.
cs.LG:We introduce a natural generalization of submodular set cover and exact active learning with a finite hypothesis class (query learning). We call this new problem interactive submodular set cover. Applications include advertising in social networks with hidden information. We give an approximation guarantee for a novel greedy algorithm and give a hardness of approximation result which matches up to constant factors. We also discuss negative results for simpler approaches and present encouraging early experimental results.
cs.LG:India is a multi-lingual country where Roman script is often used alongside different Indic scripts in a text document. To develop a script specific handwritten Optical Character Recognition (OCR) system, it is therefore necessary to identify the scripts of handwritten text correctly. In this paper, we present a system, which automatically separates the scripts of handwritten words from a document, written in Bangla or Devanagri mixed with Roman scripts. In this script separation technique, we first, extract the text lines and words from document pages using a script independent Neighboring Component Analysis technique. Then we have designed a Multi Layer Perceptron (MLP) based classifier for script separation, trained with 8 different wordlevel holistic features. Two equal sized datasets, one with Bangla and Roman scripts and the other with Devanagri and Roman scripts, are prepared for the system evaluation. On respective independent text samples, word-level script identification accuracies of 99.29% and 98.43% are achieved.
cs.LG:We address the problem of learning in an online, bandit setting where the learner must repeatedly select among $K$ actions, but only receives partial feedback based on its choices. We establish two new facts: First, using a new algorithm called Exp4.P, we show that it is possible to compete with the best in a set of $N$ experts with probability $1-\delta$ while incurring regret at most $O(\sqrt{KT\ln(N/\delta)})$ over $T$ time steps. The new algorithm is tested empirically in a large-scale, real-world dataset. Second, we give a new algorithm called VE that competes with a possibly infinite set of policies of VC-dimension $d$ while incurring regret at most $O(\sqrt{T(d\ln(T) + \ln (1/\delta))})$ with probability $1-\delta$. These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing supervised learning type guarantees for the contextual bandit setting.
cs.LG:We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared, and modify it only via a single time-dependent parameter. Our algorithm's regret bounds are worst-case optimal, and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent, which means they can exploit the structure of the actual problem instance. Critically, however, our algorithm does not need to know this structure in advance. Rather, we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.
cs.LG:Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.
cs.LG:Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled dataset. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional datasets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever.
cs.LG:A key issue in statistics and machine learning is to automatically select the "right" model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) - for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.
cs.LG:Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufficient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy.
cs.LG:We present an approach to semi-supervised learning based on an exponential family characterization. Our approach generalizes previous work on coupled priors for hybrid generative/discriminative models. Our model is more flexible and natural than previous approaches. Experimental results on several data sets show that our approach also performs better in practice.
cs.LG:In recent years, predicting the user's next request in web navigation has received much attention. An information source to be used for dealing with such problem is the left information by the previous web users stored at the web access log on the web servers. Purposed systems for this problem work based on this idea that if a large number of web users request specific pages of a website on a given session, it can be concluded that these pages are satisfying similar information needs, and therefore they are conceptually related. In this study, a new clustering approach is introduced that employs logical path storing of a website pages as another parameter which is regarded as a similarity parameter and conceptual relation between web pages. The results of simulation have shown that the proposed approach is more than others precise in determining the clusters.
cs.LG:Most Web page classification models typically apply the bag of words (BOW) model to represent the feature space. The original BOW representation, however, is unable to recognize semantic relationships between terms. One possible solution is to apply the topic model approach based on the Latent Dirichlet Allocation algorithm to cluster the term features into a set of latent topics. Terms assigned into the same topic are semantically related. In this paper, we propose a novel hierarchical classification method based on a topic model and by integrating additional term features from neighboring pages. Our hierarchical classification method consists of two phases: (1) feature representation by using a topic model and integrating neighboring pages, and (2) hierarchical Support Vector Machines (SVM) classification model constructed from a confusion matrix. From the experimental results, the approach of using the proposed hierarchical SVM model by integrating current page with neighboring pages via the topic model yielded the best performance with the accuracy equal to 90.33% and the F1 measure of 90.14%; an improvement of 5.12% and 5.13% over the original SVM model, respectively.
cs.LG:We apply the method of defensive forecasting, based on the use of game-theoretic supermartingales, to prediction with expert advice. In the traditional setting of a countable number of experts and a finite number of outcomes, the Defensive Forecasting Algorithm is very close to the well-known Aggregating Algorithm. Not only the performance guarantees but also the predictions are the same for these two methods of fundamentally different nature. We discuss also a new setting where the experts can give advice conditional on the learner's future decision. Both the algorithms can be adapted to the new setting and give the same performance guarantees as in the traditional setting. Finally, we outline an application of defensive forecasting to a setting with several loss functions.
cs.LG:This paper proposes a novel similarity measure for clustering sequential data. We first construct a common state-space by training a single probabilistic model with all the sequences in order to get a unified representation for the dataset. Then, distances are obtained attending to the transition matrices induced by each sequence in that state-space. This approach solves some of the usual overfitting and scalability issues of the existing semi-parametric techniques, that rely on training a model for each sequence. Empirical studies on both synthetic and real-world datasets illustrate the advantages of the proposed similarity measure for clustering sequences.
cs.LG:In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to $2\lambda/n$, where $\lambda$ is the real log canonical threshold and $n$ is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.
cs.LG:We present a solution to the problem of understanding a system that produces a sequence of temporally ordered observations. Our solution is based on generating and interpreting a set of temporal decision rules. A temporal decision rule is a decision rule that can be used to predict or retrodict the value of a decision attribute, using condition attributes that are observed at times other than the decision attribute's time of observation. A rule set, consisting of a set of temporal decision rules with the same decision attribute, can be interpreted by our Temporal Investigation Method for Enregistered Record Sequences (TIMERS) to signify an instantaneous, an acausal or a possibly causal relationship between the condition attributes and the decision attribute. We show the effectiveness of our method, by describing a number of experiments with both synthetic and real temporal data.
cs.LG:In this work we investigate the relationship between Bregman distances and regularized Logistic Regression model. We present a detailed study of Bregman Distance minimization, a family of generalized entropy measures associated with convex functions. We convert the L1-regularized logistic regression into this more general framework and propose a primal-dual method based algorithm for learning the parameters. We pose L1-regularized logistic regression into Bregman distance minimization and then apply non-linear constrained optimization techniques to estimate the parameters of the logistic model.
cs.LG:We describe and analyze efficient algorithms for learning a linear predictor from examples when the learner can only view a few attributes of each training example. This is the case, for instance, in medical research, where each patient participating in the experiment is only willing to go through a small number of tests. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on each training example. We demonstrate the efficiency of our algorithms by showing that when running on digit recognition data, they obtain a high prediction accuracy even when the learner gets to see only four pixels of each image.
cs.LG:We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.
cs.LG:In Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the {\it effective} sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning.
cs.LG:In this paper, we consider the distributive queue-aware power and subband allocation design for a delay-optimal OFDMA uplink system with one base station, $K$ users and $N_F$ independent subbands. Each mobile has an uplink queue with heterogeneous packet arrivals and delay requirements. We model the problem as an infinite horizon average reward Markov Decision Problem (MDP) where the control actions are functions of the instantaneous Channel State Information (CSI) as well as the joint Queue State Information (QSI). To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the subband allocation Q-factor by the sum of the per-user subband allocation Q-factor and derive a distributive online stochastic learning algorithm to estimate the per-user Q-factor and the Lagrange multipliers (LM) simultaneously and determine the control actions using an auction mechanism. We show that under the proposed auction mechanism, the distributive online learning converges almost surely (with probability 1). For illustration, we apply the proposed distributive stochastic learning framework to an application example with exponential packet size distribution. We show that the delay-optimal power control has the {\em multi-level water-filling} structure where the CSI determines the instantaneous power allocation and the QSI determines the water-level. The proposed algorithm has linear signaling overhead and computational complexity $\mathcal O(KN)$, which is desirable from an implementation perspective.
cs.LG:This paper presents a method for automated healing as part of off-line automated troubleshooting. The method combines statistical learning with constraint optimization. The automated healing aims at locally optimizing radio resource management (RRM) or system parameters of cells with poor performance in an iterative manner. The statistical learning processes the data using Logistic Regression (LR) to extract closed form (functional) relations between Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. These functional relations are then processed by an optimization engine which proposes new parameter values. The advantage of the proposed formulation is the small number of iterations required by the automated healing method to converge, making it suitable for off-line implementation. The proposed method is applied to heal an Inter-Cell Interference Coordination (ICIC) process in a 3G Long Term Evolution (LTE) network which is based on soft-frequency reuse scheme. Numerical simulations illustrate the benefits of the proposed approach.
cs.LG:Although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications such as communications. In this work, we focus our attention on the complex gaussian kernel and its possible application in the complex Kernel LMS algorithm. In order to derive the gradients needed to develop the complex kernel LMS (CKLMS), we employ the powerful tool of Wirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, the notion of Writinger's calculus is extended to include complex RKHSs. Experiments verify that the CKLMS offers significant performance improvements over the traditional complex LMS or Widely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. However, so far, the emphasis has been on batch techniques. It is only recently, that online adaptive techniques have been considered in the context of signal processing tasks. To the best of our knowledge, no kernel-based strategy has been developed, so far, that is able to deal with complex valued signals. In this paper, we take advantage of a technique called complexification of real RKHSs to attack this problem. In order to derive gradients and subgradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool ofWirtinger's Calculus, which has recently attracted much attention in the signal processing community. Writinger's calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, in this paper, the notion of Writinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be used to derive nonlinear stable algorithms, which offer significant performance improvements over the traditional complex LMS orWidely Linear complex LMS (WL-LMS) algorithms, when dealing with nonlinearities.
cs.LG:Semi-supervised support vector machines (S3VMs) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data. Though S3VMs have been found helpful in many situations, they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only. In this paper, we try to reduce the chance of performance degeneration of S3VMs. Our basic idea is that, rather than exploiting all unlabeled data, the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited, while some highly risky unlabeled instances are avoided. We propose the S3VM-\emph{us} method by using hierarchical clustering to select the unlabeled instances. Experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of S3VM-\emph{us} is much smaller than that of existing S3VMs.
cs.LG:We study prediction with expert advice in the setting where the losses are accumulated with some discounting---the impact of old losses may gradually vanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm for Regression to this case, propose a suitable new variant of exponential weights algorithm, and prove respective loss bounds.
cs.LG:In this paper, we formulate a novel problem for finding blackhole and volcano patterns in a large directed graph. Specifically, a blackhole pattern is a group which is made of a set of nodes in a way such that there are only inlinks to this group from the rest nodes in the graph. In contrast, a volcano pattern is a group which only has outlinks to the rest nodes in the graph. Both patterns can be observed in real world. For instance, in a trading network, a blackhole pattern may represent a group of traders who are manipulating the market. In the paper, we first prove that the blackhole mining problem is a dual problem of finding volcanoes. Therefore, we focus on finding the blackhole patterns. Along this line, we design two pruning schemes to guide the blackhole finding process. In the first pruning scheme, we strategically prune the search space based on a set of pattern-size-independent pruning rules and develop an iBlackhole algorithm. The second pruning scheme follows a divide-and-conquer strategy to further exploit the pruning results from the first pruning scheme. Indeed, a target directed graphs can be divided into several disconnected subgraphs by the first pruning scheme, and thus the blackhole finding can be conducted in each disconnected subgraph rather than in a large graph. Based on these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally, experimental results on real-world data show that the iBlackhole-DC algorithm can be several orders of magnitude faster than the iBlackhole algorithm, which has a huge computational advantage over a brute-force method.
cs.LG:We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is "similar" to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work.
cs.LG:We study online learning when individual instances are corrupted by adversarially chosen random noise. We assume the noise distribution is unknown, and may change over time with no restriction other than having zero mean and bounded variance. Our technique relies on a family of unbiased estimators for non-linear functions, which may be of independent interest. We show that a variant of online gradient descent can learn functions in any dot-product (e.g., polynomial) or Gaussian kernel space with any analytic convex loss function. Our variant uses randomized estimates that need to query a random number of noisy copies of each instance, where with high probability this number is upper bounded by a constant. Allowing such multiple queries cannot be avoided: Indeed, we show that online learning is in general impossible when only one noisy copy of each instance can be accessed.
cs.LG:We consider the question of the stability of evolutionary algorithms to gradual changes, or drift, in the target concept. We define an algorithm to be resistant to drift if, for some inverse polynomial drift rate in the target function, it converges to accuracy 1 -- \epsilon , with polynomial resources, and then stays within that accuracy indefinitely, except with probability \epsilon , at any one time. We show that every evolution algorithm, in the sense of Valiant (2007; 2009), can be converted using the Correlational Query technique of Feldman (2008), into such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean conjunctions, we give bounds on the rates of drift that they can resist. We develop some new evolution algorithms that are resistant to significant drift. In particular, we give an algorithm for evolving linear separators over the spherically symmetric distribution that is resistant to a drift rate of O(\epsilon /n), and another algorithm over the more general product normal distributions that resists a smaller drift rate.   The above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations.
cs.LG:We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the \emph{zero-one} loss function. Unlike most previous formulations which rely on surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural zero-one loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time $\poly(\exp(L\log(L/\epsilon)))$, for $\emph{any}$ distribution, where $L$ is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most $\epsilon$. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in $L$.
cs.LG:This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method.
cs.LG:The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\widetilde{O}(\log\frac{1}{\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\epsilon$ is related to the parameter in Tsybakov noise.
cs.LG:In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.
cs.LG:Exchangeable random variables form an important and well-studied generalization of i.i.d. variables, however simple examples show that no nontrivial concept or function classes are PAC learnable under general exchangeable data inputs $X_1,X_2,\ldots$. Inspired by the work of Berti and Rigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new paradigm, adequate for learning from exchangeable data: predictive PAC learnability. A learning rule $\mathcal L$ for a function class $\mathscr F$ is predictive PAC if for every $\e,\delta>0$ and each function $f\in {\mathscr F}$, whenever $\abs{\sigma}\geq s(\delta,\e)$, we have with confidence $1-\delta$ that the expected difference between $f(X_{n+1})$ and the image of $f\vert\sigma$ under $\mathcal L$ does not exceed $\e$ conditionally on $X_1,X_2,\ldots,X_n$. Thus, instead of learning the function $f$ as such, we are learning to a given accuracy $\e$ the predictive behaviour of $f$ at the future points $X_i(\omega)$, $i>n$ of the sample path. Using de Finetti's theorem, we show that if a universally separable function class $\mathscr F$ is distribution-free PAC learnable under i.i.d. inputs, then it is distribution-free predictive PAC learnable under exchangeable inputs, with a slightly worse sample complexity.
cs.LG:The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.
cs.LG:In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present the first model for dyadic prediction that satisfies several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) it is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to very large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are ordinal and ignore side-information when it is present. Experimental results show that the new method is competitive with state-of-the-art methods for the special cases of collaborative filtering and link prediction, and that it makes accurate predictions on nominal data.
cs.LG:We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.
cs.LG:Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. The primary mathematical tool employed in these methods is the notion of the Reproducing Kernel Hilbert Space. However, so far, the emphasis has been on batch techniques. It is only recently, that online techniques have been considered in the context of adaptive signal processing tasks. Moreover, these efforts have only been focussed on real valued data sequences. To the best of our knowledge, no adaptive kernel-based strategy has been developed, so far, for complex valued signals. Furthermore, although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications that deal with complex signals, with Communications being a typical example. In this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called \textit{complexification} of real RKHSs, or complex reproducing kernels, highlighting the use of the complex gaussian kernel. In order to derive gradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool of Wirtinger's Calculus, which has recently attracted attention in the signal processing community. To this end, in this paper, the notion of Wirtinger's calculus is extended, for the first time, to include complex RKHSs and use it to derive several realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS offers significant performance improvements over several linear and nonlinear algorithms, when dealing with nonlinearities.
cs.LG:This paper studies the MINLIP estimator for the identification of Wiener systems consisting of a sequence of a linear FIR dynamical model, and a monotonically increasing (or decreasing) static function. Given $T$ observations, this algorithm boils down to solving a convex quadratic program with $O(T)$ variables and inequality constraints, implementing an inference technique which is based entirely on model complexity control. The resulting estimates of the linear submodel are found to be almost consistent when no noise is present in the data, under a condition of smoothness of the true nonlinearity and local Persistency of Excitation (local PE) of the data. This result is novel as it does not rely on classical tools as a 'linearization' using a Taylor decomposition, nor exploits stochastic properties of the data. It is indicated how to extend the method to cope with noisy data, and empirical evidence contrasts performance of the estimator against other recently proposed techniques.
cs.LG:In response to a 1997 problem of M. Vidyasagar, we state a necessary and sufficient condition for distribution-free PAC learnability of a concept class $\mathscr C$ under the family of all non-atomic (diffuse) measures on the domain $\Omega$. Clearly, finiteness of the classical Vapnik-Chervonenkis dimension of $\mathscr C$ is a sufficient, but no longer necessary, condition. Besides, learnability of $\mathscr C$ under non-atomic measures does not imply the uniform Glivenko-Cantelli property with regard to non-atomic measures. Our learnability criterion is stated in terms of a combinatorial parameter $\VC({\mathscr C}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$ modulo countable sets. The new parameter is obtained by ``thickening up'' single points in the definition of VC dimension to uncountable ``clusters''. Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if every countable subclass of $\mathscr C$ has VC dimension $\leq d$ outside a countable subset of $\Omega$. The new parameter can be also expressed as the classical VC dimension of $\mathscr C$ calculated on a suitable subset of a compactification of $\Omega$. We do not make any measurability assumptions on $\mathscr C$, assuming instead the validity of Martin's Axiom (MA).
cs.LG:We present a new latent-variable model employing a Gaussian mixture integrated with a feature selection procedure (the Bernoulli part of the model) which together form a "Latent Bernoulli-Gauss" distribution. The model is applied to MAP estimation, clustering, feature selection and collaborative filtering and fares favorably with the state-of-the-art latent-variable models.
cs.LG:We address in this paper the problem of multi-channel signal sequence labeling. In particular, we consider the problem where the signals are contaminated by noise or may present some dephasing with respect to their labels. For that, we propose to jointly learn a SVM sample classifier with a temporal filtering of the channels. This will lead to a large margin filtering that is adapted to the specificity of each channel (noise and time-lag). We derive algorithms to solve the optimization problem and we discuss different filter regularizations for automated scaling or selection of channels. Our approach is tested on a non-linear toy example and on a BCI dataset. Results show that the classification performance on these problems can be improved by learning a large margin filtering.
cs.LG:We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a non-recursive function, etc. We further observe that Sontag's ANN is not Glivenko-Cantelli under any input distribution having a non-atomic part.
cs.LG:This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.
cs.LG:Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query `independence day' usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.
cs.LG:Recently, applying the novel data mining techniques for evaluating enterprise financial distress has received much research alternation. Support Vector Machine (SVM) and back propagation neural (BPN) network has been applied successfully in many areas with excellent generalization results, such as rule extraction, classification and evaluation. In this paper, a model based on SVM with Gaussian RBF kernel is proposed here for enterprise financial distress evaluation. BPN network is considered one of the simplest and are most general methods used for supervised training of multilayered neural network. The comparative results show that through the difference between the performance measures is marginal; SVM gives higher precision and lower error rates.
cs.LG:Most image-search approaches today are based on the text based tags associated with the images which are mostly human generated and are subject to various kinds of errors. The results of a query to the image database thus can often be misleading and may not satisfy the requirements of the user. In this work we propose our approach to automate this tagging process of images, where image results generated can be fine filtered based on a probabilistic tagging mechanism. We implement a tool which helps to automate the tagging process by maintaining a training database, wherein the system is trained to identify certain set of input images, the results generated from which are used to create a probabilistic tagging mechanism. Given a certain set of segments in an image it calculates the probability of presence of particular keywords. This probability table is further used to generate the candidate tags for input images.
cs.LG:We present a framework for discriminative sequence classification where the learner works directly in the high dimensional predictor space of all subsequences in the training set. This is possible by employing a new coordinate-descent algorithm coupled with bounding the magnitude of the gradient for selecting discriminative subsequences fast. We characterize the loss functions for which our generic learning algorithm can be applied and present concrete implementations for logistic regression (binomial log-likelihood loss) and support vector machines (squared hinge loss). Application of our algorithm to protein remote homology detection and remote fold recognition results in performance comparable to that of state-of-the-art methods (e.g., kernel support vector machines). Unlike state-of-the-art classifiers, the resulting classification models are simply lists of weighted discriminative subsequences and can thus be interpreted and related to the biological problem.
cs.LG:We present three generalisations of Kernel Principal Components Analysis (KPCA) which incorporate knowledge of the class labels of a subset of the data points. The first, MV-KPCA, penalises within class variances similar to Fisher discriminant analysis. The second, LSKPCA is a hybrid of least squares regression and kernel PCA. The final LR-KPCA is an iteratively reweighted version of the previous which achieves a sigmoid loss function on the labeled points. We provide a theoretical risk bound as well as illustrative experiments on real and toy data sets.
cs.LG:In this paper the sequential prediction problem with expert advice is considered for the case where losses of experts suffered at each step cannot be bounded in advance. We present some modification of Kalai and Vempala algorithm of following the perturbed leader where weights depend on past losses of the experts. New notions of a volume and a scaled fluctuation of a game are introduced. We present a probabilistic algorithm protected from unrestrictedly large one-step losses. This algorithm has the optimal performance in the case when the scaled fluctuations of one-step losses of experts of the pool tend to zero.
cs.LG:In prediction with expert advice the goal is to design online prediction algorithms that achieve small regret (additional loss on the whole data) compared to a reference scheme. In the simplest such scheme one compares to the loss of the best expert in hindsight. A more ambitious goal is to split the data into segments and compare to the best expert on each segment. This is appropriate if the nature of the data changes between segments. The standard fixed-share algorithm is fast and achieves small regret compared to this scheme.   Fixed share treats the experts as black boxes: there are no assumptions about how they generate their predictions. But if the experts are learning, the following question arises: should the experts learn from all data or only from data in their own segment? The original algorithm naturally addresses the first case. Here we consider the second option, which is more appropriate exactly when the nature of the data changes between segments. In general extending fixed share to this second case will slow it down by a factor of T on T outcomes. We show, however, that no such slowdown is necessary if the experts are hidden Markov models.
cs.LG:A problem posed by Freund is how to efficiently track a small pool of experts out of a much larger set. This problem was solved when Bousquet and Warmuth introduced their mixing past posteriors (MPP) algorithm in 2001.   In Freund's problem the experts would normally be considered black boxes. However, in this paper we re-examine Freund's problem in case the experts have internal structure that enables them to learn. In this case the problem has two possible interpretations: should the experts learn from all data or only from the subsequence on which they are being tracked? The MPP algorithm solves the first case. Our contribution is to generalise MPP to address the second option. The results we obtain apply to any expert structure that can be formalised using (expert) hidden Markov models. Curiously enough, for our interpretation there are \emph{two} natural reference schemes: freezing and sleeping. For each scheme, we provide an efficient prediction strategy and prove the relevant loss bound.
cs.LG:We propose a novel feature selection strategy to discover language-independent acoustic features that tend to be responsible for emotions regardless of languages, linguistics and other factors. Experimental results suggest that the language-independent feature subset discovered yields the performance comparable to the full feature set on various emotional speech corpora.
cs.LG:The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results demonstrate the efficiency and effectiveness of the proposed algorithm.
cs.LG:This paper introduces an approach to Reinforcement Learning Algorithm by comparing their immediate rewards using a variation of Q-Learning algorithm. Unlike the conventional Q-Learning, the proposed algorithm compares current reward with immediate reward of past move and work accordingly. Relative reward based Q-learning is an approach towards interactive learning. Q-Learning is a model free reinforcement learning method that used to learn the agents. It is observed that under normal circumstances algorithm take more episodes to reach optimal Q-value due to its normal reward or sometime negative reward. In this new form of algorithm agents select only those actions which have a higher immediate reward signal in comparison to previous one. The contribution of this article is the presentation of new Q-Learning Algorithm in order to maximize the performance of algorithm and reduce the number of episode required to reach optimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20 Grid world deterministic environment and the result for the two forms of Q-Learning Algorithms is given.
cs.LG:We study three families of online convex optimization algorithms: follow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual averaging (RDA), and composite-objective mirror descent. We first prove equivalence theorems that show all of these algorithms are instantiations of a general FTRL update. This provides theoretical insight on previous experimental observations. In particular, even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly, it has been observed that RDA is even more effective at producing sparsity. Our results demonstrate that FOBOS uses subgradient approximations to the L1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two, and outperforms both on a large, real-world dataset.   Our second contribution is a unified analysis which produces regret bounds that match (up to logarithmic terms) or improve the best previously known bounds. This analysis also extends these algorithms in two important ways: we support a more general type of composite objective and we analyze implicit updates, which replace the subgradient approximation of the current loss function with an exact optimization.
cs.LG:We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels - specifically, the gap in per observation probabilities between the most likely labels. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs.   We demonstrate empirically that the hybrid loss typically performs as least as well as - and often better than - both of its constituent losses on variety of tasks. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction and the effects of label dominance on these results.
cs.LG:In this paper we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in a top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy to assess the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node, are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree.
cs.LG:Margin theory provides one of the most popular explanations to the success of \texttt{AdaBoost}, where the central point lies in the recognition that \textit{margin} is the key for characterizing the performance of \texttt{AdaBoost}. This theory has been very influential, e.g., it has been used to argue that \texttt{AdaBoost} usually does not overfit since it tends to enlarge the margin even after the training error reaches zero. Previously the \textit{minimum margin bound} was established for \texttt{AdaBoost}, however, \cite{Breiman1999} pointed out that maximizing the minimum margin does not necessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006} emphasized that the margin distribution rather than minimum margin is crucial to the performance of \texttt{AdaBoost}. In this paper, we first present the \textit{$k$th margin bound} and further study on its relationship to previous work such as the minimum margin bound and Emargin bound. Then, we improve the previous empirical Bernstein bounds \citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on such findings, we defend the margin-based explanation against Breiman's doubts by proving a new generalization error bound that considers exactly the same factors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than \cite{Breiman1999}'s minimum margin bound. By incorporating factors such as average margin and variance, we present a generalization error bound that is heavily related to the whole margin distribution. We also provide margin distribution bounds for generalization error of voting classifiers in finite VC-dimension space.
cs.LG:In this work, we propose a new optimization framework for multiclass boosting learning. In the literature, AdaBoost.MO and AdaBoost.ECC are the two successful multiclass boosting algorithms, which can use binary weak learners. We explicitly derive these two algorithms' Lagrange dual problems based on their regularized loss functions. We show that the Lagrange dual formulations enable us to design totally-corrective multiclass algorithms by using the primal-dual optimization technique. Experiments on benchmark data sets suggest that our multiclass boosting can achieve a comparable generalization capability with state-of-the-art, but the convergence speed is much faster than stage-wise gradient descent boosting. In other words, the new totally corrective algorithms can maximize the margin more aggressively.
cs.LG:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for empirical risk minimization with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n}, this translates to a learning rate of O(RH/n) in the separable (L*=0) case and O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization with a smooth non-negative objective.
cs.LG:Sparse learning has recently received increasing attention in many areas including machine learning, statistics, and applied mathematics. The mixed-norm regularization based on the L1/Lq norm with q > 1 is attractive in many applications of regression and classification in that it facilitates group sparsity in the model. The resulting optimization problem is, however, challenging to solve due to the structure of the L1/Lq -regularization. Existing work deals with special cases including q = 2,infinity, and they cannot be easily extended to the general case. In this paper, we propose an efficient algorithm based on the accelerated gradient method for solving the L1/Lq -regularized problem, which is applicable for all values of q larger than 1, thus significantly extending existing work. One key building block of the proposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our theoretical analysis reveals the key properties of EP1q and illustrates why EP1q for the general q is significantly more challenging to solve than the special cases. Based on our theoretical analysis, we develop an efficient algorithm for EP1q by solving two zero finding problems. Experimental results demonstrate the efficiency of the proposed algorithm.
cs.LG:An instance-weighted variant of the support vector machine (SVM) has attracted considerable attention recently since they are useful in various machine learning tasks such as non-stationary data analysis, heteroscedastic data modeling, transfer learning, learning to rank, and transduction. An important challenge in these scenarios is to overcome the computational bottleneck---instance weights often change dynamically or adaptively, and thus the weighted SVM solutions must be repeatedly computed. In this paper, we develop an algorithm that can efficiently and exactly update the weighted SVM solutions for arbitrary change of instance weights. Technically, this contribution can be regarded as an extension of the conventional solution-path algorithm for a single regularization parameter to multiple instance-weight parameters. However, this extension gives rise to a significant problem that breakpoints (at which the solution path turns) have to be identified in high-dimensional space. To facilitate this, we introduce a parametric representation of instance weights. We also provide a geometric interpretation in weight space using a notion of critical region: a polyhedron in which the current affine solution remains to be optimal. Then we find breakpoints at intersections of the solution path and boundaries of polyhedrons. Through extensive experiments on various practical applications, we demonstrate the usefulness of the proposed algorithm.
cs.LG:Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the model's estimate of the objective and the uncertainty at any given point. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm's performance.
cs.LG:We consider the problem of energy-efficient point-to-point transmission of delay-sensitive data (e.g. multimedia data) over a fading channel. Existing research on this topic utilizes either physical-layer centric solutions, namely power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM); however, there is currently no rigorous and unified framework for simultaneously utilizing both physical-layer centric and system-level techniques to achieve the minimum possible energy consumption, under delay constraints, in the presence of stochastic and a priori unknown traffic and channel conditions. In this report, we propose such a framework. We formulate the stochastic optimization problem as a Markov decision process (MDP) and solve it online using reinforcement learning. The advantages of the proposed online method are that (i) it does not require a priori knowledge of the traffic arrival and channel statistics to determine the jointly optimal power-control, AMC, and DPM policies; (ii) it exploits partial information about the system so that less information needs to be learned than when using conventional reinforcement learning algorithms; and (iii) it obviates the need for action exploration, which severely limits the adaptation speed and run-time performance of conventional reinforcement learning algorithms. Our results show that the proposed learning algorithms can converge up to two orders of magnitude faster than a state-of-the-art learning algorithm for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms.
cs.LG:We propose a focus of attention mechanism to speed up the Perceptron algorithm. Focus of attention speeds up the Perceptron algorithm by lowering the number of features evaluated throughout training and prediction. Whereas the traditional Perceptron evaluates all the features of each example, the Attentive Perceptron evaluates less features for easy to classify examples, thereby achieving significant speedups and small losses in prediction accuracy. Focus of attention allows the Attentive Perceptron to stop the evaluation of features at any interim point and filter the example. This creates an attentive filter which concentrates computation at examples that are hard to classify, and quickly filters examples that are easy to classify.
cs.LG:In this paper, we consider a queue-aware distributive resource control algorithm for two-hop MIMO cooperative systems. We shall illustrate that relay buffering is an effective way to reduce the intrinsic half-duplex penalty in cooperative systems. The complex interactions of the queues at the source node and the relays are modeled as an average-cost infinite horizon Markov Decision Process (MDP). The traditional approach solving this MDP problem involves centralized control with huge complexity. To obtain a distributive and low complexity solution, we introduce a linear structure which approximates the value function of the associated Bellman equation by the sum of per-node value functions. We derive a distributive two-stage two-winner auction-based control policy which is a function of the local CSI and local QSI only. Furthermore, to estimate the best fit approximation parameter, we propose a distributive online stochastic learning algorithm using stochastic approximation theory. Finally, we establish technical conditions for almost-sure convergence and show that under heavy traffic, the proposed low complexity distributive control is global optimal.
cs.LG:To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach.
cs.LG:This paper describes algorithms for nonnegative matrix factorization (NMF) with the beta-divergence (beta-NMF). The beta-divergence is a family of cost functions parametrized by a single shape parameter beta that takes the Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito divergence as special cases (beta = 2,1,0, respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization (MM) algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a beta-dependent power exponent. The monotonicity of the heuristic algorithm can however be proven for beta in (0,1) using the proposed auxiliary function. Then we introduce the concept of majorization-equalization (ME) algorithm which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate the faster convergence of the ME approach. The paper also describes how the proposed algorithms can be adapted to two common variants of NMF : penalized NMF (i.e., when a penalty function of the factors is added to the criterion function) and convex-NMF (when the dictionary is assumed to belong to a known subspace).
cs.LG:Hardness results for maximum agreement problems have close connections to hardness results for proper learning in computational learning theory. In this paper we prove two hardness results for the problem of finding a low degree polynomial threshold function (PTF) which has the maximum possible agreement with a given set of labeled examples in $\R^n \times \{-1,1\}.$ We prove that for any constants $d\geq 1, \eps > 0$,   {itemize}   Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a degree-$d$ PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a degree-$d$ PTF that is consistent with a $1-\eps$ fraction of the examples.   It is $\NP$-hard to find a degree-2 PTF that is consistent with a $(\half + \eps)$ fraction of a given set of labeled examples in $\R^n \times \{-1,1\}$, even if there exists a halfspace (degree-1 PTF) that is consistent with a $1 - \eps$ fraction of the examples.   {itemize}   These results immediately imply the following hardness of learning results: (i) Assuming the Unique Games Conjecture, there is no better-than-trivial proper learning algorithm that agnostically learns degree-$d$ PTFs under arbitrary distributions; (ii) There is no better-than-trivial learning algorithm that outputs degree-2 PTFs and agnostically learns halfspaces (i.e. degree-1 PTFs) under arbitrary distributions.
cs.LG:A general framework based on Gaussian models and a MAP-EM algorithm is introduced in this paper for solving matrix/table completion problems. The numerical experiments with the standard and challenging movie ratings data show that the proposed approach, based on probably one of the simplest probabilistic models, leads to the results in the same ballpark as the state-of-the-art, at a lower computational cost.
cs.LG:This paper considers the clustering problem for large data sets. We propose an approach based on distributed optimization. The clustering problem is formulated as an optimization problem of maximizing the classification gain. We show that the optimization problem can be reformulated and decomposed into small-scale sub optimization problems by using the Dantzig-Wolfe decomposition method. Generally speaking, the Dantzig-Wolfe method can only be used for convex optimization problems, where the duality gaps are zero. Even though, the considered optimization problem in this paper is non-convex, we prove that the duality gap goes to zero, as the problem size goes to infinity. Therefore, the Dantzig-Wolfe method can be applied here. In the proposed approach, the clustering problem is iteratively solved by a group of computers coordinated by one center processor, where each computer solves one independent small-scale sub optimization problem during each iteration, and only a small amount of data communication is needed between the computers and center processor. Numerical results show that the proposed approach is effective and efficient.
cs.LG:We give sublinear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L2-SVM, for which sublinear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sublinear time algorithms achieving arbitrary approximation factor.
cs.LG:Nesterov's accelerated gradient methods (AGM) have been successfully applied in many machine learning areas. However, their empirical performance on training max-margin models has been inferior to existing specialized solvers. In this paper, we first extend AGM to strongly convex and composite objective functions with Bregman style prox-functions. Our unifying framework covers both the $\infty$-memory and 1-memory styles of AGM, tunes the Lipschiz constant adaptively, and bounds the duality gap. Then we demonstrate various ways to apply this framework of methods to a wide range of machine learning problems. Emphasis will be given on their rate of convergence and how to efficiently compute the gradient and optimize the models. The experimental results show that with our extensions AGM outperforms state-of-the-art solvers on max-margin models.
cs.LG:An importance weight quantifies the relative importance of one example over another, coming up in applications of boosting, asymmetric classification costs, reductions, and active learning. The standard approach for dealing with importance weights in gradient descent is via multiplication of the gradient. We first demonstrate the problems of this approach when importance weights are large, and argue in favor of more sophisticated ways for dealing with them. We then develop an approach which enjoys an invariance property: that updating twice with importance weight $h$ is equivalent to updating once with importance weight $2h$. For many important losses this has a closed form update which satisfies standard regret guarantees when all examples have $h=1$. We also briefly discuss two other reasonable approaches for handling large importance weights. Empirically, these approaches yield substantially superior prediction with similar computational performance while reducing the sensitivity of the algorithm to the exact setting of the learning rate. We apply these to online active learning yielding an extraordinarily fast active learning algorithm that works even in the presence of adversarial noise.
cs.LG:The note presents a modified proof of a loss bound for the exponentially weighted average forecaster with time-varying potential. The regret term of the algorithm is upper-bounded by sqrt{n ln(N)} (uniformly in n), where N is the number of experts and n is the number of steps.
cs.LG:Recommendation systems are emerging as an important business application with significant economic impact. Currently popular systems include Amazon's book recommendations, Netflix's movie recommendations, and Pandora's music recommendations. In this paper we address the problem of estimating probabilities associated with recommendation system data using non-parametric kernel smoothing. In our estimation we interpret missing items as randomly censored observations and obtain efficient computation schemes using combinatorial properties of generating functions. We demonstrate our approach with several case studies involving real world movie recommendation data. The results are comparable with state-of-the-art techniques while also providing probabilistic preference estimates outside the scope of traditional recommender systems.
cs.LG:We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.
cs.LG:In this paper, we propose a two-timescale delay-optimal dynamic clustering and power allocation design for downlink network MIMO systems. The dynamic clustering control is adaptive to the global queue state information (GQSI) only and computed at the base station controller (BSC) over a longer time scale. On the other hand, the power allocations of all the BSs in one cluster are adaptive to both intra-cluster channel state information (CCSI) and intra-cluster queue state information (CQSI), and computed at the cluster manager (CM) over a shorter time scale. We show that the two-timescale delay-optimal control can be formulated as an infinite-horizon average cost Constrained Partially Observed Markov Decision Process (CPOMDP). By exploiting the special problem structure, we shall derive an equivalent Bellman equation in terms of Pattern Selection Q-factor to solve the CPOMDP. To address the distributive requirement and the issue of exponential memory requirement and computational complexity, we approximate the Pattern Selection Q-factor by the sum of Per-cluster Potential functions and propose a novel distributive online learning algorithm to estimate the Per-cluster Potential functions (at each CM) as well as the Lagrange multipliers (LM) (at each BS). We show that the proposed distributive online learning algorithm converges almost surely (with probability 1). By exploiting the birth-death structure of the queue dynamics, we further decompose the Per-cluster Potential function into sum of Per-cluster Per-user Potential functions and formulate the instantaneous power allocation as a Per-stage QSI-aware Interference Game played among all the CMs. We also propose a QSI-aware Simultaneous Iterative Water-filling Algorithm (QSIWFA) and show that it can achieve the Nash Equilibrium (NE).
cs.LG:To attain the best learning accuracy, people move on with difficulties and frustrations. Though one can optimize the empirical objective using a given set of samples, its generalization ability to the entire sample distribution remains questionable. Even if a fair generalization guarantee is offered, one still wants to know what is to happen if the regularizer is removed, and/or how well the artificial loss (like the hinge loss) relates to the accuracy.   For such reason, this report surveys four different trials towards the learning accuracy, embracing the major advances in supervised learning theory in the past four years. Starting from the generic setting of learning, the first two trials introduce the best optimization and generalization bounds for convex learning, and the third trial gets rid of the regularizer. As an innovative attempt, the fourth trial studies the optimization when the objective is exactly the accuracy, in the special case of binary classification. This report also analyzes the last trial through experiments.
cs.LG:This report explores the use of machine learning techniques to accurately predict travel times in city streets and highways using floating car data (location information of user vehicles on a road network). The aim of this report is twofold, first we present a general architecture of solving this problem, then present and evaluate few techniques on real floating car data gathered over a month on a 5 Km highway in New Delhi.
cs.LG:This article discusses in detail the rating system that won the kaggle competition "Chess Ratings: Elo vs the rest of the world". The competition provided a historical dataset of outcomes for chess games, and aimed to discover whether novel approaches can predict the outcomes of future games, more accurately than the well-known Elo rating system. The winning rating system, called Elo++ in the rest of the article, builds upon the Elo rating system. Like Elo, Elo++ uses a single rating per player and predicts the outcome of a game, by using a logistic curve over the difference in ratings of the players. The major component of Elo++ is a regularization technique that avoids overfitting these ratings. The dataset of chess games and outcomes is relatively small and one has to be careful not to draw "too many conclusions" out of the limited data. Many approaches tested in the competition showed signs of such an overfitting. The leader-board was dominated by attempts that did a very good job on a small test dataset, but couldn't generalize well on the private hold-out dataset. The Elo++ regularization takes into account the number of games per player, the recency of these games and the ratings of the opponents. Finally, Elo++ employs a stochastic gradient descent scheme for training the ratings, and uses only two global parameters (white's advantage and regularization constant) that are optimized using cross-validation.
cs.LG:An important part of problems in statistical physics and computer science can be expressed as the computation of marginal probabilities over a Markov Random Field. The belief propagation algorithm, which is an exact procedure to compute these marginals when the underlying graph is a tree, has gained its popularity as an efficient way to approximate them in the more general case. In this paper, we focus on an aspect of the algorithm that did not get that much attention in the literature, which is the effect of the normalization of the messages. We show in particular that, for a large class of normalization strategies, it is possible to focus only on belief convergence. Following this, we express the necessary and sufficient conditions for local stability of a fixed point in terms of the graph structure and the beliefs values at the fixed point. We also explicit some connexion between the normalization constants and the underlying Bethe Free Energy.
cs.LG:We consider a retailer selling a single product with limited on-hand inventory over a finite selling season. Customer demand arrives according to a Poisson process, the rate of which is influenced by a single action taken by the retailer (such as price adjustment, sales commission, advertisement intensity, etc.). The relationship between the action and the demand rate is not known in advance. However, the retailer is able to learn the optimal action "on the fly" as she maximizes her total expected revenue based on the observed demand reactions.   Using the pricing problem as an example, we propose a dynamic "learning-while-doing" algorithm that only involves function value estimation to achieve a near-optimal performance. Our algorithm employs a series of shrinking price intervals and iteratively tests prices within that interval using a set of carefully chosen parameters. We prove that the convergence rate of our algorithm is among the fastest of all possible algorithms in terms of asymptotic "regret" (the relative loss comparing to the full information optimal solution). Our result closes the performance gaps between parametric and non-parametric learning and between a post-price mechanism and a customer-bidding mechanism. Important managerial insight from this research is that the values of information on both the parametric form of the demand function as well as each customer's exact reservation price are less important than prior literature suggests. Our results also suggest that firms would be better off to perform dynamic learning and action concurrently rather than sequentially.
cs.LG:This article presents a model which is capable of learning and abstracting new concepts based on comparing observations and finding the resemblance between the observations. In the model, the new observations are compared with the templates which have been derived from the previous experiences. In the first stage, the objects are first represented through a geometric description which is used for finding the object boundaries and a descriptor which is inspired by the human visual system and then they are fed into the model. Next, the new observations are identified through comparing them with the previously-learned templates and are used for producing new templates. The comparisons are made based on measures like Euclidean or correlation distance. The new template is created by applying onion-pealing algorithm. The algorithm consecutively uses convex hulls which are made by the points representing the objects. If the new observation is remarkably similar to one of the observed categories, it is no longer utilized in creating a new template. The existing templates are used to provide a description of the new observation. This description is provided in the templates space. Each template represents a dimension of the feature space. The degree of the resemblance each template bears to each object indicates the value associated with the object in that dimension of the templates space. In this way, the description of the new observation becomes more accurate and detailed as the time passes and the experiences increase. We have used this model for learning and recognizing the new polygons in the polygon space. Representing the polygons was made possible through employing a geometric method and a method inspired by human visual system. Various implementations of the model have been compared. The evaluation results of the model prove its efficiency in learning and deriving new templates.
cs.LG:It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet from a regularization perspective: the EigenNet has an adaptive eigenspace-based composite regularizer, which naturally generalizes the $l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and real data show that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.
cs.LG:Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.
cs.LG:We consider the problem of learning an unknown product distribution $X$ over $\{0,1\}^n$ using samples $f(X)$ where $f$ is a \emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework.   Information-theoretic arguments show that for every transformation function $f$ the corresponding learning problem can be solved to accuracy $\eps$, using $\tilde{O}(n/\eps^2)$ examples, by a generic algorithm whose running time may be exponential in $n.$ We show that this learning problem can be computationally intractable even for constant $\eps$ and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function $f(x)=w \cdot x$ with integer weights $w_i \leq n$ and prove that the corresponding learning problem requires $\Omega(n)$ samples.   As our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function $f(x)= \sum_{i=1}^n x_i$. Our algorithm learns to $\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\eps)$ number of samples that is independent of $n.$ We also give an efficient algorithm that uses $\log n \cdot \poly(1/\eps)$ samples but has running time that is only $\poly(\log n, 1/\eps).$
cs.LG:Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a two-layer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-of-the-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM$^{perf}$ in terms of $F_1$-score.
cs.LG:In this work we study parallelization of online learning, a core primitive in machine learning. In a parallel environment all known approaches for parallel online learning lead to delayed updates, where the model is updated using out-of-date information. In the worst case, or when examples are temporally correlated, delay can have a very adverse effect on the learning algorithm. Here, we analyze and present preliminary empirical results on a set of learning architectures based on a feature sharding approach that present various tradeoffs between delay, degree of parallelism, representation power and empirical performance.
cs.LG:Supervised learning is all about the ability to generalize knowledge. Specifically, the goal of the learning is to train a classifier using training data, in such a way that it will be capable of classifying new unseen data correctly. In order to acheive this goal, it is important to carefully design the learner, so it will not overfit the training data. The later can is done usually by adding a regularization term. The statistical learning theory explains the success of this method by claiming that it restricts the complexity of the learned model. This explanation, however, is rather abstract and does not have a geometric intuition. The generalization error of a classifier may be thought of as correlated with its robustness to perturbations of the data: a classifier that copes with disturbance is expected to generalize well. Indeed, Xu et al. [2009] have shown that the SVM formulation is equivalent to a robust optimization (RO) formulation, in which an adversary displaces the training and testing points within a ball of pre-determined radius. In this work we explore a different kind of robustness, namely changing each data point with a Gaussian cloud centered at the sample. Loss is evaluated as the expectation of an underlying loss function on the cloud. This setup fits the fact that in many applications, the data is sampled along with noise. We develop an RO framework, in which the adversary chooses the covariance of the noise. In our algorithm named GURU, the tuning parameter is a spectral bound on the noise, thus it can be estimated using physical or applicative considerations. Our experiments show that this framework performs as well as SVM and even slightly better in some cases. Generalizations for Mercer kernels and for the multiclass case are presented as well. We also show that our framework may be further generalized, using the technique of convex perspective functions.
cs.LG:We propose a new clustering technique that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.
cs.LG:A fundamental result of statistical learnig theory states that a concept class is PAC learnable if and only if it is a uniform Glivenko-Cantelli class if and only if the VC dimension of the class is finite. However, the theorem is only valid under special assumptions of measurability of the class, in which case the PAC learnability even becomes consistent. Otherwise, there is a classical example, constructed under the Continuum Hypothesis by Dudley and Durst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a concept class of VC dimension one which is neither uniform Glivenko-Cantelli nor consistently PAC learnable. We show that, rather surprisingly, under an additional set-theoretic hypothesis which is much milder than the Continuum Hypothesis (Martin's Axiom), PAC learnability is equivalent to finite VC dimension for every concept class.
cs.LG:Q-learning is a reliable but inefficient off-policy temporal-difference method, backing up reward only one step at a time. Replacing traces, using a recency heuristic, are more efficient but less reliable. In this work, we introduce model-free, off-policy temporal difference methods that make better use of experience than Watkins' Q(\lambda). We introduce both Optimistic Q(\lambda) and the temporal second difference trace (TSDT). TSDT is particularly powerful in deterministic domains. TSDT uses neither recency nor frequency heuristics, storing (s,a,r,s',\delta) so that off-policy updates can be performed after apparently suboptimal actions have been taken. There are additional advantages when using state abstraction, as in MAXQ. We demonstrate that TSDT does significantly better than both Q-learning and Watkins' Q(\lambda) in a deterministic cliff-walking domain. Results in a noisy cliff-walking domain are less advantageous for TSDT, but demonstrate the efficacy of Optimistic Q(\lambda), a replacing trace with some of the advantages of TSDT.
cs.LG:In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.
cs.LG:In this paper we present methods for attacking and defending $k$-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior's or process' $k$-order statistics to build a stochastic process that has those same $k$-order stationary statistics but possesses different, deliberately designed, $(k+1)$-order statistics if desired. Such a model realizes a "complexification" of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed $(k+1)$-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the $k$-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an {\em arms race} in the sense that the advantage goes to the party that has more computing resources applied to the problem.
cs.LG:We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a \emph{perturbed optimization problem} from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.
cs.LG:We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that "dogs are similar to dogs" even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.
cs.LG:We introduce an algorithm that, given n objects, learns a similarity matrix over all n^2 pairs, from crowdsourced data alone. The algorithm samples responses to adaptively chosen triplet-based relative-similarity queries. Each query has the form "is object 'a' more similar to 'b' or to 'c'?" and is chosen to be maximally informative given the preceding responses. The output is an embedding of the objects into Euclidean space (like MDS); we refer to this as the "crowd kernel." SVMs reveal that the crowd kernel captures prominent and subtle features across a number of domains, such as "is striped" among neckties and "vowel vs. consonant" among letters.
cs.LG:In this short note we prove a maximal concentration lemma for sub-Gaussian random variables stating that for independent sub-Gaussian random variables we have \[P<(\max_{1\le i\le N}S_{i}>\epsilon>) \le\exp<(-\frac{1}{N^2}\sum_{i=1}^{N}\frac{\epsilon^{2}}{2\sigma_{i}^{2}}>), \] where $S_i$ is the sum of $i$ zero mean independent sub-Gaussian random variables and $\sigma_i$ is the variance of the $i$th random variable.
cs.LG:We provide a natural learning process in which a financial trader without a risk receives a gain in case when Stock Market is inefficient. In this process, the trader rationally choose his gambles using a prediction made by a randomized calibrated algorithm. Our strategy is based on Dawid's notion of calibration with more general changing checking rules and on some modification of Kakade and Foster's randomized algorithm for computing calibrated forecasts.
cs.LG:We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale e, are explained and a few examples of their calculations are given with proofs. We then explain Sauer's Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distribution-free PAC learnable and it having finite VC dimension.   As the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale e of this new function class in terms of the Fat Shattering dimensions of the collection's classes.   We conclude this report by providing a few open questions and future research topics involving the PAC learning model.
cs.LG:In batch learning, stability together with existence and uniqueness of the solution corresponds to well-posedness of Empirical Risk Minimization (ERM) methods; recently, it was proved that CV_loo stability is necessary and sufficient for generalization and consistency of ERM. In this note, we introduce CV_on stability, which plays a similar note in online learning. We show that stochastic gradient descent (SDG) with the usual hypotheses is CVon stable and we then discuss the implications of CV_on stability for convergence of SGD.
cs.LG:Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our method both outperforms baseline methods and, in comparison to them, is faster and consumes less memory. We then demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.
cs.LG:We propose a nonparametric generalization of belief propagation, Kernel Belief Propagation (KBP), for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS), and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution, nor must their relations take any particular parametric form. Rather, the relations between variables are represented implicitly, and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (Rd, strings, groups), even where explicit parametric models are not known, or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments, we apply KBP to image denoising, depth prediction from still images, and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude, in some cases), while providing significantly more accurate results.
cs.LG:The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.
cs.LG:Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing $x^*$ that minimizes the expression $\sum_{f\in F}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where we wish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$ corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$, and we seek a shape $x$ that minimizes the sum of distances from each point in $P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and $f(x)$ is the distance from $p$ to its closest shape in $x$.   Our main result is a unified framework for constructing {\em coresets} and {\em approximate clustering} for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of $\varepsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation" of the input set $F$. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model).   We show how to generalize the results of our framework for squared distances (as in $k$-mean), distances to the $q$th power, and deterministic constructions.
cs.LG:The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.
cs.LG:This paper introduces a machine learning based collaborative multi-band spectrum sensing policy for cognitive radios. The proposed sensing policy guides secondary users to focus the search of unused radio spectrum to those frequencies that persistently provide them high data rate. The proposed policy is based on machine learning, which makes it adaptive with the temporally and spatially varying radio spectrum. Furthermore, there is no need for dynamic modeling of the primary activity since it is implicitly learned over time. Energy efficiency is achieved by minimizing the number of assigned sensors per each subband under a constraint on miss detection probability. It is important to control the missed detections because they cause collisions with primary transmissions and lead to retransmissions at both the primary and secondary user. Simulations show that the proposed machine learning based sensing policy improves the overall throughput of the secondary network and improves the energy efficiency while controlling the miss detection probability.
cs.LG:This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. In particular, we observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure between the observed variables - separating out the spurious interactions caused by the (marginalizing out of the) latent variables' time series. We develop a new method, based on convex optimization, to do so in the case when the number of latent variables is smaller than the number of observed ones. For the case when the dependency structure between the observed variables is sparse, we theoretically establish a high-dimensional scaling result for structure recovery. We verify our theoretical result with both synthetic and real data (from the stock market).
cs.LG:Estimator algorithms in learning automata are useful tools for adaptive, real-time optimization in computer science and engineering applications. This paper investigates theoretical convergence properties for a special case of estimator algorithms: the pursuit learning algorithm. In this note, we identify and fill a gap in existing proofs of probabilistic convergence for pursuit learning. It is tradition to take the pursuit learning tuning parameter to be fixed in practical applications, but our proof sheds light on the importance of a vanishing sequence of tuning parameters in a theoretical convergence analysis.
cs.LG:One of the major challenges of ECoG-based Brain-Machine Interfaces is the movement prediction of a human subject. Several methods exist to predict an arm 2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is to predict individual finger movements (5-D trajectory). The difficulty lies in the fact that there is no simple relation between ECoG signals and finger movement. We propose in this paper to decode finger flexions using switching models. This method permits to simplify the system as it is now described as an ensemble of linear models depending on an internal state. We show that an interesting accuracy prediction can be obtained by such a model.
cs.LG:Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.
cs.LG:This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using epsilon-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.
cs.LG:Motivated by the amount of code that goes unidentified on the web, we introduce a practical method for algorithmically identifying the programming language of source code. Our work is based on supervised learning and intelligent statistical features. We also explored, but abandoned, a grammatical approach. In testing, our implementation greatly outperforms that of an existing tool that relies on a Bayesian classifier. Code is written in Python and available under an MIT license.
cs.LG:Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.
cs.LG:Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior. In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm's potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm's benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.
cs.LG:We present IBSEAD or distributed autonomous entity systems based Interaction - a learning algorithm for the computer to self-evolve in a self-obsessed manner. This learning algorithm will present the computer to look at the internal and external environment in series of independent entities, which will interact with each other, with and/or without knowledge of the computer's brain. When a learning algorithm interacts, it does so by detecting and understanding the entities in the human algorithm. However, the problem with this approach is that the algorithm does not consider the interaction of the third party or unknown entities, which may be interacting with each other. These unknown entities in their interaction with the non-computer entities make an effect in the environment that influences the information and the behaviour of the computer brain. Such details and the ability to process the dynamic and unsettling nature of these interactions are absent in the current learning algorithm such as the decision tree learning algorithm. IBSEAD is able to evaluate and consider such algorithms and thus give us a better accuracy in simulation of the highly evolved nature of the human brain. Processes such as dreams, imagination and novelty, that exist in humans are not fully simulated by the existing learning algorithms. Also, Hidden Markov models (HMM) are useful in finding "hidden" entities, which may be known or unknown. However, this model fails to consider the case of unknown entities which maybe unclear or unknown. IBSEAD is better because it considers three types of entities- known, unknown and invisible. We present our case with a comparison of existing algorithms in known environments and cases and present the results of the experiments using dry run of the simulated runs of the existing machine learning algorithms versus IBSEAD.
cs.LG:In this paper, we correct an upper bound, presented in~\cite{hs-11}, on the generalisation error of classifiers learned through multiple kernel learning. The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive} dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a \emph{multiplicative} dependence on the logarithm of the number of kernels and the margin achieved by the classifier.
cs.LG:Machine Learning (ML) techniques are indispensable in a wide range of fields. Unfortunately, the exponential increase of dataset sizes are rapidly extending the runtime of sequential algorithms and threatening to slow future progress in ML. With the promise of affordable large-scale parallel computing, Cloud systems offer a viable platform to resolve the computational challenges in ML. However, designing and implementing efficient, provably correct distributed ML algorithms is often prohibitively challenging. To enable ML researchers to easily and efficiently use parallel systems, we introduced the GraphLab abstraction which is designed to represent the computational patterns in ML algorithms while permitting efficient parallel and distributed implementations. In this paper we provide a formal description of the GraphLab parallel abstraction and present an efficient distributed implementation. We conduct a comprehensive evaluation of GraphLab on three state-of-the-art ML algorithms using real large-scale data and a 64 node EC2 cluster of 512 processors. We find that GraphLab achieves orders of magnitude performance gains over Hadoop while performing comparably or superior to hand-tuned MPI implementations.
cs.LG:For large scale learning problems, it is desirable if we can obtain the optimal model parameters by going through the data in only one pass. Polyak and Juditsky (1992) showed that asymptotically the test performance of the simple average of the parameters obtained by stochastic gradient descent (SGD) is as good as that of the parameters which minimize the empirical cost. However, to our knowledge, despite its optimal asymptotic convergence rate, averaged SGD (ASGD) received little attention in recent research on large scale learning. One possible reason is that it may take a prohibitively large number of training samples for ASGD to reach its asymptotic region for most real problems. In this paper, we present a finite sample analysis for the method of Polyak and Juditsky (1992). Our analysis shows that it indeed usually takes a huge number of samples for ASGD to reach its asymptotic region for improperly chosen learning rate. More importantly, based on our analysis, we propose a simple way to properly set learning rate so that it takes a reasonable amount of data for ASGD to reach its asymptotic region. We compare ASGD using our proposed learning rate with other well known algorithms for training large scale linear classifiers. The experiments clearly show the superiority of ASGD.
cs.LG:Discovering pattern sets or global patterns is an attractive issue from the pattern mining community in order to provide useful information. By combining local patterns satisfying a joint meaning, this approach produces patterns of higher level and thus more useful for the data analyst than the usual local patterns, while reducing the number of patterns. In parallel, recent works investigating relationships between data mining and constraint programming (CP) show that the CP paradigm is a nice framework to model and mine such patterns in a declarative and generic way. We present a constraint-based language which enables us to define queries addressing patterns sets and global patterns. The usefulness of such a declarative approach is highlighted by several examples coming from the clustering based on associations. This language has been implemented in the CP framework.
cs.LG:We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.
cs.LG:This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a "value iteration" scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP.
cs.LG:We provide a formal, simple and intuitive theory of rational decision making including sequential decisions that affect the environment. The theory has a geometric flavor, which makes the arguments easy to visualize and understand. Our theory is for complete decision makers, which means that they have a complete set of preferences. Our main result shows that a complete rational decision maker implicitly has a probabilistic model of the environment. We have a countable version of this result that brings light on the issue of countable vs finite additivity by showing how it depends on the geometry of the space which we have preferences over. This is achieved through fruitfully connecting rationality with the Hahn-Banach Theorem. The theory presented here can be viewed as a formalization and extension of the betting odds approach to probability of Ramsey and De Finetti.
cs.LG:Building biological models by inferring functional dependencies from experimental data is an im- portant issue in Molecular Biology. To relieve the biologist from this traditionally manual process, various approaches have been proposed to increase the degree of automation. However, available ap- proaches often yield a single model only, rely on specific assumptions, and/or use dedicated, heuris- tic algorithms that are intolerant to changing circumstances or requirements in the view of the rapid progress made in Biotechnology. Our aim is to provide a declarative solution to the problem by ap- peal to Answer Set Programming (ASP) overcoming these difficulties. We build upon an existing approach to Automatic Network Reconstruction proposed by part of the authors. This approach has firm mathematical foundations and is well suited for ASP due to its combinatorial flavor providing a characterization of all models explaining a set of experiments. The usage of ASP has several ben- efits over the existing heuristic algorithms. First, it is declarative and thus transparent for biological experts. Second, it is elaboration tolerant and thus allows for an easy exploration and incorporation of biological constraints. Third, it allows for exploring the entire space of possible models. Finally, our approach offers an excellent performance, matching existing, special-purpose systems.
cs.LG:Detecting changes in high-dimensional time series is difficult because it involves the comparison of probability densities that need to be estimated from finite samples. In this paper, we present the first feature extraction method tailored to change point detection, which is based on an extended version of Stationary Subspace Analysis. We reduce the dimensionality of the data to the most non-stationary directions, which are most informative for detecting state changes in the time series. In extensive simulations on synthetic data we show that the accuracy of three change point detection algorithms is significantly increased by a prior feature extraction step. These findings are confirmed in an application to industrial fault monitoring.
cs.LG:We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.
cs.LG:We consider online learning in partial-monitoring games against an oblivious adversary. We show that when the number of actions available to the learner is two and the game is nontrivial then it is reducible to a bandit-like game and thus the minimax regret is $\Theta(\sqrt{T})$.
cs.LG:Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.
cs.LG:One of the most prominent challenges in clustering is "the user's dilemma," which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others.   Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of center-based approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight.
cs.LG:There has been increased interest in devising learning techniques that combine unlabeled data with labeled data ? i.e. semi-supervised learning. However, to the best of our knowledge, no study has been performed across various techniques and different types and amounts of labeled and unlabeled data. Moreover, most of the published work on semi-supervised learning techniques assumes that the labeled and unlabeled data come from the same distribution. It is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different. Not correcting for such bias can result in biased function approximation with potentially poor performance. In this paper, we present an empirical study of various semi-supervised learning techniques on a variety of datasets. We attempt to answer various questions such as the effect of independence or relevance amongst features, the effect of the size of the labeled and unlabeled sets and the effect of noise. We also investigate the impact of sample-selection bias on the semi-supervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias.
cs.LG:The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnows behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.
cs.LG:In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed.
cs.LG:We consider a bandit problem over a graph where the rewards are not directly observed. Instead, the decision maker can compare two nodes and receive (stochastic) information pertaining to the difference in their value. The graph structure describes the set of possible comparisons. Consequently, comparing between two nodes that are relatively far requires estimating the difference between every pair of nodes on the path between them. We analyze this problem from the perspective of sample complexity: How many queries are needed to find an approximately optimal node with probability more than $1-\delta$ in the PAC setup? We show that the topology of the graph plays a crucial in defining the sample complexity: graphs with a low diameter have a much better sample complexity.
cs.LG:User profiling is a useful primitive for constructing personalised services, such as content recommendation. In the present paper we investigate the feasibility of user profiling in a distributed setting, with no central authority and only local information exchanges between users. We compute a profile vector for each user (i.e., a low-dimensional vector that characterises her taste) via spectral transformation of observed user-produced ratings for items. Our two main contributions follow: i) We consider a low-rank probabilistic model of user taste. More specifically, we consider that users and items are partitioned in a constant number of classes, such that users and items within the same class are statistically identical. We prove that without prior knowledge of the compositions of the classes, based solely on few random observed ratings (namely $O(N\log N)$ such ratings for $N$ users), we can predict user preference with high probability for unrated items by running a local vote among users with similar profile vectors. In addition, we provide empirical evaluations characterising the way in which spectral profiling performance depends on the dimension of the profile space. Such evaluations are performed on a data set of real user ratings provided by Netflix. ii) We develop distributed algorithms which provably achieve an embedding of users into a low-dimensional space, based on spectral transformation. These involve simple message passing among users, and provably converge to the desired embedding. Our method essentially relies on a novel combination of gossiping and the algorithm proposed by Oja and Karhunen.
cs.LG:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper represents LDA as a factor graph within the Markov random field (MRF) framework, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Although two commonly-used approximate inference methods, such as variational Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in learning LDA, the proposed BP is competitive in both speed and accuracy as validated by encouraging experimental results on four large-scale document data sets. Furthermore, the BP algorithm has the potential to become a generic learning scheme for variants of LDA-based topic models. To this end, we show how to learn two typical variants of LDA-based topic models, such as author-topic models (ATM) and relational topic models (RTM), using BP based on the factor graph representation.
cs.LG:In machine learning, distance-based algorithms, and other approaches, use information that is represented by propositional data. However, this kind of representation can be quite restrictive and, in many cases, it requires more complex structures in order to represent data in a more natural way. Terms are the basis for functional and logic programming representation. Distances between terms are a useful tool not only to compare terms, but also to determine the search space in many of these applications. This dissertation applies distances between terms, exploiting the features of each distance and the possibility to compare from propositional data types to hierarchical representations. The distances between terms are applied through the k-NN (k-nearest neighbor) classification algorithm using XML as a common language representation. To be able to represent these data in an XML structure and to take advantage of the benefits of distance between terms, it is necessary to apply some transformations. These transformations allow the conversion of flat data into hierarchical data represented in XML, using some techniques based on intuitive associations between the names and values of variables and associations based on attribute similarity.   Several experiments with the distances between terms of Nienhuys-Cheng and Estruch et al. were performed. In the case of originally propositional data, these distances are compared to the Euclidean distance. In all cases, the experiments were performed with the distance-weighted k-nearest neighbor algorithm, using several exponents for the attraction function (weighted distance). It can be seen that in some cases, the term distances can significantly improve the results on approaches applied to flat representations.
cs.LG:In this paper we explore noise tolerant learning of classifiers. We formulate the problem as follows. We assume that there is an ${\bf unobservable}$ training set which is noise-free. The actual training set given to the learning algorithm is obtained from this ideal data set by corrupting the class label of each example. The probability that the class label of an example is corrupted is a function of the feature vector of the example. This would account for most kinds of noisy data one encounters in practice. We say that a learning method is noise tolerant if the classifiers learnt with the ideal noise-free data and with noisy data, both have the same classification accuracy on the noise-free data. In this paper we analyze the noise tolerance properties of risk minimization (under different loss functions), which is a generic method for learning classifiers. We show that risk minimization under 0-1 loss function has impressive noise tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant. We conclude the paper with some discussion on implications of these theoretical results.
cs.LG:Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain. We focus here on active learning for multi-view domains, in which there are several disjoint subsets of features (views), each of which is sufficient to learn the target concept. In this paper we make several contributions. First, we introduce Co-Testing, which is the first approach to multi-view active learning. Second, we extend the multi-view learning framework by also exploiting weak views, which are adequate only for learning a concept that is more general/specific than the target concept. Finally, we empirically show that Co-Testing outperforms existing active learners on a variety of real world domains such as wrapper induction, Web page classification, advertisement removal, and discourse tree parsing.
cs.LG:Recently, a unified framework for adaptive kernel based signal processing of complex data was presented by the authors, which, besides offering techniques to map the input data to complex Reproducing Kernel Hilbert Spaces, developed a suitable Wirtinger-like Calculus for general Hilbert Spaces. In this short paper, the extended Wirtinger's calculus is adopted to derive complex kernel-based widely-linear estimation filters. Furthermore, we illuminate several important characteristics of the widely linear filters. We show that, although in many cases the gains from adopting widely linear estimation filters, as alternatives to ordinary linear ones, are rudimentary, for the case of kernel based widely linear filters significant performance improvements can be obtained.
cs.LG:Matrix factorization from a small number of observed entries has recently garnered much attention as the key ingredient of successful recommendation systems. One unresolved problem in this area is how to adapt current methods to handle changing user preferences over time. Recent proposals to address this issue are heuristic in nature and do not fully exploit the time-dependent structure of the problem. As a principled and general temporal formulation, we propose a dynamical state space model of matrix factorization. Our proposal builds upon probabilistic matrix factorization, a Bayesian model with Gaussian priors. We utilize results in state tracking, such as the Kalman filter, to provide accurate recommendations in the presence of both process and measurement noise. We show how system parameters can be learned via expectation-maximization and provide comparisons to current published techniques.
cs.LG:The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones.   We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only fails to guarantee any useful bounds for these problems.   The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypotheses and some fixed "pivotal" hypothesis to within an absolute error of at most $\eps$ times the
cs.LG:The analysis of physiological processes over time are often given by spectrometric or gene expression profiles over time with only few time points but a large number of measured variables. The analysis of such temporal sequences is challenging and only few methods have been proposed. The information can be encoded time independent, by means of classical expression differences for a single time point or in expression profiles over time. Available methods are limited to unsupervised and semi-supervised settings. The predictive variables can be identified only by means of wrapper or post-processing techniques. This is complicated due to the small number of samples for such studies. Here, we present a supervised learning approach, termed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a supervised mapping of the temporal sequences onto a low dimensional grid. We utilize a hidden markov model (HMM) to account for the time domain and relevance learning to identify the relevant feature dimensions most predictive over time. The learned mapping can be used to visualize the temporal sequences and to predict the class of a new sequence. The relevance learning permits the identification of discriminating masses or gen expressions and prunes dimensions which are unnecessary for the classification task or encode mainly noise. In this way we obtain a very efficient learning system for temporal sequences. The results indicate that using simultaneous supervised learning and metric adaptation significantly improves the prediction accuracy for synthetically and real life data in comparison to the standard techniques. The discriminating features, identified by relevance learning, compare favorably with the results of alternative methods. Our method permits the visualization of the data on a low dimensional grid, highlighting the observed temporal structure.
cs.LG:Bayesian optimization (BO) algorithms try to optimize an unknown function that is expensive to evaluate using minimum number of evaluations/experiments. Most of the proposed algorithms in BO are sequential, where only one experiment is selected at each iteration. This method can be time inefficient when each experiment takes a long time and more than one experiment can be ran concurrently. On the other hand, requesting a fix-sized batch of experiments at each iteration causes performance inefficiency in BO compared to the sequential policies. In this paper, we present an algorithm that asks a batch of experiments at each time step t where the batch size p_t is dynamically determined in each step. Our algorithm is based on the observation that the sequence of experiments selected by the sequential policy can sometimes be almost independent from each other. Our algorithm identifies such scenarios and request those experiments at the same time without degrading the performance. We evaluate our proposed method using the Expected Improvement policy and the results show substantial speedup with little impact on the performance in eight real and synthetic benchmarks.
cs.LG:This report considers how to inject external candidate solutions into the CMA-ES algorithm. The injected solutions might stem from a gradient or a Newton step, a surrogate model optimizer or any other oracle or search mechanism. They can also be the result of a repair mechanism, for example to render infeasible solutions feasible. Only small modifications to the CMA-ES are necessary to turn injection into a reliable and effective method: too long steps need to be tightly renormalized. The main objective of this report is to reveal this simple mechanism. Depending on the source of the injected solutions, interesting variants of CMA-ES arise. When the best-ever solution is always (re-)injected, an elitist variant of CMA-ES with weighted multi-recombination arises. When \emph{all} solutions are injected from an \emph{external} source, the resulting algorithm might be viewed as \emph{adaptive encoding} with step-size control. In first experiments, injected solutions of very good quality lead to a convergence speed twice as fast as on the (simple) sphere function without injection. This means that we observe an impressive speed-up on otherwise difficult to solve functions. Single bad injected solutions on the other hand do no significant harm.
cs.LG:We propose a method to efficiently construct data-dependent kernels which can make use of large quantities of (unlabeled) data. Our construction makes an approximation in the standard construction of semi-supervised kernels in Sindhwani et al. 2005. In typical cases these kernels can be computed in nearly-linear time (in the amount of data), improving on the cubic time of the standard construction, enabling large scale semi-supervised learning in a variety of contexts. The methods are validated on semi-supervised and unsupervised problems on data sets containing upto 64,000 sample points.
cs.LG:Recent work in signal processing and statistics have focused on defining new regularization functions, which not only induce sparsity of the solution, but also take into account the structure of the problem. We present in this paper a class of convex penalties introduced in the machine learning community, which take the form of a sum of l_2 and l_infinity-norms over groups of variables. They extend the classical group-sparsity regularization in the sense that the groups possibly overlap, allowing more flexibility in the group design. We review efficient optimization methods to deal with the corresponding inverse problems, and their application to the problem of learning dictionaries of natural image patches: On the one hand, dictionary learning has indeed proven effective for various signal processing tasks. On the other hand, structured sparsity provides a natural framework for modeling dependencies between dictionary elements. We thus consider a structured sparse regularization to learn dictionaries embedded in a particular structure, for instance a tree or a two-dimensional grid. In the latter case, the results we obtain are similar to the dictionaries produced by topographic independent component analysis.
cs.LG:In this paper, we describe our approach to the Wikipedia Participation Challenge which aims to predict the number of edits a Wikipedia editor will make in the next 5 months. The best submission from our team, "zeditor", achieved 41.7% improvement over WMF's baseline predictive model and the final rank of 3rd place among 96 teams. An interesting characteristic of our approach is that only temporal dynamics features (i.e., how the number of edits changes in recent periods, etc.) are used in a self-supervised learning framework, which makes it easy to be generalised to other application domains.
cs.LG:This paper presents a method of choosing number of states of a HMM based on number of critical points of the motion capture data. The choice of Hidden Markov Models(HMM) parameters is crucial for recognizer's performance as it is the first step of the training and cannot be corrected automatically within HMM. In this article we define predictor of number of states based on number of critical points of the sequence and test its effectiveness against sample data.
cs.LG:We develop a new tool for data-dependent analysis of the exploration-exploitation trade-off in learning under limited feedback. Our tool is based on two main ingredients. The first ingredient is a new concentration inequality that makes it possible to control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. The second ingredient is an application of this inequality to the exploration-exploitation trade-off via importance weighted sampling. We apply the new tool to the stochastic multiarmed bandit problem, however, the main importance of this paper is the development and understanding of the new tool rather than improvement of existing algorithms for stochastic multiarmed bandits. In the follow-up work we demonstrate that the new tool can improve over state-of-the-art in structurally richer problems, such as stochastic multiarmed bandits with side information (Seldin et al., 2011a).
cs.LG:Structured classification tasks such as sequence labeling and dependency parsing have seen much interest by the Natural Language Processing and the machine learning communities. Several online learning algorithms were adapted for structured tasks such as Perceptron, Passive- Aggressive and the recently introduced Confidence-Weighted learning . These online algorithms are easy to implement, fast to train and yield state-of-the-art performance. However, unlike probabilistic models like Hidden Markov Model and Conditional random fields, these methods generate models that output merely a prediction with no additional information regarding confidence in the correctness of the output. In this work we fill the gap proposing few alternatives to compute the confidence in the output of non-probabilistic algorithms.We show how to compute confidence estimates in the prediction such that the confidence reflects the probability that the word is labeled correctly. We then show how to use our methods to detect mislabeled words, trade recall for precision and active learning. We evaluate our methods on four noun-phrase chunking and named entity recognition sequence labeling tasks, and on dependency parsing for 14 languages.
cs.LG:In this paper we propose and study a generalization of the standard active-learning model where a more general type of query, class conditional query, is allowed. Such queries have been quite useful in applications, but have been lacking theoretical understanding. In this work, we characterize the power of such queries under two well-known noise models. We give nearly tight upper and lower bounds on the number of queries needed to learn both for the general agnostic setting and for the bounded noise model. We further show that our methods can be made adaptive to the (unknown) noise rate, with only negligible loss in query complexity.
cs.LG:In this paper, we consider the problem of multi-armed bandits with a large, possibly infinite number of correlated arms. We assume that the arms have Bernoulli distributed rewards, independent across time, where the probabilities of success are parametrized by known attribute vectors for each arm, as well as an unknown preference vector, each of dimension $n$. For this model, we seek an algorithm with a total regret that is sub-linear in time and independent of the number of arms. We present such an algorithm, which we call the Two-Phase Algorithm, and analyze its performance. We show upper bounds on the total regret which applies uniformly in time, for both the finite and infinite arm cases. The asymptotics of the finite arm bound show that for any $f \in \omega(\log(T))$, the total regret can be made to be $O(n \cdot f(T))$. In the infinite arm case, the total regret is $O(\sqrt{n^3 T})$.
cs.LG:We present a framework for performing efficient regression in general metric spaces. Roughly speaking, our regressor predicts the value at a new point by computing a Lipschitz extension --- the smoothest function consistent with the observed data --- after performing structural risk minimization to avoid overfitting. We obtain finite-sample risk bounds with minimal structural and noise assumptions, and a natural speed-precision tradeoff. The offline (learning) and online (prediction) stages can be solved by convex programming, but this naive approach has runtime complexity $O(n^3)$, which is prohibitive for large datasets. We design instead a regression algorithm whose speed and generalization performance depend on the intrinsic dimension of the data, to which the algorithm adapts. While our main innovation is algorithmic, the statistical results may also be of independent interest.
cs.LG:Spectral clustering is a novel clustering method which can detect complex shapes of data clusters. However, it requires the eigen decomposition of the graph Laplacian matrix, which is proportion to $O(n^3)$ and thus is not suitable for large scale systems. Recently, many methods have been proposed to accelerate the computational time of spectral clustering. These approximate methods usually involve sampling techniques by which a lot information of the original data may be lost. In this work, we propose a fast and accurate spectral clustering approach using an approximate commute time embedding, which is similar to the spectral embedding. The method does not require using any sampling technique and computing any eigenvector at all. Instead it uses random projection and a linear time solver to find the approximate embedding. The experiments in several synthetic and real datasets show that the proposed approach has better clustering quality and is faster than the state-of-the-art approximate spectral clustering methods.
cs.LG:In this paper we propose a framework for solving constrained online convex optimization problem. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set $\mathcal{K}$ from which the decisions are made. While for simple shapes (e.g. Euclidean ball) the projection is straightforward, for arbitrary complex sets this is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring decisions belong to $\mathcal{K}$ for all rounds, we only require that the constraints which define the set $\mathcal{K}$ be satisfied in the long run. We show that our framework can be utilized to solve a relaxed version of online learning with side constraints addressed in \cite{DBLP:conf/colt/MannorT06} and \cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and $\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting $\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on the Mirror Prox method \citep{nemirovski-2005-prox} to solve variational inequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound for both regret and the violation of constraints when the domain $\K$ can be described by a finite number of linear constraints. Finally, we extend the result to the setting where we only have partial access to the convex set $\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm.
cs.LG:In citep{Hazan-2008-extract}, the authors showed that the regret of online linear optimization can be bounded by the total variation of the cost vectors. In this paper, we extend this result to general online convex optimization. We first analyze the limitations of the algorithm in \citep{Hazan-2008-extract} when applied it to online convex optimization. We then present two algorithms for online convex optimization whose regrets are bounded by the variation of cost functions. We finally consider the bandit setting, and present a randomized algorithm for online bandit convex optimization with a variation-based regret bound. We show that the regret bound for online bandit convex optimization is optimal when the variation of cost functions is independent of the number of trials.
cs.LG:Although exploratory behaviors are ubiquitous in the animal kingdom, their computational underpinnings are still largely unknown. Behavioral Psychology has identified learning as a primary drive underlying many exploratory behaviors. Exploration is seen as a means for an animal to gather sensory data useful for reducing its ignorance about the environment. While related problems have been addressed in Data Mining and Reinforcement Learning, the computational modeling of learning-driven exploration by embodied agents is largely unrepresented.   Here, we propose a computational theory for learning-driven exploration based on the concept of missing information that allows an agent to identify informative actions using Bayesian inference. We demonstrate that when embodiment constraints are high, agents must actively coordinate their actions to learn efficiently. Compared to earlier approaches, our exploration policy yields more efficient learning across a range of worlds with diverse structures. The improved learning in turn affords greater success in general tasks including navigation and reward gathering. We conclude by discussing how the proposed theory relates to previous information-theoretic objectives of behavior, such as predictive information and the free energy principle, and how it might contribute to a general theory of exploratory behavior.
cs.LG:This paper derives an identity connecting the square loss of ridge regression in on-line mode with the loss of the retrospectively best regressor. Some corollaries about the properties of the cumulative loss of on-line ridge regression are also obtained.
cs.LG:Unsupervised aggregation of independently built univariate predictors is explored as an alternative regularization approach for noisy, sparse datasets. Bipartite ranking algorithm Smooth Rank implementing this approach is introduced. The advantages of this algorithm are demonstrated on two types of problems. First, Smooth Rank is applied to two-class problems from bio-medical field, where ranking is often preferable to classification. In comparison against SVMs with radial and linear kernels, Smooth Rank had the best performance on 8 out of 12 benchmark benchmarks. The second area of application is survival analysis, which is reduced here to bipartite ranking in a way which allows one to use commonly accepted measures of methods performance. In comparison of Smooth Rank with Cox PH regression and CoxPath methods, Smooth Rank proved to be the best on 9 out of 10 benchmark datasets.
cs.LG:We investigate a recently proposed family of positive-definite kernels that mimic the computation in large neural networks. We examine the properties of these kernels using tools from differential geometry; specifically, we analyze the geometry of surfaces in Hilbert space that are induced by these kernels. When this geometry is described by a Riemannian manifold, we derive results for the metric, curvature, and volume element. Interestingly, though, we find that the simplest kernel in this family does not admit such an interpretation. We explore two variations of these kernels that mimic computation in neural networks with different activation functions. We experiment with these new kernels on several data sets and highlight their general trends in performance for classification.
cs.LG:We show how to incorporate information from labeled examples into nonnegative matrix factorization (NMF), a popular unsupervised learning algorithm for dimensionality reduction. In addition to mapping the data into a space of lower dimensionality, our approach aims to preserve the nonnegative components of the data that are important for classification. We identify these components from the support vectors of large-margin classifiers and derive iterative updates to preserve them in a semi-supervised version of NMF. These updates have a simple multiplicative form like their unsupervised counterparts; they are also guaranteed at each iteration to decrease their loss function---a weighted sum of I-divergences that captures the trade-off between unsupervised and supervised learning. We evaluate these updates for dimensionality reduction when they are used as a precursor to linear classification. In this role, we find that they yield much better performance than their unsupervised counterparts. We also find one unexpected benefit of the low dimensional representations discovered by our approach: often they yield more accurate classifiers than both ordinary and transductive SVMs trained in the original input space.
cs.LG:This paper provides a theoretical support for clustering aspect of the nonnegative matrix factorization (NMF). By utilizing the Karush-Kuhn-Tucker optimality conditions, we show that NMF objective is equivalent to graph clustering objective, so clustering aspect of the NMF has a solid justification. Different from previous approaches which usually discard the nonnegativity constraints, our approach guarantees the stationary point being used in deriving the equivalence is located on the feasible region in the nonnegative orthant. Additionally, since clustering capability of a matrix decomposition technique can sometimes imply its latent semantic indexing (LSI) aspect, we will also evaluate LSI aspect of the NMF by showing its capability in solving the synonymy and polysemy problems in synthetic datasets. And more extensive evaluation will be conducted by comparing LSI performances of the NMF and the singular value decomposition (SVD), the standard LSI method, using some standard datasets.
cs.LG:The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.
cs.LG:We propose a new, nonparametric approach to estimating the value function in reinforcement learning. This approach makes use of a recently developed representation of conditional distributions as functions in a reproducing kernel Hilbert space. Such representations bypass the need for estimating transition probabilities, and apply to any domain on which kernels can be defined. Our approach avoids the need to approximate intractable integrals since expectations are represented as RKHS inner products whose computation has linear complexity in the sample size. Thus, we can efficiently perform value function estimation in a wide variety of settings, including finite state spaces, continuous states spaces, and partially observable tasks where only sensor measurements are available. A second advantage of the approach is that we learn the conditional distribution representation from a training sample, and do not require an exhaustive exploration of the state space. We prove convergence of our approach either to the optimal policy, or to the closest projection of the optimal policy in our model class, under reasonable assumptions. In experiments, we demonstrate the performance of our algorithm on a learning task in a continuous state space (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. We compare with least-squares policy iteration where a Gaussian process is used for value function estimation. Our algorithm achieves better performance in both tasks.
cs.LG:Selecting the best classifier among the available ones is a difficult task, especially when only instances of one class exist. In this work we examine the notion of combining one-class classifiers as an alternative for selecting the best classifier. In particular, we propose two new one-class classification performance measures to weigh classifiers and show that a simple ensemble that implements these measures can outperform the most popular one-class ensembles. Furthermore, we propose a new one-class ensemble scheme, TUPSO, which uses meta-learning to combine one-class classifiers. Our experiments demonstrate the superiority of TUPSO over all other tested ensembles and show that the TUPSO performance is statistically indistinguishable from that of the hypothetical best classifier.
cs.LG:We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.
cs.LG:Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a {linear} dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.
cs.LG:Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, Involving Few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved, the quality of any RL method---in real-world settings---depends less on how well it scales with the number of states than on how well it scales with the number of actions. This is where our new method T-Learning excels, by evaluating the relatively few possible transits from one state to another in a policy-independent way, rather than a huge number of state-action pairs, or states in traditional policy-dependent ways. Illustrative experiments demonstrate that performance improvements of T-Learning over Q-learning can be arbitrarily large.
cs.LG:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.
cs.LG:Leveraging the power of increasing amounts of data to analyze customer base for attracting and retaining the most valuable customers is a major problem facing companies in this information age. Data mining technologies extract hidden information and knowledge from large data stored in databases or data warehouses, thereby supporting the corporate decision making process. CRM uses data mining (one of the elements of CRM) techniques to interact with customers. This study investigates the use of a technique, semi-supervised learning, for the management and analysis of customer-related data warehouse and information. The idea of semi-supervised learning is to learn not only from the labeled training data, but to exploit also the structural information in additionally available unlabeled data. The proposed semi-supervised method is a model by means of a feed-forward neural network trained by a back propagation algorithm (multi-layer perceptron) in order to predict the category of an unknown customer (potential customers). In addition, this technique can be used with Rapid Miner tools for both labeled and unlabeled data.
cs.LG:Diabetes is a major health problem in both developing and developed countries and its incidence is rising dramatically. In this study, we investigate a novel automatic approach to diagnose Diabetes disease based on Feature Weighted Support Vector Machines (FW-SVMs) and Modified Cuckoo Search (MCS). The proposed model consists of three stages: Firstly, PCA is applied to select an optimal subset of features out of set of all the features. Secondly, Mutual Information is employed to construct the FWSVM by weighting different features based on their degree of importance. Finally, since parameter selection plays a vital role in classification accuracy of SVMs, MCS is applied to select the best parameter values. The proposed MI-MCS-FWSVM method obtains 93.58% accuracy on UCI dataset. The experimental results demonstrate that our method outperforms the previous methods by not only giving more accurate results but also significantly speeding up the classification procedure.
cs.LG:We present a novel approach to learn a kernel-based regression function. It is based on the useof conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.
cs.LG:Teaching is one of the most important factors affecting any education system. Many research efforts have been conducted to facilitate the presentation modes used by instructors in classrooms as well as provide means for students to review lectures through web browsers. Other studies have been made to provide acoustical design recommendations for classrooms like room size and reverberation times. However, using acoustical features of classrooms as a way to provide education systems with feedback about the learning process was not thoroughly investigated in any of these studies. We propose a system that extracts different sound features of students and instructors, and then uses machine learning techniques to evaluate the acoustical quality of any learning environment. We infer conclusions about the students' satisfaction with the quality of lectures. Using classifiers instead of surveys and other subjective ways of measures can facilitate and speed such experiments which enables us to perform them continuously. We believe our system enables education systems to continuously review and improve their teaching strategies and acoustical quality of classrooms.
cs.LG:We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of $O(1/T)$ {assuming that the proximal step can be efficiently solved}, significantly faster than a standard subgradient descent method that has an $O(1/\sqrt{T})$ convergence rate. Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.
cs.LG:In the current competitive world, industrial companies seek to manufacture products of higher quality which can be achieved by increasing reliability, maintainability and thus the availability of products. On the other hand, improvement in products lifecycle is necessary for achieving high reliability. Typically, maintenance activities are aimed to reduce failures of industrial machinery and minimize the consequences of such failures. So the industrial companies try to improve their efficiency by using different fault detection techniques. One strategy is to process and analyze previous generated data to predict future failures. The purpose of this paper is to detect wasted parts using different data mining algorithms and compare the accuracy of these algorithms. A combination of thermal and physical characteristics has been used and the algorithms were implemented on Ahanpishegan's current data to estimate the availability of its produced parts.   Keywords: Data Mining, Fault Detection, Availability, Prediction Algorithms.
cs.LG:Clustering is considered a non-supervised learning setting, in which the goal is to partition a collection of data points into disjoint clusters. Often a bound $k$ on the number of clusters is given or assumed by the practitioner. Many versions of this problem have been defined, most notably $k$-means and $k$-median.   An underlying problem with the unsupervised nature of clustering it that of determining a similarity function. One approach for alleviating this difficulty is known as clustering with side information, alternatively, semi-supervised clustering. Here, the practitioner incorporates side information in the form of "must be clustered" or "must be separated" labels for data point pairs. Each such piece of information comes at a "query cost" (often involving human response solicitation). The collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints, possibly adding a pairwise constraint penalty function to the chosen clustering objective.   Our work is mostly related to clustering with side information. We ask how to choose the pairs of data points. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. Roughly speaking, we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, hence we don't know the bias. Using the recently introduced method of $\eps$-smooth relative regret approximations of Ailon, Begleiter and Ezra, we can show an iterative process that improves both the clustering and the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm selecting pairs uniformly.
cs.LG:Contextual bandit learning is a reinforcement learning problem where the learner repeatedly receives a set of features (context), takes an action and receives a reward based on the action and context. We consider this problem under a realizability assumption: there exists a function in a (known) function class, always capable of predicting the expected reward, given the action and context. Under this assumption, we show three things. We present a new algorithm---Regressor Elimination--- with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that for any set of policies (mapping contexts to actions), there is a distribution over rewards (given context) such that our new algorithm has constant regret unlike the previous approaches.
cs.LG:Inverse reinforcement learning (IRL) addresses the problem of recovering a task description given a demonstration of the optimal policy used to solve such a task. The optimal policy is usually provided by an expert or teacher, making IRL specially suitable for the problem of apprenticeship learning. The task description is encoded in the form of a reward function of a Markov decision process (MDP). Several algorithms have been proposed to find the reward function corresponding to a set of demonstrations. One of the algorithms that has provided best results in different applications is a gradient method to optimize a policy squared error criterion. On a parallel line of research, other authors have presented recently a gradient approximation of the maximum likelihood estimate of the reward signal. In general, both approaches approximate the gradient estimate and the criteria at different stages to make the algorithm tractable and efficient. In this work, we provide a detailed description of the different methods to highlight differences in terms of reward estimation, policy similarity and computational costs. We also provide experimental results to evaluate the differences in performance of the methods.
cs.LG:We study upper and lower bounds on the sample-complexity of learning near-optimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.
cs.LG:In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ||C||. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid's inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.
cs.LG:In this paper, we study the application of GIST SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores.
cs.LG:This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.
cs.LG:Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational data mining is used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.
cs.LG:We investigate adaptive mixture methods that linearly combine outputs of $m$ constituent filters running in parallel to model a desired signal. We use "Bregman divergences" and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of $m$ constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.
cs.LG:The past century was era of linear systems. Either systems (especially industrial ones) were simple (quasi)linear or linear approximations were accurate enough. In addition, just at the ending decades of the century profusion of computing devices were available, before then due to lack of computational resources it was not easy to evaluate available nonlinear system studies. At the moment both these two conditions changed, systems are highly complex and also pervasive amount of computation strength is cheap and easy to achieve. For recent era, a new branch of supervised learning well known as surrogate modeling (meta-modeling, surface modeling) has been devised which aimed at answering new needs of modeling realm. This short literature survey is on to introduce surrogate modeling to whom is familiar with the concepts of supervised learning. Necessity, challenges and visions of the topic are considered.
cs.LG:Bayesian model averaging (BMA) is an approach to average over alternative models; yet, it usually gets excessively concentrated around the single most probable model, therefore achieving only sub-optimal classification performance. The compression-based approach (Boulle, 2007) overcomes this problem, averaging over the different models by applying a logarithmic smoothing over the models' posterior probabilities. This approach has shown excellent performances when applied to ensembles of naive Bayes classifiers. AODE is another ensemble of models with high performance (Webb, 2005), based on a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs via BMA rather than by arithmetic mean deteriorates the performance; instead, we aggregate the SPODEs via the compression coefficients and we show that the resulting classifier obtains a slight but consistent improvement over AODE. However, an important issue in any Bayesian ensemble of models is the arbitrariness in the choice of the prior over the models. We address this problem by the paradigm of credal classification, namely by substituting the unique prior with a set of priors. Credal classifier automatically recognize the prior-dependent instances, namely the instances whose most probable class varies, when different priors are considered; in these cases, credal classifiers remain reliable by returning a set of classes rather than a single class. We thus develop the credal version of both the BMA-based and the compression-based ensemble of SPODEs, substituting the single prior over the models by a set of priors. Experiments show that both credal classifiers provide higher classification reliability than their determinate counterparts; moreover the compression-based credal classifier compares favorably to previous credal classifiers.
cs.LG:We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives.
cs.LG:Probabilistic generative modeling of data distributions can potentially exploit hidden information which is useful for discriminative classification. This observation has motivated the development of approaches that couple generative and discriminative models for classification. In this paper, we propose a new approach to couple generative and discriminative models in an unified framework based on PAC-Bayes risk theory. We first derive the model-parameter-independent stochastic feature mapping from a practical MAP classifier operating on generative models. Then we construct a linear stochastic classifier equipped with the feature mapping, and derive the explicit PAC-Bayes risk bounds for such classifier for both supervised and semi-supervised learning. Minimizing the risk bound, using an EM-like iterative procedure, results in a new posterior over hidden variables (E-step) and the update rules of model parameters (M-step). The derivation of the posterior is always feasible due to the way of equipping feature mapping and the explicit form of bounding risk. The derived posterior allows the tuning of generative models and subsequently the feature mappings for better classification. The derived update rules of the model parameters are same to those of the uncoupled models as the feature mapping is model-parameter-independent. Our experiments show that the coupling between data modeling generative model and the discriminative classifier via a stochastic feature mapping in this framework leads to a general classification tool with state-of-the-art performance.
cs.LG:Nowadays, supervised learning is commonly used in many domains. Indeed, many works propose to learn new knowledge from examples that translate the expected behaviour of the considered system. A key issue of supervised learning concerns the description language used to represent the examples. In this paper, we propose a method to evaluate the feature set used to describe them. Our method is based on the computation of the consistency of the example base. We carried out a case study in the domain of geomatic in order to evaluate the sets of measures used to characterise geographic objects. The case study shows that our method allows to give relevant evaluations of measure sets.
cs.LG:Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.
cs.LG:Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster.
cs.LG:We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.
cs.LG:Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.
cs.LG:Despite the widespread use of Clustering, there is distressingly little general theory of clustering available. Questions like "What distinguishes a clustering of data from other data partitioning?", "Are there any principles governing all clustering paradigms?", "How should a user choose an appropriate clustering algorithm for a particular task?", etc. are almost completely unanswered by the existing body of clustering literature. We consider an axiomatic approach to the theory of Clustering. We adopt the framework of Kleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, we sidestep his impossibility result and arrive at a consistent set of axioms. We suggest to extend these axioms, aiming to provide an axiomatic taxonomy of clustering paradigms. Such a taxonomy should provide users some guidance concerning the choice of the appropriate clustering paradigm for a given task. The main result of this paper is a set of abstract properties that characterize the Single-Linkage clustering function. This characterization result provides new insight into the properties of desired data groupings that make Single-Linkage the appropriate choice. We conclude by considering a taxonomy of clustering functions based on abstract properties that each satisfies.
cs.LG:A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X = x) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with t = 1/2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any t. We then present a principled algorithm to solve the extended SVM classifier for all values of t simultaneously. This has two implications: First, one can recover the entire conditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.
cs.LG:We consider MAP estimators for structured prediction with exponential family models. In particular, we concentrate on the case that efficient algorithms for uniform sampling from the output space exist. We show that under this assumption (i) exact computation of the partition function remains a hard problem, and (ii) the partition function and the gradient of the log partition function can be approximated efficiently. Our main result is an approximation scheme for the partition function based on Markov Chain Monte Carlo theory. We also show that the efficient uniform sampling assumption holds in several application settings that are of importance in machine learning.
cs.LG:We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of ~O(HSpAT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.
cs.LG:We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.
cs.LG:We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker's cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard square root of k dependence, depending on the behavior of the arms' reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms.
cs.LG:We are concerned with the issue of how to calculate the normalized maximum likelihood (NML) code-length. There is a problem that the normalization term of the NML code-length may diverge when it is continuous and unbounded and a straightforward computation of it is highly expensive when the data domain is finite . In previous works it has been investigated how to calculate the NML code-length for specific types of distributions. We first propose a general method for computing the NML code-length for the exponential family. Then we specifically focus on Gaussian mixture model (GMM), and propose a new efficient method for computing the NML to them. We develop it by generalizing Rissanen's re-normalizing technique. Then we apply this method to the clustering issue, in which a clustering structure is modeled using a GMM, and the main task is to estimate the optimal number of clusters on the basis of the NML code-length. We demonstrate using artificial data sets the superiority of the NML-based clustering over other criteria such as AIC, BIC in terms of the data size required for high accuracy rate to be achieved.
cs.LG:This paper describes the method of visualization of periodic constituents and instability areas in series of measurements, being based on the algorithm of smoothing out and concept of one-dimensional cellular automata. A method can be used at the analysis of temporal series, related to the volumes of thematic publications in web-space.
cs.LG:We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.
cs.LG:In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.
cs.LG:This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.
cs.LG:We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.
cs.LG:Unsupervised models can provide supplementary soft constraints to help classify new, "target" data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered "source" data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by naively applying classifiers learnt on the original task to the target data.
cs.LG:Numerous data mining techniques have been developed to extract information and identify patterns and predict trends from large data sets. In this study, two classification techniques, the J48 implementation of the C4.5 algorithm and a Naive Bayes classifier are applied to predict lung cancer survivability from an extensive data set with fifteen years of patient records. The purpose of the project is to verify the predictive effectiveness of the two techniques on real, historical data. Besides the performance outcome that renders J48 marginally better than the Naive Bayes technique, there is a detailed description of the data and the required pre-processing activities. The performance results confirm expectations while some of the issues that appeared during experimentation, underscore the value of having domain-specific understanding to leverage any domain-specific characteristics inherent in the data.
cs.LG:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.
cs.LG:This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.
cs.LG:We consider a multi-armed bandit setting that is inspired by real-world applications in e-commerce. In our setting, there are a few types of users, each with a specific response to the different arms. When a user enters the system, his type is unknown to the decision maker. The decision maker can either treat each user separately ignoring the previously observed users, or can attempt to take advantage of knowing that only few types exist and cluster the users according to their response to the arms. We devise algorithms that combine the usual exploration-exploitation tradeoff with clustering of users and demonstrate the value of clustering. In the process of developing algorithms for the clustered setting, we propose and analyze simple algorithms for the setup where a decision maker knows that a user belongs to one of few types, but does not know which one.
cs.LG:In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost).
cs.LG:Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work "transductively", i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an"inductive"-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\"om method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposition. Empirical results demonstrate the efficacy and efficiency of the proposed method.
cs.LG:There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for "Path Integral Policy Improvement with Covariance Matrix Adaptation". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.
cs.LG:F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.
cs.LG:We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of $O(1/T)$ where $T$ is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm.
cs.LG:Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.
cs.LG:Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.
cs.LG:We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as \emph{embeddings} in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments.
cs.LG:We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.
cs.LG:Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.
cs.LG:We consider a dynamic pricing problem under unknown demand models. In this problem a seller offers prices to a stream of customers and observes either success or failure in each sale attempt. The underlying demand model is unknown to the seller and can take one of N possible forms. In this paper, we show that this problem can be formulated as a multi-armed bandit with dependent arms. We propose a dynamic pricing policy based on the likelihood ratio test. We show that the proposed policy achieves complete learning, i.e., it offers a bounded regret where regret is defined as the revenue loss with respect to the case with a known demand model. This is in sharp contrast with the logarithmic growing regret in multi-armed bandit with independent arms.
cs.LG:Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.
cs.LG:The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.
cs.LG:In this paper we consider the problem of collectively classifying entities where relational information is available across the entities. In practice inaccurate class distribution for each entity is often available from another (external) classifier. For example this distribution could come from a classifier built using content features or a simple dictionary. Given the relational and inaccurate external classifier information, we consider two graph based settings in which the problem of collective classification can be solved. In the first setting the class distribution is used to fix labels to a subset of nodes and the labels for the remaining nodes are obtained like in a transductive setting. In the other setting the class distributions of all nodes are used to define the fitting function part of a graph regularized objective function. We define a generalized objective function that handles both the settings. Methods like harmonic Gaussian field and local-global consistency (LGC) reported in the literature can be seen as special cases. We extend the LGC and weighted vote relational neighbor classification (WvRN) methods to support usage of external classifier information. We also propose an efficient least squares regularization (LSR) based method and relate it to information regularization methods. All the methods are evaluated on several benchmark and real world datasets. Considering together speed, robustness and accuracy, experimental results indicate that the LSR and WvRN-extension methods perform better than other methods.
cs.LG:Metric learning methods have been shown to perform well on different learning tasks. Many of them rely on target neighborhood relationships that are computed in the original feature space and remain fixed throughout learning. As a result, the learned metric reflects the original neighborhood relations. We propose a novel formulation of the metric learning problem in which, in addition to the metric, the target neighborhood relations are also learned in a two-step iterative approach. The new formulation can be seen as a generalization of many existing metric learning methods. The formulation includes a target neighbor assignment rule that assigns different numbers of neighbors to instances according to their quality; `high quality' instances get more neighbors. We experiment with two of its instantiations that correspond to the metric learning algorithms LMNN and MCML and compare it to other metric learning methods on a number of datasets. The experimental results show state-of-the-art performance and provide evidence that learning the neighborhood relations does improve predictive performance.
cs.LG:We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T^{1/2} log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.
cs.LG:Semi-supervised template update systems allow to automatically take into account the intra-class variability of the biometric data over time. Such systems can be inefficient by including too many impostor's samples or skipping too many genuine's samples. In the first case, the biometric reference drifts from the real biometric data and attracts more often impostors. In the second case, the biometric reference does not evolve quickly enough and also progressively drifts from the real biometric data. We propose a hybrid system using several biometric sub-references in order to increase per- formance of self-update systems by reducing the previously cited errors. The proposition is validated for a keystroke- dynamics authentication system (this modality suffers of high variability over time) on two consequent datasets from the state of the art.
cs.LG:Most keystroke dynamics studies have been evaluated using a specific kind of dataset in which users type an imposed login and password. Moreover, these studies are optimistics since most of them use different acquisition protocols, private datasets, controlled environment, etc. In order to enhance the accuracy of keystroke dynamics' performance, the main contribution of this paper is twofold. First, we provide a new kind of dataset in which users have typed both an imposed and a chosen pairs of logins and passwords. In addition, the keystroke dynamics samples are collected in a web-based uncontrolled environment (OS, keyboards, browser, etc.). Such kind of dataset is important since it provides us more realistic results of keystroke dynamics' performance in comparison to the literature (controlled environment, etc.). Second, we present a statistical analysis of well known assertions such as the relationship between performance and password size, impact of fusion schemes on system overall performance, and others such as the relationship between performance and entropy. We put into obviousness in this paper some new results on keystroke dynamics in realistic conditions.
cs.LG:The selection of the best classification algorithm for a given dataset is a very widespread problem. It is also a complex one, in the sense it requires to make several important methodological choices. Among them, in this work we focus on the measure used to assess the classification performance and rank the algorithms. We present the most popular measures and discuss their properties. Despite the numerous measures proposed over the years, many of them turn out to be equivalent in this specific case, to have interpretation problems, or to be unsuitable for our purpose. Consequently, classic overall success rate or marginal rates should be preferred for this specific task.
cs.LG:It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.
cs.LG:Electronic health records contain rich textual data which possess critical predictive information for machine-learning based diagnostic aids. However many traditional machine learning methods fail to simultaneously integrate both vector space data and text. We present a supervised method using Laplacian eigenmaps to augment existing machine-learning methods with low-dimensional representations of textual predictors which preserve the local similarities. The proposed implementation performs alternating optimization using gradient descent. For the evaluation we applied our method to over 2,000 patient records from a large single-center pediatric cardiology practice to predict if patients were diagnosed with cardiac disease. Our method was compared with latent semantic indexing, latent Dirichlet allocation, and local Fisher discriminant analysis. The results were assessed using AUC, MCC, specificity, and sensitivity. Results indicate supervised Laplacian eigenmaps was the highest performing method in our study, achieving 0.782 and 0.374 for AUC and MCC respectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over the baseline which excluded textual data and a 2.69% and 5.35% increase in AUC and MCC respectively over unsupervised Laplacian eigenmaps. This method allows many existing machine learning predictors to effectively and efficiently utilize the potential of textual predictors.
cs.LG:This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.
cs.LG:The real challenge in pattern recognition task and machine learning process is to train a discriminator using labeled data and use it to distinguish between future data as accurate as possible. However, most of the problems in the real world have numerous data, which labeling them is a cumbersome or even an impossible matter. Semi-supervised learning is one approach to overcome these types of problems. It uses only a small set of labeled with the company of huge remain and unlabeled data to train the discriminator. In semi-supervised learning, it is very essential that which data is labeled and depend on position of data it effectiveness changes. In this paper, we proposed an evolutionary approach called Artificial Immune System (AIS) to determine which data is better to be labeled to get the high quality data. The experimental results represent the effectiveness of this algorithm in finding these data points.
cs.LG:It is often the case that, within an online recommender system, multiple users share a common account. Can such shared accounts be identified solely on the basis of the user- provided ratings? Once a shared account is identified, can the different users sharing it be identified as well? Whenever such user identification is feasible, it opens the way to possible improvements in personalized recommendations, but also raises privacy concerns. We develop a model for composite accounts based on unions of linear subspaces, and use subspace clustering for carrying out the identification task. We show that a significant fraction of such accounts is identifiable in a reliable manner, and illustrate potential uses for personalized recommendation.
cs.LG:In this paper, we attempts to learn a single metric across two heterogeneous domains where source domain is fully labeled and has many samples while target domain has only a few labeled samples but abundant unlabeled samples. To the best of our knowledge, this task is seldom touched. The proposed learning model has a simple underlying motivation: all the samples in both the source and the target domains are mapped into a common space, where both their priors P(sample)s and their posteriors P(label|sample)s are forced to be respectively aligned as much as possible. We show that the two mappings, from both the source domain and the target domain to the common space, can be reparameterized into a single positive semi-definite(PSD) matrix. Then we develop an efficient Bregman Projection algorithm to optimize the PDS matrix over which a LogDet function is used to regularize. Furthermore, we also show that this model can be easily kernelized and verify its effectiveness in crosslanguage retrieval task and cross-domain object recognition task.
